I0105 09:06:55.987840      22 e2e.go:116] Starting e2e run "2e867fcd-3821-44ab-8131-3ddf12f91b07" on Ginkgo node 1
Jan  5 09:06:55.997: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1672909615 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jan  5 09:06:56.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:06:56.082: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0105 09:06:56.082677      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0105 09:06:56.082677      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan  5 09:06:56.123: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan  5 09:06:56.166: INFO: 33 / 33 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan  5 09:06:56.166: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Jan  5 09:06:56.166: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan  5 09:06:56.180: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
Jan  5 09:06:56.180: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
Jan  5 09:06:56.180: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'csi-driver-node' (0 seconds elapsed)
Jan  5 09:06:56.180: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-omyby-v1.25.5' (0 seconds elapsed)
Jan  5 09:06:56.180: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-vot5k-v1.25.5' (0 seconds elapsed)
Jan  5 09:06:56.180: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Jan  5 09:06:56.180: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Jan  5 09:06:56.180: INFO: e2e test version: v1.25.5
Jan  5 09:06:56.184: INFO: kube-apiserver version: v1.25.5
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jan  5 09:06:56.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:06:56.189: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.108 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan  5 09:06:56.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:06:56.082: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0105 09:06:56.082677      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Jan  5 09:06:56.123: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan  5 09:06:56.166: INFO: 33 / 33 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan  5 09:06:56.166: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
    Jan  5 09:06:56.166: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan  5 09:06:56.180: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
    Jan  5 09:06:56.180: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
    Jan  5 09:06:56.180: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'csi-driver-node' (0 seconds elapsed)
    Jan  5 09:06:56.180: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-omyby-v1.25.5' (0 seconds elapsed)
    Jan  5 09:06:56.180: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-vot5k-v1.25.5' (0 seconds elapsed)
    Jan  5 09:06:56.180: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
    Jan  5 09:06:56.180: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
    Jan  5 09:06:56.180: INFO: e2e test version: v1.25.5
    Jan  5 09:06:56.184: INFO: kube-apiserver version: v1.25.5
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan  5 09:06:56.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:06:56.189: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:06:56.208
Jan  5 09:06:56.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:06:56.209
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:06:56.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:06:56.235
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:06:56.242
Jan  5 09:06:56.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4" in namespace "projected-5749" to be "Succeeded or Failed"
Jan  5 09:06:56.261: INFO: Pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.69615ms
Jan  5 09:06:58.268: INFO: Pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015382433s
Jan  5 09:07:00.270: INFO: Pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016668036s
Jan  5 09:07:02.268: INFO: Pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014669966s
Jan  5 09:07:04.267: INFO: Pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013858132s
STEP: Saw pod success 01/05/23 09:07:04.267
Jan  5 09:07:04.267: INFO: Pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4" satisfied condition "Succeeded or Failed"
Jan  5 09:07:04.272: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4 container client-container: <nil>
STEP: delete the pod 01/05/23 09:07:04.288
Jan  5 09:07:04.301: INFO: Waiting for pod downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4 to disappear
Jan  5 09:07:04.305: INFO: Pod downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 09:07:04.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5749" for this suite. 01/05/23 09:07:04.313
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":1,"skipped":8,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.111 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:06:56.208
    Jan  5 09:06:56.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:06:56.209
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:06:56.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:06:56.235
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:06:56.242
    Jan  5 09:06:56.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4" in namespace "projected-5749" to be "Succeeded or Failed"
    Jan  5 09:06:56.261: INFO: Pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.69615ms
    Jan  5 09:06:58.268: INFO: Pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015382433s
    Jan  5 09:07:00.270: INFO: Pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016668036s
    Jan  5 09:07:02.268: INFO: Pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014669966s
    Jan  5 09:07:04.267: INFO: Pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013858132s
    STEP: Saw pod success 01/05/23 09:07:04.267
    Jan  5 09:07:04.267: INFO: Pod "downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4" satisfied condition "Succeeded or Failed"
    Jan  5 09:07:04.272: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4 container client-container: <nil>
    STEP: delete the pod 01/05/23 09:07:04.288
    Jan  5 09:07:04.301: INFO: Waiting for pod downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4 to disappear
    Jan  5 09:07:04.305: INFO: Pod downwardapi-volume-33681393-8025-4c26-8672-4cb47b0008d4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 09:07:04.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5749" for this suite. 01/05/23 09:07:04.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:07:04.32
Jan  5 09:07:04.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:07:04.321
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:04.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:04.347
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-9285bbce-8f39-44fd-9e2a-a099f174aba4 01/05/23 09:07:04.354
STEP: Creating a pod to test consume configMaps 01/05/23 09:07:04.36
Jan  5 09:07:04.373: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7" in namespace "projected-5261" to be "Succeeded or Failed"
Jan  5 09:07:04.379: INFO: Pod "pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.365702ms
Jan  5 09:07:06.386: INFO: Pod "pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013407601s
Jan  5 09:07:08.388: INFO: Pod "pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014803689s
STEP: Saw pod success 01/05/23 09:07:08.388
Jan  5 09:07:08.388: INFO: Pod "pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7" satisfied condition "Succeeded or Failed"
Jan  5 09:07:08.395: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/05/23 09:07:08.454
Jan  5 09:07:08.468: INFO: Waiting for pod pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7 to disappear
Jan  5 09:07:08.474: INFO: Pod pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 09:07:08.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5261" for this suite. 01/05/23 09:07:08.49
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":2,"skipped":49,"failed":0}
------------------------------
â€¢ [4.178 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:07:04.32
    Jan  5 09:07:04.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:07:04.321
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:04.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:04.347
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-9285bbce-8f39-44fd-9e2a-a099f174aba4 01/05/23 09:07:04.354
    STEP: Creating a pod to test consume configMaps 01/05/23 09:07:04.36
    Jan  5 09:07:04.373: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7" in namespace "projected-5261" to be "Succeeded or Failed"
    Jan  5 09:07:04.379: INFO: Pod "pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.365702ms
    Jan  5 09:07:06.386: INFO: Pod "pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013407601s
    Jan  5 09:07:08.388: INFO: Pod "pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014803689s
    STEP: Saw pod success 01/05/23 09:07:08.388
    Jan  5 09:07:08.388: INFO: Pod "pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7" satisfied condition "Succeeded or Failed"
    Jan  5 09:07:08.395: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/05/23 09:07:08.454
    Jan  5 09:07:08.468: INFO: Waiting for pod pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7 to disappear
    Jan  5 09:07:08.474: INFO: Pod pod-projected-configmaps-cbbf3d83-1843-43e7-95e4-592c2b8dd0e7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 09:07:08.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5261" for this suite. 01/05/23 09:07:08.49
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:07:08.499
Jan  5 09:07:08.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 09:07:08.5
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:08.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:08.525
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 01/05/23 09:07:08.537
STEP: waiting for available Endpoint 01/05/23 09:07:08.541
STEP: listing all Endpoints 01/05/23 09:07:08.544
STEP: updating the Endpoint 01/05/23 09:07:08.549
STEP: fetching the Endpoint 01/05/23 09:07:08.559
STEP: patching the Endpoint 01/05/23 09:07:08.564
STEP: fetching the Endpoint 01/05/23 09:07:08.574
STEP: deleting the Endpoint by Collection 01/05/23 09:07:08.579
STEP: waiting for Endpoint deletion 01/05/23 09:07:08.586
STEP: fetching the Endpoint 01/05/23 09:07:08.59
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:07:08.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4558" for this suite. 01/05/23 09:07:08.602
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":3,"skipped":53,"failed":0}
------------------------------
â€¢ [0.114 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:07:08.499
    Jan  5 09:07:08.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 09:07:08.5
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:08.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:08.525
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 01/05/23 09:07:08.537
    STEP: waiting for available Endpoint 01/05/23 09:07:08.541
    STEP: listing all Endpoints 01/05/23 09:07:08.544
    STEP: updating the Endpoint 01/05/23 09:07:08.549
    STEP: fetching the Endpoint 01/05/23 09:07:08.559
    STEP: patching the Endpoint 01/05/23 09:07:08.564
    STEP: fetching the Endpoint 01/05/23 09:07:08.574
    STEP: deleting the Endpoint by Collection 01/05/23 09:07:08.579
    STEP: waiting for Endpoint deletion 01/05/23 09:07:08.586
    STEP: fetching the Endpoint 01/05/23 09:07:08.59
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:07:08.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4558" for this suite. 01/05/23 09:07:08.602
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:07:08.616
Jan  5 09:07:08.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 09:07:08.616
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:08.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:08.647
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 09:07:08.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-161" for this suite. 01/05/23 09:07:08.705
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":4,"skipped":88,"failed":0}
------------------------------
â€¢ [0.095 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:07:08.616
    Jan  5 09:07:08.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 09:07:08.616
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:08.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:08.647
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 09:07:08.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-161" for this suite. 01/05/23 09:07:08.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:07:08.711
Jan  5 09:07:08.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename daemonsets 01/05/23 09:07:08.712
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:08.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:08.734
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 01/05/23 09:07:08.783
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:07:08.789
Jan  5 09:07:08.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:07:08.810: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:07:09.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:07:09.826: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:07:10.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:07:10.826: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:07:11.828: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:07:11.828: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:07:12.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:07:12.826: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:07:13.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 09:07:13.826: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r is running 0 daemon pod, expected 1
Jan  5 09:07:14.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  5 09:07:14.827: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Getting /status 01/05/23 09:07:14.832
Jan  5 09:07:14.838: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/05/23 09:07:14.838
Jan  5 09:07:14.849: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/05/23 09:07:14.849
Jan  5 09:07:14.853: INFO: Observed &DaemonSet event: ADDED
Jan  5 09:07:14.853: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 09:07:14.854: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 09:07:14.854: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 09:07:14.854: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 09:07:14.854: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 09:07:14.854: INFO: Found daemon set daemon-set in namespace daemonsets-3882 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  5 09:07:14.854: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/05/23 09:07:14.854
STEP: watching for the daemon set status to be patched 01/05/23 09:07:14.861
Jan  5 09:07:14.867: INFO: Observed &DaemonSet event: ADDED
Jan  5 09:07:14.867: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 09:07:14.868: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 09:07:14.868: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 09:07:14.869: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 09:07:14.869: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 09:07:14.869: INFO: Observed daemon set daemon-set in namespace daemonsets-3882 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  5 09:07:14.869: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 09:07:14.869: INFO: Found daemon set daemon-set in namespace daemonsets-3882 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan  5 09:07:14.869: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/05/23 09:07:14.908
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3882, will wait for the garbage collector to delete the pods 01/05/23 09:07:14.908
Jan  5 09:07:14.971: INFO: Deleting DaemonSet.extensions daemon-set took: 6.339184ms
Jan  5 09:07:15.073: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.10561ms
Jan  5 09:07:17.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:07:17.078: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 09:07:17.086: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5515"},"items":null}

Jan  5 09:07:17.091: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5515"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:07:17.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3882" for this suite. 01/05/23 09:07:17.137
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":5,"skipped":98,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.432 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:07:08.711
    Jan  5 09:07:08.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename daemonsets 01/05/23 09:07:08.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:08.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:08.734
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 01/05/23 09:07:08.783
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:07:08.789
    Jan  5 09:07:08.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:07:08.810: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:07:09.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:07:09.826: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:07:10.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:07:10.826: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:07:11.828: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:07:11.828: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:07:12.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:07:12.826: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:07:13.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 09:07:13.826: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r is running 0 daemon pod, expected 1
    Jan  5 09:07:14.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  5 09:07:14.827: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Getting /status 01/05/23 09:07:14.832
    Jan  5 09:07:14.838: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/05/23 09:07:14.838
    Jan  5 09:07:14.849: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/05/23 09:07:14.849
    Jan  5 09:07:14.853: INFO: Observed &DaemonSet event: ADDED
    Jan  5 09:07:14.853: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 09:07:14.854: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 09:07:14.854: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 09:07:14.854: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 09:07:14.854: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 09:07:14.854: INFO: Found daemon set daemon-set in namespace daemonsets-3882 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  5 09:07:14.854: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/05/23 09:07:14.854
    STEP: watching for the daemon set status to be patched 01/05/23 09:07:14.861
    Jan  5 09:07:14.867: INFO: Observed &DaemonSet event: ADDED
    Jan  5 09:07:14.867: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 09:07:14.868: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 09:07:14.868: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 09:07:14.869: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 09:07:14.869: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 09:07:14.869: INFO: Observed daemon set daemon-set in namespace daemonsets-3882 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  5 09:07:14.869: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 09:07:14.869: INFO: Found daemon set daemon-set in namespace daemonsets-3882 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan  5 09:07:14.869: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 09:07:14.908
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3882, will wait for the garbage collector to delete the pods 01/05/23 09:07:14.908
    Jan  5 09:07:14.971: INFO: Deleting DaemonSet.extensions daemon-set took: 6.339184ms
    Jan  5 09:07:15.073: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.10561ms
    Jan  5 09:07:17.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:07:17.078: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 09:07:17.086: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5515"},"items":null}

    Jan  5 09:07:17.091: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5515"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:07:17.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3882" for this suite. 01/05/23 09:07:17.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:07:17.144
Jan  5 09:07:17.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 09:07:17.145
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:17.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:17.169
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 09:07:17.198
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:07:17.562
STEP: Deploying the webhook pod 01/05/23 09:07:17.572
STEP: Wait for the deployment to be ready 01/05/23 09:07:17.591
Jan  5 09:07:17.602: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  5 09:07:19.621: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:07:21.628: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/05/23 09:07:23.628
STEP: Verifying the service has paired with the endpoint 01/05/23 09:07:23.643
Jan  5 09:07:24.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 01/05/23 09:07:24.712
STEP: Creating a configMap that should be mutated 01/05/23 09:07:24.847
STEP: Deleting the collection of validation webhooks 01/05/23 09:07:25.819
STEP: Creating a configMap that should not be mutated 01/05/23 09:07:25.856
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:07:25.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6898" for this suite. 01/05/23 09:07:25.877
STEP: Destroying namespace "webhook-6898-markers" for this suite. 01/05/23 09:07:25.885
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":6,"skipped":116,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.792 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:07:17.144
    Jan  5 09:07:17.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 09:07:17.145
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:17.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:17.169
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 09:07:17.198
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:07:17.562
    STEP: Deploying the webhook pod 01/05/23 09:07:17.572
    STEP: Wait for the deployment to be ready 01/05/23 09:07:17.591
    Jan  5 09:07:17.602: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan  5 09:07:19.621: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:07:21.628: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 7, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/05/23 09:07:23.628
    STEP: Verifying the service has paired with the endpoint 01/05/23 09:07:23.643
    Jan  5 09:07:24.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 01/05/23 09:07:24.712
    STEP: Creating a configMap that should be mutated 01/05/23 09:07:24.847
    STEP: Deleting the collection of validation webhooks 01/05/23 09:07:25.819
    STEP: Creating a configMap that should not be mutated 01/05/23 09:07:25.856
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:07:25.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6898" for this suite. 01/05/23 09:07:25.877
    STEP: Destroying namespace "webhook-6898-markers" for this suite. 01/05/23 09:07:25.885
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:07:25.937
Jan  5 09:07:25.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 09:07:25.938
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:25.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:25.963
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 09:07:25.984
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:07:26.325
STEP: Deploying the webhook pod 01/05/23 09:07:26.331
STEP: Wait for the deployment to be ready 01/05/23 09:07:26.342
Jan  5 09:07:26.353: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 09:07:28.368
STEP: Verifying the service has paired with the endpoint 01/05/23 09:07:28.38
Jan  5 09:07:29.380: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jan  5 09:07:29.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7565-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 09:07:29.905
STEP: Creating a custom resource that should be mutated by the webhook 01/05/23 09:07:30.026
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:07:32.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7387" for this suite. 01/05/23 09:07:32.743
STEP: Destroying namespace "webhook-7387-markers" for this suite. 01/05/23 09:07:32.751
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":7,"skipped":117,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.891 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:07:25.937
    Jan  5 09:07:25.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 09:07:25.938
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:25.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:25.963
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 09:07:25.984
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:07:26.325
    STEP: Deploying the webhook pod 01/05/23 09:07:26.331
    STEP: Wait for the deployment to be ready 01/05/23 09:07:26.342
    Jan  5 09:07:26.353: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 09:07:28.368
    STEP: Verifying the service has paired with the endpoint 01/05/23 09:07:28.38
    Jan  5 09:07:29.380: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jan  5 09:07:29.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7565-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 09:07:29.905
    STEP: Creating a custom resource that should be mutated by the webhook 01/05/23 09:07:30.026
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:07:32.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7387" for this suite. 01/05/23 09:07:32.743
    STEP: Destroying namespace "webhook-7387-markers" for this suite. 01/05/23 09:07:32.751
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:07:32.829
Jan  5 09:07:32.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename job 01/05/23 09:07:32.83
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:32.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:32.866
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 01/05/23 09:07:32.873
STEP: Ensure pods equal to paralellism count is attached to the job 01/05/23 09:07:32.882
STEP: patching /status 01/05/23 09:07:36.888
STEP: updating /status 01/05/23 09:07:36.896
STEP: get /status 01/05/23 09:07:36.909
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  5 09:07:36.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5148" for this suite. 01/05/23 09:07:36.923
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":8,"skipped":126,"failed":0}
------------------------------
â€¢ [4.102 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:07:32.829
    Jan  5 09:07:32.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename job 01/05/23 09:07:32.83
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:32.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:32.866
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 01/05/23 09:07:32.873
    STEP: Ensure pods equal to paralellism count is attached to the job 01/05/23 09:07:32.882
    STEP: patching /status 01/05/23 09:07:36.888
    STEP: updating /status 01/05/23 09:07:36.896
    STEP: get /status 01/05/23 09:07:36.909
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  5 09:07:36.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5148" for this suite. 01/05/23 09:07:36.923
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:07:36.93
Jan  5 09:07:36.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 09:07:36.931
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:36.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:36.962
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-4699 01/05/23 09:07:36.971
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[] 01/05/23 09:07:36.986
Jan  5 09:07:37.002: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4699 01/05/23 09:07:37.002
Jan  5 09:07:37.013: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4699" to be "running and ready"
Jan  5 09:07:37.019: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.229639ms
Jan  5 09:07:37.019: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:07:39.027: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013937063s
Jan  5 09:07:39.027: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  5 09:07:39.027: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[pod1:[80]] 01/05/23 09:07:39.032
Jan  5 09:07:39.049: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/05/23 09:07:39.049
Jan  5 09:07:39.049: INFO: Creating new exec pod
Jan  5 09:07:39.058: INFO: Waiting up to 5m0s for pod "execpodd8gmp" in namespace "services-4699" to be "running"
Jan  5 09:07:39.064: INFO: Pod "execpodd8gmp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.136265ms
Jan  5 09:07:41.071: INFO: Pod "execpodd8gmp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012761432s
Jan  5 09:07:43.071: INFO: Pod "execpodd8gmp": Phase="Running", Reason="", readiness=true. Elapsed: 4.01305985s
Jan  5 09:07:43.071: INFO: Pod "execpodd8gmp" satisfied condition "running"
Jan  5 09:07:44.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-4699 exec execpodd8gmp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan  5 09:07:44.502: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  5 09:07:44.502: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:07:44.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-4699 exec execpodd8gmp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.120.78.168 80'
Jan  5 09:07:44.861: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.120.78.168 80\nConnection to 10.120.78.168 80 port [tcp/http] succeeded!\n"
Jan  5 09:07:44.861: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-4699 01/05/23 09:07:44.861
Jan  5 09:07:44.872: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4699" to be "running and ready"
Jan  5 09:07:44.879: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.254626ms
Jan  5 09:07:44.879: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:07:46.885: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013509975s
Jan  5 09:07:46.885: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  5 09:07:46.885: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[pod1:[80] pod2:[80]] 01/05/23 09:07:46.889
Jan  5 09:07:46.915: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/05/23 09:07:46.915
Jan  5 09:07:47.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-4699 exec execpodd8gmp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan  5 09:07:48.345: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  5 09:07:48.345: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:07:48.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-4699 exec execpodd8gmp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.120.78.168 80'
Jan  5 09:07:48.754: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.120.78.168 80\nConnection to 10.120.78.168 80 port [tcp/http] succeeded!\n"
Jan  5 09:07:48.754: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4699 01/05/23 09:07:48.754
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[pod2:[80]] 01/05/23 09:07:48.766
Jan  5 09:07:48.789: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/05/23 09:07:48.789
Jan  5 09:07:49.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-4699 exec execpodd8gmp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan  5 09:07:50.308: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  5 09:07:50.308: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:07:50.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-4699 exec execpodd8gmp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.120.78.168 80'
Jan  5 09:07:50.650: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 10.120.78.168 80\nConnection to 10.120.78.168 80 port [tcp/http] succeeded!\n"
Jan  5 09:07:50.650: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-4699 01/05/23 09:07:50.65
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[] 01/05/23 09:07:50.665
Jan  5 09:07:50.683: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:07:50.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4699" for this suite. 01/05/23 09:07:50.709
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":9,"skipped":128,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.785 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:07:36.93
    Jan  5 09:07:36.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 09:07:36.931
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:36.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:36.962
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-4699 01/05/23 09:07:36.971
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[] 01/05/23 09:07:36.986
    Jan  5 09:07:37.002: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4699 01/05/23 09:07:37.002
    Jan  5 09:07:37.013: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4699" to be "running and ready"
    Jan  5 09:07:37.019: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.229639ms
    Jan  5 09:07:37.019: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:07:39.027: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013937063s
    Jan  5 09:07:39.027: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  5 09:07:39.027: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[pod1:[80]] 01/05/23 09:07:39.032
    Jan  5 09:07:39.049: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/05/23 09:07:39.049
    Jan  5 09:07:39.049: INFO: Creating new exec pod
    Jan  5 09:07:39.058: INFO: Waiting up to 5m0s for pod "execpodd8gmp" in namespace "services-4699" to be "running"
    Jan  5 09:07:39.064: INFO: Pod "execpodd8gmp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.136265ms
    Jan  5 09:07:41.071: INFO: Pod "execpodd8gmp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012761432s
    Jan  5 09:07:43.071: INFO: Pod "execpodd8gmp": Phase="Running", Reason="", readiness=true. Elapsed: 4.01305985s
    Jan  5 09:07:43.071: INFO: Pod "execpodd8gmp" satisfied condition "running"
    Jan  5 09:07:44.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-4699 exec execpodd8gmp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan  5 09:07:44.502: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  5 09:07:44.502: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:07:44.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-4699 exec execpodd8gmp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.120.78.168 80'
    Jan  5 09:07:44.861: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.120.78.168 80\nConnection to 10.120.78.168 80 port [tcp/http] succeeded!\n"
    Jan  5 09:07:44.861: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-4699 01/05/23 09:07:44.861
    Jan  5 09:07:44.872: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4699" to be "running and ready"
    Jan  5 09:07:44.879: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.254626ms
    Jan  5 09:07:44.879: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:07:46.885: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013509975s
    Jan  5 09:07:46.885: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  5 09:07:46.885: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[pod1:[80] pod2:[80]] 01/05/23 09:07:46.889
    Jan  5 09:07:46.915: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/05/23 09:07:46.915
    Jan  5 09:07:47.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-4699 exec execpodd8gmp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan  5 09:07:48.345: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  5 09:07:48.345: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:07:48.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-4699 exec execpodd8gmp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.120.78.168 80'
    Jan  5 09:07:48.754: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.120.78.168 80\nConnection to 10.120.78.168 80 port [tcp/http] succeeded!\n"
    Jan  5 09:07:48.754: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-4699 01/05/23 09:07:48.754
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[pod2:[80]] 01/05/23 09:07:48.766
    Jan  5 09:07:48.789: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/05/23 09:07:48.789
    Jan  5 09:07:49.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-4699 exec execpodd8gmp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan  5 09:07:50.308: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  5 09:07:50.308: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:07:50.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-4699 exec execpodd8gmp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.120.78.168 80'
    Jan  5 09:07:50.650: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 10.120.78.168 80\nConnection to 10.120.78.168 80 port [tcp/http] succeeded!\n"
    Jan  5 09:07:50.650: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-4699 01/05/23 09:07:50.65
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[] 01/05/23 09:07:50.665
    Jan  5 09:07:50.683: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:07:50.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4699" for this suite. 01/05/23 09:07:50.709
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:07:50.716
Jan  5 09:07:50.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 09:07:50.716
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:50.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:50.744
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 09:07:50.767
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:07:51.027
STEP: Deploying the webhook pod 01/05/23 09:07:51.037
STEP: Wait for the deployment to be ready 01/05/23 09:07:51.049
Jan  5 09:07:51.060: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 09:07:53.081
STEP: Verifying the service has paired with the endpoint 01/05/23 09:07:53.121
Jan  5 09:07:54.122: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/05/23 09:07:54.127
STEP: create a namespace for the webhook 01/05/23 09:07:54.255
STEP: create a configmap should be unconditionally rejected by the webhook 01/05/23 09:07:54.262
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:07:54.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1127" for this suite. 01/05/23 09:07:54.341
STEP: Destroying namespace "webhook-1127-markers" for this suite. 01/05/23 09:07:54.352
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":10,"skipped":131,"failed":0}
------------------------------
â€¢ [3.676 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:07:50.716
    Jan  5 09:07:50.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 09:07:50.716
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:50.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:50.744
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 09:07:50.767
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:07:51.027
    STEP: Deploying the webhook pod 01/05/23 09:07:51.037
    STEP: Wait for the deployment to be ready 01/05/23 09:07:51.049
    Jan  5 09:07:51.060: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 09:07:53.081
    STEP: Verifying the service has paired with the endpoint 01/05/23 09:07:53.121
    Jan  5 09:07:54.122: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/05/23 09:07:54.127
    STEP: create a namespace for the webhook 01/05/23 09:07:54.255
    STEP: create a configmap should be unconditionally rejected by the webhook 01/05/23 09:07:54.262
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:07:54.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1127" for this suite. 01/05/23 09:07:54.341
    STEP: Destroying namespace "webhook-1127-markers" for this suite. 01/05/23 09:07:54.352
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:07:54.393
Jan  5 09:07:54.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename replication-controller 01/05/23 09:07:54.394
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:54.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:54.419
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 01/05/23 09:07:54.432
STEP: waiting for RC to be added 01/05/23 09:07:54.437
STEP: waiting for available Replicas 01/05/23 09:07:54.438
STEP: patching ReplicationController 01/05/23 09:07:57.084
STEP: waiting for RC to be modified 01/05/23 09:07:57.093
STEP: patching ReplicationController status 01/05/23 09:07:57.093
STEP: waiting for RC to be modified 01/05/23 09:07:57.099
STEP: waiting for available Replicas 01/05/23 09:07:57.099
STEP: fetching ReplicationController status 01/05/23 09:07:57.104
STEP: patching ReplicationController scale 01/05/23 09:07:57.109
STEP: waiting for RC to be modified 01/05/23 09:07:57.119
STEP: waiting for ReplicationController's scale to be the max amount 01/05/23 09:07:57.119
STEP: fetching ReplicationController; ensuring that it's patched 01/05/23 09:07:59.801
STEP: updating ReplicationController status 01/05/23 09:07:59.807
STEP: waiting for RC to be modified 01/05/23 09:07:59.814
STEP: listing all ReplicationControllers 01/05/23 09:07:59.814
STEP: checking that ReplicationController has expected values 01/05/23 09:07:59.821
STEP: deleting ReplicationControllers by collection 01/05/23 09:07:59.821
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/05/23 09:07:59.829
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  5 09:07:59.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2733" for this suite. 01/05/23 09:07:59.86
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":11,"skipped":182,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.474 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:07:54.393
    Jan  5 09:07:54.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename replication-controller 01/05/23 09:07:54.394
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:54.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:54.419
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 01/05/23 09:07:54.432
    STEP: waiting for RC to be added 01/05/23 09:07:54.437
    STEP: waiting for available Replicas 01/05/23 09:07:54.438
    STEP: patching ReplicationController 01/05/23 09:07:57.084
    STEP: waiting for RC to be modified 01/05/23 09:07:57.093
    STEP: patching ReplicationController status 01/05/23 09:07:57.093
    STEP: waiting for RC to be modified 01/05/23 09:07:57.099
    STEP: waiting for available Replicas 01/05/23 09:07:57.099
    STEP: fetching ReplicationController status 01/05/23 09:07:57.104
    STEP: patching ReplicationController scale 01/05/23 09:07:57.109
    STEP: waiting for RC to be modified 01/05/23 09:07:57.119
    STEP: waiting for ReplicationController's scale to be the max amount 01/05/23 09:07:57.119
    STEP: fetching ReplicationController; ensuring that it's patched 01/05/23 09:07:59.801
    STEP: updating ReplicationController status 01/05/23 09:07:59.807
    STEP: waiting for RC to be modified 01/05/23 09:07:59.814
    STEP: listing all ReplicationControllers 01/05/23 09:07:59.814
    STEP: checking that ReplicationController has expected values 01/05/23 09:07:59.821
    STEP: deleting ReplicationControllers by collection 01/05/23 09:07:59.821
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/05/23 09:07:59.829
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  5 09:07:59.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2733" for this suite. 01/05/23 09:07:59.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:07:59.867
Jan  5 09:07:59.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:07:59.868
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:59.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:59.888
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:07:59.895
Jan  5 09:07:59.906: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab" in namespace "projected-3088" to be "Succeeded or Failed"
Jan  5 09:07:59.909: INFO: Pod "downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.611153ms
Jan  5 09:08:01.914: INFO: Pod "downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008826661s
Jan  5 09:08:03.917: INFO: Pod "downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011866259s
STEP: Saw pod success 01/05/23 09:08:03.917
Jan  5 09:08:03.918: INFO: Pod "downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab" satisfied condition "Succeeded or Failed"
Jan  5 09:08:03.928: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab container client-container: <nil>
STEP: delete the pod 01/05/23 09:08:03.981
Jan  5 09:08:03.994: INFO: Waiting for pod downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab to disappear
Jan  5 09:08:03.998: INFO: Pod downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 09:08:03.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3088" for this suite. 01/05/23 09:08:04.007
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":12,"skipped":188,"failed":0}
------------------------------
â€¢ [4.146 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:07:59.867
    Jan  5 09:07:59.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:07:59.868
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:07:59.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:07:59.888
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:07:59.895
    Jan  5 09:07:59.906: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab" in namespace "projected-3088" to be "Succeeded or Failed"
    Jan  5 09:07:59.909: INFO: Pod "downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.611153ms
    Jan  5 09:08:01.914: INFO: Pod "downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008826661s
    Jan  5 09:08:03.917: INFO: Pod "downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011866259s
    STEP: Saw pod success 01/05/23 09:08:03.917
    Jan  5 09:08:03.918: INFO: Pod "downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab" satisfied condition "Succeeded or Failed"
    Jan  5 09:08:03.928: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab container client-container: <nil>
    STEP: delete the pod 01/05/23 09:08:03.981
    Jan  5 09:08:03.994: INFO: Waiting for pod downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab to disappear
    Jan  5 09:08:03.998: INFO: Pod downwardapi-volume-c060bceb-51e1-4fca-a38a-b93b69c508ab no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 09:08:03.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3088" for this suite. 01/05/23 09:08:04.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:08:04.015
Jan  5 09:08:04.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename namespaces 01/05/23 09:08:04.016
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:04.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:04.04
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 01/05/23 09:08:04.047
STEP: patching the Namespace 01/05/23 09:08:04.064
STEP: get the Namespace and ensuring it has the label 01/05/23 09:08:04.071
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:08:04.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5445" for this suite. 01/05/23 09:08:04.082
STEP: Destroying namespace "nspatchtest-26cc967a-ea8d-4728-b530-6f2c3dfed6ed-257" for this suite. 01/05/23 09:08:04.088
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":13,"skipped":212,"failed":0}
------------------------------
â€¢ [0.084 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:08:04.015
    Jan  5 09:08:04.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename namespaces 01/05/23 09:08:04.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:04.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:04.04
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 01/05/23 09:08:04.047
    STEP: patching the Namespace 01/05/23 09:08:04.064
    STEP: get the Namespace and ensuring it has the label 01/05/23 09:08:04.071
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:08:04.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5445" for this suite. 01/05/23 09:08:04.082
    STEP: Destroying namespace "nspatchtest-26cc967a-ea8d-4728-b530-6f2c3dfed6ed-257" for this suite. 01/05/23 09:08:04.088
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:08:04.1
Jan  5 09:08:04.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 09:08:04.101
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:04.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:04.124
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/05/23 09:08:04.138
Jan  5 09:08:04.151: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6113" to be "running and ready"
Jan  5 09:08:04.155: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.287128ms
Jan  5 09:08:04.155: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:08:06.162: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010588477s
Jan  5 09:08:06.162: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  5 09:08:06.162: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 01/05/23 09:08:06.166
Jan  5 09:08:06.175: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6113" to be "running and ready"
Jan  5 09:08:06.180: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.732429ms
Jan  5 09:08:06.181: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:08:08.249: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.074053537s
Jan  5 09:08:08.249: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan  5 09:08:08.249: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/05/23 09:08:08.255
Jan  5 09:08:08.268: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  5 09:08:08.274: INFO: Pod pod-with-prestop-http-hook still exists
Jan  5 09:08:10.274: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  5 09:08:10.281: INFO: Pod pod-with-prestop-http-hook still exists
Jan  5 09:08:12.275: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  5 09:08:12.281: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/05/23 09:08:12.282
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan  5 09:08:12.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6113" for this suite. 01/05/23 09:08:12.335
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":14,"skipped":213,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.242 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:08:04.1
    Jan  5 09:08:04.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 09:08:04.101
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:04.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:04.124
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/05/23 09:08:04.138
    Jan  5 09:08:04.151: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6113" to be "running and ready"
    Jan  5 09:08:04.155: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.287128ms
    Jan  5 09:08:04.155: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:08:06.162: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010588477s
    Jan  5 09:08:06.162: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  5 09:08:06.162: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 01/05/23 09:08:06.166
    Jan  5 09:08:06.175: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6113" to be "running and ready"
    Jan  5 09:08:06.180: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.732429ms
    Jan  5 09:08:06.181: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:08:08.249: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.074053537s
    Jan  5 09:08:08.249: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan  5 09:08:08.249: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/05/23 09:08:08.255
    Jan  5 09:08:08.268: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  5 09:08:08.274: INFO: Pod pod-with-prestop-http-hook still exists
    Jan  5 09:08:10.274: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  5 09:08:10.281: INFO: Pod pod-with-prestop-http-hook still exists
    Jan  5 09:08:12.275: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  5 09:08:12.281: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/05/23 09:08:12.282
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan  5 09:08:12.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6113" for this suite. 01/05/23 09:08:12.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:08:12.342
Jan  5 09:08:12.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename job 01/05/23 09:08:12.343
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:12.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:12.376
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 01/05/23 09:08:12.383
STEP: Ensuring job reaches completions 01/05/23 09:08:12.39
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  5 09:08:24.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6965" for this suite. 01/05/23 09:08:24.408
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":15,"skipped":233,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.072 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:08:12.342
    Jan  5 09:08:12.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename job 01/05/23 09:08:12.343
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:12.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:12.376
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 01/05/23 09:08:12.383
    STEP: Ensuring job reaches completions 01/05/23 09:08:12.39
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  5 09:08:24.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6965" for this suite. 01/05/23 09:08:24.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:08:24.417
Jan  5 09:08:24.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename var-expansion 01/05/23 09:08:24.417
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:24.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:24.44
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 01/05/23 09:08:24.447
Jan  5 09:08:24.499: INFO: Waiting up to 5m0s for pod "var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4" in namespace "var-expansion-7342" to be "Succeeded or Failed"
Jan  5 09:08:24.552: INFO: Pod "var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4": Phase="Pending", Reason="", readiness=false. Elapsed: 53.153988ms
Jan  5 09:08:26.567: INFO: Pod "var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4": Phase="Running", Reason="", readiness=false. Elapsed: 2.068058043s
Jan  5 09:08:28.559: INFO: Pod "var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060688572s
STEP: Saw pod success 01/05/23 09:08:28.559
Jan  5 09:08:28.559: INFO: Pod "var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4" satisfied condition "Succeeded or Failed"
Jan  5 09:08:28.568: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4 container dapi-container: <nil>
STEP: delete the pod 01/05/23 09:08:28.591
Jan  5 09:08:28.601: INFO: Waiting for pod var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4 to disappear
Jan  5 09:08:28.607: INFO: Pod var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 09:08:28.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7342" for this suite. 01/05/23 09:08:28.619
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":16,"skipped":280,"failed":0}
------------------------------
â€¢ [4.211 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:08:24.417
    Jan  5 09:08:24.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename var-expansion 01/05/23 09:08:24.417
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:24.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:24.44
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 01/05/23 09:08:24.447
    Jan  5 09:08:24.499: INFO: Waiting up to 5m0s for pod "var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4" in namespace "var-expansion-7342" to be "Succeeded or Failed"
    Jan  5 09:08:24.552: INFO: Pod "var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4": Phase="Pending", Reason="", readiness=false. Elapsed: 53.153988ms
    Jan  5 09:08:26.567: INFO: Pod "var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4": Phase="Running", Reason="", readiness=false. Elapsed: 2.068058043s
    Jan  5 09:08:28.559: INFO: Pod "var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060688572s
    STEP: Saw pod success 01/05/23 09:08:28.559
    Jan  5 09:08:28.559: INFO: Pod "var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4" satisfied condition "Succeeded or Failed"
    Jan  5 09:08:28.568: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 09:08:28.591
    Jan  5 09:08:28.601: INFO: Waiting for pod var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4 to disappear
    Jan  5 09:08:28.607: INFO: Pod var-expansion-84c25386-33ef-4041-bcf0-fd533eb1d3f4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 09:08:28.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7342" for this suite. 01/05/23 09:08:28.619
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:08:28.628
Jan  5 09:08:28.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pods 01/05/23 09:08:28.629
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:28.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:28.666
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 01/05/23 09:08:28.684
STEP: watching for Pod to be ready 01/05/23 09:08:28.696
Jan  5 09:08:28.699: INFO: observed Pod pod-test in namespace pods-3739 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan  5 09:08:28.706: INFO: observed Pod pod-test in namespace pods-3739 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC  }]
Jan  5 09:08:28.736: INFO: observed Pod pod-test in namespace pods-3739 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC  }]
Jan  5 09:08:30.170: INFO: Found Pod pod-test in namespace pods-3739 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/05/23 09:08:30.175
STEP: getting the Pod and ensuring that it's patched 01/05/23 09:08:30.189
STEP: replacing the Pod's status Ready condition to False 01/05/23 09:08:30.195
STEP: check the Pod again to ensure its Ready conditions are False 01/05/23 09:08:30.21
STEP: deleting the Pod via a Collection with a LabelSelector 01/05/23 09:08:30.21
STEP: watching for the Pod to be deleted 01/05/23 09:08:30.218
Jan  5 09:08:30.224: INFO: observed event type MODIFIED
Jan  5 09:08:32.175: INFO: observed event type MODIFIED
Jan  5 09:08:33.183: INFO: observed event type MODIFIED
Jan  5 09:08:33.189: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 09:08:33.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3739" for this suite. 01/05/23 09:08:33.205
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":17,"skipped":281,"failed":0}
------------------------------
â€¢ [4.583 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:08:28.628
    Jan  5 09:08:28.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pods 01/05/23 09:08:28.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:28.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:28.666
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 01/05/23 09:08:28.684
    STEP: watching for Pod to be ready 01/05/23 09:08:28.696
    Jan  5 09:08:28.699: INFO: observed Pod pod-test in namespace pods-3739 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan  5 09:08:28.706: INFO: observed Pod pod-test in namespace pods-3739 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC  }]
    Jan  5 09:08:28.736: INFO: observed Pod pod-test in namespace pods-3739 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC  }]
    Jan  5 09:08:30.170: INFO: Found Pod pod-test in namespace pods-3739 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:08:28 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/05/23 09:08:30.175
    STEP: getting the Pod and ensuring that it's patched 01/05/23 09:08:30.189
    STEP: replacing the Pod's status Ready condition to False 01/05/23 09:08:30.195
    STEP: check the Pod again to ensure its Ready conditions are False 01/05/23 09:08:30.21
    STEP: deleting the Pod via a Collection with a LabelSelector 01/05/23 09:08:30.21
    STEP: watching for the Pod to be deleted 01/05/23 09:08:30.218
    Jan  5 09:08:30.224: INFO: observed event type MODIFIED
    Jan  5 09:08:32.175: INFO: observed event type MODIFIED
    Jan  5 09:08:33.183: INFO: observed event type MODIFIED
    Jan  5 09:08:33.189: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 09:08:33.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3739" for this suite. 01/05/23 09:08:33.205
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:08:33.211
Jan  5 09:08:33.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 09:08:33.212
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:33.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:33.233
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan  5 09:08:33.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:08:34.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8800" for this suite. 01/05/23 09:08:34.291
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":18,"skipped":283,"failed":0}
------------------------------
â€¢ [1.087 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:08:33.211
    Jan  5 09:08:33.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 09:08:33.212
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:33.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:33.233
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan  5 09:08:33.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:08:34.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8800" for this suite. 01/05/23 09:08:34.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:08:34.3
Jan  5 09:08:34.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename daemonsets 01/05/23 09:08:34.301
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:34.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:34.324
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 01/05/23 09:08:34.365
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:08:34.373
Jan  5 09:08:34.386: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:08:34.386: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:08:35.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:08:35.405: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:08:36.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  5 09:08:36.405: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/05/23 09:08:36.411
Jan  5 09:08:36.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 09:08:36.459: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r is running 0 daemon pod, expected 1
Jan  5 09:08:37.481: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 09:08:37.481: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r is running 0 daemon pod, expected 1
Jan  5 09:08:38.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 09:08:38.476: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r is running 0 daemon pod, expected 1
Jan  5 09:08:39.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 09:08:39.476: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r is running 0 daemon pod, expected 1
Jan  5 09:08:40.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  5 09:08:40.476: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/05/23 09:08:40.481
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4230, will wait for the garbage collector to delete the pods 01/05/23 09:08:40.481
Jan  5 09:08:40.544: INFO: Deleting DaemonSet.extensions daemon-set took: 7.531455ms
Jan  5 09:08:40.646: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.525958ms
Jan  5 09:08:43.253: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:08:43.253: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 09:08:43.258: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6520"},"items":null}

Jan  5 09:08:43.263: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6520"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:08:43.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4230" for this suite. 01/05/23 09:08:43.311
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":19,"skipped":314,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.017 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:08:34.3
    Jan  5 09:08:34.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename daemonsets 01/05/23 09:08:34.301
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:34.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:34.324
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 01/05/23 09:08:34.365
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:08:34.373
    Jan  5 09:08:34.386: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:08:34.386: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:08:35.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:08:35.405: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:08:36.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  5 09:08:36.405: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/05/23 09:08:36.411
    Jan  5 09:08:36.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 09:08:36.459: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r is running 0 daemon pod, expected 1
    Jan  5 09:08:37.481: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 09:08:37.481: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r is running 0 daemon pod, expected 1
    Jan  5 09:08:38.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 09:08:38.476: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r is running 0 daemon pod, expected 1
    Jan  5 09:08:39.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 09:08:39.476: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r is running 0 daemon pod, expected 1
    Jan  5 09:08:40.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  5 09:08:40.476: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 09:08:40.481
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4230, will wait for the garbage collector to delete the pods 01/05/23 09:08:40.481
    Jan  5 09:08:40.544: INFO: Deleting DaemonSet.extensions daemon-set took: 7.531455ms
    Jan  5 09:08:40.646: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.525958ms
    Jan  5 09:08:43.253: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:08:43.253: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 09:08:43.258: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6520"},"items":null}

    Jan  5 09:08:43.263: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6520"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:08:43.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4230" for this suite. 01/05/23 09:08:43.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:08:43.319
Jan  5 09:08:43.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename svcaccounts 01/05/23 09:08:43.319
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:43.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:43.346
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jan  5 09:08:43.373: INFO: created pod
Jan  5 09:08:43.373: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1507" to be "Succeeded or Failed"
Jan  5 09:08:43.382: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.367556ms
Jan  5 09:08:45.391: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017468371s
Jan  5 09:08:47.390: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01674472s
STEP: Saw pod success 01/05/23 09:08:47.39
Jan  5 09:08:47.390: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan  5 09:09:17.391: INFO: polling logs
Jan  5 09:09:17.406: INFO: Pod logs: 
I0105 09:08:44.175912       1 log.go:195] OK: Got token
I0105 09:08:44.175953       1 log.go:195] validating with in-cluster discovery
I0105 09:08:44.176320       1 log.go:195] OK: got issuer https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io
I0105 09:08:44.176395       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io", Subject:"system:serviceaccount:svcaccounts-1507:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672910323, NotBefore:1672909723, IssuedAt:1672909723, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1507", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cb254729-ad62-4a91-a192-3d4141ce9b03"}}}
I0105 09:08:44.193476       1 log.go:195] OK: Constructed OIDC provider for issuer https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io
I0105 09:08:44.197134       1 log.go:195] OK: Validated signature on JWT
I0105 09:08:44.197231       1 log.go:195] OK: Got valid claims from token!
I0105 09:08:44.197263       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io", Subject:"system:serviceaccount:svcaccounts-1507:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672910323, NotBefore:1672909723, IssuedAt:1672909723, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1507", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cb254729-ad62-4a91-a192-3d4141ce9b03"}}}

Jan  5 09:09:17.406: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  5 09:09:17.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1507" for this suite. 01/05/23 09:09:17.422
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":20,"skipped":319,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.109 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:08:43.319
    Jan  5 09:08:43.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 09:08:43.319
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:08:43.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:08:43.346
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jan  5 09:08:43.373: INFO: created pod
    Jan  5 09:08:43.373: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1507" to be "Succeeded or Failed"
    Jan  5 09:08:43.382: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.367556ms
    Jan  5 09:08:45.391: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017468371s
    Jan  5 09:08:47.390: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01674472s
    STEP: Saw pod success 01/05/23 09:08:47.39
    Jan  5 09:08:47.390: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan  5 09:09:17.391: INFO: polling logs
    Jan  5 09:09:17.406: INFO: Pod logs: 
    I0105 09:08:44.175912       1 log.go:195] OK: Got token
    I0105 09:08:44.175953       1 log.go:195] validating with in-cluster discovery
    I0105 09:08:44.176320       1 log.go:195] OK: got issuer https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io
    I0105 09:08:44.176395       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io", Subject:"system:serviceaccount:svcaccounts-1507:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672910323, NotBefore:1672909723, IssuedAt:1672909723, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1507", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cb254729-ad62-4a91-a192-3d4141ce9b03"}}}
    I0105 09:08:44.193476       1 log.go:195] OK: Constructed OIDC provider for issuer https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io
    I0105 09:08:44.197134       1 log.go:195] OK: Validated signature on JWT
    I0105 09:08:44.197231       1 log.go:195] OK: Got valid claims from token!
    I0105 09:08:44.197263       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io", Subject:"system:serviceaccount:svcaccounts-1507:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672910323, NotBefore:1672909723, IssuedAt:1672909723, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1507", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cb254729-ad62-4a91-a192-3d4141ce9b03"}}}

    Jan  5 09:09:17.406: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  5 09:09:17.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1507" for this suite. 01/05/23 09:09:17.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:09:17.429
Jan  5 09:09:17.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename gc 01/05/23 09:09:17.429
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:17.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:17.46
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan  5 09:09:17.518: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"cd5c4195-1535-47fb-a2f5-711ddf491fc0", Controller:(*bool)(0xc0035b4c06), BlockOwnerDeletion:(*bool)(0xc0035b4c07)}}
Jan  5 09:09:17.549: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"12e526f9-6dd2-4744-9d70-2e33231888e2", Controller:(*bool)(0xc0035b4e9e), BlockOwnerDeletion:(*bool)(0xc0035b4e9f)}}
Jan  5 09:09:17.556: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7e12dba8-0d7b-4735-8ebb-975cf0ad6514", Controller:(*bool)(0xc003548d56), BlockOwnerDeletion:(*bool)(0xc003548d57)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 09:09:22.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4164" for this suite. 01/05/23 09:09:22.579
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":21,"skipped":331,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.157 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:09:17.429
    Jan  5 09:09:17.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename gc 01/05/23 09:09:17.429
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:17.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:17.46
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan  5 09:09:17.518: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"cd5c4195-1535-47fb-a2f5-711ddf491fc0", Controller:(*bool)(0xc0035b4c06), BlockOwnerDeletion:(*bool)(0xc0035b4c07)}}
    Jan  5 09:09:17.549: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"12e526f9-6dd2-4744-9d70-2e33231888e2", Controller:(*bool)(0xc0035b4e9e), BlockOwnerDeletion:(*bool)(0xc0035b4e9f)}}
    Jan  5 09:09:17.556: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7e12dba8-0d7b-4735-8ebb-975cf0ad6514", Controller:(*bool)(0xc003548d56), BlockOwnerDeletion:(*bool)(0xc003548d57)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 09:09:22.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4164" for this suite. 01/05/23 09:09:22.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:09:22.588
Jan  5 09:09:22.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 09:09:22.589
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:22.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:22.612
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/05/23 09:09:22.621
Jan  5 09:09:22.632: INFO: Waiting up to 5m0s for pod "pod-e517d2c1-dfcd-4a10-b329-67730487ff61" in namespace "emptydir-790" to be "Succeeded or Failed"
Jan  5 09:09:22.639: INFO: Pod "pod-e517d2c1-dfcd-4a10-b329-67730487ff61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.759585ms
Jan  5 09:09:24.646: INFO: Pod "pod-e517d2c1-dfcd-4a10-b329-67730487ff61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013925862s
Jan  5 09:09:26.645: INFO: Pod "pod-e517d2c1-dfcd-4a10-b329-67730487ff61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012912952s
STEP: Saw pod success 01/05/23 09:09:26.645
Jan  5 09:09:26.645: INFO: Pod "pod-e517d2c1-dfcd-4a10-b329-67730487ff61" satisfied condition "Succeeded or Failed"
Jan  5 09:09:26.650: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-e517d2c1-dfcd-4a10-b329-67730487ff61 container test-container: <nil>
STEP: delete the pod 01/05/23 09:09:26.664
Jan  5 09:09:26.685: INFO: Waiting for pod pod-e517d2c1-dfcd-4a10-b329-67730487ff61 to disappear
Jan  5 09:09:26.689: INFO: Pod pod-e517d2c1-dfcd-4a10-b329-67730487ff61 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 09:09:26.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-790" for this suite. 01/05/23 09:09:26.697
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":22,"skipped":362,"failed":0}
------------------------------
â€¢ [4.116 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:09:22.588
    Jan  5 09:09:22.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 09:09:22.589
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:22.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:22.612
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/05/23 09:09:22.621
    Jan  5 09:09:22.632: INFO: Waiting up to 5m0s for pod "pod-e517d2c1-dfcd-4a10-b329-67730487ff61" in namespace "emptydir-790" to be "Succeeded or Failed"
    Jan  5 09:09:22.639: INFO: Pod "pod-e517d2c1-dfcd-4a10-b329-67730487ff61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.759585ms
    Jan  5 09:09:24.646: INFO: Pod "pod-e517d2c1-dfcd-4a10-b329-67730487ff61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013925862s
    Jan  5 09:09:26.645: INFO: Pod "pod-e517d2c1-dfcd-4a10-b329-67730487ff61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012912952s
    STEP: Saw pod success 01/05/23 09:09:26.645
    Jan  5 09:09:26.645: INFO: Pod "pod-e517d2c1-dfcd-4a10-b329-67730487ff61" satisfied condition "Succeeded or Failed"
    Jan  5 09:09:26.650: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-e517d2c1-dfcd-4a10-b329-67730487ff61 container test-container: <nil>
    STEP: delete the pod 01/05/23 09:09:26.664
    Jan  5 09:09:26.685: INFO: Waiting for pod pod-e517d2c1-dfcd-4a10-b329-67730487ff61 to disappear
    Jan  5 09:09:26.689: INFO: Pod pod-e517d2c1-dfcd-4a10-b329-67730487ff61 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 09:09:26.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-790" for this suite. 01/05/23 09:09:26.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:09:26.705
Jan  5 09:09:26.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename resourcequota 01/05/23 09:09:26.706
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:26.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:26.727
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 01/05/23 09:09:26.733
STEP: Ensuring ResourceQuota status is calculated 01/05/23 09:09:26.739
STEP: Creating a ResourceQuota with not terminating scope 01/05/23 09:09:28.746
STEP: Ensuring ResourceQuota status is calculated 01/05/23 09:09:28.752
STEP: Creating a long running pod 01/05/23 09:09:30.762
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/05/23 09:09:30.781
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/05/23 09:09:32.787
STEP: Deleting the pod 01/05/23 09:09:34.794
STEP: Ensuring resource quota status released the pod usage 01/05/23 09:09:34.806
STEP: Creating a terminating pod 01/05/23 09:09:36.812
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/05/23 09:09:36.826
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/05/23 09:09:38.834
STEP: Deleting the pod 01/05/23 09:09:40.84
STEP: Ensuring resource quota status released the pod usage 01/05/23 09:09:40.851
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 09:09:42.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9963" for this suite. 01/05/23 09:09:42.867
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":23,"skipped":378,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.170 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:09:26.705
    Jan  5 09:09:26.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename resourcequota 01/05/23 09:09:26.706
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:26.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:26.727
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 01/05/23 09:09:26.733
    STEP: Ensuring ResourceQuota status is calculated 01/05/23 09:09:26.739
    STEP: Creating a ResourceQuota with not terminating scope 01/05/23 09:09:28.746
    STEP: Ensuring ResourceQuota status is calculated 01/05/23 09:09:28.752
    STEP: Creating a long running pod 01/05/23 09:09:30.762
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/05/23 09:09:30.781
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/05/23 09:09:32.787
    STEP: Deleting the pod 01/05/23 09:09:34.794
    STEP: Ensuring resource quota status released the pod usage 01/05/23 09:09:34.806
    STEP: Creating a terminating pod 01/05/23 09:09:36.812
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/05/23 09:09:36.826
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/05/23 09:09:38.834
    STEP: Deleting the pod 01/05/23 09:09:40.84
    STEP: Ensuring resource quota status released the pod usage 01/05/23 09:09:40.851
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 09:09:42.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9963" for this suite. 01/05/23 09:09:42.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:09:42.876
Jan  5 09:09:42.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename sysctl 01/05/23 09:09:42.876
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:42.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:42.903
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/05/23 09:09:42.91
STEP: Watching for error events or started pod 01/05/23 09:09:42.921
STEP: Waiting for pod completion 01/05/23 09:09:44.929
Jan  5 09:09:44.929: INFO: Waiting up to 3m0s for pod "sysctl-d18b8731-1c43-4a7a-9439-29ac97a79c4f" in namespace "sysctl-442" to be "completed"
Jan  5 09:09:44.934: INFO: Pod "sysctl-d18b8731-1c43-4a7a-9439-29ac97a79c4f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.350691ms
Jan  5 09:09:46.940: INFO: Pod "sysctl-d18b8731-1c43-4a7a-9439-29ac97a79c4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011589613s
Jan  5 09:09:46.940: INFO: Pod "sysctl-d18b8731-1c43-4a7a-9439-29ac97a79c4f" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/05/23 09:09:46.945
STEP: Getting logs from the pod 01/05/23 09:09:46.945
STEP: Checking that the sysctl is actually updated 01/05/23 09:09:46.963
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 09:09:46.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-442" for this suite. 01/05/23 09:09:46.973
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":24,"skipped":386,"failed":0}
------------------------------
â€¢ [4.103 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:09:42.876
    Jan  5 09:09:42.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename sysctl 01/05/23 09:09:42.876
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:42.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:42.903
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/05/23 09:09:42.91
    STEP: Watching for error events or started pod 01/05/23 09:09:42.921
    STEP: Waiting for pod completion 01/05/23 09:09:44.929
    Jan  5 09:09:44.929: INFO: Waiting up to 3m0s for pod "sysctl-d18b8731-1c43-4a7a-9439-29ac97a79c4f" in namespace "sysctl-442" to be "completed"
    Jan  5 09:09:44.934: INFO: Pod "sysctl-d18b8731-1c43-4a7a-9439-29ac97a79c4f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.350691ms
    Jan  5 09:09:46.940: INFO: Pod "sysctl-d18b8731-1c43-4a7a-9439-29ac97a79c4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011589613s
    Jan  5 09:09:46.940: INFO: Pod "sysctl-d18b8731-1c43-4a7a-9439-29ac97a79c4f" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/05/23 09:09:46.945
    STEP: Getting logs from the pod 01/05/23 09:09:46.945
    STEP: Checking that the sysctl is actually updated 01/05/23 09:09:46.963
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 09:09:46.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-442" for this suite. 01/05/23 09:09:46.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:09:46.98
Jan  5 09:09:46.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 09:09:46.981
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:47.006
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/05/23 09:09:47.013
Jan  5 09:09:47.024: INFO: Waiting up to 5m0s for pod "pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3" in namespace "emptydir-8227" to be "Succeeded or Failed"
Jan  5 09:09:47.029: INFO: Pod "pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.514476ms
Jan  5 09:09:49.037: INFO: Pod "pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012240594s
Jan  5 09:09:51.036: INFO: Pod "pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011798221s
STEP: Saw pod success 01/05/23 09:09:51.036
Jan  5 09:09:51.036: INFO: Pod "pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3" satisfied condition "Succeeded or Failed"
Jan  5 09:09:51.042: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3 container test-container: <nil>
STEP: delete the pod 01/05/23 09:09:51.056
Jan  5 09:09:51.068: INFO: Waiting for pod pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3 to disappear
Jan  5 09:09:51.077: INFO: Pod pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 09:09:51.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8227" for this suite. 01/05/23 09:09:51.087
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":25,"skipped":395,"failed":0}
------------------------------
â€¢ [4.113 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:09:46.98
    Jan  5 09:09:46.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 09:09:46.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:47.006
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/05/23 09:09:47.013
    Jan  5 09:09:47.024: INFO: Waiting up to 5m0s for pod "pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3" in namespace "emptydir-8227" to be "Succeeded or Failed"
    Jan  5 09:09:47.029: INFO: Pod "pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.514476ms
    Jan  5 09:09:49.037: INFO: Pod "pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012240594s
    Jan  5 09:09:51.036: INFO: Pod "pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011798221s
    STEP: Saw pod success 01/05/23 09:09:51.036
    Jan  5 09:09:51.036: INFO: Pod "pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3" satisfied condition "Succeeded or Failed"
    Jan  5 09:09:51.042: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3 container test-container: <nil>
    STEP: delete the pod 01/05/23 09:09:51.056
    Jan  5 09:09:51.068: INFO: Waiting for pod pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3 to disappear
    Jan  5 09:09:51.077: INFO: Pod pod-bb8a33ae-106a-4f5a-82f5-f32037e271f3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 09:09:51.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8227" for this suite. 01/05/23 09:09:51.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:09:51.095
Jan  5 09:09:51.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:09:51.096
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:51.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:51.12
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 01/05/23 09:09:51.128
Jan  5 09:09:51.140: INFO: Waiting up to 5m0s for pod "downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b" in namespace "downward-api-5968" to be "Succeeded or Failed"
Jan  5 09:09:51.146: INFO: Pod "downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.375177ms
Jan  5 09:09:53.153: INFO: Pod "downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012126894s
Jan  5 09:09:55.152: INFO: Pod "downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011687635s
STEP: Saw pod success 01/05/23 09:09:55.152
Jan  5 09:09:55.152: INFO: Pod "downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b" satisfied condition "Succeeded or Failed"
Jan  5 09:09:55.156: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b container dapi-container: <nil>
STEP: delete the pod 01/05/23 09:09:55.168
Jan  5 09:09:55.179: INFO: Waiting for pod downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b to disappear
Jan  5 09:09:55.184: INFO: Pod downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  5 09:09:55.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5968" for this suite. 01/05/23 09:09:55.193
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":26,"skipped":403,"failed":0}
------------------------------
â€¢ [4.107 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:09:51.095
    Jan  5 09:09:51.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:09:51.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:51.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:51.12
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 01/05/23 09:09:51.128
    Jan  5 09:09:51.140: INFO: Waiting up to 5m0s for pod "downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b" in namespace "downward-api-5968" to be "Succeeded or Failed"
    Jan  5 09:09:51.146: INFO: Pod "downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.375177ms
    Jan  5 09:09:53.153: INFO: Pod "downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012126894s
    Jan  5 09:09:55.152: INFO: Pod "downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011687635s
    STEP: Saw pod success 01/05/23 09:09:55.152
    Jan  5 09:09:55.152: INFO: Pod "downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b" satisfied condition "Succeeded or Failed"
    Jan  5 09:09:55.156: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b container dapi-container: <nil>
    STEP: delete the pod 01/05/23 09:09:55.168
    Jan  5 09:09:55.179: INFO: Waiting for pod downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b to disappear
    Jan  5 09:09:55.184: INFO: Pod downward-api-ba6a969a-296e-4fbc-b3c0-3e96ad649e4b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  5 09:09:55.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5968" for this suite. 01/05/23 09:09:55.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:09:55.203
Jan  5 09:09:55.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 09:09:55.203
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:55.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:55.226
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 09:09:55.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2078" for this suite. 01/05/23 09:09:55.279
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":27,"skipped":409,"failed":0}
------------------------------
â€¢ [0.083 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:09:55.203
    Jan  5 09:09:55.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 09:09:55.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:55.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:55.226
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 09:09:55.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2078" for this suite. 01/05/23 09:09:55.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:09:55.285
Jan  5 09:09:55.286: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 09:09:55.287
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:55.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:55.307
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/05/23 09:09:55.314
Jan  5 09:09:55.324: INFO: Waiting up to 5m0s for pod "pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc" in namespace "emptydir-710" to be "Succeeded or Failed"
Jan  5 09:09:55.329: INFO: Pod "pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.764332ms
Jan  5 09:09:57.337: INFO: Pod "pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01243473s
Jan  5 09:09:59.336: INFO: Pod "pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011639346s
STEP: Saw pod success 01/05/23 09:09:59.336
Jan  5 09:09:59.336: INFO: Pod "pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc" satisfied condition "Succeeded or Failed"
Jan  5 09:09:59.341: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc container test-container: <nil>
STEP: delete the pod 01/05/23 09:09:59.366
Jan  5 09:09:59.381: INFO: Waiting for pod pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc to disappear
Jan  5 09:09:59.386: INFO: Pod pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 09:09:59.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-710" for this suite. 01/05/23 09:09:59.395
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":28,"skipped":419,"failed":0}
------------------------------
â€¢ [4.118 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:09:55.285
    Jan  5 09:09:55.286: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 09:09:55.287
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:55.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:55.307
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/05/23 09:09:55.314
    Jan  5 09:09:55.324: INFO: Waiting up to 5m0s for pod "pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc" in namespace "emptydir-710" to be "Succeeded or Failed"
    Jan  5 09:09:55.329: INFO: Pod "pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.764332ms
    Jan  5 09:09:57.337: INFO: Pod "pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01243473s
    Jan  5 09:09:59.336: INFO: Pod "pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011639346s
    STEP: Saw pod success 01/05/23 09:09:59.336
    Jan  5 09:09:59.336: INFO: Pod "pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc" satisfied condition "Succeeded or Failed"
    Jan  5 09:09:59.341: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc container test-container: <nil>
    STEP: delete the pod 01/05/23 09:09:59.366
    Jan  5 09:09:59.381: INFO: Waiting for pod pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc to disappear
    Jan  5 09:09:59.386: INFO: Pod pod-f164cdb6-001c-4fb4-8c69-9bb3765950fc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 09:09:59.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-710" for this suite. 01/05/23 09:09:59.395
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:09:59.404
Jan  5 09:09:59.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename endpointslice 01/05/23 09:09:59.405
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:59.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:59.428
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan  5 09:09:59.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-149" for this suite. 01/05/23 09:09:59.495
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":29,"skipped":421,"failed":0}
------------------------------
â€¢ [0.097 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:09:59.404
    Jan  5 09:09:59.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename endpointslice 01/05/23 09:09:59.405
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:59.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:59.428
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan  5 09:09:59.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-149" for this suite. 01/05/23 09:09:59.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:09:59.504
Jan  5 09:09:59.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename runtimeclass 01/05/23 09:09:59.506
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:59.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:59.53
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan  5 09:09:59.570: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6801 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  5 09:09:59.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6801" for this suite. 01/05/23 09:09:59.593
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":30,"skipped":435,"failed":0}
------------------------------
â€¢ [0.095 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:09:59.504
    Jan  5 09:09:59.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 09:09:59.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:59.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:59.53
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan  5 09:09:59.570: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6801 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  5 09:09:59.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6801" for this suite. 01/05/23 09:09:59.593
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:09:59.599
Jan  5 09:09:59.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename deployment 01/05/23 09:09:59.6
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:59.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:59.621
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan  5 09:09:59.635: INFO: Creating deployment "test-recreate-deployment"
Jan  5 09:09:59.641: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan  5 09:09:59.650: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan  5 09:10:01.661: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan  5 09:10:01.666: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan  5 09:10:01.678: INFO: Updating deployment test-recreate-deployment
Jan  5 09:10:01.678: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 09:10:01.766: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4198  a405eade-7550-451d-a651-b1f0b073da0e 7117 2 2023-01-05 09:09:59 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e7f8c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-05 09:10:01 +0000 UTC,LastTransitionTime:2023-01-05 09:10:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-05 09:10:01 +0000 UTC,LastTransitionTime:2023-01-05 09:09:59 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan  5 09:10:01.772: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-4198  5eb0cefb-42f1-48e1-96a6-b943d40e1805 7116 1 2023-01-05 09:10:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment a405eade-7550-451d-a651-b1f0b073da0e 0xc000f2e2f0 0xc000f2e2f1}] [] [{kube-controller-manager Update apps/v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a405eade-7550-451d-a651-b1f0b073da0e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000f2e398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 09:10:01.772: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan  5 09:10:01.773: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-4198  b31555cf-8df8-49a9-8472-fa406dff76bb 7106 2 2023-01-05 09:09:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment a405eade-7550-451d-a651-b1f0b073da0e 0xc000f2e1e7 0xc000f2e1e8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a405eade-7550-451d-a651-b1f0b073da0e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000f2e298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 09:10:01.777: INFO: Pod "test-recreate-deployment-9d58999df-7drq8" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-7drq8 test-recreate-deployment-9d58999df- deployment-4198  f070987a-e3e8-4ebe-ae91-9cef664985b6 7115 0 2023-01-05 09:10:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 5eb0cefb-42f1-48e1-96a6-b943d40e1805 0xc001e7fc40 0xc001e7fc41}] [] [{kube-controller-manager Update v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5eb0cefb-42f1-48e1-96a6-b943d40e1805\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjpcj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjpcj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:10:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:10:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:10:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:10:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:,StartTime:2023-01-05 09:10:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 09:10:01.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4198" for this suite. 01/05/23 09:10:01.787
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":31,"skipped":437,"failed":0}
------------------------------
â€¢ [2.196 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:09:59.599
    Jan  5 09:09:59.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename deployment 01/05/23 09:09:59.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:09:59.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:09:59.621
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan  5 09:09:59.635: INFO: Creating deployment "test-recreate-deployment"
    Jan  5 09:09:59.641: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan  5 09:09:59.650: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan  5 09:10:01.661: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan  5 09:10:01.666: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan  5 09:10:01.678: INFO: Updating deployment test-recreate-deployment
    Jan  5 09:10:01.678: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 09:10:01.766: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4198  a405eade-7550-451d-a651-b1f0b073da0e 7117 2 2023-01-05 09:09:59 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e7f8c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-05 09:10:01 +0000 UTC,LastTransitionTime:2023-01-05 09:10:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-05 09:10:01 +0000 UTC,LastTransitionTime:2023-01-05 09:09:59 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan  5 09:10:01.772: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-4198  5eb0cefb-42f1-48e1-96a6-b943d40e1805 7116 1 2023-01-05 09:10:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment a405eade-7550-451d-a651-b1f0b073da0e 0xc000f2e2f0 0xc000f2e2f1}] [] [{kube-controller-manager Update apps/v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a405eade-7550-451d-a651-b1f0b073da0e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000f2e398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 09:10:01.772: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan  5 09:10:01.773: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-4198  b31555cf-8df8-49a9-8472-fa406dff76bb 7106 2 2023-01-05 09:09:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment a405eade-7550-451d-a651-b1f0b073da0e 0xc000f2e1e7 0xc000f2e1e8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a405eade-7550-451d-a651-b1f0b073da0e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000f2e298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 09:10:01.777: INFO: Pod "test-recreate-deployment-9d58999df-7drq8" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-7drq8 test-recreate-deployment-9d58999df- deployment-4198  f070987a-e3e8-4ebe-ae91-9cef664985b6 7115 0 2023-01-05 09:10:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 5eb0cefb-42f1-48e1-96a6-b943d40e1805 0xc001e7fc40 0xc001e7fc41}] [] [{kube-controller-manager Update v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5eb0cefb-42f1-48e1-96a6-b943d40e1805\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 09:10:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjpcj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjpcj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:10:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:10:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:10:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:10:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:,StartTime:2023-01-05 09:10:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 09:10:01.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4198" for this suite. 01/05/23 09:10:01.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:01.798
Jan  5 09:10:01.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubelet-test 01/05/23 09:10:01.799
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:01.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:01.838
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan  5 09:10:01.857: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs85e746a6-1e97-4844-a8c3-1810fad2eab9" in namespace "kubelet-test-7597" to be "running and ready"
Jan  5 09:10:01.862: INFO: Pod "busybox-readonly-fs85e746a6-1e97-4844-a8c3-1810fad2eab9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.177462ms
Jan  5 09:10:01.862: INFO: The phase of Pod busybox-readonly-fs85e746a6-1e97-4844-a8c3-1810fad2eab9 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:10:03.870: INFO: Pod "busybox-readonly-fs85e746a6-1e97-4844-a8c3-1810fad2eab9": Phase="Running", Reason="", readiness=true. Elapsed: 2.013195311s
Jan  5 09:10:03.870: INFO: The phase of Pod busybox-readonly-fs85e746a6-1e97-4844-a8c3-1810fad2eab9 is Running (Ready = true)
Jan  5 09:10:03.870: INFO: Pod "busybox-readonly-fs85e746a6-1e97-4844-a8c3-1810fad2eab9" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  5 09:10:03.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7597" for this suite. 01/05/23 09:10:03.908
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":32,"skipped":472,"failed":0}
------------------------------
â€¢ [2.118 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:01.798
    Jan  5 09:10:01.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 09:10:01.799
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:01.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:01.838
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan  5 09:10:01.857: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs85e746a6-1e97-4844-a8c3-1810fad2eab9" in namespace "kubelet-test-7597" to be "running and ready"
    Jan  5 09:10:01.862: INFO: Pod "busybox-readonly-fs85e746a6-1e97-4844-a8c3-1810fad2eab9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.177462ms
    Jan  5 09:10:01.862: INFO: The phase of Pod busybox-readonly-fs85e746a6-1e97-4844-a8c3-1810fad2eab9 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:10:03.870: INFO: Pod "busybox-readonly-fs85e746a6-1e97-4844-a8c3-1810fad2eab9": Phase="Running", Reason="", readiness=true. Elapsed: 2.013195311s
    Jan  5 09:10:03.870: INFO: The phase of Pod busybox-readonly-fs85e746a6-1e97-4844-a8c3-1810fad2eab9 is Running (Ready = true)
    Jan  5 09:10:03.870: INFO: Pod "busybox-readonly-fs85e746a6-1e97-4844-a8c3-1810fad2eab9" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  5 09:10:03.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7597" for this suite. 01/05/23 09:10:03.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:03.917
Jan  5 09:10:03.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 09:10:03.918
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:03.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:03.947
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-3b72316f-78d1-441d-99d4-1d22257642a9 01/05/23 09:10:03.964
STEP: Creating the pod 01/05/23 09:10:03.97
Jan  5 09:10:03.982: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b0ee686-b742-4805-9b5d-4b7bb0cafdff" in namespace "configmap-1255" to be "running"
Jan  5 09:10:03.989: INFO: Pod "pod-configmaps-8b0ee686-b742-4805-9b5d-4b7bb0cafdff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.501455ms
Jan  5 09:10:05.995: INFO: Pod "pod-configmaps-8b0ee686-b742-4805-9b5d-4b7bb0cafdff": Phase="Running", Reason="", readiness=false. Elapsed: 2.012305637s
Jan  5 09:10:05.995: INFO: Pod "pod-configmaps-8b0ee686-b742-4805-9b5d-4b7bb0cafdff" satisfied condition "running"
STEP: Waiting for pod with text data 01/05/23 09:10:05.995
STEP: Waiting for pod with binary data 01/05/23 09:10:06.007
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 09:10:06.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1255" for this suite. 01/05/23 09:10:06.105
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":33,"skipped":483,"failed":0}
------------------------------
â€¢ [2.195 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:03.917
    Jan  5 09:10:03.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 09:10:03.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:03.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:03.947
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-3b72316f-78d1-441d-99d4-1d22257642a9 01/05/23 09:10:03.964
    STEP: Creating the pod 01/05/23 09:10:03.97
    Jan  5 09:10:03.982: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b0ee686-b742-4805-9b5d-4b7bb0cafdff" in namespace "configmap-1255" to be "running"
    Jan  5 09:10:03.989: INFO: Pod "pod-configmaps-8b0ee686-b742-4805-9b5d-4b7bb0cafdff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.501455ms
    Jan  5 09:10:05.995: INFO: Pod "pod-configmaps-8b0ee686-b742-4805-9b5d-4b7bb0cafdff": Phase="Running", Reason="", readiness=false. Elapsed: 2.012305637s
    Jan  5 09:10:05.995: INFO: Pod "pod-configmaps-8b0ee686-b742-4805-9b5d-4b7bb0cafdff" satisfied condition "running"
    STEP: Waiting for pod with text data 01/05/23 09:10:05.995
    STEP: Waiting for pod with binary data 01/05/23 09:10:06.007
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 09:10:06.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1255" for this suite. 01/05/23 09:10:06.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:06.113
Jan  5 09:10:06.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 09:10:06.113
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:06.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:06.136
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-a2f28768-5a7d-4acf-b6bb-8206a2153348 01/05/23 09:10:06.144
STEP: Creating a pod to test consume secrets 01/05/23 09:10:06.149
Jan  5 09:10:06.159: INFO: Waiting up to 5m0s for pod "pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87" in namespace "secrets-8669" to be "Succeeded or Failed"
Jan  5 09:10:06.169: INFO: Pod "pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87": Phase="Pending", Reason="", readiness=false. Elapsed: 9.949349ms
Jan  5 09:10:08.176: INFO: Pod "pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87": Phase="Running", Reason="", readiness=false. Elapsed: 2.016776924s
Jan  5 09:10:10.175: INFO: Pod "pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016572036s
STEP: Saw pod success 01/05/23 09:10:10.175
Jan  5 09:10:10.176: INFO: Pod "pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87" satisfied condition "Succeeded or Failed"
Jan  5 09:10:10.180: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t pod pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 09:10:10.197
Jan  5 09:10:10.209: INFO: Waiting for pod pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87 to disappear
Jan  5 09:10:10.215: INFO: Pod pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 09:10:10.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8669" for this suite. 01/05/23 09:10:10.224
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":34,"skipped":525,"failed":0}
------------------------------
â€¢ [4.117 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:06.113
    Jan  5 09:10:06.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 09:10:06.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:06.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:06.136
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-a2f28768-5a7d-4acf-b6bb-8206a2153348 01/05/23 09:10:06.144
    STEP: Creating a pod to test consume secrets 01/05/23 09:10:06.149
    Jan  5 09:10:06.159: INFO: Waiting up to 5m0s for pod "pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87" in namespace "secrets-8669" to be "Succeeded or Failed"
    Jan  5 09:10:06.169: INFO: Pod "pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87": Phase="Pending", Reason="", readiness=false. Elapsed: 9.949349ms
    Jan  5 09:10:08.176: INFO: Pod "pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87": Phase="Running", Reason="", readiness=false. Elapsed: 2.016776924s
    Jan  5 09:10:10.175: INFO: Pod "pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016572036s
    STEP: Saw pod success 01/05/23 09:10:10.175
    Jan  5 09:10:10.176: INFO: Pod "pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87" satisfied condition "Succeeded or Failed"
    Jan  5 09:10:10.180: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t pod pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 09:10:10.197
    Jan  5 09:10:10.209: INFO: Waiting for pod pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87 to disappear
    Jan  5 09:10:10.215: INFO: Pod pod-secrets-caad3a2f-7fa1-46f6-b5cf-f528fab45d87 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 09:10:10.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8669" for this suite. 01/05/23 09:10:10.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:10.231
Jan  5 09:10:10.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename resourcequota 01/05/23 09:10:10.231
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:10.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:10.259
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 01/05/23 09:10:10.266
STEP: Creating a ResourceQuota 01/05/23 09:10:15.273
STEP: Ensuring resource quota status is calculated 01/05/23 09:10:15.278
STEP: Creating a Service 01/05/23 09:10:17.295
STEP: Creating a NodePort Service 01/05/23 09:10:17.309
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/05/23 09:10:17.329
STEP: Ensuring resource quota status captures service creation 01/05/23 09:10:17.353
STEP: Deleting Services 01/05/23 09:10:19.36
STEP: Ensuring resource quota status released usage 01/05/23 09:10:19.388
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 09:10:21.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9670" for this suite. 01/05/23 09:10:21.403
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":35,"skipped":534,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.179 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:10.231
    Jan  5 09:10:10.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename resourcequota 01/05/23 09:10:10.231
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:10.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:10.259
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 01/05/23 09:10:10.266
    STEP: Creating a ResourceQuota 01/05/23 09:10:15.273
    STEP: Ensuring resource quota status is calculated 01/05/23 09:10:15.278
    STEP: Creating a Service 01/05/23 09:10:17.295
    STEP: Creating a NodePort Service 01/05/23 09:10:17.309
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/05/23 09:10:17.329
    STEP: Ensuring resource quota status captures service creation 01/05/23 09:10:17.353
    STEP: Deleting Services 01/05/23 09:10:19.36
    STEP: Ensuring resource quota status released usage 01/05/23 09:10:19.388
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 09:10:21.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9670" for this suite. 01/05/23 09:10:21.403
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:21.41
Jan  5 09:10:21.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename resourcequota 01/05/23 09:10:21.411
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:21.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:21.433
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 01/05/23 09:10:21.44
STEP: Getting a ResourceQuota 01/05/23 09:10:21.446
STEP: Listing all ResourceQuotas with LabelSelector 01/05/23 09:10:21.45
STEP: Patching the ResourceQuota 01/05/23 09:10:21.455
STEP: Deleting a Collection of ResourceQuotas 01/05/23 09:10:21.46
STEP: Verifying the deleted ResourceQuota 01/05/23 09:10:21.47
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 09:10:21.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6837" for this suite. 01/05/23 09:10:21.483
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":36,"skipped":535,"failed":0}
------------------------------
â€¢ [0.080 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:21.41
    Jan  5 09:10:21.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename resourcequota 01/05/23 09:10:21.411
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:21.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:21.433
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 01/05/23 09:10:21.44
    STEP: Getting a ResourceQuota 01/05/23 09:10:21.446
    STEP: Listing all ResourceQuotas with LabelSelector 01/05/23 09:10:21.45
    STEP: Patching the ResourceQuota 01/05/23 09:10:21.455
    STEP: Deleting a Collection of ResourceQuotas 01/05/23 09:10:21.46
    STEP: Verifying the deleted ResourceQuota 01/05/23 09:10:21.47
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 09:10:21.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6837" for this suite. 01/05/23 09:10:21.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:21.492
Jan  5 09:10:21.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubelet-test 01/05/23 09:10:21.493
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:21.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:21.516
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  5 09:10:25.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3641" for this suite. 01/05/23 09:10:25.565
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":37,"skipped":574,"failed":0}
------------------------------
â€¢ [4.080 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:21.492
    Jan  5 09:10:21.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 09:10:21.493
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:21.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:21.516
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  5 09:10:25.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3641" for this suite. 01/05/23 09:10:25.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:25.575
Jan  5 09:10:25.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename events 01/05/23 09:10:25.576
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:25.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:25.6
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/05/23 09:10:25.607
STEP: listing all events in all namespaces 01/05/23 09:10:25.612
STEP: patching the test event 01/05/23 09:10:25.625
STEP: fetching the test event 01/05/23 09:10:25.631
STEP: updating the test event 01/05/23 09:10:25.636
STEP: getting the test event 01/05/23 09:10:25.647
STEP: deleting the test event 01/05/23 09:10:25.652
STEP: listing all events in all namespaces 01/05/23 09:10:25.658
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan  5 09:10:25.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5056" for this suite. 01/05/23 09:10:25.679
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":38,"skipped":609,"failed":0}
------------------------------
â€¢ [0.110 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:25.575
    Jan  5 09:10:25.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename events 01/05/23 09:10:25.576
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:25.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:25.6
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/05/23 09:10:25.607
    STEP: listing all events in all namespaces 01/05/23 09:10:25.612
    STEP: patching the test event 01/05/23 09:10:25.625
    STEP: fetching the test event 01/05/23 09:10:25.631
    STEP: updating the test event 01/05/23 09:10:25.636
    STEP: getting the test event 01/05/23 09:10:25.647
    STEP: deleting the test event 01/05/23 09:10:25.652
    STEP: listing all events in all namespaces 01/05/23 09:10:25.658
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan  5 09:10:25.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5056" for this suite. 01/05/23 09:10:25.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:25.686
Jan  5 09:10:25.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:10:25.687
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:25.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:25.717
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 01/05/23 09:10:25.727
Jan  5 09:10:25.741: INFO: Waiting up to 5m0s for pod "downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d" in namespace "downward-api-5003" to be "Succeeded or Failed"
Jan  5 09:10:25.757: INFO: Pod "downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.495066ms
Jan  5 09:10:27.763: INFO: Pod "downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021622342s
Jan  5 09:10:29.763: INFO: Pod "downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022084447s
STEP: Saw pod success 01/05/23 09:10:29.763
Jan  5 09:10:29.763: INFO: Pod "downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d" satisfied condition "Succeeded or Failed"
Jan  5 09:10:29.767: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d container dapi-container: <nil>
STEP: delete the pod 01/05/23 09:10:29.778
Jan  5 09:10:29.789: INFO: Waiting for pod downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d to disappear
Jan  5 09:10:29.793: INFO: Pod downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  5 09:10:29.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5003" for this suite. 01/05/23 09:10:29.801
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":39,"skipped":631,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:25.686
    Jan  5 09:10:25.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:10:25.687
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:25.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:25.717
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 01/05/23 09:10:25.727
    Jan  5 09:10:25.741: INFO: Waiting up to 5m0s for pod "downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d" in namespace "downward-api-5003" to be "Succeeded or Failed"
    Jan  5 09:10:25.757: INFO: Pod "downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.495066ms
    Jan  5 09:10:27.763: INFO: Pod "downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021622342s
    Jan  5 09:10:29.763: INFO: Pod "downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022084447s
    STEP: Saw pod success 01/05/23 09:10:29.763
    Jan  5 09:10:29.763: INFO: Pod "downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d" satisfied condition "Succeeded or Failed"
    Jan  5 09:10:29.767: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d container dapi-container: <nil>
    STEP: delete the pod 01/05/23 09:10:29.778
    Jan  5 09:10:29.789: INFO: Waiting for pod downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d to disappear
    Jan  5 09:10:29.793: INFO: Pod downward-api-2a73032b-e651-4d6a-a7bd-aec807d29b6d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  5 09:10:29.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5003" for this suite. 01/05/23 09:10:29.801
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:29.806
Jan  5 09:10:29.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename proxy 01/05/23 09:10:29.807
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:29.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:29.83
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/05/23 09:10:29.85
STEP: creating replication controller proxy-service-6dcrq in namespace proxy-4961 01/05/23 09:10:29.85
I0105 09:10:29.858967      22 runners.go:193] Created replication controller with name: proxy-service-6dcrq, namespace: proxy-4961, replica count: 1
I0105 09:10:30.909462      22 runners.go:193] proxy-service-6dcrq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0105 09:10:31.909627      22 runners.go:193] proxy-service-6dcrq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0105 09:10:32.909935      22 runners.go:193] proxy-service-6dcrq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 09:10:32.915: INFO: setup took 3.077204839s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/05/23 09:10:32.915
Jan  5 09:10:32.974: INFO: (0) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 58.654428ms)
Jan  5 09:10:32.974: INFO: (0) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 59.078336ms)
Jan  5 09:10:32.974: INFO: (0) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 59.134111ms)
Jan  5 09:10:32.979: INFO: (0) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 64.354337ms)
Jan  5 09:10:32.980: INFO: (0) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 64.799996ms)
Jan  5 09:10:32.980: INFO: (0) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 64.562634ms)
Jan  5 09:10:32.980: INFO: (0) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 64.783195ms)
Jan  5 09:10:32.980: INFO: (0) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 64.981874ms)
Jan  5 09:10:32.980: INFO: (0) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 65.019154ms)
Jan  5 09:10:32.981: INFO: (0) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 65.552968ms)
Jan  5 09:10:32.981: INFO: (0) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 65.406324ms)
Jan  5 09:10:32.984: INFO: (0) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 68.656964ms)
Jan  5 09:10:32.984: INFO: (0) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 68.4882ms)
Jan  5 09:10:32.984: INFO: (0) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 68.630586ms)
Jan  5 09:10:32.992: INFO: (0) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 76.613855ms)
Jan  5 09:10:32.992: INFO: (0) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 76.612962ms)
Jan  5 09:10:33.004: INFO: (1) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 12.245741ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 28.024379ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 28.039408ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 28.185009ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 28.070185ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 28.131349ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 28.146647ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 28.103697ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 28.138632ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 28.291316ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 28.21829ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 28.333785ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 28.188235ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 28.236435ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 28.367609ms)
Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 28.251613ms)
Jan  5 09:10:33.036: INFO: (2) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 15.16761ms)
Jan  5 09:10:33.036: INFO: (2) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 15.200551ms)
Jan  5 09:10:33.036: INFO: (2) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 15.368204ms)
Jan  5 09:10:33.036: INFO: (2) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 15.276904ms)
Jan  5 09:10:33.036: INFO: (2) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 15.363817ms)
Jan  5 09:10:33.041: INFO: (2) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 20.61559ms)
Jan  5 09:10:33.041: INFO: (2) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 20.650004ms)
Jan  5 09:10:33.041: INFO: (2) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 20.811004ms)
Jan  5 09:10:33.041: INFO: (2) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 20.832584ms)
Jan  5 09:10:33.041: INFO: (2) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 20.879511ms)
Jan  5 09:10:33.042: INFO: (2) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 21.130618ms)
Jan  5 09:10:33.042: INFO: (2) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 21.192695ms)
Jan  5 09:10:33.046: INFO: (2) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 25.93939ms)
Jan  5 09:10:33.051: INFO: (2) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 30.100785ms)
Jan  5 09:10:33.051: INFO: (2) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 30.296249ms)
Jan  5 09:10:33.051: INFO: (2) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 30.198868ms)
Jan  5 09:10:33.063: INFO: (3) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 12.41147ms)
Jan  5 09:10:33.063: INFO: (3) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 12.583881ms)
Jan  5 09:10:33.064: INFO: (3) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 12.730995ms)
Jan  5 09:10:33.071: INFO: (3) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 20.006296ms)
Jan  5 09:10:33.071: INFO: (3) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 20.370625ms)
Jan  5 09:10:33.071: INFO: (3) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 20.435956ms)
Jan  5 09:10:33.071: INFO: (3) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 20.514773ms)
Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 20.74951ms)
Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 20.779966ms)
Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 20.7927ms)
Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 21.047052ms)
Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 20.871285ms)
Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 21.012659ms)
Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 20.995367ms)
Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 21.038246ms)
Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 21.208724ms)
Jan  5 09:10:33.092: INFO: (4) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 19.58902ms)
Jan  5 09:10:33.092: INFO: (4) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 19.858642ms)
Jan  5 09:10:33.092: INFO: (4) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 19.94943ms)
Jan  5 09:10:33.092: INFO: (4) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 20.039958ms)
Jan  5 09:10:33.092: INFO: (4) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 20.136999ms)
Jan  5 09:10:33.093: INFO: (4) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 20.219131ms)
Jan  5 09:10:33.093: INFO: (4) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 20.211348ms)
Jan  5 09:10:33.093: INFO: (4) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 20.721006ms)
Jan  5 09:10:33.093: INFO: (4) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 20.966162ms)
Jan  5 09:10:33.093: INFO: (4) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 20.968958ms)
Jan  5 09:10:33.094: INFO: (4) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 21.500417ms)
Jan  5 09:10:33.095: INFO: (4) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 22.592229ms)
Jan  5 09:10:33.098: INFO: (4) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 25.381761ms)
Jan  5 09:10:33.100: INFO: (4) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 27.824426ms)
Jan  5 09:10:33.101: INFO: (4) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 28.150514ms)
Jan  5 09:10:33.101: INFO: (4) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 28.720095ms)
Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 27.61123ms)
Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 27.65943ms)
Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 27.68635ms)
Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 27.713921ms)
Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 27.613124ms)
Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 28.160473ms)
Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 28.251543ms)
Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 28.05123ms)
Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 28.189677ms)
Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 28.397434ms)
Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 28.04118ms)
Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 28.18585ms)
Jan  5 09:10:33.138: INFO: (5) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 36.671925ms)
Jan  5 09:10:33.138: INFO: (5) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 36.583149ms)
Jan  5 09:10:33.138: INFO: (5) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 36.513619ms)
Jan  5 09:10:33.138: INFO: (5) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 36.666564ms)
Jan  5 09:10:33.152: INFO: (6) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 14.246396ms)
Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 15.64572ms)
Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 16.034985ms)
Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 15.739615ms)
Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 16.103833ms)
Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 15.74774ms)
Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 15.884094ms)
Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 15.868646ms)
Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 15.863395ms)
Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 15.970776ms)
Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 16.123218ms)
Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 16.0537ms)
Jan  5 09:10:33.158: INFO: (6) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 20.05144ms)
Jan  5 09:10:33.161: INFO: (6) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 22.261776ms)
Jan  5 09:10:33.161: INFO: (6) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 22.525817ms)
Jan  5 09:10:33.161: INFO: (6) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 22.60839ms)
Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 22.110914ms)
Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 22.000197ms)
Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 21.896966ms)
Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 22.029252ms)
Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 22.102197ms)
Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 21.970942ms)
Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 22.238561ms)
Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 22.363423ms)
Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 22.006048ms)
Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 21.965092ms)
Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 21.920389ms)
Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 22.153543ms)
Jan  5 09:10:33.186: INFO: (7) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 25.003787ms)
Jan  5 09:10:33.187: INFO: (7) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 25.932526ms)
Jan  5 09:10:33.187: INFO: (7) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 25.839754ms)
Jan  5 09:10:33.187: INFO: (7) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 26.048211ms)
Jan  5 09:10:33.199: INFO: (8) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 11.600541ms)
Jan  5 09:10:33.200: INFO: (8) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 12.139054ms)
Jan  5 09:10:33.200: INFO: (8) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 13.0607ms)
Jan  5 09:10:33.201: INFO: (8) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 13.288904ms)
Jan  5 09:10:33.204: INFO: (8) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 16.736621ms)
Jan  5 09:10:33.204: INFO: (8) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 17.018636ms)
Jan  5 09:10:33.204: INFO: (8) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 17.184565ms)
Jan  5 09:10:33.205: INFO: (8) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 17.350453ms)
Jan  5 09:10:33.205: INFO: (8) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 17.106298ms)
Jan  5 09:10:33.205: INFO: (8) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 17.105707ms)
Jan  5 09:10:33.205: INFO: (8) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 17.196116ms)
Jan  5 09:10:33.205: INFO: (8) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 17.162012ms)
Jan  5 09:10:33.206: INFO: (8) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 18.206407ms)
Jan  5 09:10:33.209: INFO: (8) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 21.819732ms)
Jan  5 09:10:33.212: INFO: (8) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 24.343368ms)
Jan  5 09:10:33.212: INFO: (8) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 24.383844ms)
Jan  5 09:10:33.229: INFO: (9) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 17.342168ms)
Jan  5 09:10:33.229: INFO: (9) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 17.280403ms)
Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 17.33278ms)
Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 17.211885ms)
Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 17.258713ms)
Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 17.335725ms)
Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 17.463914ms)
Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 17.278188ms)
Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 17.414021ms)
Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 17.876882ms)
Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 17.83238ms)
Jan  5 09:10:33.231: INFO: (9) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 18.71923ms)
Jan  5 09:10:33.236: INFO: (9) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 23.876468ms)
Jan  5 09:10:33.236: INFO: (9) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 24.016028ms)
Jan  5 09:10:33.236: INFO: (9) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 24.056945ms)
Jan  5 09:10:33.236: INFO: (9) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 24.127486ms)
Jan  5 09:10:33.251: INFO: (10) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 14.737661ms)
Jan  5 09:10:33.251: INFO: (10) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 14.729626ms)
Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 14.922263ms)
Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 15.030115ms)
Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 15.098762ms)
Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 15.053429ms)
Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 15.127736ms)
Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 15.299146ms)
Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 15.205119ms)
Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 15.348356ms)
Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 15.271765ms)
Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 15.588865ms)
Jan  5 09:10:33.259: INFO: (10) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 22.052184ms)
Jan  5 09:10:33.259: INFO: (10) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 22.111695ms)
Jan  5 09:10:33.259: INFO: (10) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 22.219477ms)
Jan  5 09:10:33.259: INFO: (10) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 22.430859ms)
Jan  5 09:10:33.279: INFO: (11) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 19.972063ms)
Jan  5 09:10:33.279: INFO: (11) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 20.0884ms)
Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 20.366977ms)
Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 20.286738ms)
Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 20.249508ms)
Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 20.409256ms)
Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 20.490597ms)
Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 20.580805ms)
Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 20.524921ms)
Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 20.549647ms)
Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 20.74446ms)
Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 20.77101ms)
Jan  5 09:10:33.283: INFO: (11) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 24.096678ms)
Jan  5 09:10:33.284: INFO: (11) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 24.412036ms)
Jan  5 09:10:33.284: INFO: (11) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 24.505741ms)
Jan  5 09:10:33.284: INFO: (11) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 24.539133ms)
Jan  5 09:10:33.295: INFO: (12) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 10.649501ms)
Jan  5 09:10:33.295: INFO: (12) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 10.672965ms)
Jan  5 09:10:33.296: INFO: (12) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 12.307807ms)
Jan  5 09:10:33.297: INFO: (12) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 12.535602ms)
Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 16.469634ms)
Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 16.474554ms)
Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 16.550004ms)
Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 16.536118ms)
Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 16.465195ms)
Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 16.492145ms)
Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 16.722184ms)
Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 16.719539ms)
Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 16.600687ms)
Jan  5 09:10:33.303: INFO: (12) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 19.414134ms)
Jan  5 09:10:33.309: INFO: (12) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 24.624131ms)
Jan  5 09:10:33.309: INFO: (12) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 24.950428ms)
Jan  5 09:10:33.325: INFO: (13) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 16.305898ms)
Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 16.488889ms)
Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 16.944197ms)
Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 16.975425ms)
Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 16.880688ms)
Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 17.181378ms)
Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 17.368657ms)
Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 17.486657ms)
Jan  5 09:10:33.328: INFO: (13) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 18.285364ms)
Jan  5 09:10:33.328: INFO: (13) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 18.386801ms)
Jan  5 09:10:33.328: INFO: (13) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 18.545688ms)
Jan  5 09:10:33.328: INFO: (13) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 18.741101ms)
Jan  5 09:10:33.335: INFO: (13) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 25.798828ms)
Jan  5 09:10:33.335: INFO: (13) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 25.639902ms)
Jan  5 09:10:33.335: INFO: (13) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 25.434299ms)
Jan  5 09:10:33.335: INFO: (13) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 26.243295ms)
Jan  5 09:10:33.346: INFO: (14) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 10.141736ms)
Jan  5 09:10:33.346: INFO: (14) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 10.59546ms)
Jan  5 09:10:33.347: INFO: (14) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 10.827402ms)
Jan  5 09:10:33.347: INFO: (14) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 11.606112ms)
Jan  5 09:10:33.347: INFO: (14) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 11.807857ms)
Jan  5 09:10:33.348: INFO: (14) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 11.893106ms)
Jan  5 09:10:33.350: INFO: (14) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 13.989598ms)
Jan  5 09:10:33.356: INFO: (14) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 20.311325ms)
Jan  5 09:10:33.356: INFO: (14) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 20.269186ms)
Jan  5 09:10:33.356: INFO: (14) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 20.372819ms)
Jan  5 09:10:33.356: INFO: (14) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 20.431959ms)
Jan  5 09:10:33.356: INFO: (14) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 20.339377ms)
Jan  5 09:10:33.356: INFO: (14) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 20.686323ms)
Jan  5 09:10:33.357: INFO: (14) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 21.183006ms)
Jan  5 09:10:33.357: INFO: (14) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 21.206861ms)
Jan  5 09:10:33.361: INFO: (14) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 25.363909ms)
Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 14.432942ms)
Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 14.87764ms)
Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 14.831124ms)
Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 14.962639ms)
Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 14.873132ms)
Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 14.886287ms)
Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 14.987615ms)
Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 15.26352ms)
Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 15.047506ms)
Jan  5 09:10:33.377: INFO: (15) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 15.17833ms)
Jan  5 09:10:33.377: INFO: (15) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 15.501021ms)
Jan  5 09:10:33.377: INFO: (15) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 15.541105ms)
Jan  5 09:10:33.382: INFO: (15) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 20.601063ms)
Jan  5 09:10:33.382: INFO: (15) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 20.786869ms)
Jan  5 09:10:33.382: INFO: (15) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 20.754699ms)
Jan  5 09:10:33.382: INFO: (15) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 20.996921ms)
Jan  5 09:10:33.401: INFO: (16) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 18.834846ms)
Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 19.271628ms)
Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 19.289913ms)
Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 19.00911ms)
Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 19.001416ms)
Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 19.156204ms)
Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 19.058382ms)
Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 19.018047ms)
Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 19.117141ms)
Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 19.164319ms)
Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 19.25546ms)
Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 19.277639ms)
Jan  5 09:10:33.407: INFO: (16) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 24.185414ms)
Jan  5 09:10:33.407: INFO: (16) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 24.603833ms)
Jan  5 09:10:33.407: INFO: (16) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 24.52068ms)
Jan  5 09:10:33.407: INFO: (16) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 24.80577ms)
Jan  5 09:10:33.425: INFO: (17) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 17.98283ms)
Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 18.165901ms)
Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 18.295342ms)
Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 18.260376ms)
Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 18.223417ms)
Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 18.455841ms)
Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 18.639172ms)
Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 18.903484ms)
Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 18.945011ms)
Jan  5 09:10:33.427: INFO: (17) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 19.103495ms)
Jan  5 09:10:33.427: INFO: (17) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 18.927999ms)
Jan  5 09:10:33.428: INFO: (17) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 20.054105ms)
Jan  5 09:10:33.434: INFO: (17) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 26.074129ms)
Jan  5 09:10:33.434: INFO: (17) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 26.422287ms)
Jan  5 09:10:33.434: INFO: (17) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 26.468554ms)
Jan  5 09:10:33.434: INFO: (17) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 26.482951ms)
Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 18.318134ms)
Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 18.492139ms)
Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 18.347939ms)
Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 18.471078ms)
Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 18.536641ms)
Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 18.78895ms)
Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 18.648449ms)
Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 18.836079ms)
Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 18.456742ms)
Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 18.704533ms)
Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 18.736022ms)
Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 19.034436ms)
Jan  5 09:10:33.460: INFO: (18) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 26.131478ms)
Jan  5 09:10:33.461: INFO: (18) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 26.367426ms)
Jan  5 09:10:33.461: INFO: (18) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 26.563282ms)
Jan  5 09:10:33.461: INFO: (18) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 26.813456ms)
Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 18.818926ms)
Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 18.780725ms)
Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 18.808658ms)
Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 18.979775ms)
Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 18.879369ms)
Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 19.047462ms)
Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 18.943177ms)
Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 19.126178ms)
Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 19.158649ms)
Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 18.99304ms)
Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 19.258624ms)
Jan  5 09:10:33.481: INFO: (19) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 19.255047ms)
Jan  5 09:10:33.492: INFO: (19) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 30.385555ms)
Jan  5 09:10:33.492: INFO: (19) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 30.450404ms)
Jan  5 09:10:33.492: INFO: (19) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 30.401093ms)
Jan  5 09:10:33.492: INFO: (19) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 30.401003ms)
STEP: deleting ReplicationController proxy-service-6dcrq in namespace proxy-4961, will wait for the garbage collector to delete the pods 01/05/23 09:10:33.492
Jan  5 09:10:33.557: INFO: Deleting ReplicationController proxy-service-6dcrq took: 7.851173ms
Jan  5 09:10:33.657: INFO: Terminating ReplicationController proxy-service-6dcrq pods took: 100.655413ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan  5 09:10:35.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4961" for this suite. 01/05/23 09:10:35.874
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":40,"skipped":635,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.076 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:29.806
    Jan  5 09:10:29.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename proxy 01/05/23 09:10:29.807
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:29.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:29.83
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/05/23 09:10:29.85
    STEP: creating replication controller proxy-service-6dcrq in namespace proxy-4961 01/05/23 09:10:29.85
    I0105 09:10:29.858967      22 runners.go:193] Created replication controller with name: proxy-service-6dcrq, namespace: proxy-4961, replica count: 1
    I0105 09:10:30.909462      22 runners.go:193] proxy-service-6dcrq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0105 09:10:31.909627      22 runners.go:193] proxy-service-6dcrq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0105 09:10:32.909935      22 runners.go:193] proxy-service-6dcrq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 09:10:32.915: INFO: setup took 3.077204839s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/05/23 09:10:32.915
    Jan  5 09:10:32.974: INFO: (0) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 58.654428ms)
    Jan  5 09:10:32.974: INFO: (0) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 59.078336ms)
    Jan  5 09:10:32.974: INFO: (0) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 59.134111ms)
    Jan  5 09:10:32.979: INFO: (0) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 64.354337ms)
    Jan  5 09:10:32.980: INFO: (0) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 64.799996ms)
    Jan  5 09:10:32.980: INFO: (0) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 64.562634ms)
    Jan  5 09:10:32.980: INFO: (0) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 64.783195ms)
    Jan  5 09:10:32.980: INFO: (0) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 64.981874ms)
    Jan  5 09:10:32.980: INFO: (0) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 65.019154ms)
    Jan  5 09:10:32.981: INFO: (0) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 65.552968ms)
    Jan  5 09:10:32.981: INFO: (0) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 65.406324ms)
    Jan  5 09:10:32.984: INFO: (0) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 68.656964ms)
    Jan  5 09:10:32.984: INFO: (0) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 68.4882ms)
    Jan  5 09:10:32.984: INFO: (0) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 68.630586ms)
    Jan  5 09:10:32.992: INFO: (0) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 76.613855ms)
    Jan  5 09:10:32.992: INFO: (0) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 76.612962ms)
    Jan  5 09:10:33.004: INFO: (1) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 12.245741ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 28.024379ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 28.039408ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 28.185009ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 28.070185ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 28.131349ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 28.146647ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 28.103697ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 28.138632ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 28.291316ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 28.21829ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 28.333785ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 28.188235ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 28.236435ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 28.367609ms)
    Jan  5 09:10:33.020: INFO: (1) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 28.251613ms)
    Jan  5 09:10:33.036: INFO: (2) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 15.16761ms)
    Jan  5 09:10:33.036: INFO: (2) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 15.200551ms)
    Jan  5 09:10:33.036: INFO: (2) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 15.368204ms)
    Jan  5 09:10:33.036: INFO: (2) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 15.276904ms)
    Jan  5 09:10:33.036: INFO: (2) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 15.363817ms)
    Jan  5 09:10:33.041: INFO: (2) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 20.61559ms)
    Jan  5 09:10:33.041: INFO: (2) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 20.650004ms)
    Jan  5 09:10:33.041: INFO: (2) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 20.811004ms)
    Jan  5 09:10:33.041: INFO: (2) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 20.832584ms)
    Jan  5 09:10:33.041: INFO: (2) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 20.879511ms)
    Jan  5 09:10:33.042: INFO: (2) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 21.130618ms)
    Jan  5 09:10:33.042: INFO: (2) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 21.192695ms)
    Jan  5 09:10:33.046: INFO: (2) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 25.93939ms)
    Jan  5 09:10:33.051: INFO: (2) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 30.100785ms)
    Jan  5 09:10:33.051: INFO: (2) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 30.296249ms)
    Jan  5 09:10:33.051: INFO: (2) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 30.198868ms)
    Jan  5 09:10:33.063: INFO: (3) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 12.41147ms)
    Jan  5 09:10:33.063: INFO: (3) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 12.583881ms)
    Jan  5 09:10:33.064: INFO: (3) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 12.730995ms)
    Jan  5 09:10:33.071: INFO: (3) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 20.006296ms)
    Jan  5 09:10:33.071: INFO: (3) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 20.370625ms)
    Jan  5 09:10:33.071: INFO: (3) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 20.435956ms)
    Jan  5 09:10:33.071: INFO: (3) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 20.514773ms)
    Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 20.74951ms)
    Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 20.779966ms)
    Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 20.7927ms)
    Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 21.047052ms)
    Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 20.871285ms)
    Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 21.012659ms)
    Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 20.995367ms)
    Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 21.038246ms)
    Jan  5 09:10:33.072: INFO: (3) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 21.208724ms)
    Jan  5 09:10:33.092: INFO: (4) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 19.58902ms)
    Jan  5 09:10:33.092: INFO: (4) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 19.858642ms)
    Jan  5 09:10:33.092: INFO: (4) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 19.94943ms)
    Jan  5 09:10:33.092: INFO: (4) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 20.039958ms)
    Jan  5 09:10:33.092: INFO: (4) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 20.136999ms)
    Jan  5 09:10:33.093: INFO: (4) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 20.219131ms)
    Jan  5 09:10:33.093: INFO: (4) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 20.211348ms)
    Jan  5 09:10:33.093: INFO: (4) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 20.721006ms)
    Jan  5 09:10:33.093: INFO: (4) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 20.966162ms)
    Jan  5 09:10:33.093: INFO: (4) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 20.968958ms)
    Jan  5 09:10:33.094: INFO: (4) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 21.500417ms)
    Jan  5 09:10:33.095: INFO: (4) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 22.592229ms)
    Jan  5 09:10:33.098: INFO: (4) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 25.381761ms)
    Jan  5 09:10:33.100: INFO: (4) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 27.824426ms)
    Jan  5 09:10:33.101: INFO: (4) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 28.150514ms)
    Jan  5 09:10:33.101: INFO: (4) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 28.720095ms)
    Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 27.61123ms)
    Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 27.65943ms)
    Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 27.68635ms)
    Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 27.713921ms)
    Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 27.613124ms)
    Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 28.160473ms)
    Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 28.251543ms)
    Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 28.05123ms)
    Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 28.189677ms)
    Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 28.397434ms)
    Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 28.04118ms)
    Jan  5 09:10:33.129: INFO: (5) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 28.18585ms)
    Jan  5 09:10:33.138: INFO: (5) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 36.671925ms)
    Jan  5 09:10:33.138: INFO: (5) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 36.583149ms)
    Jan  5 09:10:33.138: INFO: (5) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 36.513619ms)
    Jan  5 09:10:33.138: INFO: (5) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 36.666564ms)
    Jan  5 09:10:33.152: INFO: (6) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 14.246396ms)
    Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 15.64572ms)
    Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 16.034985ms)
    Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 15.739615ms)
    Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 16.103833ms)
    Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 15.74774ms)
    Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 15.884094ms)
    Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 15.868646ms)
    Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 15.863395ms)
    Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 15.970776ms)
    Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 16.123218ms)
    Jan  5 09:10:33.154: INFO: (6) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 16.0537ms)
    Jan  5 09:10:33.158: INFO: (6) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 20.05144ms)
    Jan  5 09:10:33.161: INFO: (6) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 22.261776ms)
    Jan  5 09:10:33.161: INFO: (6) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 22.525817ms)
    Jan  5 09:10:33.161: INFO: (6) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 22.60839ms)
    Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 22.110914ms)
    Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 22.000197ms)
    Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 21.896966ms)
    Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 22.029252ms)
    Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 22.102197ms)
    Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 21.970942ms)
    Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 22.238561ms)
    Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 22.363423ms)
    Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 22.006048ms)
    Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 21.965092ms)
    Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 21.920389ms)
    Jan  5 09:10:33.183: INFO: (7) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 22.153543ms)
    Jan  5 09:10:33.186: INFO: (7) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 25.003787ms)
    Jan  5 09:10:33.187: INFO: (7) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 25.932526ms)
    Jan  5 09:10:33.187: INFO: (7) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 25.839754ms)
    Jan  5 09:10:33.187: INFO: (7) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 26.048211ms)
    Jan  5 09:10:33.199: INFO: (8) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 11.600541ms)
    Jan  5 09:10:33.200: INFO: (8) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 12.139054ms)
    Jan  5 09:10:33.200: INFO: (8) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 13.0607ms)
    Jan  5 09:10:33.201: INFO: (8) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 13.288904ms)
    Jan  5 09:10:33.204: INFO: (8) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 16.736621ms)
    Jan  5 09:10:33.204: INFO: (8) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 17.018636ms)
    Jan  5 09:10:33.204: INFO: (8) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 17.184565ms)
    Jan  5 09:10:33.205: INFO: (8) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 17.350453ms)
    Jan  5 09:10:33.205: INFO: (8) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 17.106298ms)
    Jan  5 09:10:33.205: INFO: (8) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 17.105707ms)
    Jan  5 09:10:33.205: INFO: (8) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 17.196116ms)
    Jan  5 09:10:33.205: INFO: (8) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 17.162012ms)
    Jan  5 09:10:33.206: INFO: (8) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 18.206407ms)
    Jan  5 09:10:33.209: INFO: (8) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 21.819732ms)
    Jan  5 09:10:33.212: INFO: (8) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 24.343368ms)
    Jan  5 09:10:33.212: INFO: (8) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 24.383844ms)
    Jan  5 09:10:33.229: INFO: (9) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 17.342168ms)
    Jan  5 09:10:33.229: INFO: (9) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 17.280403ms)
    Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 17.33278ms)
    Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 17.211885ms)
    Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 17.258713ms)
    Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 17.335725ms)
    Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 17.463914ms)
    Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 17.278188ms)
    Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 17.414021ms)
    Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 17.876882ms)
    Jan  5 09:10:33.230: INFO: (9) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 17.83238ms)
    Jan  5 09:10:33.231: INFO: (9) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 18.71923ms)
    Jan  5 09:10:33.236: INFO: (9) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 23.876468ms)
    Jan  5 09:10:33.236: INFO: (9) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 24.016028ms)
    Jan  5 09:10:33.236: INFO: (9) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 24.056945ms)
    Jan  5 09:10:33.236: INFO: (9) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 24.127486ms)
    Jan  5 09:10:33.251: INFO: (10) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 14.737661ms)
    Jan  5 09:10:33.251: INFO: (10) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 14.729626ms)
    Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 14.922263ms)
    Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 15.030115ms)
    Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 15.098762ms)
    Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 15.053429ms)
    Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 15.127736ms)
    Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 15.299146ms)
    Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 15.205119ms)
    Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 15.348356ms)
    Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 15.271765ms)
    Jan  5 09:10:33.252: INFO: (10) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 15.588865ms)
    Jan  5 09:10:33.259: INFO: (10) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 22.052184ms)
    Jan  5 09:10:33.259: INFO: (10) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 22.111695ms)
    Jan  5 09:10:33.259: INFO: (10) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 22.219477ms)
    Jan  5 09:10:33.259: INFO: (10) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 22.430859ms)
    Jan  5 09:10:33.279: INFO: (11) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 19.972063ms)
    Jan  5 09:10:33.279: INFO: (11) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 20.0884ms)
    Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 20.366977ms)
    Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 20.286738ms)
    Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 20.249508ms)
    Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 20.409256ms)
    Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 20.490597ms)
    Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 20.580805ms)
    Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 20.524921ms)
    Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 20.549647ms)
    Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 20.74446ms)
    Jan  5 09:10:33.280: INFO: (11) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 20.77101ms)
    Jan  5 09:10:33.283: INFO: (11) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 24.096678ms)
    Jan  5 09:10:33.284: INFO: (11) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 24.412036ms)
    Jan  5 09:10:33.284: INFO: (11) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 24.505741ms)
    Jan  5 09:10:33.284: INFO: (11) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 24.539133ms)
    Jan  5 09:10:33.295: INFO: (12) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 10.649501ms)
    Jan  5 09:10:33.295: INFO: (12) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 10.672965ms)
    Jan  5 09:10:33.296: INFO: (12) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 12.307807ms)
    Jan  5 09:10:33.297: INFO: (12) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 12.535602ms)
    Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 16.469634ms)
    Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 16.474554ms)
    Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 16.550004ms)
    Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 16.536118ms)
    Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 16.465195ms)
    Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 16.492145ms)
    Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 16.722184ms)
    Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 16.719539ms)
    Jan  5 09:10:33.301: INFO: (12) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 16.600687ms)
    Jan  5 09:10:33.303: INFO: (12) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 19.414134ms)
    Jan  5 09:10:33.309: INFO: (12) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 24.624131ms)
    Jan  5 09:10:33.309: INFO: (12) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 24.950428ms)
    Jan  5 09:10:33.325: INFO: (13) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 16.305898ms)
    Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 16.488889ms)
    Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 16.944197ms)
    Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 16.975425ms)
    Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 16.880688ms)
    Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 17.181378ms)
    Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 17.368657ms)
    Jan  5 09:10:33.326: INFO: (13) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 17.486657ms)
    Jan  5 09:10:33.328: INFO: (13) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 18.285364ms)
    Jan  5 09:10:33.328: INFO: (13) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 18.386801ms)
    Jan  5 09:10:33.328: INFO: (13) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 18.545688ms)
    Jan  5 09:10:33.328: INFO: (13) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 18.741101ms)
    Jan  5 09:10:33.335: INFO: (13) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 25.798828ms)
    Jan  5 09:10:33.335: INFO: (13) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 25.639902ms)
    Jan  5 09:10:33.335: INFO: (13) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 25.434299ms)
    Jan  5 09:10:33.335: INFO: (13) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 26.243295ms)
    Jan  5 09:10:33.346: INFO: (14) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 10.141736ms)
    Jan  5 09:10:33.346: INFO: (14) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 10.59546ms)
    Jan  5 09:10:33.347: INFO: (14) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 10.827402ms)
    Jan  5 09:10:33.347: INFO: (14) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 11.606112ms)
    Jan  5 09:10:33.347: INFO: (14) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 11.807857ms)
    Jan  5 09:10:33.348: INFO: (14) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 11.893106ms)
    Jan  5 09:10:33.350: INFO: (14) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 13.989598ms)
    Jan  5 09:10:33.356: INFO: (14) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 20.311325ms)
    Jan  5 09:10:33.356: INFO: (14) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 20.269186ms)
    Jan  5 09:10:33.356: INFO: (14) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 20.372819ms)
    Jan  5 09:10:33.356: INFO: (14) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 20.431959ms)
    Jan  5 09:10:33.356: INFO: (14) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 20.339377ms)
    Jan  5 09:10:33.356: INFO: (14) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 20.686323ms)
    Jan  5 09:10:33.357: INFO: (14) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 21.183006ms)
    Jan  5 09:10:33.357: INFO: (14) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 21.206861ms)
    Jan  5 09:10:33.361: INFO: (14) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 25.363909ms)
    Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 14.432942ms)
    Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 14.87764ms)
    Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 14.831124ms)
    Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 14.962639ms)
    Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 14.873132ms)
    Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 14.886287ms)
    Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 14.987615ms)
    Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 15.26352ms)
    Jan  5 09:10:33.376: INFO: (15) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 15.047506ms)
    Jan  5 09:10:33.377: INFO: (15) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 15.17833ms)
    Jan  5 09:10:33.377: INFO: (15) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 15.501021ms)
    Jan  5 09:10:33.377: INFO: (15) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 15.541105ms)
    Jan  5 09:10:33.382: INFO: (15) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 20.601063ms)
    Jan  5 09:10:33.382: INFO: (15) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 20.786869ms)
    Jan  5 09:10:33.382: INFO: (15) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 20.754699ms)
    Jan  5 09:10:33.382: INFO: (15) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 20.996921ms)
    Jan  5 09:10:33.401: INFO: (16) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 18.834846ms)
    Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 19.271628ms)
    Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 19.289913ms)
    Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 19.00911ms)
    Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 19.001416ms)
    Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 19.156204ms)
    Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 19.058382ms)
    Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 19.018047ms)
    Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 19.117141ms)
    Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 19.164319ms)
    Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 19.25546ms)
    Jan  5 09:10:33.402: INFO: (16) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 19.277639ms)
    Jan  5 09:10:33.407: INFO: (16) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 24.185414ms)
    Jan  5 09:10:33.407: INFO: (16) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 24.603833ms)
    Jan  5 09:10:33.407: INFO: (16) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 24.52068ms)
    Jan  5 09:10:33.407: INFO: (16) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 24.80577ms)
    Jan  5 09:10:33.425: INFO: (17) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 17.98283ms)
    Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 18.165901ms)
    Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 18.295342ms)
    Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 18.260376ms)
    Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 18.223417ms)
    Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 18.455841ms)
    Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 18.639172ms)
    Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 18.903484ms)
    Jan  5 09:10:33.426: INFO: (17) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 18.945011ms)
    Jan  5 09:10:33.427: INFO: (17) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 19.103495ms)
    Jan  5 09:10:33.427: INFO: (17) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 18.927999ms)
    Jan  5 09:10:33.428: INFO: (17) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 20.054105ms)
    Jan  5 09:10:33.434: INFO: (17) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 26.074129ms)
    Jan  5 09:10:33.434: INFO: (17) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 26.422287ms)
    Jan  5 09:10:33.434: INFO: (17) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 26.468554ms)
    Jan  5 09:10:33.434: INFO: (17) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 26.482951ms)
    Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 18.318134ms)
    Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 18.492139ms)
    Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 18.347939ms)
    Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 18.471078ms)
    Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 18.536641ms)
    Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 18.78895ms)
    Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 18.648449ms)
    Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 18.836079ms)
    Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 18.456742ms)
    Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 18.704533ms)
    Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 18.736022ms)
    Jan  5 09:10:33.453: INFO: (18) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 19.034436ms)
    Jan  5 09:10:33.460: INFO: (18) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 26.131478ms)
    Jan  5 09:10:33.461: INFO: (18) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 26.367426ms)
    Jan  5 09:10:33.461: INFO: (18) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 26.563282ms)
    Jan  5 09:10:33.461: INFO: (18) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 26.813456ms)
    Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 18.818926ms)
    Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:162/proxy/: bar (200; 18.780725ms)
    Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 18.808658ms)
    Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">test<... (200; 18.979775ms)
    Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:460/proxy/: tls baz (200; 18.879369ms)
    Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname1/proxy/: foo (200; 19.047462ms)
    Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname1/proxy/: tls baz (200; 18.943177ms)
    Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:443/proxy/tlsrewritem... (200; 19.126178ms)
    Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm/proxy/rewriteme">test</a> (200; 19.158649ms)
    Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4961/pods/http:proxy-service-6dcrq-bz2jm:1080/proxy/rewriteme">... (200; 18.99304ms)
    Jan  5 09:10:33.480: INFO: (19) /api/v1/namespaces/proxy-4961/pods/https:proxy-service-6dcrq-bz2jm:462/proxy/: tls qux (200; 19.258624ms)
    Jan  5 09:10:33.481: INFO: (19) /api/v1/namespaces/proxy-4961/services/https:proxy-service-6dcrq:tlsportname2/proxy/: tls qux (200; 19.255047ms)
    Jan  5 09:10:33.492: INFO: (19) /api/v1/namespaces/proxy-4961/services/proxy-service-6dcrq:portname2/proxy/: bar (200; 30.385555ms)
    Jan  5 09:10:33.492: INFO: (19) /api/v1/namespaces/proxy-4961/pods/proxy-service-6dcrq-bz2jm:160/proxy/: foo (200; 30.450404ms)
    Jan  5 09:10:33.492: INFO: (19) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname1/proxy/: foo (200; 30.401093ms)
    Jan  5 09:10:33.492: INFO: (19) /api/v1/namespaces/proxy-4961/services/http:proxy-service-6dcrq:portname2/proxy/: bar (200; 30.401003ms)
    STEP: deleting ReplicationController proxy-service-6dcrq in namespace proxy-4961, will wait for the garbage collector to delete the pods 01/05/23 09:10:33.492
    Jan  5 09:10:33.557: INFO: Deleting ReplicationController proxy-service-6dcrq took: 7.851173ms
    Jan  5 09:10:33.657: INFO: Terminating ReplicationController proxy-service-6dcrq pods took: 100.655413ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan  5 09:10:35.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-4961" for this suite. 01/05/23 09:10:35.874
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:35.885
Jan  5 09:10:35.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename disruption 01/05/23 09:10:35.886
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:35.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:35.913
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 01/05/23 09:10:35.92
STEP: Waiting for the pdb to be processed 01/05/23 09:10:35.926
STEP: updating the pdb 01/05/23 09:10:37.938
STEP: Waiting for the pdb to be processed 01/05/23 09:10:37.948
STEP: patching the pdb 01/05/23 09:10:39.96
STEP: Waiting for the pdb to be processed 01/05/23 09:10:39.972
STEP: Waiting for the pdb to be deleted 01/05/23 09:10:41.992
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  5 09:10:41.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1583" for this suite. 01/05/23 09:10:42.006
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":41,"skipped":636,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.128 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:35.885
    Jan  5 09:10:35.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename disruption 01/05/23 09:10:35.886
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:35.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:35.913
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 01/05/23 09:10:35.92
    STEP: Waiting for the pdb to be processed 01/05/23 09:10:35.926
    STEP: updating the pdb 01/05/23 09:10:37.938
    STEP: Waiting for the pdb to be processed 01/05/23 09:10:37.948
    STEP: patching the pdb 01/05/23 09:10:39.96
    STEP: Waiting for the pdb to be processed 01/05/23 09:10:39.972
    STEP: Waiting for the pdb to be deleted 01/05/23 09:10:41.992
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  5 09:10:41.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1583" for this suite. 01/05/23 09:10:42.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:42.014
Jan  5 09:10:42.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 09:10:42.015
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:42.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:42.038
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 01/05/23 09:10:42.046
Jan  5 09:10:42.057: INFO: Waiting up to 5m0s for pod "pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74" in namespace "emptydir-913" to be "Succeeded or Failed"
Jan  5 09:10:42.064: INFO: Pod "pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74": Phase="Pending", Reason="", readiness=false. Elapsed: 6.977068ms
Jan  5 09:10:44.079: INFO: Pod "pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021314447s
Jan  5 09:10:46.072: INFO: Pod "pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014718102s
STEP: Saw pod success 01/05/23 09:10:46.072
Jan  5 09:10:46.072: INFO: Pod "pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74" satisfied condition "Succeeded or Failed"
Jan  5 09:10:46.077: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74 container test-container: <nil>
STEP: delete the pod 01/05/23 09:10:46.092
Jan  5 09:10:46.102: INFO: Waiting for pod pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74 to disappear
Jan  5 09:10:46.106: INFO: Pod pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 09:10:46.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-913" for this suite. 01/05/23 09:10:46.116
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":42,"skipped":654,"failed":0}
------------------------------
â€¢ [4.108 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:42.014
    Jan  5 09:10:42.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 09:10:42.015
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:42.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:42.038
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/05/23 09:10:42.046
    Jan  5 09:10:42.057: INFO: Waiting up to 5m0s for pod "pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74" in namespace "emptydir-913" to be "Succeeded or Failed"
    Jan  5 09:10:42.064: INFO: Pod "pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74": Phase="Pending", Reason="", readiness=false. Elapsed: 6.977068ms
    Jan  5 09:10:44.079: INFO: Pod "pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021314447s
    Jan  5 09:10:46.072: INFO: Pod "pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014718102s
    STEP: Saw pod success 01/05/23 09:10:46.072
    Jan  5 09:10:46.072: INFO: Pod "pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74" satisfied condition "Succeeded or Failed"
    Jan  5 09:10:46.077: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74 container test-container: <nil>
    STEP: delete the pod 01/05/23 09:10:46.092
    Jan  5 09:10:46.102: INFO: Waiting for pod pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74 to disappear
    Jan  5 09:10:46.106: INFO: Pod pod-2291df03-e8e1-4cec-b293-fbceb7a2bb74 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 09:10:46.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-913" for this suite. 01/05/23 09:10:46.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:46.125
Jan  5 09:10:46.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pods 01/05/23 09:10:46.126
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:46.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:46.153
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 01/05/23 09:10:46.161
Jan  5 09:10:46.171: INFO: Waiting up to 5m0s for pod "pod-pqpq9" in namespace "pods-4121" to be "running"
Jan  5 09:10:46.176: INFO: Pod "pod-pqpq9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.970684ms
Jan  5 09:10:48.183: INFO: Pod "pod-pqpq9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011415384s
Jan  5 09:10:48.183: INFO: Pod "pod-pqpq9" satisfied condition "running"
STEP: patching /status 01/05/23 09:10:48.183
Jan  5 09:10:48.192: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 09:10:48.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4121" for this suite. 01/05/23 09:10:48.201
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":43,"skipped":674,"failed":0}
------------------------------
â€¢ [2.084 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:46.125
    Jan  5 09:10:46.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pods 01/05/23 09:10:46.126
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:46.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:46.153
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 01/05/23 09:10:46.161
    Jan  5 09:10:46.171: INFO: Waiting up to 5m0s for pod "pod-pqpq9" in namespace "pods-4121" to be "running"
    Jan  5 09:10:46.176: INFO: Pod "pod-pqpq9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.970684ms
    Jan  5 09:10:48.183: INFO: Pod "pod-pqpq9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011415384s
    Jan  5 09:10:48.183: INFO: Pod "pod-pqpq9" satisfied condition "running"
    STEP: patching /status 01/05/23 09:10:48.183
    Jan  5 09:10:48.192: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 09:10:48.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4121" for this suite. 01/05/23 09:10:48.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:48.214
Jan  5 09:10:48.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:10:48.215
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:48.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:48.238
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-2213fc84-f5d4-4943-8868-212a6189e9c1 01/05/23 09:10:48.246
STEP: Creating a pod to test consume secrets 01/05/23 09:10:48.251
Jan  5 09:10:48.264: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c" in namespace "projected-8739" to be "Succeeded or Failed"
Jan  5 09:10:48.270: INFO: Pod "pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042038ms
Jan  5 09:10:50.276: INFO: Pod "pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01232706s
Jan  5 09:10:52.276: INFO: Pod "pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012412691s
STEP: Saw pod success 01/05/23 09:10:52.276
Jan  5 09:10:52.276: INFO: Pod "pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c" satisfied condition "Succeeded or Failed"
Jan  5 09:10:52.281: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 09:10:52.297
Jan  5 09:10:52.311: INFO: Waiting for pod pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c to disappear
Jan  5 09:10:52.319: INFO: Pod pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 09:10:52.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8739" for this suite. 01/05/23 09:10:52.333
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":44,"skipped":744,"failed":0}
------------------------------
â€¢ [4.126 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:48.214
    Jan  5 09:10:48.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:10:48.215
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:48.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:48.238
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-2213fc84-f5d4-4943-8868-212a6189e9c1 01/05/23 09:10:48.246
    STEP: Creating a pod to test consume secrets 01/05/23 09:10:48.251
    Jan  5 09:10:48.264: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c" in namespace "projected-8739" to be "Succeeded or Failed"
    Jan  5 09:10:48.270: INFO: Pod "pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042038ms
    Jan  5 09:10:50.276: INFO: Pod "pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01232706s
    Jan  5 09:10:52.276: INFO: Pod "pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012412691s
    STEP: Saw pod success 01/05/23 09:10:52.276
    Jan  5 09:10:52.276: INFO: Pod "pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c" satisfied condition "Succeeded or Failed"
    Jan  5 09:10:52.281: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 09:10:52.297
    Jan  5 09:10:52.311: INFO: Waiting for pod pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c to disappear
    Jan  5 09:10:52.319: INFO: Pod pod-projected-secrets-bc74c805-8127-4bd4-a646-d06fd3330f0c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 09:10:52.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8739" for this suite. 01/05/23 09:10:52.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:10:52.343
Jan  5 09:10:52.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-probe 01/05/23 09:10:52.343
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:52.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:52.371
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe in namespace container-probe-5164 01/05/23 09:10:52.378
Jan  5 09:10:52.390: INFO: Waiting up to 5m0s for pod "busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe" in namespace "container-probe-5164" to be "not pending"
Jan  5 09:10:52.397: INFO: Pod "busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.332652ms
Jan  5 09:10:54.403: INFO: Pod "busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.013658499s
Jan  5 09:10:54.403: INFO: Pod "busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe" satisfied condition "not pending"
Jan  5 09:10:54.404: INFO: Started pod busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe in namespace container-probe-5164
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 09:10:54.404
Jan  5 09:10:54.409: INFO: Initial restart count of pod busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe is 0
Jan  5 09:11:44.587: INFO: Restart count of pod container-probe-5164/busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe is now 1 (50.177713634s elapsed)
STEP: deleting the pod 01/05/23 09:11:44.587
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 09:11:44.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5164" for this suite. 01/05/23 09:11:44.619
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":45,"skipped":792,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.283 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:10:52.343
    Jan  5 09:10:52.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-probe 01/05/23 09:10:52.343
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:10:52.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:10:52.371
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe in namespace container-probe-5164 01/05/23 09:10:52.378
    Jan  5 09:10:52.390: INFO: Waiting up to 5m0s for pod "busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe" in namespace "container-probe-5164" to be "not pending"
    Jan  5 09:10:52.397: INFO: Pod "busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.332652ms
    Jan  5 09:10:54.403: INFO: Pod "busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.013658499s
    Jan  5 09:10:54.403: INFO: Pod "busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe" satisfied condition "not pending"
    Jan  5 09:10:54.404: INFO: Started pod busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe in namespace container-probe-5164
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 09:10:54.404
    Jan  5 09:10:54.409: INFO: Initial restart count of pod busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe is 0
    Jan  5 09:11:44.587: INFO: Restart count of pod container-probe-5164/busybox-25d57ac0-f5bf-4327-9b49-9e43a5823bbe is now 1 (50.177713634s elapsed)
    STEP: deleting the pod 01/05/23 09:11:44.587
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 09:11:44.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5164" for this suite. 01/05/23 09:11:44.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:11:44.626
Jan  5 09:11:44.626: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename watch 01/05/23 09:11:44.627
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:11:44.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:11:44.649
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/05/23 09:11:44.656
STEP: creating a new configmap 01/05/23 09:11:44.659
STEP: modifying the configmap once 01/05/23 09:11:44.665
STEP: closing the watch once it receives two notifications 01/05/23 09:11:44.676
Jan  5 09:11:44.676: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2720  a35f525b-9beb-4e2f-9e4d-1504bc84cf81 7906 0 2023-01-05 09:11:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 09:11:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:11:44.676: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2720  a35f525b-9beb-4e2f-9e4d-1504bc84cf81 7907 0 2023-01-05 09:11:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 09:11:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/05/23 09:11:44.676
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/05/23 09:11:44.686
STEP: deleting the configmap 01/05/23 09:11:44.689
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/05/23 09:11:44.695
Jan  5 09:11:44.695: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2720  a35f525b-9beb-4e2f-9e4d-1504bc84cf81 7908 0 2023-01-05 09:11:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 09:11:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:11:44.695: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2720  a35f525b-9beb-4e2f-9e4d-1504bc84cf81 7909 0 2023-01-05 09:11:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 09:11:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  5 09:11:44.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2720" for this suite. 01/05/23 09:11:44.704
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":46,"skipped":817,"failed":0}
------------------------------
â€¢ [0.091 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:11:44.626
    Jan  5 09:11:44.626: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename watch 01/05/23 09:11:44.627
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:11:44.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:11:44.649
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/05/23 09:11:44.656
    STEP: creating a new configmap 01/05/23 09:11:44.659
    STEP: modifying the configmap once 01/05/23 09:11:44.665
    STEP: closing the watch once it receives two notifications 01/05/23 09:11:44.676
    Jan  5 09:11:44.676: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2720  a35f525b-9beb-4e2f-9e4d-1504bc84cf81 7906 0 2023-01-05 09:11:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 09:11:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:11:44.676: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2720  a35f525b-9beb-4e2f-9e4d-1504bc84cf81 7907 0 2023-01-05 09:11:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 09:11:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/05/23 09:11:44.676
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/05/23 09:11:44.686
    STEP: deleting the configmap 01/05/23 09:11:44.689
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/05/23 09:11:44.695
    Jan  5 09:11:44.695: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2720  a35f525b-9beb-4e2f-9e4d-1504bc84cf81 7908 0 2023-01-05 09:11:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 09:11:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:11:44.695: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2720  a35f525b-9beb-4e2f-9e4d-1504bc84cf81 7909 0 2023-01-05 09:11:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 09:11:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  5 09:11:44.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2720" for this suite. 01/05/23 09:11:44.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:11:44.718
Jan  5 09:11:44.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename events 01/05/23 09:11:44.719
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:11:44.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:11:44.745
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/05/23 09:11:44.752
STEP: listing events in all namespaces 01/05/23 09:11:44.759
STEP: listing events in test namespace 01/05/23 09:11:44.771
STEP: listing events with field selection filtering on source 01/05/23 09:11:44.776
STEP: listing events with field selection filtering on reportingController 01/05/23 09:11:44.78
STEP: getting the test event 01/05/23 09:11:44.784
STEP: patching the test event 01/05/23 09:11:44.789
STEP: getting the test event 01/05/23 09:11:44.798
STEP: updating the test event 01/05/23 09:11:44.802
STEP: getting the test event 01/05/23 09:11:44.808
STEP: deleting the test event 01/05/23 09:11:44.813
STEP: listing events in all namespaces 01/05/23 09:11:44.818
STEP: listing events in test namespace 01/05/23 09:11:44.828
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan  5 09:11:44.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-478" for this suite. 01/05/23 09:11:44.865
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":47,"skipped":827,"failed":0}
------------------------------
â€¢ [0.154 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:11:44.718
    Jan  5 09:11:44.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename events 01/05/23 09:11:44.719
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:11:44.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:11:44.745
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/05/23 09:11:44.752
    STEP: listing events in all namespaces 01/05/23 09:11:44.759
    STEP: listing events in test namespace 01/05/23 09:11:44.771
    STEP: listing events with field selection filtering on source 01/05/23 09:11:44.776
    STEP: listing events with field selection filtering on reportingController 01/05/23 09:11:44.78
    STEP: getting the test event 01/05/23 09:11:44.784
    STEP: patching the test event 01/05/23 09:11:44.789
    STEP: getting the test event 01/05/23 09:11:44.798
    STEP: updating the test event 01/05/23 09:11:44.802
    STEP: getting the test event 01/05/23 09:11:44.808
    STEP: deleting the test event 01/05/23 09:11:44.813
    STEP: listing events in all namespaces 01/05/23 09:11:44.818
    STEP: listing events in test namespace 01/05/23 09:11:44.828
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan  5 09:11:44.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-478" for this suite. 01/05/23 09:11:44.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:11:44.873
Jan  5 09:11:44.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:11:44.874
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:11:44.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:11:44.896
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:11:44.905
Jan  5 09:11:44.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb" in namespace "downward-api-9106" to be "Succeeded or Failed"
Jan  5 09:11:44.924: INFO: Pod "downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.784522ms
Jan  5 09:11:46.932: INFO: Pod "downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0148609s
Jan  5 09:11:48.932: INFO: Pod "downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015409586s
STEP: Saw pod success 01/05/23 09:11:48.932
Jan  5 09:11:48.932: INFO: Pod "downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb" satisfied condition "Succeeded or Failed"
Jan  5 09:11:48.937: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb container client-container: <nil>
STEP: delete the pod 01/05/23 09:11:48.952
Jan  5 09:11:48.963: INFO: Waiting for pod downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb to disappear
Jan  5 09:11:48.967: INFO: Pod downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 09:11:48.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9106" for this suite. 01/05/23 09:11:48.975
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":48,"skipped":845,"failed":0}
------------------------------
â€¢ [4.108 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:11:44.873
    Jan  5 09:11:44.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:11:44.874
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:11:44.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:11:44.896
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:11:44.905
    Jan  5 09:11:44.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb" in namespace "downward-api-9106" to be "Succeeded or Failed"
    Jan  5 09:11:44.924: INFO: Pod "downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.784522ms
    Jan  5 09:11:46.932: INFO: Pod "downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0148609s
    Jan  5 09:11:48.932: INFO: Pod "downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015409586s
    STEP: Saw pod success 01/05/23 09:11:48.932
    Jan  5 09:11:48.932: INFO: Pod "downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb" satisfied condition "Succeeded or Failed"
    Jan  5 09:11:48.937: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb container client-container: <nil>
    STEP: delete the pod 01/05/23 09:11:48.952
    Jan  5 09:11:48.963: INFO: Waiting for pod downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb to disappear
    Jan  5 09:11:48.967: INFO: Pod downwardapi-volume-7086a331-de9c-4ea4-a934-977f28ec13eb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 09:11:48.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9106" for this suite. 01/05/23 09:11:48.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:11:48.983
Jan  5 09:11:48.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:11:48.983
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:11:49.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:11:49.007
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 01/05/23 09:11:49.014
Jan  5 09:11:49.025: INFO: Waiting up to 5m0s for pod "labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e" in namespace "downward-api-7874" to be "running and ready"
Jan  5 09:11:49.031: INFO: Pod "labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.974321ms
Jan  5 09:11:49.031: INFO: The phase of Pod labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:11:51.037: INFO: Pod "labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012254568s
Jan  5 09:11:51.037: INFO: The phase of Pod labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e is Running (Ready = true)
Jan  5 09:11:51.037: INFO: Pod "labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e" satisfied condition "running and ready"
Jan  5 09:11:51.569: INFO: Successfully updated pod "labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 09:11:53.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7874" for this suite. 01/05/23 09:11:53.602
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":49,"skipped":859,"failed":0}
------------------------------
â€¢ [4.625 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:11:48.983
    Jan  5 09:11:48.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:11:48.983
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:11:49.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:11:49.007
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 01/05/23 09:11:49.014
    Jan  5 09:11:49.025: INFO: Waiting up to 5m0s for pod "labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e" in namespace "downward-api-7874" to be "running and ready"
    Jan  5 09:11:49.031: INFO: Pod "labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.974321ms
    Jan  5 09:11:49.031: INFO: The phase of Pod labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:11:51.037: INFO: Pod "labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012254568s
    Jan  5 09:11:51.037: INFO: The phase of Pod labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e is Running (Ready = true)
    Jan  5 09:11:51.037: INFO: Pod "labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e" satisfied condition "running and ready"
    Jan  5 09:11:51.569: INFO: Successfully updated pod "labelsupdate7c68ccaa-d459-4397-82e0-d0072170021e"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 09:11:53.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7874" for this suite. 01/05/23 09:11:53.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:11:53.609
Jan  5 09:11:53.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:11:53.61
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:11:53.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:11:53.632
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:11:53.64
Jan  5 09:11:53.650: INFO: Waiting up to 5m0s for pod "downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d" in namespace "downward-api-229" to be "Succeeded or Failed"
Jan  5 09:11:53.658: INFO: Pod "downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009505ms
Jan  5 09:11:55.665: INFO: Pod "downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014725596s
Jan  5 09:11:57.665: INFO: Pod "downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014310658s
STEP: Saw pod success 01/05/23 09:11:57.665
Jan  5 09:11:57.665: INFO: Pod "downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d" satisfied condition "Succeeded or Failed"
Jan  5 09:11:57.671: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d container client-container: <nil>
STEP: delete the pod 01/05/23 09:11:57.684
Jan  5 09:11:57.694: INFO: Waiting for pod downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d to disappear
Jan  5 09:11:57.700: INFO: Pod downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 09:11:57.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-229" for this suite. 01/05/23 09:11:57.71
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":50,"skipped":865,"failed":0}
------------------------------
â€¢ [4.109 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:11:53.609
    Jan  5 09:11:53.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:11:53.61
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:11:53.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:11:53.632
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:11:53.64
    Jan  5 09:11:53.650: INFO: Waiting up to 5m0s for pod "downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d" in namespace "downward-api-229" to be "Succeeded or Failed"
    Jan  5 09:11:53.658: INFO: Pod "downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009505ms
    Jan  5 09:11:55.665: INFO: Pod "downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014725596s
    Jan  5 09:11:57.665: INFO: Pod "downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014310658s
    STEP: Saw pod success 01/05/23 09:11:57.665
    Jan  5 09:11:57.665: INFO: Pod "downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d" satisfied condition "Succeeded or Failed"
    Jan  5 09:11:57.671: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d container client-container: <nil>
    STEP: delete the pod 01/05/23 09:11:57.684
    Jan  5 09:11:57.694: INFO: Waiting for pod downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d to disappear
    Jan  5 09:11:57.700: INFO: Pod downwardapi-volume-568f646f-2be0-4b8f-938d-bb62b9cb250d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 09:11:57.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-229" for this suite. 01/05/23 09:11:57.71
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:11:57.718
Jan  5 09:11:57.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:11:57.719
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:11:57.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:11:57.749
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-93d00243-f5cd-46fa-a76e-87d2cdc65cc6 01/05/23 09:11:57.757
STEP: Creating a pod to test consume configMaps 01/05/23 09:11:57.763
Jan  5 09:11:57.774: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca" in namespace "projected-658" to be "Succeeded or Failed"
Jan  5 09:11:57.780: INFO: Pod "pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.462732ms
Jan  5 09:11:59.787: INFO: Pod "pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012890057s
Jan  5 09:12:01.788: INFO: Pod "pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013651565s
STEP: Saw pod success 01/05/23 09:12:01.788
Jan  5 09:12:01.788: INFO: Pod "pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca" satisfied condition "Succeeded or Failed"
Jan  5 09:12:01.792: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca container agnhost-container: <nil>
STEP: delete the pod 01/05/23 09:12:01.805
Jan  5 09:12:01.816: INFO: Waiting for pod pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca to disappear
Jan  5 09:12:01.820: INFO: Pod pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 09:12:01.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-658" for this suite. 01/05/23 09:12:01.829
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":51,"skipped":866,"failed":0}
------------------------------
â€¢ [4.118 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:11:57.718
    Jan  5 09:11:57.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:11:57.719
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:11:57.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:11:57.749
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-93d00243-f5cd-46fa-a76e-87d2cdc65cc6 01/05/23 09:11:57.757
    STEP: Creating a pod to test consume configMaps 01/05/23 09:11:57.763
    Jan  5 09:11:57.774: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca" in namespace "projected-658" to be "Succeeded or Failed"
    Jan  5 09:11:57.780: INFO: Pod "pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.462732ms
    Jan  5 09:11:59.787: INFO: Pod "pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012890057s
    Jan  5 09:12:01.788: INFO: Pod "pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013651565s
    STEP: Saw pod success 01/05/23 09:12:01.788
    Jan  5 09:12:01.788: INFO: Pod "pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca" satisfied condition "Succeeded or Failed"
    Jan  5 09:12:01.792: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 09:12:01.805
    Jan  5 09:12:01.816: INFO: Waiting for pod pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca to disappear
    Jan  5 09:12:01.820: INFO: Pod pod-projected-configmaps-3cd01096-2d4c-4333-b590-8ac853a282ca no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 09:12:01.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-658" for this suite. 01/05/23 09:12:01.829
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:01.836
Jan  5 09:12:01.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename svcaccounts 01/05/23 09:12:01.837
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:01.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:01.863
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jan  5 09:12:01.886: INFO: Waiting up to 5m0s for pod "pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9" in namespace "svcaccounts-4824" to be "running"
Jan  5 09:12:01.891: INFO: Pod "pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.670056ms
Jan  5 09:12:03.898: INFO: Pod "pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.012171291s
Jan  5 09:12:03.898: INFO: Pod "pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9" satisfied condition "running"
STEP: reading a file in the container 01/05/23 09:12:03.898
Jan  5 09:12:03.898: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4824 pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/05/23 09:12:04.353
Jan  5 09:12:04.354: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4824 pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/05/23 09:12:04.84
Jan  5 09:12:04.840: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4824 pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan  5 09:12:05.392: INFO: Got root ca configmap in namespace "svcaccounts-4824"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  5 09:12:05.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4824" for this suite. 01/05/23 09:12:05.406
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":52,"skipped":866,"failed":0}
------------------------------
â€¢ [3.576 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:01.836
    Jan  5 09:12:01.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 09:12:01.837
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:01.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:01.863
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jan  5 09:12:01.886: INFO: Waiting up to 5m0s for pod "pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9" in namespace "svcaccounts-4824" to be "running"
    Jan  5 09:12:01.891: INFO: Pod "pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.670056ms
    Jan  5 09:12:03.898: INFO: Pod "pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.012171291s
    Jan  5 09:12:03.898: INFO: Pod "pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9" satisfied condition "running"
    STEP: reading a file in the container 01/05/23 09:12:03.898
    Jan  5 09:12:03.898: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4824 pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/05/23 09:12:04.353
    Jan  5 09:12:04.354: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4824 pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/05/23 09:12:04.84
    Jan  5 09:12:04.840: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4824 pod-service-account-b9987fb5-09d6-4615-b572-56dd4e6df0b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan  5 09:12:05.392: INFO: Got root ca configmap in namespace "svcaccounts-4824"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  5 09:12:05.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4824" for this suite. 01/05/23 09:12:05.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:05.413
Jan  5 09:12:05.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename daemonsets 01/05/23 09:12:05.413
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:05.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:05.451
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 01/05/23 09:12:05.493
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:12:05.504
Jan  5 09:12:05.518: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:12:05.518: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:12:06.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:12:06.534: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:12:07.533: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  5 09:12:07.533: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: listing all DeamonSets 01/05/23 09:12:07.538
STEP: DeleteCollection of the DaemonSets 01/05/23 09:12:07.543
STEP: Verify that ReplicaSets have been deleted 01/05/23 09:12:07.55
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan  5 09:12:07.569: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8161"},"items":null}

Jan  5 09:12:07.579: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8161"},"items":[{"metadata":{"name":"daemon-set-jb96n","generateName":"daemon-set-","namespace":"daemonsets-8957","uid":"2499816d-4333-47da-8efb-5c1af9b1b8ba","resourceVersion":"8155","creationTimestamp":"2023-01-05T09:12:05Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2bad915d-6ea0-4b78-8582-b3a0f553888c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bad915d-6ea0-4b78-8582-b3a0f553888c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vnk65","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vnk65","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"}],"hostIP":"10.250.0.128","podIP":"10.96.3.54","podIPs":[{"ip":"10.96.3.54"}],"startTime":"2023-01-05T09:12:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T09:12:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://476116c57200aa54b9d4351d4c75f55e60f99ba686aaf6e408c59f7f5e5cc5e4","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jhc5w","generateName":"daemon-set-","namespace":"daemonsets-8957","uid":"6f22ed74-5bdc-4e7b-9bcf-76cfd97e9a7d","resourceVersion":"8144","creationTimestamp":"2023-01-05T09:12:05Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2bad915d-6ea0-4b78-8582-b3a0f553888c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bad915d-6ea0-4b78-8582-b3a0f553888c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tg2nj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tg2nj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:06Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:06Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"}],"hostIP":"10.250.1.19","podIP":"10.96.1.230","podIPs":[{"ip":"10.96.1.230"}],"startTime":"2023-01-05T09:12:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T09:12:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6e62a0ae7c18a0688e6ee650c739d2c95e2ef69aee7a01364569762363cf3dc5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kvqrx","generateName":"daemon-set-","namespace":"daemonsets-8957","uid":"e17d8d50-02f0-4a08-bd03-39a1e929e685","resourceVersion":"8146","creationTimestamp":"2023-01-05T09:12:05Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2bad915d-6ea0-4b78-8582-b3a0f553888c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bad915d-6ea0-4b78-8582-b3a0f553888c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2zj85","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2zj85","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:06Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:06Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"}],"hostIP":"10.250.2.138","podIP":"10.96.2.104","podIPs":[{"ip":"10.96.2.104"}],"startTime":"2023-01-05T09:12:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T09:12:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://0aa52b2a80913ffa9723c44b00f51cb524ae75ee0cbea0c0587f20fd1fc33e84","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vqncr","generateName":"daemon-set-","namespace":"daemonsets-8957","uid":"d0754ab2-5321-46bf-9fd1-d05c699968bb","resourceVersion":"8142","creationTimestamp":"2023-01-05T09:12:05Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2bad915d-6ea0-4b78-8582-b3a0f553888c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bad915d-6ea0-4b78-8582-b3a0f553888c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kcnjk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kcnjk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:06Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:06Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"}],"hostIP":"10.250.0.174","podIP":"10.96.0.191","podIPs":[{"ip":"10.96.0.191"}],"startTime":"2023-01-05T09:12:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T09:12:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://bba3b3848f73622330468bc4dac0574a16df9cb25dcc29c76897e2591a2e59e8","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:12:07.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8957" for this suite. 01/05/23 09:12:07.621
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":53,"skipped":885,"failed":0}
------------------------------
â€¢ [2.215 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:05.413
    Jan  5 09:12:05.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename daemonsets 01/05/23 09:12:05.413
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:05.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:05.451
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 01/05/23 09:12:05.493
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:12:05.504
    Jan  5 09:12:05.518: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:12:05.518: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:12:06.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:12:06.534: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:12:07.533: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  5 09:12:07.533: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: listing all DeamonSets 01/05/23 09:12:07.538
    STEP: DeleteCollection of the DaemonSets 01/05/23 09:12:07.543
    STEP: Verify that ReplicaSets have been deleted 01/05/23 09:12:07.55
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jan  5 09:12:07.569: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8161"},"items":null}

    Jan  5 09:12:07.579: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8161"},"items":[{"metadata":{"name":"daemon-set-jb96n","generateName":"daemon-set-","namespace":"daemonsets-8957","uid":"2499816d-4333-47da-8efb-5c1af9b1b8ba","resourceVersion":"8155","creationTimestamp":"2023-01-05T09:12:05Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2bad915d-6ea0-4b78-8582-b3a0f553888c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bad915d-6ea0-4b78-8582-b3a0f553888c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vnk65","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vnk65","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"}],"hostIP":"10.250.0.128","podIP":"10.96.3.54","podIPs":[{"ip":"10.96.3.54"}],"startTime":"2023-01-05T09:12:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T09:12:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://476116c57200aa54b9d4351d4c75f55e60f99ba686aaf6e408c59f7f5e5cc5e4","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jhc5w","generateName":"daemon-set-","namespace":"daemonsets-8957","uid":"6f22ed74-5bdc-4e7b-9bcf-76cfd97e9a7d","resourceVersion":"8144","creationTimestamp":"2023-01-05T09:12:05Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2bad915d-6ea0-4b78-8582-b3a0f553888c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bad915d-6ea0-4b78-8582-b3a0f553888c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tg2nj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tg2nj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:06Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:06Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"}],"hostIP":"10.250.1.19","podIP":"10.96.1.230","podIPs":[{"ip":"10.96.1.230"}],"startTime":"2023-01-05T09:12:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T09:12:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6e62a0ae7c18a0688e6ee650c739d2c95e2ef69aee7a01364569762363cf3dc5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kvqrx","generateName":"daemon-set-","namespace":"daemonsets-8957","uid":"e17d8d50-02f0-4a08-bd03-39a1e929e685","resourceVersion":"8146","creationTimestamp":"2023-01-05T09:12:05Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2bad915d-6ea0-4b78-8582-b3a0f553888c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bad915d-6ea0-4b78-8582-b3a0f553888c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2zj85","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2zj85","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:06Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:06Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"}],"hostIP":"10.250.2.138","podIP":"10.96.2.104","podIPs":[{"ip":"10.96.2.104"}],"startTime":"2023-01-05T09:12:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T09:12:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://0aa52b2a80913ffa9723c44b00f51cb524ae75ee0cbea0c0587f20fd1fc33e84","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vqncr","generateName":"daemon-set-","namespace":"daemonsets-8957","uid":"d0754ab2-5321-46bf-9fd1-d05c699968bb","resourceVersion":"8142","creationTimestamp":"2023-01-05T09:12:05Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2bad915d-6ea0-4b78-8582-b3a0f553888c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bad915d-6ea0-4b78-8582-b3a0f553888c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T09:12:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kcnjk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kcnjk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:06Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:06Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T09:12:05Z"}],"hostIP":"10.250.0.174","podIP":"10.96.0.191","podIPs":[{"ip":"10.96.0.191"}],"startTime":"2023-01-05T09:12:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T09:12:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://bba3b3848f73622330468bc4dac0574a16df9cb25dcc29c76897e2591a2e59e8","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:12:07.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8957" for this suite. 01/05/23 09:12:07.621
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:07.628
Jan  5 09:12:07.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename statefulset 01/05/23 09:12:07.629
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:07.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:07.653
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3335 01/05/23 09:12:07.66
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-3335 01/05/23 09:12:07.674
Jan  5 09:12:07.683: INFO: Found 0 stateful pods, waiting for 1
Jan  5 09:12:17.691: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/05/23 09:12:17.7
STEP: Getting /status 01/05/23 09:12:17.71
Jan  5 09:12:17.716: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/05/23 09:12:17.716
Jan  5 09:12:17.728: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/05/23 09:12:17.728
Jan  5 09:12:17.731: INFO: Observed &StatefulSet event: ADDED
Jan  5 09:12:17.731: INFO: Found Statefulset ss in namespace statefulset-3335 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 09:12:17.731: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/05/23 09:12:17.731
Jan  5 09:12:17.731: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  5 09:12:17.739: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/05/23 09:12:17.739
Jan  5 09:12:17.744: INFO: Observed &StatefulSet event: ADDED
Jan  5 09:12:17.744: INFO: Observed Statefulset ss in namespace statefulset-3335 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 09:12:17.745: INFO: Observed &StatefulSet event: MODIFIED
Jan  5 09:12:17.745: INFO: Found Statefulset ss in namespace statefulset-3335 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 09:12:17.745: INFO: Deleting all statefulset in ns statefulset-3335
Jan  5 09:12:17.749: INFO: Scaling statefulset ss to 0
Jan  5 09:12:27.772: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 09:12:27.777: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 09:12:27.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3335" for this suite. 01/05/23 09:12:27.803
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":54,"skipped":887,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.185 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:07.628
    Jan  5 09:12:07.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename statefulset 01/05/23 09:12:07.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:07.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:07.653
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3335 01/05/23 09:12:07.66
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-3335 01/05/23 09:12:07.674
    Jan  5 09:12:07.683: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 09:12:17.691: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/05/23 09:12:17.7
    STEP: Getting /status 01/05/23 09:12:17.71
    Jan  5 09:12:17.716: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/05/23 09:12:17.716
    Jan  5 09:12:17.728: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/05/23 09:12:17.728
    Jan  5 09:12:17.731: INFO: Observed &StatefulSet event: ADDED
    Jan  5 09:12:17.731: INFO: Found Statefulset ss in namespace statefulset-3335 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 09:12:17.731: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/05/23 09:12:17.731
    Jan  5 09:12:17.731: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  5 09:12:17.739: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/05/23 09:12:17.739
    Jan  5 09:12:17.744: INFO: Observed &StatefulSet event: ADDED
    Jan  5 09:12:17.744: INFO: Observed Statefulset ss in namespace statefulset-3335 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 09:12:17.745: INFO: Observed &StatefulSet event: MODIFIED
    Jan  5 09:12:17.745: INFO: Found Statefulset ss in namespace statefulset-3335 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 09:12:17.745: INFO: Deleting all statefulset in ns statefulset-3335
    Jan  5 09:12:17.749: INFO: Scaling statefulset ss to 0
    Jan  5 09:12:27.772: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 09:12:27.777: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 09:12:27.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3335" for this suite. 01/05/23 09:12:27.803
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:27.813
Jan  5 09:12:27.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 09:12:27.814
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:27.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:27.835
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 01/05/23 09:12:27.843
Jan  5 09:12:27.843: INFO: Creating e2e-svc-a-s59q5
Jan  5 09:12:27.859: INFO: Creating e2e-svc-b-77v2v
Jan  5 09:12:27.873: INFO: Creating e2e-svc-c-29xmq
STEP: deleting service collection 01/05/23 09:12:27.892
Jan  5 09:12:27.924: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:12:27.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4986" for this suite. 01/05/23 09:12:27.933
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":55,"skipped":888,"failed":0}
------------------------------
â€¢ [0.126 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:27.813
    Jan  5 09:12:27.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 09:12:27.814
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:27.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:27.835
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 01/05/23 09:12:27.843
    Jan  5 09:12:27.843: INFO: Creating e2e-svc-a-s59q5
    Jan  5 09:12:27.859: INFO: Creating e2e-svc-b-77v2v
    Jan  5 09:12:27.873: INFO: Creating e2e-svc-c-29xmq
    STEP: deleting service collection 01/05/23 09:12:27.892
    Jan  5 09:12:27.924: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:12:27.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4986" for this suite. 01/05/23 09:12:27.933
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:27.939
Jan  5 09:12:27.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename runtimeclass 01/05/23 09:12:27.941
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:27.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:27.965
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  5 09:12:27.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8470" for this suite. 01/05/23 09:12:27.987
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":56,"skipped":890,"failed":0}
------------------------------
â€¢ [0.054 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:27.939
    Jan  5 09:12:27.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 09:12:27.941
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:27.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:27.965
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  5 09:12:27.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8470" for this suite. 01/05/23 09:12:27.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:27.996
Jan  5 09:12:27.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename csistoragecapacity 01/05/23 09:12:27.997
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:28.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:28.019
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/05/23 09:12:28.028
STEP: getting /apis/storage.k8s.io 01/05/23 09:12:28.034
STEP: getting /apis/storage.k8s.io/v1 01/05/23 09:12:28.038
STEP: creating 01/05/23 09:12:28.041
STEP: watching 01/05/23 09:12:28.058
Jan  5 09:12:28.058: INFO: starting watch
STEP: getting 01/05/23 09:12:28.07
STEP: listing in namespace 01/05/23 09:12:28.074
STEP: listing across namespaces 01/05/23 09:12:28.079
STEP: patching 01/05/23 09:12:28.084
STEP: updating 01/05/23 09:12:28.091
Jan  5 09:12:28.097: INFO: waiting for watch events with expected annotations in namespace
Jan  5 09:12:28.097: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/05/23 09:12:28.097
STEP: deleting a collection 01/05/23 09:12:28.111
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jan  5 09:12:28.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-5264" for this suite. 01/05/23 09:12:28.134
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":57,"skipped":900,"failed":0}
------------------------------
â€¢ [0.144 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:27.996
    Jan  5 09:12:27.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename csistoragecapacity 01/05/23 09:12:27.997
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:28.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:28.019
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/05/23 09:12:28.028
    STEP: getting /apis/storage.k8s.io 01/05/23 09:12:28.034
    STEP: getting /apis/storage.k8s.io/v1 01/05/23 09:12:28.038
    STEP: creating 01/05/23 09:12:28.041
    STEP: watching 01/05/23 09:12:28.058
    Jan  5 09:12:28.058: INFO: starting watch
    STEP: getting 01/05/23 09:12:28.07
    STEP: listing in namespace 01/05/23 09:12:28.074
    STEP: listing across namespaces 01/05/23 09:12:28.079
    STEP: patching 01/05/23 09:12:28.084
    STEP: updating 01/05/23 09:12:28.091
    Jan  5 09:12:28.097: INFO: waiting for watch events with expected annotations in namespace
    Jan  5 09:12:28.097: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/05/23 09:12:28.097
    STEP: deleting a collection 01/05/23 09:12:28.111
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jan  5 09:12:28.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-5264" for this suite. 01/05/23 09:12:28.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:28.142
Jan  5 09:12:28.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:12:28.142
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:28.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:28.165
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:12:28.172
Jan  5 09:12:28.186: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971" in namespace "downward-api-5503" to be "Succeeded or Failed"
Jan  5 09:12:28.194: INFO: Pod "downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971": Phase="Pending", Reason="", readiness=false. Elapsed: 7.536097ms
Jan  5 09:12:30.201: INFO: Pod "downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014905809s
Jan  5 09:12:32.203: INFO: Pod "downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016323485s
STEP: Saw pod success 01/05/23 09:12:32.203
Jan  5 09:12:32.203: INFO: Pod "downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971" satisfied condition "Succeeded or Failed"
Jan  5 09:12:32.209: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971 container client-container: <nil>
STEP: delete the pod 01/05/23 09:12:32.221
Jan  5 09:12:32.237: INFO: Waiting for pod downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971 to disappear
Jan  5 09:12:32.242: INFO: Pod downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 09:12:32.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5503" for this suite. 01/05/23 09:12:32.255
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":58,"skipped":964,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:28.142
    Jan  5 09:12:28.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:12:28.142
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:28.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:28.165
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:12:28.172
    Jan  5 09:12:28.186: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971" in namespace "downward-api-5503" to be "Succeeded or Failed"
    Jan  5 09:12:28.194: INFO: Pod "downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971": Phase="Pending", Reason="", readiness=false. Elapsed: 7.536097ms
    Jan  5 09:12:30.201: INFO: Pod "downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014905809s
    Jan  5 09:12:32.203: INFO: Pod "downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016323485s
    STEP: Saw pod success 01/05/23 09:12:32.203
    Jan  5 09:12:32.203: INFO: Pod "downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971" satisfied condition "Succeeded or Failed"
    Jan  5 09:12:32.209: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971 container client-container: <nil>
    STEP: delete the pod 01/05/23 09:12:32.221
    Jan  5 09:12:32.237: INFO: Waiting for pod downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971 to disappear
    Jan  5 09:12:32.242: INFO: Pod downwardapi-volume-34fc9b62-15ff-4c52-9774-e436332cc971 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 09:12:32.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5503" for this suite. 01/05/23 09:12:32.255
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:32.263
Jan  5 09:12:32.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 09:12:32.264
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:32.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:32.286
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 09:12:32.306
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:12:32.692
STEP: Deploying the webhook pod 01/05/23 09:12:32.7
STEP: Wait for the deployment to be ready 01/05/23 09:12:32.712
Jan  5 09:12:32.720: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 09:12:34.737
STEP: Verifying the service has paired with the endpoint 01/05/23 09:12:34.748
Jan  5 09:12:35.749: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/05/23 09:12:35.755
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/05/23 09:12:35.881
STEP: Creating a dummy validating-webhook-configuration object 01/05/23 09:12:36.007
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/05/23 09:12:36.11
STEP: Creating a dummy mutating-webhook-configuration object 01/05/23 09:12:36.116
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/05/23 09:12:36.214
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:12:36.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5839" for this suite. 01/05/23 09:12:36.239
STEP: Destroying namespace "webhook-5839-markers" for this suite. 01/05/23 09:12:36.245
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":59,"skipped":966,"failed":0}
------------------------------
â€¢ [4.016 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:32.263
    Jan  5 09:12:32.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 09:12:32.264
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:32.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:32.286
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 09:12:32.306
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:12:32.692
    STEP: Deploying the webhook pod 01/05/23 09:12:32.7
    STEP: Wait for the deployment to be ready 01/05/23 09:12:32.712
    Jan  5 09:12:32.720: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 09:12:34.737
    STEP: Verifying the service has paired with the endpoint 01/05/23 09:12:34.748
    Jan  5 09:12:35.749: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/05/23 09:12:35.755
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/05/23 09:12:35.881
    STEP: Creating a dummy validating-webhook-configuration object 01/05/23 09:12:36.007
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/05/23 09:12:36.11
    STEP: Creating a dummy mutating-webhook-configuration object 01/05/23 09:12:36.116
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/05/23 09:12:36.214
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:12:36.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5839" for this suite. 01/05/23 09:12:36.239
    STEP: Destroying namespace "webhook-5839-markers" for this suite. 01/05/23 09:12:36.245
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:36.282
Jan  5 09:12:36.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:12:36.283
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:36.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:36.313
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:12:36.32
Jan  5 09:12:36.331: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1" in namespace "downward-api-7653" to be "Succeeded or Failed"
Jan  5 09:12:36.337: INFO: Pod "downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.810692ms
Jan  5 09:12:38.344: INFO: Pod "downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012880652s
Jan  5 09:12:40.345: INFO: Pod "downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014437221s
STEP: Saw pod success 01/05/23 09:12:40.345
Jan  5 09:12:40.345: INFO: Pod "downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1" satisfied condition "Succeeded or Failed"
Jan  5 09:12:40.351: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1 container client-container: <nil>
STEP: delete the pod 01/05/23 09:12:40.365
Jan  5 09:12:40.377: INFO: Waiting for pod downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1 to disappear
Jan  5 09:12:40.381: INFO: Pod downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 09:12:40.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7653" for this suite. 01/05/23 09:12:40.391
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":60,"skipped":1016,"failed":0}
------------------------------
â€¢ [4.119 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:36.282
    Jan  5 09:12:36.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:12:36.283
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:36.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:36.313
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:12:36.32
    Jan  5 09:12:36.331: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1" in namespace "downward-api-7653" to be "Succeeded or Failed"
    Jan  5 09:12:36.337: INFO: Pod "downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.810692ms
    Jan  5 09:12:38.344: INFO: Pod "downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012880652s
    Jan  5 09:12:40.345: INFO: Pod "downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014437221s
    STEP: Saw pod success 01/05/23 09:12:40.345
    Jan  5 09:12:40.345: INFO: Pod "downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1" satisfied condition "Succeeded or Failed"
    Jan  5 09:12:40.351: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1 container client-container: <nil>
    STEP: delete the pod 01/05/23 09:12:40.365
    Jan  5 09:12:40.377: INFO: Waiting for pod downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1 to disappear
    Jan  5 09:12:40.381: INFO: Pod downwardapi-volume-89b91591-e363-49af-88ab-50c41a9476c1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 09:12:40.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7653" for this suite. 01/05/23 09:12:40.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:40.402
Jan  5 09:12:40.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename svcaccounts 01/05/23 09:12:40.403
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:40.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:40.43
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jan  5 09:12:40.443: INFO: Got root ca configmap in namespace "svcaccounts-7791"
Jan  5 09:12:40.449: INFO: Deleted root ca configmap in namespace "svcaccounts-7791"
STEP: waiting for a new root ca configmap created 01/05/23 09:12:40.95
Jan  5 09:12:40.958: INFO: Recreated root ca configmap in namespace "svcaccounts-7791"
Jan  5 09:12:40.969: INFO: Updated root ca configmap in namespace "svcaccounts-7791"
STEP: waiting for the root ca configmap reconciled 01/05/23 09:12:41.47
Jan  5 09:12:41.476: INFO: Reconciled root ca configmap in namespace "svcaccounts-7791"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  5 09:12:41.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7791" for this suite. 01/05/23 09:12:41.486
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":61,"skipped":1024,"failed":0}
------------------------------
â€¢ [1.092 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:40.402
    Jan  5 09:12:40.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 09:12:40.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:40.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:40.43
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jan  5 09:12:40.443: INFO: Got root ca configmap in namespace "svcaccounts-7791"
    Jan  5 09:12:40.449: INFO: Deleted root ca configmap in namespace "svcaccounts-7791"
    STEP: waiting for a new root ca configmap created 01/05/23 09:12:40.95
    Jan  5 09:12:40.958: INFO: Recreated root ca configmap in namespace "svcaccounts-7791"
    Jan  5 09:12:40.969: INFO: Updated root ca configmap in namespace "svcaccounts-7791"
    STEP: waiting for the root ca configmap reconciled 01/05/23 09:12:41.47
    Jan  5 09:12:41.476: INFO: Reconciled root ca configmap in namespace "svcaccounts-7791"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  5 09:12:41.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7791" for this suite. 01/05/23 09:12:41.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:41.494
Jan  5 09:12:41.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 09:12:41.495
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:41.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:41.518
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 09:12:41.526
Jan  5 09:12:41.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-8015 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan  5 09:12:41.602: INFO: stderr: ""
Jan  5 09:12:41.602: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/05/23 09:12:41.602
STEP: verifying the pod e2e-test-httpd-pod was created 01/05/23 09:12:46.654
Jan  5 09:12:46.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-8015 get pod e2e-test-httpd-pod -o json'
Jan  5 09:12:46.721: INFO: stderr: ""
Jan  5 09:12:46.721: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-01-05T09:12:41Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8015\",\n        \"resourceVersion\": \"8563\",\n        \"uid\": \"4f80eb03-1a23-469b-8aca-c1fbe1db9684\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io\"\n                    }\n                ],\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-q24tv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-q24tv\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T09:12:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T09:12:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T09:12:42Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T09:12:41Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://f3fa26fa929ba4fdd7130796357195b474960be5589d2170fddc87fe53d1bbdb\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-05T09:12:42Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.2.138\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.96.2.214\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.96.2.214\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-05T09:12:41Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/05/23 09:12:46.721
Jan  5 09:12:46.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-8015 replace -f -'
Jan  5 09:12:47.526: INFO: stderr: ""
Jan  5 09:12:47.526: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/05/23 09:12:47.526
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jan  5 09:12:47.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-8015 delete pods e2e-test-httpd-pod'
Jan  5 09:12:49.039: INFO: stderr: ""
Jan  5 09:12:49.039: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 09:12:49.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8015" for this suite. 01/05/23 09:12:49.048
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":62,"skipped":1064,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.560 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:41.494
    Jan  5 09:12:41.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 09:12:41.495
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:41.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:41.518
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 09:12:41.526
    Jan  5 09:12:41.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-8015 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan  5 09:12:41.602: INFO: stderr: ""
    Jan  5 09:12:41.602: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/05/23 09:12:41.602
    STEP: verifying the pod e2e-test-httpd-pod was created 01/05/23 09:12:46.654
    Jan  5 09:12:46.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-8015 get pod e2e-test-httpd-pod -o json'
    Jan  5 09:12:46.721: INFO: stderr: ""
    Jan  5 09:12:46.721: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-01-05T09:12:41Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8015\",\n        \"resourceVersion\": \"8563\",\n        \"uid\": \"4f80eb03-1a23-469b-8aca-c1fbe1db9684\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io\"\n                    }\n                ],\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-q24tv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-q24tv\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T09:12:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T09:12:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T09:12:42Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T09:12:41Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://f3fa26fa929ba4fdd7130796357195b474960be5589d2170fddc87fe53d1bbdb\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-05T09:12:42Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.2.138\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.96.2.214\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.96.2.214\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-05T09:12:41Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/05/23 09:12:46.721
    Jan  5 09:12:46.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-8015 replace -f -'
    Jan  5 09:12:47.526: INFO: stderr: ""
    Jan  5 09:12:47.526: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/05/23 09:12:47.526
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jan  5 09:12:47.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-8015 delete pods e2e-test-httpd-pod'
    Jan  5 09:12:49.039: INFO: stderr: ""
    Jan  5 09:12:49.039: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 09:12:49.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8015" for this suite. 01/05/23 09:12:49.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:49.054
Jan  5 09:12:49.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename controllerrevisions 01/05/23 09:12:49.055
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:49.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:49.078
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-fsvbz-daemon-set" 01/05/23 09:12:49.126
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:12:49.132
Jan  5 09:12:49.144: INFO: Number of nodes with available pods controlled by daemonset e2e-fsvbz-daemon-set: 0
Jan  5 09:12:49.144: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:12:50.163: INFO: Number of nodes with available pods controlled by daemonset e2e-fsvbz-daemon-set: 1
Jan  5 09:12:50.163: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:12:51.161: INFO: Number of nodes with available pods controlled by daemonset e2e-fsvbz-daemon-set: 4
Jan  5 09:12:51.161: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-fsvbz-daemon-set
STEP: Confirm DaemonSet "e2e-fsvbz-daemon-set" successfully created with "daemonset-name=e2e-fsvbz-daemon-set" label 01/05/23 09:12:51.165
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-fsvbz-daemon-set" 01/05/23 09:12:51.175
Jan  5 09:12:51.180: INFO: Located ControllerRevision: "e2e-fsvbz-daemon-set-6978966c88"
STEP: Patching ControllerRevision "e2e-fsvbz-daemon-set-6978966c88" 01/05/23 09:12:51.184
Jan  5 09:12:51.189: INFO: e2e-fsvbz-daemon-set-6978966c88 has been patched
STEP: Create a new ControllerRevision 01/05/23 09:12:51.189
Jan  5 09:12:51.195: INFO: Created ControllerRevision: e2e-fsvbz-daemon-set-6f8cd4bf78
STEP: Confirm that there are two ControllerRevisions 01/05/23 09:12:51.195
Jan  5 09:12:51.195: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 09:12:51.199: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-fsvbz-daemon-set-6978966c88" 01/05/23 09:12:51.199
STEP: Confirm that there is only one ControllerRevision 01/05/23 09:12:51.205
Jan  5 09:12:51.205: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 09:12:51.211: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-fsvbz-daemon-set-6f8cd4bf78" 01/05/23 09:12:51.216
Jan  5 09:12:51.227: INFO: e2e-fsvbz-daemon-set-6f8cd4bf78 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/05/23 09:12:51.227
W0105 09:12:51.240029      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/05/23 09:12:51.24
Jan  5 09:12:51.240: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 09:12:52.244: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 09:12:52.252: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-fsvbz-daemon-set-6f8cd4bf78=updated" 01/05/23 09:12:52.252
STEP: Confirm that there is only one ControllerRevision 01/05/23 09:12:52.26
Jan  5 09:12:52.260: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 09:12:52.265: INFO: Found 1 ControllerRevisions
Jan  5 09:12:52.272: INFO: ControllerRevision "e2e-fsvbz-daemon-set-6dfb875f5" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-fsvbz-daemon-set" 01/05/23 09:12:52.277
STEP: deleting DaemonSet.extensions e2e-fsvbz-daemon-set in namespace controllerrevisions-6113, will wait for the garbage collector to delete the pods 01/05/23 09:12:52.277
Jan  5 09:12:52.339: INFO: Deleting DaemonSet.extensions e2e-fsvbz-daemon-set took: 7.193321ms
Jan  5 09:12:52.440: INFO: Terminating DaemonSet.extensions e2e-fsvbz-daemon-set pods took: 100.534208ms
Jan  5 09:12:53.845: INFO: Number of nodes with available pods controlled by daemonset e2e-fsvbz-daemon-set: 0
Jan  5 09:12:53.845: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-fsvbz-daemon-set
Jan  5 09:12:53.851: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8693"},"items":null}

Jan  5 09:12:53.856: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8693"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:12:53.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-6113" for this suite. 01/05/23 09:12:53.901
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":63,"skipped":1070,"failed":0}
------------------------------
â€¢ [4.855 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:49.054
    Jan  5 09:12:49.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename controllerrevisions 01/05/23 09:12:49.055
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:49.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:49.078
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-fsvbz-daemon-set" 01/05/23 09:12:49.126
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:12:49.132
    Jan  5 09:12:49.144: INFO: Number of nodes with available pods controlled by daemonset e2e-fsvbz-daemon-set: 0
    Jan  5 09:12:49.144: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:12:50.163: INFO: Number of nodes with available pods controlled by daemonset e2e-fsvbz-daemon-set: 1
    Jan  5 09:12:50.163: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:12:51.161: INFO: Number of nodes with available pods controlled by daemonset e2e-fsvbz-daemon-set: 4
    Jan  5 09:12:51.161: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-fsvbz-daemon-set
    STEP: Confirm DaemonSet "e2e-fsvbz-daemon-set" successfully created with "daemonset-name=e2e-fsvbz-daemon-set" label 01/05/23 09:12:51.165
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-fsvbz-daemon-set" 01/05/23 09:12:51.175
    Jan  5 09:12:51.180: INFO: Located ControllerRevision: "e2e-fsvbz-daemon-set-6978966c88"
    STEP: Patching ControllerRevision "e2e-fsvbz-daemon-set-6978966c88" 01/05/23 09:12:51.184
    Jan  5 09:12:51.189: INFO: e2e-fsvbz-daemon-set-6978966c88 has been patched
    STEP: Create a new ControllerRevision 01/05/23 09:12:51.189
    Jan  5 09:12:51.195: INFO: Created ControllerRevision: e2e-fsvbz-daemon-set-6f8cd4bf78
    STEP: Confirm that there are two ControllerRevisions 01/05/23 09:12:51.195
    Jan  5 09:12:51.195: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 09:12:51.199: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-fsvbz-daemon-set-6978966c88" 01/05/23 09:12:51.199
    STEP: Confirm that there is only one ControllerRevision 01/05/23 09:12:51.205
    Jan  5 09:12:51.205: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 09:12:51.211: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-fsvbz-daemon-set-6f8cd4bf78" 01/05/23 09:12:51.216
    Jan  5 09:12:51.227: INFO: e2e-fsvbz-daemon-set-6f8cd4bf78 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/05/23 09:12:51.227
    W0105 09:12:51.240029      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/05/23 09:12:51.24
    Jan  5 09:12:51.240: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 09:12:52.244: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 09:12:52.252: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-fsvbz-daemon-set-6f8cd4bf78=updated" 01/05/23 09:12:52.252
    STEP: Confirm that there is only one ControllerRevision 01/05/23 09:12:52.26
    Jan  5 09:12:52.260: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 09:12:52.265: INFO: Found 1 ControllerRevisions
    Jan  5 09:12:52.272: INFO: ControllerRevision "e2e-fsvbz-daemon-set-6dfb875f5" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-fsvbz-daemon-set" 01/05/23 09:12:52.277
    STEP: deleting DaemonSet.extensions e2e-fsvbz-daemon-set in namespace controllerrevisions-6113, will wait for the garbage collector to delete the pods 01/05/23 09:12:52.277
    Jan  5 09:12:52.339: INFO: Deleting DaemonSet.extensions e2e-fsvbz-daemon-set took: 7.193321ms
    Jan  5 09:12:52.440: INFO: Terminating DaemonSet.extensions e2e-fsvbz-daemon-set pods took: 100.534208ms
    Jan  5 09:12:53.845: INFO: Number of nodes with available pods controlled by daemonset e2e-fsvbz-daemon-set: 0
    Jan  5 09:12:53.845: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-fsvbz-daemon-set
    Jan  5 09:12:53.851: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8693"},"items":null}

    Jan  5 09:12:53.856: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8693"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:12:53.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-6113" for this suite. 01/05/23 09:12:53.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:12:53.911
Jan  5 09:12:53.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename crd-watch 01/05/23 09:12:53.911
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:53.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:53.937
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan  5 09:12:53.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Creating first CR  01/05/23 09:12:56.504
Jan  5 09:12:56.512: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T09:12:56Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T09:12:56Z]] name:name1 resourceVersion:8715 uid:62a29ee8-3f66-4836-b16d-9f43e2a77e76] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/05/23 09:13:06.512
Jan  5 09:13:06.519: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T09:13:06Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T09:13:06Z]] name:name2 resourceVersion:8767 uid:eeaa2062-540a-4e66-be66-c8182b569627] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/05/23 09:13:16.519
Jan  5 09:13:16.534: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T09:12:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T09:13:16Z]] name:name1 resourceVersion:8816 uid:62a29ee8-3f66-4836-b16d-9f43e2a77e76] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/05/23 09:13:26.534
Jan  5 09:13:26.541: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T09:13:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T09:13:26Z]] name:name2 resourceVersion:8866 uid:eeaa2062-540a-4e66-be66-c8182b569627] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/05/23 09:13:36.542
Jan  5 09:13:36.551: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T09:12:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T09:13:16Z]] name:name1 resourceVersion:8912 uid:62a29ee8-3f66-4836-b16d-9f43e2a77e76] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/05/23 09:13:46.551
Jan  5 09:13:46.558: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T09:13:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T09:13:26Z]] name:name2 resourceVersion:8959 uid:eeaa2062-540a-4e66-be66-c8182b569627] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:13:57.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8952" for this suite. 01/05/23 09:13:57.082
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":64,"skipped":1082,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.179 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:12:53.911
    Jan  5 09:12:53.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename crd-watch 01/05/23 09:12:53.911
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:12:53.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:12:53.937
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan  5 09:12:53.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Creating first CR  01/05/23 09:12:56.504
    Jan  5 09:12:56.512: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T09:12:56Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T09:12:56Z]] name:name1 resourceVersion:8715 uid:62a29ee8-3f66-4836-b16d-9f43e2a77e76] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/05/23 09:13:06.512
    Jan  5 09:13:06.519: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T09:13:06Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T09:13:06Z]] name:name2 resourceVersion:8767 uid:eeaa2062-540a-4e66-be66-c8182b569627] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/05/23 09:13:16.519
    Jan  5 09:13:16.534: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T09:12:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T09:13:16Z]] name:name1 resourceVersion:8816 uid:62a29ee8-3f66-4836-b16d-9f43e2a77e76] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/05/23 09:13:26.534
    Jan  5 09:13:26.541: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T09:13:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T09:13:26Z]] name:name2 resourceVersion:8866 uid:eeaa2062-540a-4e66-be66-c8182b569627] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/05/23 09:13:36.542
    Jan  5 09:13:36.551: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T09:12:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T09:13:16Z]] name:name1 resourceVersion:8912 uid:62a29ee8-3f66-4836-b16d-9f43e2a77e76] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/05/23 09:13:46.551
    Jan  5 09:13:46.558: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T09:13:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T09:13:26Z]] name:name2 resourceVersion:8959 uid:eeaa2062-540a-4e66-be66-c8182b569627] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:13:57.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-8952" for this suite. 01/05/23 09:13:57.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:13:57.091
Jan  5 09:13:57.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename replication-controller 01/05/23 09:13:57.092
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:13:57.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:13:57.114
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jan  5 09:13:57.120: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/05/23 09:13:58.137
STEP: Checking rc "condition-test" has the desired failure condition set 01/05/23 09:13:58.144
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/05/23 09:13:59.155
Jan  5 09:13:59.188: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/05/23 09:13:59.188
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  5 09:14:00.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8497" for this suite. 01/05/23 09:14:00.214
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":65,"skipped":1125,"failed":0}
------------------------------
â€¢ [3.129 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:13:57.091
    Jan  5 09:13:57.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename replication-controller 01/05/23 09:13:57.092
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:13:57.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:13:57.114
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jan  5 09:13:57.120: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/05/23 09:13:58.137
    STEP: Checking rc "condition-test" has the desired failure condition set 01/05/23 09:13:58.144
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/05/23 09:13:59.155
    Jan  5 09:13:59.188: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/05/23 09:13:59.188
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  5 09:14:00.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8497" for this suite. 01/05/23 09:14:00.214
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:14:00.22
Jan  5 09:14:00.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:14:00.221
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:00.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:00.245
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 01/05/23 09:14:00.252
Jan  5 09:14:00.262: INFO: Waiting up to 5m0s for pod "downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2" in namespace "downward-api-6580" to be "Succeeded or Failed"
Jan  5 09:14:00.267: INFO: Pod "downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.392659ms
Jan  5 09:14:02.275: INFO: Pod "downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013162762s
Jan  5 09:14:04.273: INFO: Pod "downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011306596s
STEP: Saw pod success 01/05/23 09:14:04.273
Jan  5 09:14:04.273: INFO: Pod "downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2" satisfied condition "Succeeded or Failed"
Jan  5 09:14:04.278: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2 container dapi-container: <nil>
STEP: delete the pod 01/05/23 09:14:04.294
Jan  5 09:14:04.306: INFO: Waiting for pod downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2 to disappear
Jan  5 09:14:04.309: INFO: Pod downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  5 09:14:04.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6580" for this suite. 01/05/23 09:14:04.318
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":66,"skipped":1126,"failed":0}
------------------------------
â€¢ [4.104 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:14:00.22
    Jan  5 09:14:00.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:14:00.221
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:00.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:00.245
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 01/05/23 09:14:00.252
    Jan  5 09:14:00.262: INFO: Waiting up to 5m0s for pod "downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2" in namespace "downward-api-6580" to be "Succeeded or Failed"
    Jan  5 09:14:00.267: INFO: Pod "downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.392659ms
    Jan  5 09:14:02.275: INFO: Pod "downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013162762s
    Jan  5 09:14:04.273: INFO: Pod "downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011306596s
    STEP: Saw pod success 01/05/23 09:14:04.273
    Jan  5 09:14:04.273: INFO: Pod "downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2" satisfied condition "Succeeded or Failed"
    Jan  5 09:14:04.278: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 09:14:04.294
    Jan  5 09:14:04.306: INFO: Waiting for pod downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2 to disappear
    Jan  5 09:14:04.309: INFO: Pod downward-api-12938083-d5b4-4fd6-8e44-8a6348704de2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  5 09:14:04.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6580" for this suite. 01/05/23 09:14:04.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:14:04.325
Jan  5 09:14:04.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename subpath 01/05/23 09:14:04.326
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:04.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:04.355
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 09:14:04.362
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-4rmh 01/05/23 09:14:04.372
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 09:14:04.372
Jan  5 09:14:04.383: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4rmh" in namespace "subpath-1639" to be "Succeeded or Failed"
Jan  5 09:14:04.398: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Pending", Reason="", readiness=false. Elapsed: 14.972801ms
Jan  5 09:14:06.407: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 2.024123905s
Jan  5 09:14:08.406: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 4.023044643s
Jan  5 09:14:10.403: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 6.020786445s
Jan  5 09:14:12.404: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 8.020868802s
Jan  5 09:14:14.406: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 10.022917003s
Jan  5 09:14:16.404: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 12.021273353s
Jan  5 09:14:18.406: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 14.023001966s
Jan  5 09:14:20.404: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 16.021540544s
Jan  5 09:14:22.405: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 18.022399211s
Jan  5 09:14:24.406: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 20.022832883s
Jan  5 09:14:26.405: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=false. Elapsed: 22.022710667s
Jan  5 09:14:28.405: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.022719789s
STEP: Saw pod success 01/05/23 09:14:28.405
Jan  5 09:14:28.406: INFO: Pod "pod-subpath-test-configmap-4rmh" satisfied condition "Succeeded or Failed"
Jan  5 09:14:28.411: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-subpath-test-configmap-4rmh container test-container-subpath-configmap-4rmh: <nil>
STEP: delete the pod 01/05/23 09:14:28.427
Jan  5 09:14:28.443: INFO: Waiting for pod pod-subpath-test-configmap-4rmh to disappear
Jan  5 09:14:28.446: INFO: Pod pod-subpath-test-configmap-4rmh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4rmh 01/05/23 09:14:28.446
Jan  5 09:14:28.447: INFO: Deleting pod "pod-subpath-test-configmap-4rmh" in namespace "subpath-1639"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  5 09:14:28.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1639" for this suite. 01/05/23 09:14:28.46
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":67,"skipped":1139,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.142 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:14:04.325
    Jan  5 09:14:04.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename subpath 01/05/23 09:14:04.326
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:04.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:04.355
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 09:14:04.362
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-4rmh 01/05/23 09:14:04.372
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 09:14:04.372
    Jan  5 09:14:04.383: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4rmh" in namespace "subpath-1639" to be "Succeeded or Failed"
    Jan  5 09:14:04.398: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Pending", Reason="", readiness=false. Elapsed: 14.972801ms
    Jan  5 09:14:06.407: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 2.024123905s
    Jan  5 09:14:08.406: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 4.023044643s
    Jan  5 09:14:10.403: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 6.020786445s
    Jan  5 09:14:12.404: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 8.020868802s
    Jan  5 09:14:14.406: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 10.022917003s
    Jan  5 09:14:16.404: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 12.021273353s
    Jan  5 09:14:18.406: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 14.023001966s
    Jan  5 09:14:20.404: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 16.021540544s
    Jan  5 09:14:22.405: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 18.022399211s
    Jan  5 09:14:24.406: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=true. Elapsed: 20.022832883s
    Jan  5 09:14:26.405: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Running", Reason="", readiness=false. Elapsed: 22.022710667s
    Jan  5 09:14:28.405: INFO: Pod "pod-subpath-test-configmap-4rmh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.022719789s
    STEP: Saw pod success 01/05/23 09:14:28.405
    Jan  5 09:14:28.406: INFO: Pod "pod-subpath-test-configmap-4rmh" satisfied condition "Succeeded or Failed"
    Jan  5 09:14:28.411: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-subpath-test-configmap-4rmh container test-container-subpath-configmap-4rmh: <nil>
    STEP: delete the pod 01/05/23 09:14:28.427
    Jan  5 09:14:28.443: INFO: Waiting for pod pod-subpath-test-configmap-4rmh to disappear
    Jan  5 09:14:28.446: INFO: Pod pod-subpath-test-configmap-4rmh no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-4rmh 01/05/23 09:14:28.446
    Jan  5 09:14:28.447: INFO: Deleting pod "pod-subpath-test-configmap-4rmh" in namespace "subpath-1639"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  5 09:14:28.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1639" for this suite. 01/05/23 09:14:28.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:14:28.47
Jan  5 09:14:28.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename replication-controller 01/05/23 09:14:28.47
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:28.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:28.494
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15 01/05/23 09:14:28.501
Jan  5 09:14:28.512: INFO: Pod name my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15: Found 0 pods out of 1
Jan  5 09:14:33.520: INFO: Pod name my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15: Found 1 pods out of 1
Jan  5 09:14:33.520: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15" are running
Jan  5 09:14:33.520: INFO: Waiting up to 5m0s for pod "my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15-xk4jc" in namespace "replication-controller-1394" to be "running"
Jan  5 09:14:33.525: INFO: Pod "my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15-xk4jc": Phase="Running", Reason="", readiness=true. Elapsed: 4.97868ms
Jan  5 09:14:33.525: INFO: Pod "my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15-xk4jc" satisfied condition "running"
Jan  5 09:14:33.525: INFO: Pod "my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15-xk4jc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:14:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:14:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:14:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:14:28 +0000 UTC Reason: Message:}])
Jan  5 09:14:33.525: INFO: Trying to dial the pod
Jan  5 09:14:38.648: INFO: Controller my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15: Got expected result from replica 1 [my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15-xk4jc]: "my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15-xk4jc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  5 09:14:38.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1394" for this suite. 01/05/23 09:14:38.657
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":68,"skipped":1167,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.193 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:14:28.47
    Jan  5 09:14:28.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename replication-controller 01/05/23 09:14:28.47
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:28.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:28.494
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15 01/05/23 09:14:28.501
    Jan  5 09:14:28.512: INFO: Pod name my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15: Found 0 pods out of 1
    Jan  5 09:14:33.520: INFO: Pod name my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15: Found 1 pods out of 1
    Jan  5 09:14:33.520: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15" are running
    Jan  5 09:14:33.520: INFO: Waiting up to 5m0s for pod "my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15-xk4jc" in namespace "replication-controller-1394" to be "running"
    Jan  5 09:14:33.525: INFO: Pod "my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15-xk4jc": Phase="Running", Reason="", readiness=true. Elapsed: 4.97868ms
    Jan  5 09:14:33.525: INFO: Pod "my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15-xk4jc" satisfied condition "running"
    Jan  5 09:14:33.525: INFO: Pod "my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15-xk4jc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:14:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:14:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:14:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:14:28 +0000 UTC Reason: Message:}])
    Jan  5 09:14:33.525: INFO: Trying to dial the pod
    Jan  5 09:14:38.648: INFO: Controller my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15: Got expected result from replica 1 [my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15-xk4jc]: "my-hostname-basic-b9395285-8ec8-473f-9164-f899b0955f15-xk4jc", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  5 09:14:38.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1394" for this suite. 01/05/23 09:14:38.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:14:38.666
Jan  5 09:14:38.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-runtime 01/05/23 09:14:38.667
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:38.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:38.691
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 01/05/23 09:14:38.707
STEP: wait for the container to reach Succeeded 01/05/23 09:14:38.724
STEP: get the container status 01/05/23 09:14:42.753
STEP: the container should be terminated 01/05/23 09:14:42.758
STEP: the termination message should be set 01/05/23 09:14:42.758
Jan  5 09:14:42.758: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/05/23 09:14:42.758
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  5 09:14:42.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8838" for this suite. 01/05/23 09:14:42.786
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":69,"skipped":1200,"failed":0}
------------------------------
â€¢ [4.134 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:14:38.666
    Jan  5 09:14:38.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-runtime 01/05/23 09:14:38.667
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:38.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:38.691
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 01/05/23 09:14:38.707
    STEP: wait for the container to reach Succeeded 01/05/23 09:14:38.724
    STEP: get the container status 01/05/23 09:14:42.753
    STEP: the container should be terminated 01/05/23 09:14:42.758
    STEP: the termination message should be set 01/05/23 09:14:42.758
    Jan  5 09:14:42.758: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/05/23 09:14:42.758
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  5 09:14:42.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8838" for this suite. 01/05/23 09:14:42.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:14:42.801
Jan  5 09:14:42.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename init-container 01/05/23 09:14:42.802
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:42.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:42.843
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 01/05/23 09:14:42.85
Jan  5 09:14:42.850: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 09:14:47.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9211" for this suite. 01/05/23 09:14:47.259
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":70,"skipped":1207,"failed":0}
------------------------------
â€¢ [4.465 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:14:42.801
    Jan  5 09:14:42.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename init-container 01/05/23 09:14:42.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:42.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:42.843
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 01/05/23 09:14:42.85
    Jan  5 09:14:42.850: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 09:14:47.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9211" for this suite. 01/05/23 09:14:47.259
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:14:47.266
Jan  5 09:14:47.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 09:14:47.267
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:47.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:47.29
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-9ad34351-f5ba-41be-8c1f-927ede1be8ba 01/05/23 09:14:47.296
STEP: Creating a pod to test consume secrets 01/05/23 09:14:47.303
Jan  5 09:14:47.314: INFO: Waiting up to 5m0s for pod "pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45" in namespace "secrets-5724" to be "Succeeded or Failed"
Jan  5 09:14:47.319: INFO: Pod "pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45": Phase="Pending", Reason="", readiness=false. Elapsed: 5.180706ms
Jan  5 09:14:49.326: INFO: Pod "pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012334033s
Jan  5 09:14:51.325: INFO: Pod "pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011110798s
STEP: Saw pod success 01/05/23 09:14:51.325
Jan  5 09:14:51.325: INFO: Pod "pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45" satisfied condition "Succeeded or Failed"
Jan  5 09:14:51.329: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 09:14:51.354
Jan  5 09:14:51.367: INFO: Waiting for pod pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45 to disappear
Jan  5 09:14:51.371: INFO: Pod pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 09:14:51.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5724" for this suite. 01/05/23 09:14:51.381
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":71,"skipped":1207,"failed":0}
------------------------------
â€¢ [4.122 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:14:47.266
    Jan  5 09:14:47.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 09:14:47.267
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:47.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:47.29
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-9ad34351-f5ba-41be-8c1f-927ede1be8ba 01/05/23 09:14:47.296
    STEP: Creating a pod to test consume secrets 01/05/23 09:14:47.303
    Jan  5 09:14:47.314: INFO: Waiting up to 5m0s for pod "pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45" in namespace "secrets-5724" to be "Succeeded or Failed"
    Jan  5 09:14:47.319: INFO: Pod "pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45": Phase="Pending", Reason="", readiness=false. Elapsed: 5.180706ms
    Jan  5 09:14:49.326: INFO: Pod "pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012334033s
    Jan  5 09:14:51.325: INFO: Pod "pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011110798s
    STEP: Saw pod success 01/05/23 09:14:51.325
    Jan  5 09:14:51.325: INFO: Pod "pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45" satisfied condition "Succeeded or Failed"
    Jan  5 09:14:51.329: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 09:14:51.354
    Jan  5 09:14:51.367: INFO: Waiting for pod pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45 to disappear
    Jan  5 09:14:51.371: INFO: Pod pod-secrets-31d8e6a3-054c-4fd8-9781-529530bb6c45 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 09:14:51.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5724" for this suite. 01/05/23 09:14:51.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:14:51.39
Jan  5 09:14:51.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename job 01/05/23 09:14:51.391
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:51.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:51.415
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 01/05/23 09:14:51.422
STEP: Ensuring active pods == parallelism 01/05/23 09:14:51.428
STEP: delete a job 01/05/23 09:14:53.435
STEP: deleting Job.batch foo in namespace job-6176, will wait for the garbage collector to delete the pods 01/05/23 09:14:53.435
Jan  5 09:14:53.498: INFO: Deleting Job.batch foo took: 7.618697ms
Jan  5 09:14:53.599: INFO: Terminating Job.batch foo pods took: 100.617512ms
STEP: Ensuring job was deleted 01/05/23 09:15:26.2
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  5 09:15:26.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6176" for this suite. 01/05/23 09:15:26.216
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":72,"skipped":1241,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.831 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:14:51.39
    Jan  5 09:14:51.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename job 01/05/23 09:14:51.391
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:14:51.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:14:51.415
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 01/05/23 09:14:51.422
    STEP: Ensuring active pods == parallelism 01/05/23 09:14:51.428
    STEP: delete a job 01/05/23 09:14:53.435
    STEP: deleting Job.batch foo in namespace job-6176, will wait for the garbage collector to delete the pods 01/05/23 09:14:53.435
    Jan  5 09:14:53.498: INFO: Deleting Job.batch foo took: 7.618697ms
    Jan  5 09:14:53.599: INFO: Terminating Job.batch foo pods took: 100.617512ms
    STEP: Ensuring job was deleted 01/05/23 09:15:26.2
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  5 09:15:26.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6176" for this suite. 01/05/23 09:15:26.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:15:26.224
Jan  5 09:15:26.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 09:15:26.225
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:15:26.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:15:26.25
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-938bb64d-cc41-4cb7-83b4-53ab5d0e752d 01/05/23 09:15:26.257
STEP: Creating a pod to test consume configMaps 01/05/23 09:15:26.261
Jan  5 09:15:26.272: INFO: Waiting up to 5m0s for pod "pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a" in namespace "configmap-4368" to be "Succeeded or Failed"
Jan  5 09:15:26.277: INFO: Pod "pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.655706ms
Jan  5 09:15:28.284: INFO: Pod "pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a": Phase="Running", Reason="", readiness=false. Elapsed: 2.012077092s
Jan  5 09:15:30.285: INFO: Pod "pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013004732s
STEP: Saw pod success 01/05/23 09:15:30.285
Jan  5 09:15:30.285: INFO: Pod "pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a" satisfied condition "Succeeded or Failed"
Jan  5 09:15:30.290: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a container agnhost-container: <nil>
STEP: delete the pod 01/05/23 09:15:30.302
Jan  5 09:15:30.316: INFO: Waiting for pod pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a to disappear
Jan  5 09:15:30.321: INFO: Pod pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 09:15:30.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4368" for this suite. 01/05/23 09:15:30.331
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":73,"skipped":1261,"failed":0}
------------------------------
â€¢ [4.113 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:15:26.224
    Jan  5 09:15:26.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 09:15:26.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:15:26.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:15:26.25
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-938bb64d-cc41-4cb7-83b4-53ab5d0e752d 01/05/23 09:15:26.257
    STEP: Creating a pod to test consume configMaps 01/05/23 09:15:26.261
    Jan  5 09:15:26.272: INFO: Waiting up to 5m0s for pod "pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a" in namespace "configmap-4368" to be "Succeeded or Failed"
    Jan  5 09:15:26.277: INFO: Pod "pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.655706ms
    Jan  5 09:15:28.284: INFO: Pod "pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a": Phase="Running", Reason="", readiness=false. Elapsed: 2.012077092s
    Jan  5 09:15:30.285: INFO: Pod "pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013004732s
    STEP: Saw pod success 01/05/23 09:15:30.285
    Jan  5 09:15:30.285: INFO: Pod "pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a" satisfied condition "Succeeded or Failed"
    Jan  5 09:15:30.290: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 09:15:30.302
    Jan  5 09:15:30.316: INFO: Waiting for pod pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a to disappear
    Jan  5 09:15:30.321: INFO: Pod pod-configmaps-da27490c-ed68-4ed3-943d-eb483fd0977a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 09:15:30.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4368" for this suite. 01/05/23 09:15:30.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:15:30.337
Jan  5 09:15:30.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 09:15:30.338
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:15:30.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:15:30.358
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-6045 01/05/23 09:15:30.365
STEP: creating service affinity-clusterip-transition in namespace services-6045 01/05/23 09:15:30.365
STEP: creating replication controller affinity-clusterip-transition in namespace services-6045 01/05/23 09:15:30.387
I0105 09:15:30.437602      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6045, replica count: 3
I0105 09:15:33.488140      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 09:15:33.501: INFO: Creating new exec pod
Jan  5 09:15:33.511: INFO: Waiting up to 5m0s for pod "execpod-affinityfthvt" in namespace "services-6045" to be "running"
Jan  5 09:15:33.517: INFO: Pod "execpod-affinityfthvt": Phase="Pending", Reason="", readiness=false. Elapsed: 5.913056ms
Jan  5 09:15:35.547: INFO: Pod "execpod-affinityfthvt": Phase="Running", Reason="", readiness=true. Elapsed: 2.03604131s
Jan  5 09:15:35.548: INFO: Pod "execpod-affinityfthvt" satisfied condition "running"
Jan  5 09:15:36.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6045 exec execpod-affinityfthvt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan  5 09:15:37.071: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan  5 09:15:37.071: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:15:37.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6045 exec execpod-affinityfthvt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.7.117 80'
Jan  5 09:15:37.597: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.7.117 80\nConnection to 10.113.7.117 80 port [tcp/http] succeeded!\n"
Jan  5 09:15:37.597: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:15:37.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6045 exec execpod-affinityfthvt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.113.7.117:80/ ; done'
Jan  5 09:15:38.129: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n"
Jan  5 09:15:38.129: INFO: stdout: "\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-rjwrp\naffinity-clusterip-transition-p4clj\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-p4clj\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-rjwrp\naffinity-clusterip-transition-p4clj\naffinity-clusterip-transition-p4clj\naffinity-clusterip-transition-rjwrp\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-p4clj\naffinity-clusterip-transition-f9m4q"
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-rjwrp
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-p4clj
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-p4clj
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-rjwrp
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-p4clj
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-p4clj
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-rjwrp
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-p4clj
Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6045 exec execpod-affinityfthvt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.113.7.117:80/ ; done'
Jan  5 09:15:38.748: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n"
Jan  5 09:15:38.748: INFO: stdout: "\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q"
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
Jan  5 09:15:38.748: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6045, will wait for the garbage collector to delete the pods 01/05/23 09:15:38.761
Jan  5 09:15:38.823: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.239145ms
Jan  5 09:15:38.924: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.001492ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:15:41.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6045" for this suite. 01/05/23 09:15:41.246
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":74,"skipped":1267,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.915 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:15:30.337
    Jan  5 09:15:30.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 09:15:30.338
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:15:30.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:15:30.358
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-6045 01/05/23 09:15:30.365
    STEP: creating service affinity-clusterip-transition in namespace services-6045 01/05/23 09:15:30.365
    STEP: creating replication controller affinity-clusterip-transition in namespace services-6045 01/05/23 09:15:30.387
    I0105 09:15:30.437602      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6045, replica count: 3
    I0105 09:15:33.488140      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 09:15:33.501: INFO: Creating new exec pod
    Jan  5 09:15:33.511: INFO: Waiting up to 5m0s for pod "execpod-affinityfthvt" in namespace "services-6045" to be "running"
    Jan  5 09:15:33.517: INFO: Pod "execpod-affinityfthvt": Phase="Pending", Reason="", readiness=false. Elapsed: 5.913056ms
    Jan  5 09:15:35.547: INFO: Pod "execpod-affinityfthvt": Phase="Running", Reason="", readiness=true. Elapsed: 2.03604131s
    Jan  5 09:15:35.548: INFO: Pod "execpod-affinityfthvt" satisfied condition "running"
    Jan  5 09:15:36.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6045 exec execpod-affinityfthvt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jan  5 09:15:37.071: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan  5 09:15:37.071: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:15:37.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6045 exec execpod-affinityfthvt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.7.117 80'
    Jan  5 09:15:37.597: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.7.117 80\nConnection to 10.113.7.117 80 port [tcp/http] succeeded!\n"
    Jan  5 09:15:37.597: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:15:37.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6045 exec execpod-affinityfthvt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.113.7.117:80/ ; done'
    Jan  5 09:15:38.129: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n"
    Jan  5 09:15:38.129: INFO: stdout: "\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-rjwrp\naffinity-clusterip-transition-p4clj\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-p4clj\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-rjwrp\naffinity-clusterip-transition-p4clj\naffinity-clusterip-transition-p4clj\naffinity-clusterip-transition-rjwrp\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-p4clj\naffinity-clusterip-transition-f9m4q"
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-rjwrp
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-p4clj
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-p4clj
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-rjwrp
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-p4clj
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-p4clj
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-rjwrp
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-p4clj
    Jan  5 09:15:38.129: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6045 exec execpod-affinityfthvt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.113.7.117:80/ ; done'
    Jan  5 09:15:38.748: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.7.117:80/\n"
    Jan  5 09:15:38.748: INFO: stdout: "\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q\naffinity-clusterip-transition-f9m4q"
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Received response from host: affinity-clusterip-transition-f9m4q
    Jan  5 09:15:38.748: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6045, will wait for the garbage collector to delete the pods 01/05/23 09:15:38.761
    Jan  5 09:15:38.823: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.239145ms
    Jan  5 09:15:38.924: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.001492ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:15:41.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6045" for this suite. 01/05/23 09:15:41.246
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:15:41.253
Jan  5 09:15:41.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename security-context-test 01/05/23 09:15:41.254
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:15:41.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:15:41.276
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jan  5 09:15:41.294: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b6dedd00-7a1e-4bfd-a571-5370daa70b6e" in namespace "security-context-test-8292" to be "Succeeded or Failed"
Jan  5 09:15:41.301: INFO: Pod "busybox-privileged-false-b6dedd00-7a1e-4bfd-a571-5370daa70b6e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.192465ms
Jan  5 09:15:43.313: INFO: Pod "busybox-privileged-false-b6dedd00-7a1e-4bfd-a571-5370daa70b6e": Phase="Running", Reason="", readiness=false. Elapsed: 2.018365495s
Jan  5 09:15:45.309: INFO: Pod "busybox-privileged-false-b6dedd00-7a1e-4bfd-a571-5370daa70b6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015299144s
Jan  5 09:15:45.310: INFO: Pod "busybox-privileged-false-b6dedd00-7a1e-4bfd-a571-5370daa70b6e" satisfied condition "Succeeded or Failed"
Jan  5 09:15:45.324: INFO: Got logs for pod "busybox-privileged-false-b6dedd00-7a1e-4bfd-a571-5370daa70b6e": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  5 09:15:45.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8292" for this suite. 01/05/23 09:15:45.333
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":75,"skipped":1282,"failed":0}
------------------------------
â€¢ [4.086 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:15:41.253
    Jan  5 09:15:41.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename security-context-test 01/05/23 09:15:41.254
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:15:41.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:15:41.276
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jan  5 09:15:41.294: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b6dedd00-7a1e-4bfd-a571-5370daa70b6e" in namespace "security-context-test-8292" to be "Succeeded or Failed"
    Jan  5 09:15:41.301: INFO: Pod "busybox-privileged-false-b6dedd00-7a1e-4bfd-a571-5370daa70b6e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.192465ms
    Jan  5 09:15:43.313: INFO: Pod "busybox-privileged-false-b6dedd00-7a1e-4bfd-a571-5370daa70b6e": Phase="Running", Reason="", readiness=false. Elapsed: 2.018365495s
    Jan  5 09:15:45.309: INFO: Pod "busybox-privileged-false-b6dedd00-7a1e-4bfd-a571-5370daa70b6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015299144s
    Jan  5 09:15:45.310: INFO: Pod "busybox-privileged-false-b6dedd00-7a1e-4bfd-a571-5370daa70b6e" satisfied condition "Succeeded or Failed"
    Jan  5 09:15:45.324: INFO: Got logs for pod "busybox-privileged-false-b6dedd00-7a1e-4bfd-a571-5370daa70b6e": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  5 09:15:45.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-8292" for this suite. 01/05/23 09:15:45.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:15:45.344
Jan  5 09:15:45.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename security-context-test 01/05/23 09:15:45.345
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:15:45.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:15:45.368
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jan  5 09:15:45.384: INFO: Waiting up to 5m0s for pod "busybox-user-65534-e25eaedd-98d0-467c-86a0-a95e2b7b347a" in namespace "security-context-test-1578" to be "Succeeded or Failed"
Jan  5 09:15:45.390: INFO: Pod "busybox-user-65534-e25eaedd-98d0-467c-86a0-a95e2b7b347a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.730367ms
Jan  5 09:15:47.402: INFO: Pod "busybox-user-65534-e25eaedd-98d0-467c-86a0-a95e2b7b347a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017822546s
Jan  5 09:15:49.398: INFO: Pod "busybox-user-65534-e25eaedd-98d0-467c-86a0-a95e2b7b347a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013607713s
Jan  5 09:15:49.398: INFO: Pod "busybox-user-65534-e25eaedd-98d0-467c-86a0-a95e2b7b347a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  5 09:15:49.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1578" for this suite. 01/05/23 09:15:49.41
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":76,"skipped":1361,"failed":0}
------------------------------
â€¢ [4.074 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:15:45.344
    Jan  5 09:15:45.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename security-context-test 01/05/23 09:15:45.345
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:15:45.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:15:45.368
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jan  5 09:15:45.384: INFO: Waiting up to 5m0s for pod "busybox-user-65534-e25eaedd-98d0-467c-86a0-a95e2b7b347a" in namespace "security-context-test-1578" to be "Succeeded or Failed"
    Jan  5 09:15:45.390: INFO: Pod "busybox-user-65534-e25eaedd-98d0-467c-86a0-a95e2b7b347a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.730367ms
    Jan  5 09:15:47.402: INFO: Pod "busybox-user-65534-e25eaedd-98d0-467c-86a0-a95e2b7b347a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017822546s
    Jan  5 09:15:49.398: INFO: Pod "busybox-user-65534-e25eaedd-98d0-467c-86a0-a95e2b7b347a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013607713s
    Jan  5 09:15:49.398: INFO: Pod "busybox-user-65534-e25eaedd-98d0-467c-86a0-a95e2b7b347a" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  5 09:15:49.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1578" for this suite. 01/05/23 09:15:49.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:15:49.421
Jan  5 09:15:49.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename statefulset 01/05/23 09:15:49.422
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:15:49.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:15:49.448
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4581 01/05/23 09:15:49.455
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 01/05/23 09:15:49.462
STEP: Creating stateful set ss in namespace statefulset-4581 01/05/23 09:15:49.471
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4581 01/05/23 09:15:49.478
Jan  5 09:15:49.488: INFO: Found 0 stateful pods, waiting for 1
Jan  5 09:15:59.497: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/05/23 09:15:59.497
Jan  5 09:15:59.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 09:16:00.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 09:16:00.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 09:16:00.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 09:16:00.032: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  5 09:16:10.039: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 09:16:10.039: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 09:16:10.060: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999986s
Jan  5 09:16:11.068: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99341158s
Jan  5 09:16:12.076: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985927641s
Jan  5 09:16:13.082: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.978122353s
Jan  5 09:16:14.091: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.972159839s
Jan  5 09:16:15.097: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.963221628s
Jan  5 09:16:16.104: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.956623753s
Jan  5 09:16:17.111: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.949612788s
Jan  5 09:16:18.118: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.941937443s
Jan  5 09:16:19.125: INFO: Verifying statefulset ss doesn't scale past 1 for another 935.849644ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4581 01/05/23 09:16:20.125
Jan  5 09:16:20.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:16:20.600: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 09:16:20.600: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 09:16:20.600: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 09:16:20.605: INFO: Found 1 stateful pods, waiting for 3
Jan  5 09:16:30.617: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 09:16:30.617: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 09:16:30.617: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/05/23 09:16:30.617
STEP: Scale down will halt with unhealthy stateful pod 01/05/23 09:16:30.617
Jan  5 09:16:30.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 09:16:31.079: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 09:16:31.079: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 09:16:31.079: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 09:16:31.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 09:16:31.597: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 09:16:31.597: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 09:16:31.597: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 09:16:31.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 09:16:32.113: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 09:16:32.113: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 09:16:32.113: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 09:16:32.113: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 09:16:32.118: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan  5 09:16:42.132: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 09:16:42.132: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 09:16:42.132: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 09:16:42.153: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999859s
Jan  5 09:16:43.160: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989710147s
Jan  5 09:16:44.167: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982955988s
Jan  5 09:16:45.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976542069s
Jan  5 09:16:46.186: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964895594s
Jan  5 09:16:47.193: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957519424s
Jan  5 09:16:48.202: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95019319s
Jan  5 09:16:49.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.94101426s
Jan  5 09:16:50.217: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.933898857s
Jan  5 09:16:51.225: INFO: Verifying statefulset ss doesn't scale past 3 for another 925.070559ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4581 01/05/23 09:16:52.225
Jan  5 09:16:52.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:16:52.752: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 09:16:52.752: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 09:16:52.752: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 09:16:52.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:16:53.155: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 09:16:53.155: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 09:16:53.155: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 09:16:53.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:16:53.630: INFO: rc: 1
Jan  5 09:16:53.630: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: failed to exec in container: container is in CONTAINER_EXITED state

error:
exit status 1
Jan  5 09:17:03.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:17:03.701: INFO: rc: 1
Jan  5 09:17:03.701: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:17:13.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:17:13.766: INFO: rc: 1
Jan  5 09:17:13.766: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:17:23.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:17:23.843: INFO: rc: 1
Jan  5 09:17:23.843: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:17:33.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:17:33.920: INFO: rc: 1
Jan  5 09:17:33.920: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:17:43.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:17:43.990: INFO: rc: 1
Jan  5 09:17:43.990: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:17:53.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:17:54.060: INFO: rc: 1
Jan  5 09:17:54.060: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:18:04.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:18:04.134: INFO: rc: 1
Jan  5 09:18:04.134: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:18:14.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:18:14.207: INFO: rc: 1
Jan  5 09:18:14.207: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:18:24.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:18:24.270: INFO: rc: 1
Jan  5 09:18:24.270: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:18:34.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:18:34.337: INFO: rc: 1
Jan  5 09:18:34.337: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:18:44.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:18:44.406: INFO: rc: 1
Jan  5 09:18:44.406: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:18:54.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:18:54.468: INFO: rc: 1
Jan  5 09:18:54.468: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:19:04.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:19:04.543: INFO: rc: 1
Jan  5 09:19:04.543: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:19:14.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:19:14.616: INFO: rc: 1
Jan  5 09:19:14.616: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:19:24.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:19:24.689: INFO: rc: 1
Jan  5 09:19:24.689: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:19:34.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:19:34.754: INFO: rc: 1
Jan  5 09:19:34.754: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:19:44.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:19:44.825: INFO: rc: 1
Jan  5 09:19:44.825: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:19:54.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:19:54.893: INFO: rc: 1
Jan  5 09:19:54.893: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:20:04.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:20:04.966: INFO: rc: 1
Jan  5 09:20:04.966: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:20:14.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:20:15.032: INFO: rc: 1
Jan  5 09:20:15.032: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:20:25.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:20:25.104: INFO: rc: 1
Jan  5 09:20:25.104: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:20:35.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:20:35.179: INFO: rc: 1
Jan  5 09:20:35.179: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:20:45.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:20:45.246: INFO: rc: 1
Jan  5 09:20:45.246: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:20:55.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:20:55.316: INFO: rc: 1
Jan  5 09:20:55.316: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:21:05.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:21:05.383: INFO: rc: 1
Jan  5 09:21:05.383: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:21:15.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:21:15.457: INFO: rc: 1
Jan  5 09:21:15.457: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:21:25.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:21:25.524: INFO: rc: 1
Jan  5 09:21:25.524: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:21:35.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:21:35.591: INFO: rc: 1
Jan  5 09:21:35.591: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:21:45.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:21:45.665: INFO: rc: 1
Jan  5 09:21:45.665: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan  5 09:21:55.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:21:55.746: INFO: rc: 1
Jan  5 09:21:55.746: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Jan  5 09:21:55.746: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/05/23 09:21:55.764
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 09:21:55.764: INFO: Deleting all statefulset in ns statefulset-4581
Jan  5 09:21:55.769: INFO: Scaling statefulset ss to 0
Jan  5 09:21:55.783: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 09:21:55.787: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 09:21:55.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4581" for this suite. 01/05/23 09:21:55.814
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":77,"skipped":1399,"failed":0}
------------------------------
â€¢ [SLOW TEST] [366.401 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:15:49.421
    Jan  5 09:15:49.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename statefulset 01/05/23 09:15:49.422
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:15:49.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:15:49.448
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4581 01/05/23 09:15:49.455
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/05/23 09:15:49.462
    STEP: Creating stateful set ss in namespace statefulset-4581 01/05/23 09:15:49.471
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4581 01/05/23 09:15:49.478
    Jan  5 09:15:49.488: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 09:15:59.497: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/05/23 09:15:59.497
    Jan  5 09:15:59.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 09:16:00.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 09:16:00.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 09:16:00.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 09:16:00.032: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan  5 09:16:10.039: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 09:16:10.039: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 09:16:10.060: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999986s
    Jan  5 09:16:11.068: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99341158s
    Jan  5 09:16:12.076: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985927641s
    Jan  5 09:16:13.082: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.978122353s
    Jan  5 09:16:14.091: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.972159839s
    Jan  5 09:16:15.097: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.963221628s
    Jan  5 09:16:16.104: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.956623753s
    Jan  5 09:16:17.111: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.949612788s
    Jan  5 09:16:18.118: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.941937443s
    Jan  5 09:16:19.125: INFO: Verifying statefulset ss doesn't scale past 1 for another 935.849644ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4581 01/05/23 09:16:20.125
    Jan  5 09:16:20.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:16:20.600: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 09:16:20.600: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 09:16:20.600: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 09:16:20.605: INFO: Found 1 stateful pods, waiting for 3
    Jan  5 09:16:30.617: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 09:16:30.617: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 09:16:30.617: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/05/23 09:16:30.617
    STEP: Scale down will halt with unhealthy stateful pod 01/05/23 09:16:30.617
    Jan  5 09:16:30.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 09:16:31.079: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 09:16:31.079: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 09:16:31.079: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 09:16:31.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 09:16:31.597: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 09:16:31.597: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 09:16:31.597: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 09:16:31.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 09:16:32.113: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 09:16:32.113: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 09:16:32.113: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 09:16:32.113: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 09:16:32.118: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan  5 09:16:42.132: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 09:16:42.132: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 09:16:42.132: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 09:16:42.153: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999859s
    Jan  5 09:16:43.160: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989710147s
    Jan  5 09:16:44.167: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982955988s
    Jan  5 09:16:45.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976542069s
    Jan  5 09:16:46.186: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964895594s
    Jan  5 09:16:47.193: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957519424s
    Jan  5 09:16:48.202: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95019319s
    Jan  5 09:16:49.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.94101426s
    Jan  5 09:16:50.217: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.933898857s
    Jan  5 09:16:51.225: INFO: Verifying statefulset ss doesn't scale past 3 for another 925.070559ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4581 01/05/23 09:16:52.225
    Jan  5 09:16:52.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:16:52.752: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 09:16:52.752: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 09:16:52.752: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 09:16:52.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:16:53.155: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 09:16:53.155: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 09:16:53.155: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 09:16:53.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:16:53.630: INFO: rc: 1
    Jan  5 09:16:53.630: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    error: Internal error occurred: error executing command in container: failed to exec in container: container is in CONTAINER_EXITED state

    error:
    exit status 1
    Jan  5 09:17:03.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:17:03.701: INFO: rc: 1
    Jan  5 09:17:03.701: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:17:13.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:17:13.766: INFO: rc: 1
    Jan  5 09:17:13.766: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:17:23.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:17:23.843: INFO: rc: 1
    Jan  5 09:17:23.843: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:17:33.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:17:33.920: INFO: rc: 1
    Jan  5 09:17:33.920: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:17:43.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:17:43.990: INFO: rc: 1
    Jan  5 09:17:43.990: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:17:53.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:17:54.060: INFO: rc: 1
    Jan  5 09:17:54.060: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:18:04.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:18:04.134: INFO: rc: 1
    Jan  5 09:18:04.134: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:18:14.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:18:14.207: INFO: rc: 1
    Jan  5 09:18:14.207: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:18:24.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:18:24.270: INFO: rc: 1
    Jan  5 09:18:24.270: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:18:34.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:18:34.337: INFO: rc: 1
    Jan  5 09:18:34.337: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:18:44.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:18:44.406: INFO: rc: 1
    Jan  5 09:18:44.406: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:18:54.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:18:54.468: INFO: rc: 1
    Jan  5 09:18:54.468: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:19:04.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:19:04.543: INFO: rc: 1
    Jan  5 09:19:04.543: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:19:14.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:19:14.616: INFO: rc: 1
    Jan  5 09:19:14.616: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:19:24.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:19:24.689: INFO: rc: 1
    Jan  5 09:19:24.689: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:19:34.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:19:34.754: INFO: rc: 1
    Jan  5 09:19:34.754: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:19:44.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:19:44.825: INFO: rc: 1
    Jan  5 09:19:44.825: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:19:54.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:19:54.893: INFO: rc: 1
    Jan  5 09:19:54.893: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:20:04.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:20:04.966: INFO: rc: 1
    Jan  5 09:20:04.966: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:20:14.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:20:15.032: INFO: rc: 1
    Jan  5 09:20:15.032: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:20:25.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:20:25.104: INFO: rc: 1
    Jan  5 09:20:25.104: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:20:35.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:20:35.179: INFO: rc: 1
    Jan  5 09:20:35.179: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:20:45.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:20:45.246: INFO: rc: 1
    Jan  5 09:20:45.246: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:20:55.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:20:55.316: INFO: rc: 1
    Jan  5 09:20:55.316: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:21:05.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:21:05.383: INFO: rc: 1
    Jan  5 09:21:05.383: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:21:15.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:21:15.457: INFO: rc: 1
    Jan  5 09:21:15.457: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:21:25.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:21:25.524: INFO: rc: 1
    Jan  5 09:21:25.524: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:21:35.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:21:35.591: INFO: rc: 1
    Jan  5 09:21:35.591: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:21:45.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:21:45.665: INFO: rc: 1
    Jan  5 09:21:45.665: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan  5 09:21:55.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4581 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:21:55.746: INFO: rc: 1
    Jan  5 09:21:55.746: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
    Jan  5 09:21:55.746: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/05/23 09:21:55.764
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 09:21:55.764: INFO: Deleting all statefulset in ns statefulset-4581
    Jan  5 09:21:55.769: INFO: Scaling statefulset ss to 0
    Jan  5 09:21:55.783: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 09:21:55.787: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 09:21:55.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4581" for this suite. 01/05/23 09:21:55.814
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:21:55.822
Jan  5 09:21:55.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 09:21:55.824
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:21:55.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:21:55.855
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 01/05/23 09:21:55.863
Jan  5 09:21:55.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 create -f -'
Jan  5 09:21:56.664: INFO: stderr: ""
Jan  5 09:21:56.664: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 09:21:56.664
Jan  5 09:21:56.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 09:21:56.733: INFO: stderr: ""
Jan  5 09:21:56.733: INFO: stdout: "update-demo-nautilus-fmx24 update-demo-nautilus-t5zh6 "
Jan  5 09:21:56.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 09:21:56.796: INFO: stderr: ""
Jan  5 09:21:56.796: INFO: stdout: ""
Jan  5 09:21:56.796: INFO: update-demo-nautilus-fmx24 is created but not running
Jan  5 09:22:01.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 09:22:01.867: INFO: stderr: ""
Jan  5 09:22:01.867: INFO: stdout: "update-demo-nautilus-fmx24 update-demo-nautilus-t5zh6 "
Jan  5 09:22:01.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 09:22:01.940: INFO: stderr: ""
Jan  5 09:22:01.940: INFO: stdout: "true"
Jan  5 09:22:01.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 09:22:02.009: INFO: stderr: ""
Jan  5 09:22:02.009: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 09:22:02.009: INFO: validating pod update-demo-nautilus-fmx24
Jan  5 09:22:02.120: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 09:22:02.120: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 09:22:02.120: INFO: update-demo-nautilus-fmx24 is verified up and running
Jan  5 09:22:02.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-t5zh6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 09:22:02.186: INFO: stderr: ""
Jan  5 09:22:02.186: INFO: stdout: ""
Jan  5 09:22:02.186: INFO: update-demo-nautilus-t5zh6 is created but not running
Jan  5 09:22:07.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 09:22:07.255: INFO: stderr: ""
Jan  5 09:22:07.255: INFO: stdout: "update-demo-nautilus-fmx24 update-demo-nautilus-t5zh6 "
Jan  5 09:22:07.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 09:22:07.317: INFO: stderr: ""
Jan  5 09:22:07.317: INFO: stdout: "true"
Jan  5 09:22:07.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 09:22:07.386: INFO: stderr: ""
Jan  5 09:22:07.386: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 09:22:07.386: INFO: validating pod update-demo-nautilus-fmx24
Jan  5 09:22:07.400: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 09:22:07.400: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 09:22:07.400: INFO: update-demo-nautilus-fmx24 is verified up and running
Jan  5 09:22:07.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-t5zh6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 09:22:07.469: INFO: stderr: ""
Jan  5 09:22:07.469: INFO: stdout: "true"
Jan  5 09:22:07.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-t5zh6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 09:22:07.538: INFO: stderr: ""
Jan  5 09:22:07.538: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 09:22:07.538: INFO: validating pod update-demo-nautilus-t5zh6
Jan  5 09:22:07.629: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 09:22:07.629: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 09:22:07.629: INFO: update-demo-nautilus-t5zh6 is verified up and running
STEP: scaling down the replication controller 01/05/23 09:22:07.629
Jan  5 09:22:07.630: INFO: scanned /root for discovery docs: <nil>
Jan  5 09:22:07.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan  5 09:22:08.717: INFO: stderr: ""
Jan  5 09:22:08.717: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 09:22:08.717
Jan  5 09:22:08.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 09:22:08.785: INFO: stderr: ""
Jan  5 09:22:08.785: INFO: stdout: "update-demo-nautilus-fmx24 update-demo-nautilus-t5zh6 "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/05/23 09:22:08.785
Jan  5 09:22:13.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 09:22:13.854: INFO: stderr: ""
Jan  5 09:22:13.854: INFO: stdout: "update-demo-nautilus-fmx24 "
Jan  5 09:22:13.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 09:22:13.925: INFO: stderr: ""
Jan  5 09:22:13.925: INFO: stdout: "true"
Jan  5 09:22:13.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 09:22:13.989: INFO: stderr: ""
Jan  5 09:22:13.990: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 09:22:13.990: INFO: validating pod update-demo-nautilus-fmx24
Jan  5 09:22:14.001: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 09:22:14.001: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 09:22:14.001: INFO: update-demo-nautilus-fmx24 is verified up and running
STEP: scaling up the replication controller 01/05/23 09:22:14.001
Jan  5 09:22:14.002: INFO: scanned /root for discovery docs: <nil>
Jan  5 09:22:14.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan  5 09:22:15.086: INFO: stderr: ""
Jan  5 09:22:15.086: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 09:22:15.086
Jan  5 09:22:15.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 09:22:15.151: INFO: stderr: ""
Jan  5 09:22:15.151: INFO: stdout: "update-demo-nautilus-fmx24 update-demo-nautilus-vm7tw "
Jan  5 09:22:15.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 09:22:15.217: INFO: stderr: ""
Jan  5 09:22:15.217: INFO: stdout: "true"
Jan  5 09:22:15.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 09:22:15.279: INFO: stderr: ""
Jan  5 09:22:15.279: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 09:22:15.279: INFO: validating pod update-demo-nautilus-fmx24
Jan  5 09:22:15.293: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 09:22:15.293: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 09:22:15.293: INFO: update-demo-nautilus-fmx24 is verified up and running
Jan  5 09:22:15.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-vm7tw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 09:22:15.359: INFO: stderr: ""
Jan  5 09:22:15.359: INFO: stdout: "true"
Jan  5 09:22:15.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-vm7tw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 09:22:15.429: INFO: stderr: ""
Jan  5 09:22:15.429: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 09:22:15.429: INFO: validating pod update-demo-nautilus-vm7tw
Jan  5 09:22:15.537: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 09:22:15.537: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 09:22:15.537: INFO: update-demo-nautilus-vm7tw is verified up and running
STEP: using delete to clean up resources 01/05/23 09:22:15.537
Jan  5 09:22:15.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 delete --grace-period=0 --force -f -'
Jan  5 09:22:15.602: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 09:22:15.602: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  5 09:22:15.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get rc,svc -l name=update-demo --no-headers'
Jan  5 09:22:15.678: INFO: stderr: "No resources found in kubectl-7435 namespace.\n"
Jan  5 09:22:15.678: INFO: stdout: ""
Jan  5 09:22:15.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  5 09:22:15.760: INFO: stderr: ""
Jan  5 09:22:15.760: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 09:22:15.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7435" for this suite. 01/05/23 09:22:15.769
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":78,"skipped":1400,"failed":0}
------------------------------
â€¢ [SLOW TEST] [19.953 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:21:55.822
    Jan  5 09:21:55.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 09:21:55.824
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:21:55.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:21:55.855
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 01/05/23 09:21:55.863
    Jan  5 09:21:55.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 create -f -'
    Jan  5 09:21:56.664: INFO: stderr: ""
    Jan  5 09:21:56.664: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 09:21:56.664
    Jan  5 09:21:56.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 09:21:56.733: INFO: stderr: ""
    Jan  5 09:21:56.733: INFO: stdout: "update-demo-nautilus-fmx24 update-demo-nautilus-t5zh6 "
    Jan  5 09:21:56.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 09:21:56.796: INFO: stderr: ""
    Jan  5 09:21:56.796: INFO: stdout: ""
    Jan  5 09:21:56.796: INFO: update-demo-nautilus-fmx24 is created but not running
    Jan  5 09:22:01.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 09:22:01.867: INFO: stderr: ""
    Jan  5 09:22:01.867: INFO: stdout: "update-demo-nautilus-fmx24 update-demo-nautilus-t5zh6 "
    Jan  5 09:22:01.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 09:22:01.940: INFO: stderr: ""
    Jan  5 09:22:01.940: INFO: stdout: "true"
    Jan  5 09:22:01.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 09:22:02.009: INFO: stderr: ""
    Jan  5 09:22:02.009: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 09:22:02.009: INFO: validating pod update-demo-nautilus-fmx24
    Jan  5 09:22:02.120: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 09:22:02.120: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 09:22:02.120: INFO: update-demo-nautilus-fmx24 is verified up and running
    Jan  5 09:22:02.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-t5zh6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 09:22:02.186: INFO: stderr: ""
    Jan  5 09:22:02.186: INFO: stdout: ""
    Jan  5 09:22:02.186: INFO: update-demo-nautilus-t5zh6 is created but not running
    Jan  5 09:22:07.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 09:22:07.255: INFO: stderr: ""
    Jan  5 09:22:07.255: INFO: stdout: "update-demo-nautilus-fmx24 update-demo-nautilus-t5zh6 "
    Jan  5 09:22:07.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 09:22:07.317: INFO: stderr: ""
    Jan  5 09:22:07.317: INFO: stdout: "true"
    Jan  5 09:22:07.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 09:22:07.386: INFO: stderr: ""
    Jan  5 09:22:07.386: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 09:22:07.386: INFO: validating pod update-demo-nautilus-fmx24
    Jan  5 09:22:07.400: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 09:22:07.400: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 09:22:07.400: INFO: update-demo-nautilus-fmx24 is verified up and running
    Jan  5 09:22:07.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-t5zh6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 09:22:07.469: INFO: stderr: ""
    Jan  5 09:22:07.469: INFO: stdout: "true"
    Jan  5 09:22:07.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-t5zh6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 09:22:07.538: INFO: stderr: ""
    Jan  5 09:22:07.538: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 09:22:07.538: INFO: validating pod update-demo-nautilus-t5zh6
    Jan  5 09:22:07.629: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 09:22:07.629: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 09:22:07.629: INFO: update-demo-nautilus-t5zh6 is verified up and running
    STEP: scaling down the replication controller 01/05/23 09:22:07.629
    Jan  5 09:22:07.630: INFO: scanned /root for discovery docs: <nil>
    Jan  5 09:22:07.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan  5 09:22:08.717: INFO: stderr: ""
    Jan  5 09:22:08.717: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 09:22:08.717
    Jan  5 09:22:08.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 09:22:08.785: INFO: stderr: ""
    Jan  5 09:22:08.785: INFO: stdout: "update-demo-nautilus-fmx24 update-demo-nautilus-t5zh6 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/05/23 09:22:08.785
    Jan  5 09:22:13.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 09:22:13.854: INFO: stderr: ""
    Jan  5 09:22:13.854: INFO: stdout: "update-demo-nautilus-fmx24 "
    Jan  5 09:22:13.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 09:22:13.925: INFO: stderr: ""
    Jan  5 09:22:13.925: INFO: stdout: "true"
    Jan  5 09:22:13.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 09:22:13.989: INFO: stderr: ""
    Jan  5 09:22:13.990: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 09:22:13.990: INFO: validating pod update-demo-nautilus-fmx24
    Jan  5 09:22:14.001: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 09:22:14.001: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 09:22:14.001: INFO: update-demo-nautilus-fmx24 is verified up and running
    STEP: scaling up the replication controller 01/05/23 09:22:14.001
    Jan  5 09:22:14.002: INFO: scanned /root for discovery docs: <nil>
    Jan  5 09:22:14.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan  5 09:22:15.086: INFO: stderr: ""
    Jan  5 09:22:15.086: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 09:22:15.086
    Jan  5 09:22:15.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 09:22:15.151: INFO: stderr: ""
    Jan  5 09:22:15.151: INFO: stdout: "update-demo-nautilus-fmx24 update-demo-nautilus-vm7tw "
    Jan  5 09:22:15.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 09:22:15.217: INFO: stderr: ""
    Jan  5 09:22:15.217: INFO: stdout: "true"
    Jan  5 09:22:15.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-fmx24 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 09:22:15.279: INFO: stderr: ""
    Jan  5 09:22:15.279: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 09:22:15.279: INFO: validating pod update-demo-nautilus-fmx24
    Jan  5 09:22:15.293: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 09:22:15.293: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 09:22:15.293: INFO: update-demo-nautilus-fmx24 is verified up and running
    Jan  5 09:22:15.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-vm7tw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 09:22:15.359: INFO: stderr: ""
    Jan  5 09:22:15.359: INFO: stdout: "true"
    Jan  5 09:22:15.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods update-demo-nautilus-vm7tw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 09:22:15.429: INFO: stderr: ""
    Jan  5 09:22:15.429: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 09:22:15.429: INFO: validating pod update-demo-nautilus-vm7tw
    Jan  5 09:22:15.537: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 09:22:15.537: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 09:22:15.537: INFO: update-demo-nautilus-vm7tw is verified up and running
    STEP: using delete to clean up resources 01/05/23 09:22:15.537
    Jan  5 09:22:15.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 delete --grace-period=0 --force -f -'
    Jan  5 09:22:15.602: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 09:22:15.602: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan  5 09:22:15.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get rc,svc -l name=update-demo --no-headers'
    Jan  5 09:22:15.678: INFO: stderr: "No resources found in kubectl-7435 namespace.\n"
    Jan  5 09:22:15.678: INFO: stdout: ""
    Jan  5 09:22:15.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7435 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  5 09:22:15.760: INFO: stderr: ""
    Jan  5 09:22:15.760: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 09:22:15.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7435" for this suite. 01/05/23 09:22:15.769
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:22:15.775
Jan  5 09:22:15.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename replicaset 01/05/23 09:22:15.776
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:22:15.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:22:15.796
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan  5 09:22:15.825: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  5 09:22:20.831: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 09:22:20.831
STEP: Scaling up "test-rs" replicaset  01/05/23 09:22:20.832
Jan  5 09:22:20.844: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/05/23 09:22:20.844
W0105 09:22:20.853857      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  5 09:22:20.858: INFO: observed ReplicaSet test-rs in namespace replicaset-8825 with ReadyReplicas 1, AvailableReplicas 1
Jan  5 09:22:20.880: INFO: observed ReplicaSet test-rs in namespace replicaset-8825 with ReadyReplicas 1, AvailableReplicas 1
Jan  5 09:22:20.895: INFO: observed ReplicaSet test-rs in namespace replicaset-8825 with ReadyReplicas 1, AvailableReplicas 1
Jan  5 09:22:20.901: INFO: observed ReplicaSet test-rs in namespace replicaset-8825 with ReadyReplicas 1, AvailableReplicas 1
Jan  5 09:22:22.125: INFO: observed ReplicaSet test-rs in namespace replicaset-8825 with ReadyReplicas 2, AvailableReplicas 2
Jan  5 09:22:23.427: INFO: observed Replicaset test-rs in namespace replicaset-8825 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  5 09:22:23.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8825" for this suite. 01/05/23 09:22:23.436
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":79,"skipped":1401,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.668 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:22:15.775
    Jan  5 09:22:15.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename replicaset 01/05/23 09:22:15.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:22:15.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:22:15.796
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan  5 09:22:15.825: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  5 09:22:20.831: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 09:22:20.831
    STEP: Scaling up "test-rs" replicaset  01/05/23 09:22:20.832
    Jan  5 09:22:20.844: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/05/23 09:22:20.844
    W0105 09:22:20.853857      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  5 09:22:20.858: INFO: observed ReplicaSet test-rs in namespace replicaset-8825 with ReadyReplicas 1, AvailableReplicas 1
    Jan  5 09:22:20.880: INFO: observed ReplicaSet test-rs in namespace replicaset-8825 with ReadyReplicas 1, AvailableReplicas 1
    Jan  5 09:22:20.895: INFO: observed ReplicaSet test-rs in namespace replicaset-8825 with ReadyReplicas 1, AvailableReplicas 1
    Jan  5 09:22:20.901: INFO: observed ReplicaSet test-rs in namespace replicaset-8825 with ReadyReplicas 1, AvailableReplicas 1
    Jan  5 09:22:22.125: INFO: observed ReplicaSet test-rs in namespace replicaset-8825 with ReadyReplicas 2, AvailableReplicas 2
    Jan  5 09:22:23.427: INFO: observed Replicaset test-rs in namespace replicaset-8825 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  5 09:22:23.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8825" for this suite. 01/05/23 09:22:23.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:22:23.443
Jan  5 09:22:23.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 09:22:23.445
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:22:23.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:22:23.469
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 01/05/23 09:22:23.477
STEP: fetching the ConfigMap 01/05/23 09:22:23.484
STEP: patching the ConfigMap 01/05/23 09:22:23.489
STEP: listing all ConfigMaps in all namespaces with a label selector 01/05/23 09:22:23.497
STEP: deleting the ConfigMap by collection with a label selector 01/05/23 09:22:23.503
STEP: listing all ConfigMaps in test namespace 01/05/23 09:22:23.509
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 09:22:23.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-574" for this suite. 01/05/23 09:22:23.522
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":80,"skipped":1407,"failed":0}
------------------------------
â€¢ [0.084 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:22:23.443
    Jan  5 09:22:23.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 09:22:23.445
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:22:23.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:22:23.469
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 01/05/23 09:22:23.477
    STEP: fetching the ConfigMap 01/05/23 09:22:23.484
    STEP: patching the ConfigMap 01/05/23 09:22:23.489
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/05/23 09:22:23.497
    STEP: deleting the ConfigMap by collection with a label selector 01/05/23 09:22:23.503
    STEP: listing all ConfigMaps in test namespace 01/05/23 09:22:23.509
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 09:22:23.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-574" for this suite. 01/05/23 09:22:23.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:22:23.53
Jan  5 09:22:23.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pods 01/05/23 09:22:23.531
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:22:23.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:22:23.551
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 01/05/23 09:22:23.557
STEP: setting up watch 01/05/23 09:22:23.557
STEP: submitting the pod to kubernetes 01/05/23 09:22:23.662
STEP: verifying the pod is in kubernetes 01/05/23 09:22:23.678
STEP: verifying pod creation was observed 01/05/23 09:22:23.686
Jan  5 09:22:23.686: INFO: Waiting up to 5m0s for pod "pod-submit-remove-59014882-5f7d-4571-adc4-2e50d27d19a7" in namespace "pods-1584" to be "running"
Jan  5 09:22:23.690: INFO: Pod "pod-submit-remove-59014882-5f7d-4571-adc4-2e50d27d19a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.45314ms
Jan  5 09:22:25.696: INFO: Pod "pod-submit-remove-59014882-5f7d-4571-adc4-2e50d27d19a7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010114151s
Jan  5 09:22:25.696: INFO: Pod "pod-submit-remove-59014882-5f7d-4571-adc4-2e50d27d19a7" satisfied condition "running"
STEP: deleting the pod gracefully 01/05/23 09:22:25.701
STEP: verifying pod deletion was observed 01/05/23 09:22:25.71
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 09:22:27.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1584" for this suite. 01/05/23 09:22:27.971
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":81,"skipped":1423,"failed":0}
------------------------------
â€¢ [4.448 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:22:23.53
    Jan  5 09:22:23.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pods 01/05/23 09:22:23.531
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:22:23.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:22:23.551
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 01/05/23 09:22:23.557
    STEP: setting up watch 01/05/23 09:22:23.557
    STEP: submitting the pod to kubernetes 01/05/23 09:22:23.662
    STEP: verifying the pod is in kubernetes 01/05/23 09:22:23.678
    STEP: verifying pod creation was observed 01/05/23 09:22:23.686
    Jan  5 09:22:23.686: INFO: Waiting up to 5m0s for pod "pod-submit-remove-59014882-5f7d-4571-adc4-2e50d27d19a7" in namespace "pods-1584" to be "running"
    Jan  5 09:22:23.690: INFO: Pod "pod-submit-remove-59014882-5f7d-4571-adc4-2e50d27d19a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.45314ms
    Jan  5 09:22:25.696: INFO: Pod "pod-submit-remove-59014882-5f7d-4571-adc4-2e50d27d19a7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010114151s
    Jan  5 09:22:25.696: INFO: Pod "pod-submit-remove-59014882-5f7d-4571-adc4-2e50d27d19a7" satisfied condition "running"
    STEP: deleting the pod gracefully 01/05/23 09:22:25.701
    STEP: verifying pod deletion was observed 01/05/23 09:22:25.71
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 09:22:27.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1584" for this suite. 01/05/23 09:22:27.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:22:27.981
Jan  5 09:22:27.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename statefulset 01/05/23 09:22:27.982
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:22:27.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:22:28.006
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5420 01/05/23 09:22:28.013
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-5420 01/05/23 09:22:28.019
Jan  5 09:22:28.034: INFO: Found 0 stateful pods, waiting for 1
Jan  5 09:22:38.041: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/05/23 09:22:38.05
STEP: updating a scale subresource 01/05/23 09:22:38.055
STEP: verifying the statefulset Spec.Replicas was modified 01/05/23 09:22:38.061
STEP: Patch a scale subresource 01/05/23 09:22:38.065
STEP: verifying the statefulset Spec.Replicas was modified 01/05/23 09:22:38.073
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 09:22:38.081: INFO: Deleting all statefulset in ns statefulset-5420
Jan  5 09:22:38.089: INFO: Scaling statefulset ss to 0
Jan  5 09:22:48.125: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 09:22:48.129: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 09:22:48.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5420" for this suite. 01/05/23 09:22:48.154
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":82,"skipped":1449,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.180 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:22:27.981
    Jan  5 09:22:27.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename statefulset 01/05/23 09:22:27.982
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:22:27.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:22:28.006
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5420 01/05/23 09:22:28.013
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-5420 01/05/23 09:22:28.019
    Jan  5 09:22:28.034: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 09:22:38.041: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/05/23 09:22:38.05
    STEP: updating a scale subresource 01/05/23 09:22:38.055
    STEP: verifying the statefulset Spec.Replicas was modified 01/05/23 09:22:38.061
    STEP: Patch a scale subresource 01/05/23 09:22:38.065
    STEP: verifying the statefulset Spec.Replicas was modified 01/05/23 09:22:38.073
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 09:22:38.081: INFO: Deleting all statefulset in ns statefulset-5420
    Jan  5 09:22:38.089: INFO: Scaling statefulset ss to 0
    Jan  5 09:22:48.125: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 09:22:48.129: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 09:22:48.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5420" for this suite. 01/05/23 09:22:48.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:22:48.164
Jan  5 09:22:48.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename daemonsets 01/05/23 09:22:48.165
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:22:48.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:22:48.19
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jan  5 09:22:48.236: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:22:48.242
Jan  5 09:22:48.253: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:22:48.253: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:22:49.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 09:22:49.270: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:22:50.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  5 09:22:50.270: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Update daemon pods image. 01/05/23 09:22:50.294
STEP: Check that daemon pods images are updated. 01/05/23 09:22:50.31
Jan  5 09:22:50.318: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:50.318: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:50.318: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:51.336: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:51.336: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:51.336: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:52.335: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:52.335: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:52.335: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:53.334: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:53.334: INFO: Pod daemon-set-f2z9m is not available
Jan  5 09:22:53.334: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:53.334: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:54.334: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:54.334: INFO: Pod daemon-set-f2z9m is not available
Jan  5 09:22:54.334: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:54.334: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:55.335: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:55.335: INFO: Pod daemon-set-f2z9m is not available
Jan  5 09:22:55.335: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:55.335: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:56.335: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:56.335: INFO: Pod daemon-set-f2z9m is not available
Jan  5 09:22:56.335: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:56.335: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:57.336: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:57.336: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:58.337: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:58.337: INFO: Pod daemon-set-jkpd7 is not available
Jan  5 09:22:58.337: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:59.333: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:22:59.333: INFO: Pod daemon-set-jkpd7 is not available
Jan  5 09:22:59.333: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:23:00.336: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:23:01.334: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 09:23:01.334: INFO: Pod daemon-set-ss5dq is not available
Jan  5 09:23:03.334: INFO: Pod daemon-set-4pfbv is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/05/23 09:23:03.343
Jan  5 09:23:03.360: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 09:23:03.360: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv is running 0 daemon pod, expected 1
Jan  5 09:23:04.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  5 09:23:04.377: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/05/23 09:23:04.403
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8126, will wait for the garbage collector to delete the pods 01/05/23 09:23:04.403
Jan  5 09:23:04.466: INFO: Deleting DaemonSet.extensions daemon-set took: 8.026136ms
Jan  5 09:23:04.566: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.39436ms
Jan  5 09:23:07.272: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:23:07.272: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 09:23:07.277: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12333"},"items":null}

Jan  5 09:23:07.282: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12333"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:23:07.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8126" for this suite. 01/05/23 09:23:07.322
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":83,"skipped":1482,"failed":0}
------------------------------
â€¢ [SLOW TEST] [19.165 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:22:48.164
    Jan  5 09:22:48.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename daemonsets 01/05/23 09:22:48.165
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:22:48.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:22:48.19
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jan  5 09:22:48.236: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:22:48.242
    Jan  5 09:22:48.253: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:22:48.253: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:22:49.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 09:22:49.270: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:22:50.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  5 09:22:50.270: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Update daemon pods image. 01/05/23 09:22:50.294
    STEP: Check that daemon pods images are updated. 01/05/23 09:22:50.31
    Jan  5 09:22:50.318: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:50.318: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:50.318: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:51.336: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:51.336: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:51.336: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:52.335: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:52.335: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:52.335: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:53.334: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:53.334: INFO: Pod daemon-set-f2z9m is not available
    Jan  5 09:22:53.334: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:53.334: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:54.334: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:54.334: INFO: Pod daemon-set-f2z9m is not available
    Jan  5 09:22:54.334: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:54.334: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:55.335: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:55.335: INFO: Pod daemon-set-f2z9m is not available
    Jan  5 09:22:55.335: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:55.335: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:56.335: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:56.335: INFO: Pod daemon-set-f2z9m is not available
    Jan  5 09:22:56.335: INFO: Wrong image for pod: daemon-set-mlbzj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:56.335: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:57.336: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:57.336: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:58.337: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:58.337: INFO: Pod daemon-set-jkpd7 is not available
    Jan  5 09:22:58.337: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:59.333: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:22:59.333: INFO: Pod daemon-set-jkpd7 is not available
    Jan  5 09:22:59.333: INFO: Wrong image for pod: daemon-set-z7kjb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:23:00.336: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:23:01.334: INFO: Wrong image for pod: daemon-set-86m69. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 09:23:01.334: INFO: Pod daemon-set-ss5dq is not available
    Jan  5 09:23:03.334: INFO: Pod daemon-set-4pfbv is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/05/23 09:23:03.343
    Jan  5 09:23:03.360: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 09:23:03.360: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv is running 0 daemon pod, expected 1
    Jan  5 09:23:04.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  5 09:23:04.377: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 09:23:04.403
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8126, will wait for the garbage collector to delete the pods 01/05/23 09:23:04.403
    Jan  5 09:23:04.466: INFO: Deleting DaemonSet.extensions daemon-set took: 8.026136ms
    Jan  5 09:23:04.566: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.39436ms
    Jan  5 09:23:07.272: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:23:07.272: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 09:23:07.277: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12333"},"items":null}

    Jan  5 09:23:07.282: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12333"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:23:07.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8126" for this suite. 01/05/23 09:23:07.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:23:07.329
Jan  5 09:23:07.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 09:23:07.33
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:23:07.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:23:07.355
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1132 01/05/23 09:23:07.361
STEP: changing the ExternalName service to type=ClusterIP 01/05/23 09:23:07.368
STEP: creating replication controller externalname-service in namespace services-1132 01/05/23 09:23:07.389
I0105 09:23:07.401536      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1132, replica count: 2
I0105 09:23:10.453391      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 09:23:10.453: INFO: Creating new exec pod
Jan  5 09:23:10.465: INFO: Waiting up to 5m0s for pod "execpod6zjr6" in namespace "services-1132" to be "running"
Jan  5 09:23:10.475: INFO: Pod "execpod6zjr6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.599855ms
Jan  5 09:23:12.482: INFO: Pod "execpod6zjr6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017066863s
Jan  5 09:23:12.482: INFO: Pod "execpod6zjr6" satisfied condition "running"
Jan  5 09:23:13.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-1132 exec execpod6zjr6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan  5 09:23:13.933: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  5 09:23:13.933: INFO: stdout: "externalname-service-hxlhh"
Jan  5 09:23:13.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-1132 exec execpod6zjr6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.124.116.72 80'
Jan  5 09:23:14.394: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.124.116.72 80\nConnection to 10.124.116.72 80 port [tcp/http] succeeded!\n"
Jan  5 09:23:14.394: INFO: stdout: ""
Jan  5 09:23:15.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-1132 exec execpod6zjr6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.124.116.72 80'
Jan  5 09:23:15.915: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.124.116.72 80\nConnection to 10.124.116.72 80 port [tcp/http] succeeded!\n"
Jan  5 09:23:15.915: INFO: stdout: "externalname-service-hxlhh"
Jan  5 09:23:15.915: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:23:15.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1132" for this suite. 01/05/23 09:23:15.939
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":84,"skipped":1491,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.616 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:23:07.329
    Jan  5 09:23:07.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 09:23:07.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:23:07.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:23:07.355
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1132 01/05/23 09:23:07.361
    STEP: changing the ExternalName service to type=ClusterIP 01/05/23 09:23:07.368
    STEP: creating replication controller externalname-service in namespace services-1132 01/05/23 09:23:07.389
    I0105 09:23:07.401536      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1132, replica count: 2
    I0105 09:23:10.453391      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 09:23:10.453: INFO: Creating new exec pod
    Jan  5 09:23:10.465: INFO: Waiting up to 5m0s for pod "execpod6zjr6" in namespace "services-1132" to be "running"
    Jan  5 09:23:10.475: INFO: Pod "execpod6zjr6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.599855ms
    Jan  5 09:23:12.482: INFO: Pod "execpod6zjr6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017066863s
    Jan  5 09:23:12.482: INFO: Pod "execpod6zjr6" satisfied condition "running"
    Jan  5 09:23:13.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-1132 exec execpod6zjr6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan  5 09:23:13.933: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  5 09:23:13.933: INFO: stdout: "externalname-service-hxlhh"
    Jan  5 09:23:13.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-1132 exec execpod6zjr6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.124.116.72 80'
    Jan  5 09:23:14.394: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.124.116.72 80\nConnection to 10.124.116.72 80 port [tcp/http] succeeded!\n"
    Jan  5 09:23:14.394: INFO: stdout: ""
    Jan  5 09:23:15.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-1132 exec execpod6zjr6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.124.116.72 80'
    Jan  5 09:23:15.915: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.124.116.72 80\nConnection to 10.124.116.72 80 port [tcp/http] succeeded!\n"
    Jan  5 09:23:15.915: INFO: stdout: "externalname-service-hxlhh"
    Jan  5 09:23:15.915: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:23:15.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1132" for this suite. 01/05/23 09:23:15.939
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:23:15.948
Jan  5 09:23:15.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename gc 01/05/23 09:23:15.949
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:23:15.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:23:15.977
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/05/23 09:23:15.996
STEP: delete the rc 01/05/23 09:23:21.01
STEP: wait for the rc to be deleted 01/05/23 09:23:21.019
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/05/23 09:23:26.025
STEP: Gathering metrics 01/05/23 09:23:56.039
W0105 09:23:56.055077      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 09:23:56.055: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan  5 09:23:56.055: INFO: Deleting pod "simpletest.rc-2dmcw" in namespace "gc-5145"
Jan  5 09:23:56.064: INFO: Deleting pod "simpletest.rc-2hl5s" in namespace "gc-5145"
Jan  5 09:23:56.078: INFO: Deleting pod "simpletest.rc-2jbbj" in namespace "gc-5145"
Jan  5 09:23:56.090: INFO: Deleting pod "simpletest.rc-2p9fq" in namespace "gc-5145"
Jan  5 09:23:56.100: INFO: Deleting pod "simpletest.rc-2xzrs" in namespace "gc-5145"
Jan  5 09:23:56.117: INFO: Deleting pod "simpletest.rc-48gk8" in namespace "gc-5145"
Jan  5 09:23:56.135: INFO: Deleting pod "simpletest.rc-4glxh" in namespace "gc-5145"
Jan  5 09:23:56.152: INFO: Deleting pod "simpletest.rc-4jqmj" in namespace "gc-5145"
Jan  5 09:23:56.164: INFO: Deleting pod "simpletest.rc-4nxfb" in namespace "gc-5145"
Jan  5 09:23:56.175: INFO: Deleting pod "simpletest.rc-4vh6r" in namespace "gc-5145"
Jan  5 09:23:56.185: INFO: Deleting pod "simpletest.rc-56xbv" in namespace "gc-5145"
Jan  5 09:23:56.196: INFO: Deleting pod "simpletest.rc-5bjcl" in namespace "gc-5145"
Jan  5 09:23:56.206: INFO: Deleting pod "simpletest.rc-5gnzb" in namespace "gc-5145"
Jan  5 09:23:56.219: INFO: Deleting pod "simpletest.rc-5srb6" in namespace "gc-5145"
Jan  5 09:23:56.235: INFO: Deleting pod "simpletest.rc-67lkf" in namespace "gc-5145"
Jan  5 09:23:56.247: INFO: Deleting pod "simpletest.rc-72ntf" in namespace "gc-5145"
Jan  5 09:23:56.258: INFO: Deleting pod "simpletest.rc-7794r" in namespace "gc-5145"
Jan  5 09:23:56.269: INFO: Deleting pod "simpletest.rc-78hck" in namespace "gc-5145"
Jan  5 09:23:56.286: INFO: Deleting pod "simpletest.rc-7bhxj" in namespace "gc-5145"
Jan  5 09:23:56.300: INFO: Deleting pod "simpletest.rc-7lnm4" in namespace "gc-5145"
Jan  5 09:23:56.313: INFO: Deleting pod "simpletest.rc-7tjk8" in namespace "gc-5145"
Jan  5 09:23:56.326: INFO: Deleting pod "simpletest.rc-7tt5t" in namespace "gc-5145"
Jan  5 09:23:56.334: INFO: Deleting pod "simpletest.rc-82gxm" in namespace "gc-5145"
Jan  5 09:23:56.345: INFO: Deleting pod "simpletest.rc-89pdn" in namespace "gc-5145"
Jan  5 09:23:56.358: INFO: Deleting pod "simpletest.rc-89wg6" in namespace "gc-5145"
Jan  5 09:23:56.368: INFO: Deleting pod "simpletest.rc-8b7s5" in namespace "gc-5145"
Jan  5 09:23:56.391: INFO: Deleting pod "simpletest.rc-8d4qs" in namespace "gc-5145"
Jan  5 09:23:56.412: INFO: Deleting pod "simpletest.rc-8kw8t" in namespace "gc-5145"
Jan  5 09:23:56.422: INFO: Deleting pod "simpletest.rc-8lvmp" in namespace "gc-5145"
Jan  5 09:23:56.434: INFO: Deleting pod "simpletest.rc-9k9vn" in namespace "gc-5145"
Jan  5 09:23:56.443: INFO: Deleting pod "simpletest.rc-9zzbg" in namespace "gc-5145"
Jan  5 09:23:56.453: INFO: Deleting pod "simpletest.rc-bdvdv" in namespace "gc-5145"
Jan  5 09:23:56.467: INFO: Deleting pod "simpletest.rc-bxqvv" in namespace "gc-5145"
Jan  5 09:23:56.478: INFO: Deleting pod "simpletest.rc-c6bj5" in namespace "gc-5145"
Jan  5 09:23:56.486: INFO: Deleting pod "simpletest.rc-c78rw" in namespace "gc-5145"
Jan  5 09:23:56.502: INFO: Deleting pod "simpletest.rc-cdjwh" in namespace "gc-5145"
Jan  5 09:23:56.512: INFO: Deleting pod "simpletest.rc-cvmjg" in namespace "gc-5145"
Jan  5 09:23:56.522: INFO: Deleting pod "simpletest.rc-cxfdl" in namespace "gc-5145"
Jan  5 09:23:56.534: INFO: Deleting pod "simpletest.rc-ddqkv" in namespace "gc-5145"
Jan  5 09:23:56.546: INFO: Deleting pod "simpletest.rc-dngsj" in namespace "gc-5145"
Jan  5 09:23:56.556: INFO: Deleting pod "simpletest.rc-dpjrk" in namespace "gc-5145"
Jan  5 09:23:56.570: INFO: Deleting pod "simpletest.rc-dv9qz" in namespace "gc-5145"
Jan  5 09:23:56.582: INFO: Deleting pod "simpletest.rc-dzs86" in namespace "gc-5145"
Jan  5 09:23:56.592: INFO: Deleting pod "simpletest.rc-f6z62" in namespace "gc-5145"
Jan  5 09:23:56.601: INFO: Deleting pod "simpletest.rc-fchd2" in namespace "gc-5145"
Jan  5 09:23:56.612: INFO: Deleting pod "simpletest.rc-ffzxm" in namespace "gc-5145"
Jan  5 09:23:56.624: INFO: Deleting pod "simpletest.rc-flk4v" in namespace "gc-5145"
Jan  5 09:23:56.633: INFO: Deleting pod "simpletest.rc-fxcbp" in namespace "gc-5145"
Jan  5 09:23:56.645: INFO: Deleting pod "simpletest.rc-g476g" in namespace "gc-5145"
Jan  5 09:23:56.656: INFO: Deleting pod "simpletest.rc-gh6cn" in namespace "gc-5145"
Jan  5 09:23:56.670: INFO: Deleting pod "simpletest.rc-gmr59" in namespace "gc-5145"
Jan  5 09:23:56.679: INFO: Deleting pod "simpletest.rc-gwccm" in namespace "gc-5145"
Jan  5 09:23:56.689: INFO: Deleting pod "simpletest.rc-hc52l" in namespace "gc-5145"
Jan  5 09:23:56.701: INFO: Deleting pod "simpletest.rc-hfr4w" in namespace "gc-5145"
Jan  5 09:23:56.713: INFO: Deleting pod "simpletest.rc-hn6vk" in namespace "gc-5145"
Jan  5 09:23:56.723: INFO: Deleting pod "simpletest.rc-hvfqt" in namespace "gc-5145"
Jan  5 09:23:56.731: INFO: Deleting pod "simpletest.rc-hw8dn" in namespace "gc-5145"
Jan  5 09:23:56.740: INFO: Deleting pod "simpletest.rc-j52g9" in namespace "gc-5145"
Jan  5 09:23:56.750: INFO: Deleting pod "simpletest.rc-jf4zq" in namespace "gc-5145"
Jan  5 09:23:56.766: INFO: Deleting pod "simpletest.rc-kf7hl" in namespace "gc-5145"
Jan  5 09:23:56.777: INFO: Deleting pod "simpletest.rc-kkcj9" in namespace "gc-5145"
Jan  5 09:23:56.787: INFO: Deleting pod "simpletest.rc-l7z8s" in namespace "gc-5145"
Jan  5 09:23:56.798: INFO: Deleting pod "simpletest.rc-lms99" in namespace "gc-5145"
Jan  5 09:23:56.811: INFO: Deleting pod "simpletest.rc-lrd9t" in namespace "gc-5145"
Jan  5 09:23:56.837: INFO: Deleting pod "simpletest.rc-lrkbw" in namespace "gc-5145"
Jan  5 09:23:56.888: INFO: Deleting pod "simpletest.rc-nrglf" in namespace "gc-5145"
Jan  5 09:23:56.938: INFO: Deleting pod "simpletest.rc-nt76l" in namespace "gc-5145"
Jan  5 09:23:56.992: INFO: Deleting pod "simpletest.rc-nxxsb" in namespace "gc-5145"
Jan  5 09:23:57.038: INFO: Deleting pod "simpletest.rc-p2z5g" in namespace "gc-5145"
Jan  5 09:23:57.090: INFO: Deleting pod "simpletest.rc-pmvcw" in namespace "gc-5145"
Jan  5 09:23:57.139: INFO: Deleting pod "simpletest.rc-pw96r" in namespace "gc-5145"
Jan  5 09:23:57.195: INFO: Deleting pod "simpletest.rc-q597t" in namespace "gc-5145"
Jan  5 09:23:57.240: INFO: Deleting pod "simpletest.rc-qscfw" in namespace "gc-5145"
Jan  5 09:23:57.290: INFO: Deleting pod "simpletest.rc-qttlk" in namespace "gc-5145"
Jan  5 09:23:57.341: INFO: Deleting pod "simpletest.rc-qw7jm" in namespace "gc-5145"
Jan  5 09:23:57.387: INFO: Deleting pod "simpletest.rc-qxtwd" in namespace "gc-5145"
Jan  5 09:23:57.476: INFO: Deleting pod "simpletest.rc-r6x69" in namespace "gc-5145"
Jan  5 09:23:57.487: INFO: Deleting pod "simpletest.rc-r7j9d" in namespace "gc-5145"
Jan  5 09:23:57.544: INFO: Deleting pod "simpletest.rc-rp5jh" in namespace "gc-5145"
Jan  5 09:23:57.590: INFO: Deleting pod "simpletest.rc-rtg9n" in namespace "gc-5145"
Jan  5 09:23:57.638: INFO: Deleting pod "simpletest.rc-rz7h8" in namespace "gc-5145"
Jan  5 09:23:57.688: INFO: Deleting pod "simpletest.rc-s8hgx" in namespace "gc-5145"
Jan  5 09:23:57.740: INFO: Deleting pod "simpletest.rc-sfvxk" in namespace "gc-5145"
Jan  5 09:23:57.787: INFO: Deleting pod "simpletest.rc-sg9zp" in namespace "gc-5145"
Jan  5 09:23:57.838: INFO: Deleting pod "simpletest.rc-shgj9" in namespace "gc-5145"
Jan  5 09:23:57.889: INFO: Deleting pod "simpletest.rc-sswqf" in namespace "gc-5145"
Jan  5 09:23:57.937: INFO: Deleting pod "simpletest.rc-tcsmx" in namespace "gc-5145"
Jan  5 09:23:57.994: INFO: Deleting pod "simpletest.rc-ttcv9" in namespace "gc-5145"
Jan  5 09:23:58.037: INFO: Deleting pod "simpletest.rc-v62ds" in namespace "gc-5145"
Jan  5 09:23:58.094: INFO: Deleting pod "simpletest.rc-v6q2d" in namespace "gc-5145"
Jan  5 09:23:58.162: INFO: Deleting pod "simpletest.rc-v8dc7" in namespace "gc-5145"
Jan  5 09:23:58.188: INFO: Deleting pod "simpletest.rc-v8grc" in namespace "gc-5145"
Jan  5 09:23:58.237: INFO: Deleting pod "simpletest.rc-v8m6s" in namespace "gc-5145"
Jan  5 09:23:58.291: INFO: Deleting pod "simpletest.rc-vdqds" in namespace "gc-5145"
Jan  5 09:23:58.342: INFO: Deleting pod "simpletest.rc-vz8mz" in namespace "gc-5145"
Jan  5 09:23:58.385: INFO: Deleting pod "simpletest.rc-wfwzb" in namespace "gc-5145"
Jan  5 09:23:58.436: INFO: Deleting pod "simpletest.rc-wrc2q" in namespace "gc-5145"
Jan  5 09:23:58.502: INFO: Deleting pod "simpletest.rc-ww4r2" in namespace "gc-5145"
Jan  5 09:23:58.549: INFO: Deleting pod "simpletest.rc-zpfzs" in namespace "gc-5145"
Jan  5 09:23:58.589: INFO: Deleting pod "simpletest.rc-zqxsw" in namespace "gc-5145"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 09:23:58.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5145" for this suite. 01/05/23 09:23:58.689
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":85,"skipped":1561,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.788 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:23:15.948
    Jan  5 09:23:15.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename gc 01/05/23 09:23:15.949
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:23:15.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:23:15.977
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/05/23 09:23:15.996
    STEP: delete the rc 01/05/23 09:23:21.01
    STEP: wait for the rc to be deleted 01/05/23 09:23:21.019
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/05/23 09:23:26.025
    STEP: Gathering metrics 01/05/23 09:23:56.039
    W0105 09:23:56.055077      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 09:23:56.055: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan  5 09:23:56.055: INFO: Deleting pod "simpletest.rc-2dmcw" in namespace "gc-5145"
    Jan  5 09:23:56.064: INFO: Deleting pod "simpletest.rc-2hl5s" in namespace "gc-5145"
    Jan  5 09:23:56.078: INFO: Deleting pod "simpletest.rc-2jbbj" in namespace "gc-5145"
    Jan  5 09:23:56.090: INFO: Deleting pod "simpletest.rc-2p9fq" in namespace "gc-5145"
    Jan  5 09:23:56.100: INFO: Deleting pod "simpletest.rc-2xzrs" in namespace "gc-5145"
    Jan  5 09:23:56.117: INFO: Deleting pod "simpletest.rc-48gk8" in namespace "gc-5145"
    Jan  5 09:23:56.135: INFO: Deleting pod "simpletest.rc-4glxh" in namespace "gc-5145"
    Jan  5 09:23:56.152: INFO: Deleting pod "simpletest.rc-4jqmj" in namespace "gc-5145"
    Jan  5 09:23:56.164: INFO: Deleting pod "simpletest.rc-4nxfb" in namespace "gc-5145"
    Jan  5 09:23:56.175: INFO: Deleting pod "simpletest.rc-4vh6r" in namespace "gc-5145"
    Jan  5 09:23:56.185: INFO: Deleting pod "simpletest.rc-56xbv" in namespace "gc-5145"
    Jan  5 09:23:56.196: INFO: Deleting pod "simpletest.rc-5bjcl" in namespace "gc-5145"
    Jan  5 09:23:56.206: INFO: Deleting pod "simpletest.rc-5gnzb" in namespace "gc-5145"
    Jan  5 09:23:56.219: INFO: Deleting pod "simpletest.rc-5srb6" in namespace "gc-5145"
    Jan  5 09:23:56.235: INFO: Deleting pod "simpletest.rc-67lkf" in namespace "gc-5145"
    Jan  5 09:23:56.247: INFO: Deleting pod "simpletest.rc-72ntf" in namespace "gc-5145"
    Jan  5 09:23:56.258: INFO: Deleting pod "simpletest.rc-7794r" in namespace "gc-5145"
    Jan  5 09:23:56.269: INFO: Deleting pod "simpletest.rc-78hck" in namespace "gc-5145"
    Jan  5 09:23:56.286: INFO: Deleting pod "simpletest.rc-7bhxj" in namespace "gc-5145"
    Jan  5 09:23:56.300: INFO: Deleting pod "simpletest.rc-7lnm4" in namespace "gc-5145"
    Jan  5 09:23:56.313: INFO: Deleting pod "simpletest.rc-7tjk8" in namespace "gc-5145"
    Jan  5 09:23:56.326: INFO: Deleting pod "simpletest.rc-7tt5t" in namespace "gc-5145"
    Jan  5 09:23:56.334: INFO: Deleting pod "simpletest.rc-82gxm" in namespace "gc-5145"
    Jan  5 09:23:56.345: INFO: Deleting pod "simpletest.rc-89pdn" in namespace "gc-5145"
    Jan  5 09:23:56.358: INFO: Deleting pod "simpletest.rc-89wg6" in namespace "gc-5145"
    Jan  5 09:23:56.368: INFO: Deleting pod "simpletest.rc-8b7s5" in namespace "gc-5145"
    Jan  5 09:23:56.391: INFO: Deleting pod "simpletest.rc-8d4qs" in namespace "gc-5145"
    Jan  5 09:23:56.412: INFO: Deleting pod "simpletest.rc-8kw8t" in namespace "gc-5145"
    Jan  5 09:23:56.422: INFO: Deleting pod "simpletest.rc-8lvmp" in namespace "gc-5145"
    Jan  5 09:23:56.434: INFO: Deleting pod "simpletest.rc-9k9vn" in namespace "gc-5145"
    Jan  5 09:23:56.443: INFO: Deleting pod "simpletest.rc-9zzbg" in namespace "gc-5145"
    Jan  5 09:23:56.453: INFO: Deleting pod "simpletest.rc-bdvdv" in namespace "gc-5145"
    Jan  5 09:23:56.467: INFO: Deleting pod "simpletest.rc-bxqvv" in namespace "gc-5145"
    Jan  5 09:23:56.478: INFO: Deleting pod "simpletest.rc-c6bj5" in namespace "gc-5145"
    Jan  5 09:23:56.486: INFO: Deleting pod "simpletest.rc-c78rw" in namespace "gc-5145"
    Jan  5 09:23:56.502: INFO: Deleting pod "simpletest.rc-cdjwh" in namespace "gc-5145"
    Jan  5 09:23:56.512: INFO: Deleting pod "simpletest.rc-cvmjg" in namespace "gc-5145"
    Jan  5 09:23:56.522: INFO: Deleting pod "simpletest.rc-cxfdl" in namespace "gc-5145"
    Jan  5 09:23:56.534: INFO: Deleting pod "simpletest.rc-ddqkv" in namespace "gc-5145"
    Jan  5 09:23:56.546: INFO: Deleting pod "simpletest.rc-dngsj" in namespace "gc-5145"
    Jan  5 09:23:56.556: INFO: Deleting pod "simpletest.rc-dpjrk" in namespace "gc-5145"
    Jan  5 09:23:56.570: INFO: Deleting pod "simpletest.rc-dv9qz" in namespace "gc-5145"
    Jan  5 09:23:56.582: INFO: Deleting pod "simpletest.rc-dzs86" in namespace "gc-5145"
    Jan  5 09:23:56.592: INFO: Deleting pod "simpletest.rc-f6z62" in namespace "gc-5145"
    Jan  5 09:23:56.601: INFO: Deleting pod "simpletest.rc-fchd2" in namespace "gc-5145"
    Jan  5 09:23:56.612: INFO: Deleting pod "simpletest.rc-ffzxm" in namespace "gc-5145"
    Jan  5 09:23:56.624: INFO: Deleting pod "simpletest.rc-flk4v" in namespace "gc-5145"
    Jan  5 09:23:56.633: INFO: Deleting pod "simpletest.rc-fxcbp" in namespace "gc-5145"
    Jan  5 09:23:56.645: INFO: Deleting pod "simpletest.rc-g476g" in namespace "gc-5145"
    Jan  5 09:23:56.656: INFO: Deleting pod "simpletest.rc-gh6cn" in namespace "gc-5145"
    Jan  5 09:23:56.670: INFO: Deleting pod "simpletest.rc-gmr59" in namespace "gc-5145"
    Jan  5 09:23:56.679: INFO: Deleting pod "simpletest.rc-gwccm" in namespace "gc-5145"
    Jan  5 09:23:56.689: INFO: Deleting pod "simpletest.rc-hc52l" in namespace "gc-5145"
    Jan  5 09:23:56.701: INFO: Deleting pod "simpletest.rc-hfr4w" in namespace "gc-5145"
    Jan  5 09:23:56.713: INFO: Deleting pod "simpletest.rc-hn6vk" in namespace "gc-5145"
    Jan  5 09:23:56.723: INFO: Deleting pod "simpletest.rc-hvfqt" in namespace "gc-5145"
    Jan  5 09:23:56.731: INFO: Deleting pod "simpletest.rc-hw8dn" in namespace "gc-5145"
    Jan  5 09:23:56.740: INFO: Deleting pod "simpletest.rc-j52g9" in namespace "gc-5145"
    Jan  5 09:23:56.750: INFO: Deleting pod "simpletest.rc-jf4zq" in namespace "gc-5145"
    Jan  5 09:23:56.766: INFO: Deleting pod "simpletest.rc-kf7hl" in namespace "gc-5145"
    Jan  5 09:23:56.777: INFO: Deleting pod "simpletest.rc-kkcj9" in namespace "gc-5145"
    Jan  5 09:23:56.787: INFO: Deleting pod "simpletest.rc-l7z8s" in namespace "gc-5145"
    Jan  5 09:23:56.798: INFO: Deleting pod "simpletest.rc-lms99" in namespace "gc-5145"
    Jan  5 09:23:56.811: INFO: Deleting pod "simpletest.rc-lrd9t" in namespace "gc-5145"
    Jan  5 09:23:56.837: INFO: Deleting pod "simpletest.rc-lrkbw" in namespace "gc-5145"
    Jan  5 09:23:56.888: INFO: Deleting pod "simpletest.rc-nrglf" in namespace "gc-5145"
    Jan  5 09:23:56.938: INFO: Deleting pod "simpletest.rc-nt76l" in namespace "gc-5145"
    Jan  5 09:23:56.992: INFO: Deleting pod "simpletest.rc-nxxsb" in namespace "gc-5145"
    Jan  5 09:23:57.038: INFO: Deleting pod "simpletest.rc-p2z5g" in namespace "gc-5145"
    Jan  5 09:23:57.090: INFO: Deleting pod "simpletest.rc-pmvcw" in namespace "gc-5145"
    Jan  5 09:23:57.139: INFO: Deleting pod "simpletest.rc-pw96r" in namespace "gc-5145"
    Jan  5 09:23:57.195: INFO: Deleting pod "simpletest.rc-q597t" in namespace "gc-5145"
    Jan  5 09:23:57.240: INFO: Deleting pod "simpletest.rc-qscfw" in namespace "gc-5145"
    Jan  5 09:23:57.290: INFO: Deleting pod "simpletest.rc-qttlk" in namespace "gc-5145"
    Jan  5 09:23:57.341: INFO: Deleting pod "simpletest.rc-qw7jm" in namespace "gc-5145"
    Jan  5 09:23:57.387: INFO: Deleting pod "simpletest.rc-qxtwd" in namespace "gc-5145"
    Jan  5 09:23:57.476: INFO: Deleting pod "simpletest.rc-r6x69" in namespace "gc-5145"
    Jan  5 09:23:57.487: INFO: Deleting pod "simpletest.rc-r7j9d" in namespace "gc-5145"
    Jan  5 09:23:57.544: INFO: Deleting pod "simpletest.rc-rp5jh" in namespace "gc-5145"
    Jan  5 09:23:57.590: INFO: Deleting pod "simpletest.rc-rtg9n" in namespace "gc-5145"
    Jan  5 09:23:57.638: INFO: Deleting pod "simpletest.rc-rz7h8" in namespace "gc-5145"
    Jan  5 09:23:57.688: INFO: Deleting pod "simpletest.rc-s8hgx" in namespace "gc-5145"
    Jan  5 09:23:57.740: INFO: Deleting pod "simpletest.rc-sfvxk" in namespace "gc-5145"
    Jan  5 09:23:57.787: INFO: Deleting pod "simpletest.rc-sg9zp" in namespace "gc-5145"
    Jan  5 09:23:57.838: INFO: Deleting pod "simpletest.rc-shgj9" in namespace "gc-5145"
    Jan  5 09:23:57.889: INFO: Deleting pod "simpletest.rc-sswqf" in namespace "gc-5145"
    Jan  5 09:23:57.937: INFO: Deleting pod "simpletest.rc-tcsmx" in namespace "gc-5145"
    Jan  5 09:23:57.994: INFO: Deleting pod "simpletest.rc-ttcv9" in namespace "gc-5145"
    Jan  5 09:23:58.037: INFO: Deleting pod "simpletest.rc-v62ds" in namespace "gc-5145"
    Jan  5 09:23:58.094: INFO: Deleting pod "simpletest.rc-v6q2d" in namespace "gc-5145"
    Jan  5 09:23:58.162: INFO: Deleting pod "simpletest.rc-v8dc7" in namespace "gc-5145"
    Jan  5 09:23:58.188: INFO: Deleting pod "simpletest.rc-v8grc" in namespace "gc-5145"
    Jan  5 09:23:58.237: INFO: Deleting pod "simpletest.rc-v8m6s" in namespace "gc-5145"
    Jan  5 09:23:58.291: INFO: Deleting pod "simpletest.rc-vdqds" in namespace "gc-5145"
    Jan  5 09:23:58.342: INFO: Deleting pod "simpletest.rc-vz8mz" in namespace "gc-5145"
    Jan  5 09:23:58.385: INFO: Deleting pod "simpletest.rc-wfwzb" in namespace "gc-5145"
    Jan  5 09:23:58.436: INFO: Deleting pod "simpletest.rc-wrc2q" in namespace "gc-5145"
    Jan  5 09:23:58.502: INFO: Deleting pod "simpletest.rc-ww4r2" in namespace "gc-5145"
    Jan  5 09:23:58.549: INFO: Deleting pod "simpletest.rc-zpfzs" in namespace "gc-5145"
    Jan  5 09:23:58.589: INFO: Deleting pod "simpletest.rc-zqxsw" in namespace "gc-5145"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 09:23:58.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5145" for this suite. 01/05/23 09:23:58.689
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:23:58.736
Jan  5 09:23:58.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 09:23:58.737
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:23:58.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:23:58.759
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 09:23:58.779
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:23:59.339
STEP: Deploying the webhook pod 01/05/23 09:23:59.346
STEP: Wait for the deployment to be ready 01/05/23 09:23:59.357
Jan  5 09:23:59.370: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 09:24:01.387
STEP: Verifying the service has paired with the endpoint 01/05/23 09:24:01.421
Jan  5 09:24:02.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 01/05/23 09:24:02.427
STEP: create a pod 01/05/23 09:24:02.552
Jan  5 09:24:02.562: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-143" to be "running"
Jan  5 09:24:02.568: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.129724ms
Jan  5 09:24:04.575: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012466942s
Jan  5 09:24:04.575: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/05/23 09:24:04.575
Jan  5 09:24:04.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=webhook-143 attach --namespace=webhook-143 to-be-attached-pod -i -c=container1'
Jan  5 09:24:04.741: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:24:04.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-143" for this suite. 01/05/23 09:24:04.758
STEP: Destroying namespace "webhook-143-markers" for this suite. 01/05/23 09:24:04.765
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":86,"skipped":1562,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.069 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:23:58.736
    Jan  5 09:23:58.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 09:23:58.737
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:23:58.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:23:58.759
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 09:23:58.779
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:23:59.339
    STEP: Deploying the webhook pod 01/05/23 09:23:59.346
    STEP: Wait for the deployment to be ready 01/05/23 09:23:59.357
    Jan  5 09:23:59.370: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 09:24:01.387
    STEP: Verifying the service has paired with the endpoint 01/05/23 09:24:01.421
    Jan  5 09:24:02.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 01/05/23 09:24:02.427
    STEP: create a pod 01/05/23 09:24:02.552
    Jan  5 09:24:02.562: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-143" to be "running"
    Jan  5 09:24:02.568: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.129724ms
    Jan  5 09:24:04.575: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012466942s
    Jan  5 09:24:04.575: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/05/23 09:24:04.575
    Jan  5 09:24:04.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=webhook-143 attach --namespace=webhook-143 to-be-attached-pod -i -c=container1'
    Jan  5 09:24:04.741: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:24:04.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-143" for this suite. 01/05/23 09:24:04.758
    STEP: Destroying namespace "webhook-143-markers" for this suite. 01/05/23 09:24:04.765
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:24:04.808
Jan  5 09:24:04.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename watch 01/05/23 09:24:04.81
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:04.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:04.834
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/05/23 09:24:04.841
STEP: creating a new configmap 01/05/23 09:24:04.844
STEP: modifying the configmap once 01/05/23 09:24:04.85
STEP: changing the label value of the configmap 01/05/23 09:24:04.859
STEP: Expecting to observe a delete notification for the watched object 01/05/23 09:24:04.868
Jan  5 09:24:04.868: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3338  744be266-2d76-4407-a556-0e145a99481c 13674 0 2023-01-05 09:24:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:24:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:24:04.869: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3338  744be266-2d76-4407-a556-0e145a99481c 13675 0 2023-01-05 09:24:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:24:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:24:04.869: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3338  744be266-2d76-4407-a556-0e145a99481c 13676 0 2023-01-05 09:24:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:24:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/05/23 09:24:04.869
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/05/23 09:24:04.879
STEP: changing the label value of the configmap back 01/05/23 09:24:14.88
STEP: modifying the configmap a third time 01/05/23 09:24:14.893
STEP: deleting the configmap 01/05/23 09:24:14.905
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/05/23 09:24:14.919
Jan  5 09:24:14.920: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3338  744be266-2d76-4407-a556-0e145a99481c 13740 0 2023-01-05 09:24:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:24:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:24:14.920: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3338  744be266-2d76-4407-a556-0e145a99481c 13741 0 2023-01-05 09:24:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:24:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:24:14.920: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3338  744be266-2d76-4407-a556-0e145a99481c 13743 0 2023-01-05 09:24:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:24:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  5 09:24:14.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3338" for this suite. 01/05/23 09:24:14.939
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":87,"skipped":1597,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.138 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:24:04.808
    Jan  5 09:24:04.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename watch 01/05/23 09:24:04.81
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:04.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:04.834
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/05/23 09:24:04.841
    STEP: creating a new configmap 01/05/23 09:24:04.844
    STEP: modifying the configmap once 01/05/23 09:24:04.85
    STEP: changing the label value of the configmap 01/05/23 09:24:04.859
    STEP: Expecting to observe a delete notification for the watched object 01/05/23 09:24:04.868
    Jan  5 09:24:04.868: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3338  744be266-2d76-4407-a556-0e145a99481c 13674 0 2023-01-05 09:24:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:24:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:24:04.869: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3338  744be266-2d76-4407-a556-0e145a99481c 13675 0 2023-01-05 09:24:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:24:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:24:04.869: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3338  744be266-2d76-4407-a556-0e145a99481c 13676 0 2023-01-05 09:24:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:24:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/05/23 09:24:04.869
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/05/23 09:24:04.879
    STEP: changing the label value of the configmap back 01/05/23 09:24:14.88
    STEP: modifying the configmap a third time 01/05/23 09:24:14.893
    STEP: deleting the configmap 01/05/23 09:24:14.905
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/05/23 09:24:14.919
    Jan  5 09:24:14.920: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3338  744be266-2d76-4407-a556-0e145a99481c 13740 0 2023-01-05 09:24:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:24:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:24:14.920: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3338  744be266-2d76-4407-a556-0e145a99481c 13741 0 2023-01-05 09:24:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:24:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:24:14.920: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3338  744be266-2d76-4407-a556-0e145a99481c 13743 0 2023-01-05 09:24:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:24:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  5 09:24:14.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3338" for this suite. 01/05/23 09:24:14.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:24:14.948
Jan  5 09:24:14.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 09:24:14.949
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:14.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:14.972
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-cfca39a4-48f8-4717-9628-fab6c00ee84c 01/05/23 09:24:14.979
STEP: Creating a pod to test consume configMaps 01/05/23 09:24:14.984
Jan  5 09:24:14.994: INFO: Waiting up to 5m0s for pod "pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f" in namespace "configmap-9830" to be "Succeeded or Failed"
Jan  5 09:24:15.000: INFO: Pod "pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.793907ms
Jan  5 09:24:17.007: INFO: Pod "pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012543035s
Jan  5 09:24:19.007: INFO: Pod "pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013177683s
STEP: Saw pod success 01/05/23 09:24:19.007
Jan  5 09:24:19.008: INFO: Pod "pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f" satisfied condition "Succeeded or Failed"
Jan  5 09:24:19.012: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f container agnhost-container: <nil>
STEP: delete the pod 01/05/23 09:24:19.029
Jan  5 09:24:19.041: INFO: Waiting for pod pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f to disappear
Jan  5 09:24:19.045: INFO: Pod pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 09:24:19.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9830" for this suite. 01/05/23 09:24:19.053
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":88,"skipped":1616,"failed":0}
------------------------------
â€¢ [4.111 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:24:14.948
    Jan  5 09:24:14.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 09:24:14.949
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:14.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:14.972
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-cfca39a4-48f8-4717-9628-fab6c00ee84c 01/05/23 09:24:14.979
    STEP: Creating a pod to test consume configMaps 01/05/23 09:24:14.984
    Jan  5 09:24:14.994: INFO: Waiting up to 5m0s for pod "pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f" in namespace "configmap-9830" to be "Succeeded or Failed"
    Jan  5 09:24:15.000: INFO: Pod "pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.793907ms
    Jan  5 09:24:17.007: INFO: Pod "pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012543035s
    Jan  5 09:24:19.007: INFO: Pod "pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013177683s
    STEP: Saw pod success 01/05/23 09:24:19.007
    Jan  5 09:24:19.008: INFO: Pod "pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f" satisfied condition "Succeeded or Failed"
    Jan  5 09:24:19.012: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 09:24:19.029
    Jan  5 09:24:19.041: INFO: Waiting for pod pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f to disappear
    Jan  5 09:24:19.045: INFO: Pod pod-configmaps-de4c504b-8504-4ebe-839b-fdd8987a918f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 09:24:19.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9830" for this suite. 01/05/23 09:24:19.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:24:19.061
Jan  5 09:24:19.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:24:19.061
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:19.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:19.085
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:24:19.092
Jan  5 09:24:19.107: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70" in namespace "projected-7236" to be "Succeeded or Failed"
Jan  5 09:24:19.116: INFO: Pod "downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70": Phase="Pending", Reason="", readiness=false. Elapsed: 9.014204ms
Jan  5 09:24:21.123: INFO: Pod "downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016277361s
Jan  5 09:24:23.122: INFO: Pod "downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015637354s
STEP: Saw pod success 01/05/23 09:24:23.122
Jan  5 09:24:23.122: INFO: Pod "downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70" satisfied condition "Succeeded or Failed"
Jan  5 09:24:23.128: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70 container client-container: <nil>
STEP: delete the pod 01/05/23 09:24:23.149
Jan  5 09:24:23.162: INFO: Waiting for pod downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70 to disappear
Jan  5 09:24:23.167: INFO: Pod downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 09:24:23.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7236" for this suite. 01/05/23 09:24:23.177
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":89,"skipped":1638,"failed":0}
------------------------------
â€¢ [4.121 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:24:19.061
    Jan  5 09:24:19.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:24:19.061
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:19.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:19.085
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:24:19.092
    Jan  5 09:24:19.107: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70" in namespace "projected-7236" to be "Succeeded or Failed"
    Jan  5 09:24:19.116: INFO: Pod "downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70": Phase="Pending", Reason="", readiness=false. Elapsed: 9.014204ms
    Jan  5 09:24:21.123: INFO: Pod "downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016277361s
    Jan  5 09:24:23.122: INFO: Pod "downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015637354s
    STEP: Saw pod success 01/05/23 09:24:23.122
    Jan  5 09:24:23.122: INFO: Pod "downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70" satisfied condition "Succeeded or Failed"
    Jan  5 09:24:23.128: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70 container client-container: <nil>
    STEP: delete the pod 01/05/23 09:24:23.149
    Jan  5 09:24:23.162: INFO: Waiting for pod downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70 to disappear
    Jan  5 09:24:23.167: INFO: Pod downwardapi-volume-35a6f1f7-1a9a-4de9-b87c-3b0ac7320c70 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 09:24:23.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7236" for this suite. 01/05/23 09:24:23.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:24:23.183
Jan  5 09:24:23.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename resourcequota 01/05/23 09:24:23.184
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:23.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:23.206
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 01/05/23 09:24:23.213
STEP: Getting a ResourceQuota 01/05/23 09:24:23.219
STEP: Updating a ResourceQuota 01/05/23 09:24:23.254
STEP: Verifying a ResourceQuota was modified 01/05/23 09:24:23.26
STEP: Deleting a ResourceQuota 01/05/23 09:24:23.265
STEP: Verifying the deleted ResourceQuota 01/05/23 09:24:23.27
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 09:24:23.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-244" for this suite. 01/05/23 09:24:23.286
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":90,"skipped":1647,"failed":0}
------------------------------
â€¢ [0.109 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:24:23.183
    Jan  5 09:24:23.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename resourcequota 01/05/23 09:24:23.184
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:23.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:23.206
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 01/05/23 09:24:23.213
    STEP: Getting a ResourceQuota 01/05/23 09:24:23.219
    STEP: Updating a ResourceQuota 01/05/23 09:24:23.254
    STEP: Verifying a ResourceQuota was modified 01/05/23 09:24:23.26
    STEP: Deleting a ResourceQuota 01/05/23 09:24:23.265
    STEP: Verifying the deleted ResourceQuota 01/05/23 09:24:23.27
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 09:24:23.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-244" for this suite. 01/05/23 09:24:23.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:24:23.296
Jan  5 09:24:23.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:24:23.297
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:23.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:23.318
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-0ef35c5f-8bd7-4a03-a9ed-496b44f4362a 01/05/23 09:24:23.325
STEP: Creating secret with name secret-projected-all-test-volume-c5645353-a4a0-4e36-bd56-512be028f30b 01/05/23 09:24:23.33
STEP: Creating a pod to test Check all projections for projected volume plugin 01/05/23 09:24:23.335
Jan  5 09:24:23.345: INFO: Waiting up to 5m0s for pod "projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074" in namespace "projected-8895" to be "Succeeded or Failed"
Jan  5 09:24:23.349: INFO: Pod "projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074": Phase="Pending", Reason="", readiness=false. Elapsed: 4.275649ms
Jan  5 09:24:25.356: INFO: Pod "projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011009618s
Jan  5 09:24:27.356: INFO: Pod "projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01115418s
STEP: Saw pod success 01/05/23 09:24:27.356
Jan  5 09:24:27.356: INFO: Pod "projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074" satisfied condition "Succeeded or Failed"
Jan  5 09:24:27.361: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074 container projected-all-volume-test: <nil>
STEP: delete the pod 01/05/23 09:24:27.372
Jan  5 09:24:27.385: INFO: Waiting for pod projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074 to disappear
Jan  5 09:24:27.389: INFO: Pod projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jan  5 09:24:27.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8895" for this suite. 01/05/23 09:24:27.397
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":91,"skipped":1657,"failed":0}
------------------------------
â€¢ [4.106 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:24:23.296
    Jan  5 09:24:23.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:24:23.297
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:23.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:23.318
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-0ef35c5f-8bd7-4a03-a9ed-496b44f4362a 01/05/23 09:24:23.325
    STEP: Creating secret with name secret-projected-all-test-volume-c5645353-a4a0-4e36-bd56-512be028f30b 01/05/23 09:24:23.33
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/05/23 09:24:23.335
    Jan  5 09:24:23.345: INFO: Waiting up to 5m0s for pod "projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074" in namespace "projected-8895" to be "Succeeded or Failed"
    Jan  5 09:24:23.349: INFO: Pod "projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074": Phase="Pending", Reason="", readiness=false. Elapsed: 4.275649ms
    Jan  5 09:24:25.356: INFO: Pod "projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011009618s
    Jan  5 09:24:27.356: INFO: Pod "projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01115418s
    STEP: Saw pod success 01/05/23 09:24:27.356
    Jan  5 09:24:27.356: INFO: Pod "projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074" satisfied condition "Succeeded or Failed"
    Jan  5 09:24:27.361: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/05/23 09:24:27.372
    Jan  5 09:24:27.385: INFO: Waiting for pod projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074 to disappear
    Jan  5 09:24:27.389: INFO: Pod projected-volume-0dce7de2-1d3d-41fb-bc47-735db3a9d074 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jan  5 09:24:27.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8895" for this suite. 01/05/23 09:24:27.397
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:24:27.402
Jan  5 09:24:27.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename resourcequota 01/05/23 09:24:27.403
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:27.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:27.427
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 01/05/23 09:24:44.441
STEP: Creating a ResourceQuota 01/05/23 09:24:49.448
STEP: Ensuring resource quota status is calculated 01/05/23 09:24:49.455
STEP: Creating a ConfigMap 01/05/23 09:24:51.462
STEP: Ensuring resource quota status captures configMap creation 01/05/23 09:24:51.472
STEP: Deleting a ConfigMap 01/05/23 09:24:53.491
STEP: Ensuring resource quota status released usage 01/05/23 09:24:53.498
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 09:24:55.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4292" for this suite. 01/05/23 09:24:55.515
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":92,"skipped":1659,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.120 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:24:27.402
    Jan  5 09:24:27.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename resourcequota 01/05/23 09:24:27.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:27.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:27.427
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 01/05/23 09:24:44.441
    STEP: Creating a ResourceQuota 01/05/23 09:24:49.448
    STEP: Ensuring resource quota status is calculated 01/05/23 09:24:49.455
    STEP: Creating a ConfigMap 01/05/23 09:24:51.462
    STEP: Ensuring resource quota status captures configMap creation 01/05/23 09:24:51.472
    STEP: Deleting a ConfigMap 01/05/23 09:24:53.491
    STEP: Ensuring resource quota status released usage 01/05/23 09:24:53.498
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 09:24:55.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4292" for this suite. 01/05/23 09:24:55.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:24:55.523
Jan  5 09:24:55.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 09:24:55.524
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:55.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:55.554
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-779c1fb3-738e-4c6c-be6a-edd6abfde081 01/05/23 09:24:55.561
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan  5 09:24:55.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1490" for this suite. 01/05/23 09:24:55.571
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":93,"skipped":1675,"failed":0}
------------------------------
â€¢ [0.053 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:24:55.523
    Jan  5 09:24:55.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 09:24:55.524
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:55.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:55.554
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-779c1fb3-738e-4c6c-be6a-edd6abfde081 01/05/23 09:24:55.561
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 09:24:55.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1490" for this suite. 01/05/23 09:24:55.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:24:55.577
Jan  5 09:24:55.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir-wrapper 01/05/23 09:24:55.578
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:55.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:55.629
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/05/23 09:24:55.636
STEP: Creating RC which spawns configmap-volume pods 01/05/23 09:24:55.942
Jan  5 09:24:55.961: INFO: Pod name wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b: Found 0 pods out of 5
Jan  5 09:25:00.984: INFO: Pod name wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/05/23 09:25:00.984
Jan  5 09:25:00.984: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-6qtnj" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:00.991: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-6qtnj": Phase="Running", Reason="", readiness=true. Elapsed: 6.996594ms
Jan  5 09:25:00.991: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-6qtnj" satisfied condition "running"
Jan  5 09:25:00.991: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-7rzr5" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:00.999: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-7rzr5": Phase="Running", Reason="", readiness=true. Elapsed: 8.210382ms
Jan  5 09:25:00.999: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-7rzr5" satisfied condition "running"
Jan  5 09:25:00.999: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-8gr68" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:01.006: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-8gr68": Phase="Running", Reason="", readiness=true. Elapsed: 6.331661ms
Jan  5 09:25:01.006: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-8gr68" satisfied condition "running"
Jan  5 09:25:01.006: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-hwq88" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:01.013: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-hwq88": Phase="Running", Reason="", readiness=true. Elapsed: 7.311873ms
Jan  5 09:25:01.013: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-hwq88" satisfied condition "running"
Jan  5 09:25:01.013: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-rmk8w" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:01.019: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-rmk8w": Phase="Running", Reason="", readiness=true. Elapsed: 5.881169ms
Jan  5 09:25:01.019: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-rmk8w" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b in namespace emptydir-wrapper-767, will wait for the garbage collector to delete the pods 01/05/23 09:25:01.019
Jan  5 09:25:01.085: INFO: Deleting ReplicationController wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b took: 9.108188ms
Jan  5 09:25:01.186: INFO: Terminating ReplicationController wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b pods took: 101.103941ms
STEP: Creating RC which spawns configmap-volume pods 01/05/23 09:25:02.297
Jan  5 09:25:02.314: INFO: Pod name wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9: Found 0 pods out of 5
Jan  5 09:25:07.334: INFO: Pod name wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/05/23 09:25:07.334
Jan  5 09:25:07.334: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-6bjpx" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:07.344: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-6bjpx": Phase="Running", Reason="", readiness=true. Elapsed: 10.137052ms
Jan  5 09:25:07.344: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-6bjpx" satisfied condition "running"
Jan  5 09:25:07.344: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-7cjxm" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:07.356: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-7cjxm": Phase="Running", Reason="", readiness=true. Elapsed: 11.625834ms
Jan  5 09:25:07.356: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-7cjxm" satisfied condition "running"
Jan  5 09:25:07.356: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-b9l95" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:07.364: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-b9l95": Phase="Running", Reason="", readiness=true. Elapsed: 8.208268ms
Jan  5 09:25:07.364: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-b9l95" satisfied condition "running"
Jan  5 09:25:07.364: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-hfx4m" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:07.371: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-hfx4m": Phase="Running", Reason="", readiness=true. Elapsed: 6.676897ms
Jan  5 09:25:07.371: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-hfx4m" satisfied condition "running"
Jan  5 09:25:07.371: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-mklh6" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:07.378: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-mklh6": Phase="Running", Reason="", readiness=true. Elapsed: 7.316913ms
Jan  5 09:25:07.378: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-mklh6" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9 in namespace emptydir-wrapper-767, will wait for the garbage collector to delete the pods 01/05/23 09:25:07.378
Jan  5 09:25:07.443: INFO: Deleting ReplicationController wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9 took: 7.331809ms
Jan  5 09:25:07.544: INFO: Terminating ReplicationController wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9 pods took: 100.813551ms
STEP: Creating RC which spawns configmap-volume pods 01/05/23 09:25:08.855
Jan  5 09:25:08.880: INFO: Pod name wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05: Found 0 pods out of 5
Jan  5 09:25:13.899: INFO: Pod name wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/05/23 09:25:13.899
Jan  5 09:25:13.899: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-7dl2h" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:13.906: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-7dl2h": Phase="Running", Reason="", readiness=true. Elapsed: 7.329635ms
Jan  5 09:25:13.906: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-7dl2h" satisfied condition "running"
Jan  5 09:25:13.906: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-b62l4" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:13.913: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-b62l4": Phase="Running", Reason="", readiness=true. Elapsed: 7.099236ms
Jan  5 09:25:13.913: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-b62l4" satisfied condition "running"
Jan  5 09:25:13.913: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-fxr94" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:13.921: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-fxr94": Phase="Running", Reason="", readiness=true. Elapsed: 7.15485ms
Jan  5 09:25:13.921: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-fxr94" satisfied condition "running"
Jan  5 09:25:13.921: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-l7hl5" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:13.928: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-l7hl5": Phase="Running", Reason="", readiness=true. Elapsed: 7.806157ms
Jan  5 09:25:13.928: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-l7hl5" satisfied condition "running"
Jan  5 09:25:13.928: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-tv8qn" in namespace "emptydir-wrapper-767" to be "running"
Jan  5 09:25:13.935: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-tv8qn": Phase="Running", Reason="", readiness=true. Elapsed: 6.873644ms
Jan  5 09:25:13.935: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-tv8qn" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05 in namespace emptydir-wrapper-767, will wait for the garbage collector to delete the pods 01/05/23 09:25:13.935
Jan  5 09:25:14.000: INFO: Deleting ReplicationController wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05 took: 7.427659ms
Jan  5 09:25:14.101: INFO: Terminating ReplicationController wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05 pods took: 101.11376ms
STEP: Cleaning up the configMaps 01/05/23 09:25:15.302
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan  5 09:25:15.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-767" for this suite. 01/05/23 09:25:15.594
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":94,"skipped":1689,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.024 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:24:55.577
    Jan  5 09:24:55.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir-wrapper 01/05/23 09:24:55.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:24:55.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:24:55.629
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/05/23 09:24:55.636
    STEP: Creating RC which spawns configmap-volume pods 01/05/23 09:24:55.942
    Jan  5 09:24:55.961: INFO: Pod name wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b: Found 0 pods out of 5
    Jan  5 09:25:00.984: INFO: Pod name wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/05/23 09:25:00.984
    Jan  5 09:25:00.984: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-6qtnj" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:00.991: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-6qtnj": Phase="Running", Reason="", readiness=true. Elapsed: 6.996594ms
    Jan  5 09:25:00.991: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-6qtnj" satisfied condition "running"
    Jan  5 09:25:00.991: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-7rzr5" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:00.999: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-7rzr5": Phase="Running", Reason="", readiness=true. Elapsed: 8.210382ms
    Jan  5 09:25:00.999: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-7rzr5" satisfied condition "running"
    Jan  5 09:25:00.999: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-8gr68" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:01.006: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-8gr68": Phase="Running", Reason="", readiness=true. Elapsed: 6.331661ms
    Jan  5 09:25:01.006: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-8gr68" satisfied condition "running"
    Jan  5 09:25:01.006: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-hwq88" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:01.013: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-hwq88": Phase="Running", Reason="", readiness=true. Elapsed: 7.311873ms
    Jan  5 09:25:01.013: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-hwq88" satisfied condition "running"
    Jan  5 09:25:01.013: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-rmk8w" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:01.019: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-rmk8w": Phase="Running", Reason="", readiness=true. Elapsed: 5.881169ms
    Jan  5 09:25:01.019: INFO: Pod "wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b-rmk8w" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b in namespace emptydir-wrapper-767, will wait for the garbage collector to delete the pods 01/05/23 09:25:01.019
    Jan  5 09:25:01.085: INFO: Deleting ReplicationController wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b took: 9.108188ms
    Jan  5 09:25:01.186: INFO: Terminating ReplicationController wrapped-volume-race-ccd45c18-fed6-456c-ae7c-8cc797819f0b pods took: 101.103941ms
    STEP: Creating RC which spawns configmap-volume pods 01/05/23 09:25:02.297
    Jan  5 09:25:02.314: INFO: Pod name wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9: Found 0 pods out of 5
    Jan  5 09:25:07.334: INFO: Pod name wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/05/23 09:25:07.334
    Jan  5 09:25:07.334: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-6bjpx" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:07.344: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-6bjpx": Phase="Running", Reason="", readiness=true. Elapsed: 10.137052ms
    Jan  5 09:25:07.344: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-6bjpx" satisfied condition "running"
    Jan  5 09:25:07.344: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-7cjxm" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:07.356: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-7cjxm": Phase="Running", Reason="", readiness=true. Elapsed: 11.625834ms
    Jan  5 09:25:07.356: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-7cjxm" satisfied condition "running"
    Jan  5 09:25:07.356: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-b9l95" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:07.364: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-b9l95": Phase="Running", Reason="", readiness=true. Elapsed: 8.208268ms
    Jan  5 09:25:07.364: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-b9l95" satisfied condition "running"
    Jan  5 09:25:07.364: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-hfx4m" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:07.371: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-hfx4m": Phase="Running", Reason="", readiness=true. Elapsed: 6.676897ms
    Jan  5 09:25:07.371: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-hfx4m" satisfied condition "running"
    Jan  5 09:25:07.371: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-mklh6" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:07.378: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-mklh6": Phase="Running", Reason="", readiness=true. Elapsed: 7.316913ms
    Jan  5 09:25:07.378: INFO: Pod "wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9-mklh6" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9 in namespace emptydir-wrapper-767, will wait for the garbage collector to delete the pods 01/05/23 09:25:07.378
    Jan  5 09:25:07.443: INFO: Deleting ReplicationController wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9 took: 7.331809ms
    Jan  5 09:25:07.544: INFO: Terminating ReplicationController wrapped-volume-race-921e863e-fb28-46a4-aec2-16ce1f1c4bc9 pods took: 100.813551ms
    STEP: Creating RC which spawns configmap-volume pods 01/05/23 09:25:08.855
    Jan  5 09:25:08.880: INFO: Pod name wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05: Found 0 pods out of 5
    Jan  5 09:25:13.899: INFO: Pod name wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/05/23 09:25:13.899
    Jan  5 09:25:13.899: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-7dl2h" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:13.906: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-7dl2h": Phase="Running", Reason="", readiness=true. Elapsed: 7.329635ms
    Jan  5 09:25:13.906: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-7dl2h" satisfied condition "running"
    Jan  5 09:25:13.906: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-b62l4" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:13.913: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-b62l4": Phase="Running", Reason="", readiness=true. Elapsed: 7.099236ms
    Jan  5 09:25:13.913: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-b62l4" satisfied condition "running"
    Jan  5 09:25:13.913: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-fxr94" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:13.921: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-fxr94": Phase="Running", Reason="", readiness=true. Elapsed: 7.15485ms
    Jan  5 09:25:13.921: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-fxr94" satisfied condition "running"
    Jan  5 09:25:13.921: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-l7hl5" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:13.928: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-l7hl5": Phase="Running", Reason="", readiness=true. Elapsed: 7.806157ms
    Jan  5 09:25:13.928: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-l7hl5" satisfied condition "running"
    Jan  5 09:25:13.928: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-tv8qn" in namespace "emptydir-wrapper-767" to be "running"
    Jan  5 09:25:13.935: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-tv8qn": Phase="Running", Reason="", readiness=true. Elapsed: 6.873644ms
    Jan  5 09:25:13.935: INFO: Pod "wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05-tv8qn" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05 in namespace emptydir-wrapper-767, will wait for the garbage collector to delete the pods 01/05/23 09:25:13.935
    Jan  5 09:25:14.000: INFO: Deleting ReplicationController wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05 took: 7.427659ms
    Jan  5 09:25:14.101: INFO: Terminating ReplicationController wrapped-volume-race-3946475a-0201-4f6c-807c-31d986b6ea05 pods took: 101.11376ms
    STEP: Cleaning up the configMaps 01/05/23 09:25:15.302
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan  5 09:25:15.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-767" for this suite. 01/05/23 09:25:15.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:25:15.603
Jan  5 09:25:15.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename deployment 01/05/23 09:25:15.604
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:25:15.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:25:15.641
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan  5 09:25:15.648: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan  5 09:25:15.658: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  5 09:25:20.669: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 09:25:20.669
Jan  5 09:25:20.669: INFO: Creating deployment "test-rolling-update-deployment"
Jan  5 09:25:20.674: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan  5 09:25:20.683: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan  5 09:25:22.698: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan  5 09:25:22.704: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 09:25:22.727: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3839  14612598-0500-4fe8-be11-55530b4c6213 14481 1 2023-01-05 09:25:20 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-05 09:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:25:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b65808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 09:25:20 +0000 UTC,LastTransitionTime:2023-01-05 09:25:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-05 09:25:22 +0000 UTC,LastTransitionTime:2023-01-05 09:25:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  5 09:25:22.736: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-3839  b6ebb2ca-a631-4d60-9607-83150fdd8aa8 14473 1 2023-01-05 09:25:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 14612598-0500-4fe8-be11-55530b4c6213 0xc001b65cf7 0xc001b65cf8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 09:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14612598-0500-4fe8-be11-55530b4c6213\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:25:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b65da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 09:25:22.736: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan  5 09:25:22.736: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3839  46c2b856-dbd7-414d-b5a4-8d52009bcbcc 14480 2 2023-01-05 09:25:15 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 14612598-0500-4fe8-be11-55530b4c6213 0xc001b65bc7 0xc001b65bc8}] [] [{e2e.test Update apps/v1 2023-01-05 09:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:25:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14612598-0500-4fe8-be11-55530b4c6213\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:25:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001b65c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 09:25:22.745: INFO: Pod "test-rolling-update-deployment-78f575d8ff-hszsn" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-hszsn test-rolling-update-deployment-78f575d8ff- deployment-3839  cc9cb6a7-3c65-4423-9aa2-60b68cd98f4c 14472 0 2023-01-05 09:25:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff b6ebb2ca-a631-4d60-9607-83150fdd8aa8 0xc0039b85e7 0xc0039b85e8}] [] [{kube-controller-manager Update v1 2023-01-05 09:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6ebb2ca-a631-4d60-9607-83150fdd8aa8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 09:25:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nbxbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nbxbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:25:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:25:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:25:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:25:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.27,StartTime:2023-01-05 09:25:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 09:25:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://45c4b60b94082772d178049db4ae9ea6ecd0d8e3acce83d356c693bbe6216dce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 09:25:22.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3839" for this suite. 01/05/23 09:25:22.773
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":95,"skipped":1694,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.181 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:25:15.603
    Jan  5 09:25:15.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename deployment 01/05/23 09:25:15.604
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:25:15.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:25:15.641
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan  5 09:25:15.648: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan  5 09:25:15.658: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  5 09:25:20.669: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 09:25:20.669
    Jan  5 09:25:20.669: INFO: Creating deployment "test-rolling-update-deployment"
    Jan  5 09:25:20.674: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan  5 09:25:20.683: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan  5 09:25:22.698: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan  5 09:25:22.704: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 09:25:22.727: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3839  14612598-0500-4fe8-be11-55530b4c6213 14481 1 2023-01-05 09:25:20 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-05 09:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:25:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b65808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 09:25:20 +0000 UTC,LastTransitionTime:2023-01-05 09:25:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-05 09:25:22 +0000 UTC,LastTransitionTime:2023-01-05 09:25:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  5 09:25:22.736: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-3839  b6ebb2ca-a631-4d60-9607-83150fdd8aa8 14473 1 2023-01-05 09:25:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 14612598-0500-4fe8-be11-55530b4c6213 0xc001b65cf7 0xc001b65cf8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 09:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14612598-0500-4fe8-be11-55530b4c6213\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:25:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b65da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 09:25:22.736: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan  5 09:25:22.736: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3839  46c2b856-dbd7-414d-b5a4-8d52009bcbcc 14480 2 2023-01-05 09:25:15 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 14612598-0500-4fe8-be11-55530b4c6213 0xc001b65bc7 0xc001b65bc8}] [] [{e2e.test Update apps/v1 2023-01-05 09:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:25:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14612598-0500-4fe8-be11-55530b4c6213\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:25:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001b65c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 09:25:22.745: INFO: Pod "test-rolling-update-deployment-78f575d8ff-hszsn" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-hszsn test-rolling-update-deployment-78f575d8ff- deployment-3839  cc9cb6a7-3c65-4423-9aa2-60b68cd98f4c 14472 0 2023-01-05 09:25:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff b6ebb2ca-a631-4d60-9607-83150fdd8aa8 0xc0039b85e7 0xc0039b85e8}] [] [{kube-controller-manager Update v1 2023-01-05 09:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6ebb2ca-a631-4d60-9607-83150fdd8aa8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 09:25:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nbxbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nbxbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:25:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:25:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:25:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:25:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.27,StartTime:2023-01-05 09:25:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 09:25:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://45c4b60b94082772d178049db4ae9ea6ecd0d8e3acce83d356c693bbe6216dce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 09:25:22.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3839" for this suite. 01/05/23 09:25:22.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:25:22.787
Jan  5 09:25:22.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename namespaces 01/05/23 09:25:22.788
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:25:22.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:25:22.814
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 01/05/23 09:25:22.821
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:25:22.837
STEP: Creating a service in the namespace 01/05/23 09:25:22.844
STEP: Deleting the namespace 01/05/23 09:25:22.858
STEP: Waiting for the namespace to be removed. 01/05/23 09:25:22.866
STEP: Recreating the namespace 01/05/23 09:25:28.873
STEP: Verifying there is no service in the namespace 01/05/23 09:25:28.89
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:25:28.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6817" for this suite. 01/05/23 09:25:28.903
STEP: Destroying namespace "nsdeletetest-1616" for this suite. 01/05/23 09:25:28.909
Jan  5 09:25:28.914: INFO: Namespace nsdeletetest-1616 was already deleted
STEP: Destroying namespace "nsdeletetest-2130" for this suite. 01/05/23 09:25:28.914
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":96,"skipped":1720,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.134 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:25:22.787
    Jan  5 09:25:22.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename namespaces 01/05/23 09:25:22.788
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:25:22.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:25:22.814
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 01/05/23 09:25:22.821
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:25:22.837
    STEP: Creating a service in the namespace 01/05/23 09:25:22.844
    STEP: Deleting the namespace 01/05/23 09:25:22.858
    STEP: Waiting for the namespace to be removed. 01/05/23 09:25:22.866
    STEP: Recreating the namespace 01/05/23 09:25:28.873
    STEP: Verifying there is no service in the namespace 01/05/23 09:25:28.89
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:25:28.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6817" for this suite. 01/05/23 09:25:28.903
    STEP: Destroying namespace "nsdeletetest-1616" for this suite. 01/05/23 09:25:28.909
    Jan  5 09:25:28.914: INFO: Namespace nsdeletetest-1616 was already deleted
    STEP: Destroying namespace "nsdeletetest-2130" for this suite. 01/05/23 09:25:28.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:25:28.923
Jan  5 09:25:28.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pods 01/05/23 09:25:28.924
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:25:28.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:25:28.948
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 01/05/23 09:25:28.954
Jan  5 09:25:28.964: INFO: created test-pod-1
Jan  5 09:25:28.972: INFO: created test-pod-2
Jan  5 09:25:28.980: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/05/23 09:25:28.98
Jan  5 09:25:28.980: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-8574' to be running and ready
Jan  5 09:25:28.996: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 09:25:28.996: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 09:25:28.996: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 09:25:28.996: INFO: 0 / 3 pods in namespace 'pods-8574' are running and ready (0 seconds elapsed)
Jan  5 09:25:28.996: INFO: expected 0 pod replicas in namespace 'pods-8574', 0 are Running and Ready.
Jan  5 09:25:28.996: INFO: POD         NODE                                                        PHASE    GRACE  CONDITIONS
Jan  5 09:25:28.996: INFO: test-pod-1  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC  }]
Jan  5 09:25:28.996: INFO: test-pod-2  shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC  }]
Jan  5 09:25:28.996: INFO: test-pod-3  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC  }]
Jan  5 09:25:28.996: INFO: 
Jan  5 09:25:31.013: INFO: 3 / 3 pods in namespace 'pods-8574' are running and ready (2 seconds elapsed)
Jan  5 09:25:31.013: INFO: expected 0 pod replicas in namespace 'pods-8574', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/05/23 09:25:31.031
Jan  5 09:25:31.037: INFO: Pod quantity 3 is different from expected quantity 0
Jan  5 09:25:32.044: INFO: Pod quantity 3 is different from expected quantity 0
Jan  5 09:25:33.045: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 09:25:34.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8574" for this suite. 01/05/23 09:25:34.054
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":97,"skipped":1802,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.138 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:25:28.923
    Jan  5 09:25:28.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pods 01/05/23 09:25:28.924
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:25:28.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:25:28.948
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 01/05/23 09:25:28.954
    Jan  5 09:25:28.964: INFO: created test-pod-1
    Jan  5 09:25:28.972: INFO: created test-pod-2
    Jan  5 09:25:28.980: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/05/23 09:25:28.98
    Jan  5 09:25:28.980: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-8574' to be running and ready
    Jan  5 09:25:28.996: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 09:25:28.996: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 09:25:28.996: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 09:25:28.996: INFO: 0 / 3 pods in namespace 'pods-8574' are running and ready (0 seconds elapsed)
    Jan  5 09:25:28.996: INFO: expected 0 pod replicas in namespace 'pods-8574', 0 are Running and Ready.
    Jan  5 09:25:28.996: INFO: POD         NODE                                                        PHASE    GRACE  CONDITIONS
    Jan  5 09:25:28.996: INFO: test-pod-1  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC  }]
    Jan  5 09:25:28.996: INFO: test-pod-2  shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC  }]
    Jan  5 09:25:28.996: INFO: test-pod-3  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:25:28 +0000 UTC  }]
    Jan  5 09:25:28.996: INFO: 
    Jan  5 09:25:31.013: INFO: 3 / 3 pods in namespace 'pods-8574' are running and ready (2 seconds elapsed)
    Jan  5 09:25:31.013: INFO: expected 0 pod replicas in namespace 'pods-8574', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/05/23 09:25:31.031
    Jan  5 09:25:31.037: INFO: Pod quantity 3 is different from expected quantity 0
    Jan  5 09:25:32.044: INFO: Pod quantity 3 is different from expected quantity 0
    Jan  5 09:25:33.045: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 09:25:34.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8574" for this suite. 01/05/23 09:25:34.054
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:25:34.061
Jan  5 09:25:34.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename replicaset 01/05/23 09:25:34.062
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:25:34.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:25:34.091
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/05/23 09:25:34.104
STEP: Verify that the required pods have come up. 01/05/23 09:25:34.11
Jan  5 09:25:34.114: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  5 09:25:39.121: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 09:25:39.121
STEP: Getting /status 01/05/23 09:25:39.121
Jan  5 09:25:39.127: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/05/23 09:25:39.127
Jan  5 09:25:39.138: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/05/23 09:25:39.138
Jan  5 09:25:39.143: INFO: Observed &ReplicaSet event: ADDED
Jan  5 09:25:39.143: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 09:25:39.143: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 09:25:39.143: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 09:25:39.144: INFO: Found replicaset test-rs in namespace replicaset-147 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  5 09:25:39.144: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/05/23 09:25:39.144
Jan  5 09:25:39.144: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  5 09:25:39.153: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/05/23 09:25:39.153
Jan  5 09:25:39.157: INFO: Observed &ReplicaSet event: ADDED
Jan  5 09:25:39.157: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 09:25:39.157: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 09:25:39.157: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 09:25:39.157: INFO: Observed replicaset test-rs in namespace replicaset-147 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 09:25:39.157: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 09:25:39.157: INFO: Found replicaset test-rs in namespace replicaset-147 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan  5 09:25:39.157: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  5 09:25:39.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-147" for this suite. 01/05/23 09:25:39.166
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":98,"skipped":1804,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.113 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:25:34.061
    Jan  5 09:25:34.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename replicaset 01/05/23 09:25:34.062
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:25:34.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:25:34.091
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/05/23 09:25:34.104
    STEP: Verify that the required pods have come up. 01/05/23 09:25:34.11
    Jan  5 09:25:34.114: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  5 09:25:39.121: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 09:25:39.121
    STEP: Getting /status 01/05/23 09:25:39.121
    Jan  5 09:25:39.127: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/05/23 09:25:39.127
    Jan  5 09:25:39.138: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/05/23 09:25:39.138
    Jan  5 09:25:39.143: INFO: Observed &ReplicaSet event: ADDED
    Jan  5 09:25:39.143: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 09:25:39.143: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 09:25:39.143: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 09:25:39.144: INFO: Found replicaset test-rs in namespace replicaset-147 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  5 09:25:39.144: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/05/23 09:25:39.144
    Jan  5 09:25:39.144: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  5 09:25:39.153: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/05/23 09:25:39.153
    Jan  5 09:25:39.157: INFO: Observed &ReplicaSet event: ADDED
    Jan  5 09:25:39.157: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 09:25:39.157: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 09:25:39.157: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 09:25:39.157: INFO: Observed replicaset test-rs in namespace replicaset-147 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 09:25:39.157: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 09:25:39.157: INFO: Found replicaset test-rs in namespace replicaset-147 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan  5 09:25:39.157: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  5 09:25:39.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-147" for this suite. 01/05/23 09:25:39.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:25:39.175
Jan  5 09:25:39.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pod-network-test 01/05/23 09:25:39.175
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:25:39.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:25:39.209
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-1539 01/05/23 09:25:39.216
STEP: creating a selector 01/05/23 09:25:39.216
STEP: Creating the service pods in kubernetes 01/05/23 09:25:39.217
Jan  5 09:25:39.217: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  5 09:25:39.289: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1539" to be "running and ready"
Jan  5 09:25:39.296: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.640708ms
Jan  5 09:25:39.296: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:25:41.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013596697s
Jan  5 09:25:41.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 09:25:43.304: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01439488s
Jan  5 09:25:43.304: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 09:25:45.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014093211s
Jan  5 09:25:45.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 09:25:47.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013707404s
Jan  5 09:25:47.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 09:25:49.302: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013249699s
Jan  5 09:25:49.302: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 09:25:51.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013910817s
Jan  5 09:25:51.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 09:25:53.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013838621s
Jan  5 09:25:53.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 09:25:55.302: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013025189s
Jan  5 09:25:55.302: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 09:25:57.302: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012815937s
Jan  5 09:25:57.302: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 09:25:59.301: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.012003784s
Jan  5 09:25:59.301: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 09:26:01.305: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015725845s
Jan  5 09:26:01.305: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  5 09:26:01.305: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  5 09:26:01.313: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1539" to be "running and ready"
Jan  5 09:26:01.318: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.970918ms
Jan  5 09:26:01.318: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  5 09:26:01.318: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  5 09:26:01.324: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1539" to be "running and ready"
Jan  5 09:26:01.329: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.153666ms
Jan  5 09:26:01.329: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  5 09:26:01.329: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan  5 09:26:01.334: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1539" to be "running and ready"
Jan  5 09:26:01.339: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.206677ms
Jan  5 09:26:01.339: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan  5 09:26:01.339: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 01/05/23 09:26:01.344
Jan  5 09:26:01.362: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1539" to be "running"
Jan  5 09:26:01.368: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.796323ms
Jan  5 09:26:03.374: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011759414s
Jan  5 09:26:03.374: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  5 09:26:03.378: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1539" to be "running"
Jan  5 09:26:03.383: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.069591ms
Jan  5 09:26:03.383: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan  5 09:26:03.388: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jan  5 09:26:03.388: INFO: Going to poll 10.96.3.123 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jan  5 09:26:03.398: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.3.123:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1539 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:26:03.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:26:03.399: INFO: ExecWithOptions: Clientset creation
Jan  5 09:26:03.399: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-1539/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.3.123%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 09:26:03.803: INFO: Found all 1 expected endpoints: [netserver-0]
Jan  5 09:26:03.803: INFO: Going to poll 10.96.1.159 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jan  5 09:26:03.809: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.1.159:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1539 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:26:03.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:26:03.810: INFO: ExecWithOptions: Clientset creation
Jan  5 09:26:03.810: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-1539/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.1.159%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 09:26:04.250: INFO: Found all 1 expected endpoints: [netserver-1]
Jan  5 09:26:04.251: INFO: Going to poll 10.96.0.117 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jan  5 09:26:04.256: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.0.117:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1539 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:26:04.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:26:04.257: INFO: ExecWithOptions: Clientset creation
Jan  5 09:26:04.257: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-1539/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.0.117%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 09:26:04.750: INFO: Found all 1 expected endpoints: [netserver-2]
Jan  5 09:26:04.750: INFO: Going to poll 10.96.2.238 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jan  5 09:26:04.758: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.2.238:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1539 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:26:04.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:26:04.758: INFO: ExecWithOptions: Clientset creation
Jan  5 09:26:04.758: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-1539/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.2.238%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 09:26:05.226: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan  5 09:26:05.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1539" for this suite. 01/05/23 09:26:05.235
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":99,"skipped":1812,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.067 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:25:39.175
    Jan  5 09:25:39.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pod-network-test 01/05/23 09:25:39.175
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:25:39.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:25:39.209
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-1539 01/05/23 09:25:39.216
    STEP: creating a selector 01/05/23 09:25:39.216
    STEP: Creating the service pods in kubernetes 01/05/23 09:25:39.217
    Jan  5 09:25:39.217: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  5 09:25:39.289: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1539" to be "running and ready"
    Jan  5 09:25:39.296: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.640708ms
    Jan  5 09:25:39.296: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:25:41.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013596697s
    Jan  5 09:25:41.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 09:25:43.304: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01439488s
    Jan  5 09:25:43.304: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 09:25:45.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014093211s
    Jan  5 09:25:45.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 09:25:47.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013707404s
    Jan  5 09:25:47.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 09:25:49.302: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013249699s
    Jan  5 09:25:49.302: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 09:25:51.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013910817s
    Jan  5 09:25:51.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 09:25:53.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013838621s
    Jan  5 09:25:53.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 09:25:55.302: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013025189s
    Jan  5 09:25:55.302: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 09:25:57.302: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012815937s
    Jan  5 09:25:57.302: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 09:25:59.301: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.012003784s
    Jan  5 09:25:59.301: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 09:26:01.305: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015725845s
    Jan  5 09:26:01.305: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  5 09:26:01.305: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  5 09:26:01.313: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1539" to be "running and ready"
    Jan  5 09:26:01.318: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.970918ms
    Jan  5 09:26:01.318: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  5 09:26:01.318: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  5 09:26:01.324: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1539" to be "running and ready"
    Jan  5 09:26:01.329: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.153666ms
    Jan  5 09:26:01.329: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  5 09:26:01.329: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan  5 09:26:01.334: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1539" to be "running and ready"
    Jan  5 09:26:01.339: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.206677ms
    Jan  5 09:26:01.339: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan  5 09:26:01.339: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 01/05/23 09:26:01.344
    Jan  5 09:26:01.362: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1539" to be "running"
    Jan  5 09:26:01.368: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.796323ms
    Jan  5 09:26:03.374: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011759414s
    Jan  5 09:26:03.374: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  5 09:26:03.378: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1539" to be "running"
    Jan  5 09:26:03.383: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.069591ms
    Jan  5 09:26:03.383: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan  5 09:26:03.388: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Jan  5 09:26:03.388: INFO: Going to poll 10.96.3.123 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Jan  5 09:26:03.398: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.3.123:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1539 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:26:03.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:26:03.399: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:26:03.399: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-1539/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.3.123%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 09:26:03.803: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan  5 09:26:03.803: INFO: Going to poll 10.96.1.159 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Jan  5 09:26:03.809: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.1.159:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1539 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:26:03.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:26:03.810: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:26:03.810: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-1539/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.1.159%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 09:26:04.250: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan  5 09:26:04.251: INFO: Going to poll 10.96.0.117 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Jan  5 09:26:04.256: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.0.117:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1539 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:26:04.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:26:04.257: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:26:04.257: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-1539/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.0.117%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 09:26:04.750: INFO: Found all 1 expected endpoints: [netserver-2]
    Jan  5 09:26:04.750: INFO: Going to poll 10.96.2.238 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Jan  5 09:26:04.758: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.2.238:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1539 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:26:04.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:26:04.758: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:26:04.758: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-1539/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.2.238%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 09:26:05.226: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan  5 09:26:05.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1539" for this suite. 01/05/23 09:26:05.235
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:26:05.242
Jan  5 09:26:05.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename conformance-tests 01/05/23 09:26:05.243
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:26:05.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:26:05.264
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/05/23 09:26:05.27
Jan  5 09:26:05.270: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jan  5 09:26:05.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-7914" for this suite. 01/05/23 09:26:05.296
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":100,"skipped":1816,"failed":0}
------------------------------
â€¢ [0.060 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:26:05.242
    Jan  5 09:26:05.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename conformance-tests 01/05/23 09:26:05.243
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:26:05.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:26:05.264
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/05/23 09:26:05.27
    Jan  5 09:26:05.270: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jan  5 09:26:05.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-7914" for this suite. 01/05/23 09:26:05.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:26:05.305
Jan  5 09:26:05.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 09:26:05.306
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:26:05.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:26:05.342
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-9943/configmap-test-716b6a91-5e50-4e76-a308-c87c3a153e99 01/05/23 09:26:05.349
STEP: Creating a pod to test consume configMaps 01/05/23 09:26:05.356
Jan  5 09:26:05.366: INFO: Waiting up to 5m0s for pod "pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89" in namespace "configmap-9943" to be "Succeeded or Failed"
Jan  5 09:26:05.371: INFO: Pod "pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.567778ms
Jan  5 09:26:07.377: INFO: Pod "pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010644279s
Jan  5 09:26:09.377: INFO: Pod "pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010781408s
STEP: Saw pod success 01/05/23 09:26:09.377
Jan  5 09:26:09.377: INFO: Pod "pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89" satisfied condition "Succeeded or Failed"
Jan  5 09:26:09.400: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89 container env-test: <nil>
STEP: delete the pod 01/05/23 09:26:09.419
Jan  5 09:26:09.435: INFO: Waiting for pod pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89 to disappear
Jan  5 09:26:09.440: INFO: Pod pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 09:26:09.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9943" for this suite. 01/05/23 09:26:09.449
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":101,"skipped":1822,"failed":0}
------------------------------
â€¢ [4.152 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:26:05.305
    Jan  5 09:26:05.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 09:26:05.306
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:26:05.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:26:05.342
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-9943/configmap-test-716b6a91-5e50-4e76-a308-c87c3a153e99 01/05/23 09:26:05.349
    STEP: Creating a pod to test consume configMaps 01/05/23 09:26:05.356
    Jan  5 09:26:05.366: INFO: Waiting up to 5m0s for pod "pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89" in namespace "configmap-9943" to be "Succeeded or Failed"
    Jan  5 09:26:05.371: INFO: Pod "pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.567778ms
    Jan  5 09:26:07.377: INFO: Pod "pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010644279s
    Jan  5 09:26:09.377: INFO: Pod "pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010781408s
    STEP: Saw pod success 01/05/23 09:26:09.377
    Jan  5 09:26:09.377: INFO: Pod "pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89" satisfied condition "Succeeded or Failed"
    Jan  5 09:26:09.400: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89 container env-test: <nil>
    STEP: delete the pod 01/05/23 09:26:09.419
    Jan  5 09:26:09.435: INFO: Waiting for pod pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89 to disappear
    Jan  5 09:26:09.440: INFO: Pod pod-configmaps-d25aaa75-373c-425a-bf21-a4d30d127d89 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 09:26:09.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9943" for this suite. 01/05/23 09:26:09.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:26:09.458
Jan  5 09:26:09.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-probe 01/05/23 09:26:09.459
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:26:09.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:26:09.483
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac in namespace container-probe-9224 01/05/23 09:26:09.491
Jan  5 09:26:09.504: INFO: Waiting up to 5m0s for pod "busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac" in namespace "container-probe-9224" to be "not pending"
Jan  5 09:26:09.511: INFO: Pod "busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.895459ms
Jan  5 09:26:11.517: INFO: Pod "busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac": Phase="Running", Reason="", readiness=true. Elapsed: 2.013705043s
Jan  5 09:26:11.517: INFO: Pod "busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac" satisfied condition "not pending"
Jan  5 09:26:11.517: INFO: Started pod busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac in namespace container-probe-9224
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 09:26:11.517
Jan  5 09:26:11.523: INFO: Initial restart count of pod busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac is 0
STEP: deleting the pod 01/05/23 09:30:12.382
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 09:30:12.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9224" for this suite. 01/05/23 09:30:12.406
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":102,"skipped":1854,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.955 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:26:09.458
    Jan  5 09:26:09.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-probe 01/05/23 09:26:09.459
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:26:09.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:26:09.483
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac in namespace container-probe-9224 01/05/23 09:26:09.491
    Jan  5 09:26:09.504: INFO: Waiting up to 5m0s for pod "busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac" in namespace "container-probe-9224" to be "not pending"
    Jan  5 09:26:09.511: INFO: Pod "busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.895459ms
    Jan  5 09:26:11.517: INFO: Pod "busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac": Phase="Running", Reason="", readiness=true. Elapsed: 2.013705043s
    Jan  5 09:26:11.517: INFO: Pod "busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac" satisfied condition "not pending"
    Jan  5 09:26:11.517: INFO: Started pod busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac in namespace container-probe-9224
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 09:26:11.517
    Jan  5 09:26:11.523: INFO: Initial restart count of pod busybox-83a0e6b7-a8af-4310-9026-ade8ac89e8ac is 0
    STEP: deleting the pod 01/05/23 09:30:12.382
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 09:30:12.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9224" for this suite. 01/05/23 09:30:12.406
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:30:12.415
Jan  5 09:30:12.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename subpath 01/05/23 09:30:12.416
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:30:12.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:30:12.441
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 09:30:12.448
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-lgfl 01/05/23 09:30:12.459
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 09:30:12.46
Jan  5 09:30:12.476: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lgfl" in namespace "subpath-943" to be "Succeeded or Failed"
Jan  5 09:30:12.483: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.244786ms
Jan  5 09:30:14.490: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 2.013552403s
Jan  5 09:30:16.490: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 4.013304827s
Jan  5 09:30:18.491: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 6.014919897s
Jan  5 09:30:20.491: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 8.014375942s
Jan  5 09:30:22.490: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 10.013280917s
Jan  5 09:30:24.489: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 12.012959734s
Jan  5 09:30:26.490: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 14.01329303s
Jan  5 09:30:28.490: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 16.013348212s
Jan  5 09:30:30.524: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 18.047656086s
Jan  5 09:30:32.489: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 20.012227847s
Jan  5 09:30:34.491: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=false. Elapsed: 22.01490609s
Jan  5 09:30:36.490: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013790142s
STEP: Saw pod success 01/05/23 09:30:36.49
Jan  5 09:30:36.490: INFO: Pod "pod-subpath-test-secret-lgfl" satisfied condition "Succeeded or Failed"
Jan  5 09:30:36.495: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-subpath-test-secret-lgfl container test-container-subpath-secret-lgfl: <nil>
STEP: delete the pod 01/05/23 09:30:36.522
Jan  5 09:30:36.535: INFO: Waiting for pod pod-subpath-test-secret-lgfl to disappear
Jan  5 09:30:36.539: INFO: Pod pod-subpath-test-secret-lgfl no longer exists
STEP: Deleting pod pod-subpath-test-secret-lgfl 01/05/23 09:30:36.539
Jan  5 09:30:36.539: INFO: Deleting pod "pod-subpath-test-secret-lgfl" in namespace "subpath-943"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  5 09:30:36.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-943" for this suite. 01/05/23 09:30:36.553
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":103,"skipped":1857,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.143 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:30:12.415
    Jan  5 09:30:12.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename subpath 01/05/23 09:30:12.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:30:12.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:30:12.441
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 09:30:12.448
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-lgfl 01/05/23 09:30:12.459
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 09:30:12.46
    Jan  5 09:30:12.476: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lgfl" in namespace "subpath-943" to be "Succeeded or Failed"
    Jan  5 09:30:12.483: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.244786ms
    Jan  5 09:30:14.490: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 2.013552403s
    Jan  5 09:30:16.490: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 4.013304827s
    Jan  5 09:30:18.491: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 6.014919897s
    Jan  5 09:30:20.491: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 8.014375942s
    Jan  5 09:30:22.490: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 10.013280917s
    Jan  5 09:30:24.489: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 12.012959734s
    Jan  5 09:30:26.490: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 14.01329303s
    Jan  5 09:30:28.490: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 16.013348212s
    Jan  5 09:30:30.524: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 18.047656086s
    Jan  5 09:30:32.489: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=true. Elapsed: 20.012227847s
    Jan  5 09:30:34.491: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Running", Reason="", readiness=false. Elapsed: 22.01490609s
    Jan  5 09:30:36.490: INFO: Pod "pod-subpath-test-secret-lgfl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013790142s
    STEP: Saw pod success 01/05/23 09:30:36.49
    Jan  5 09:30:36.490: INFO: Pod "pod-subpath-test-secret-lgfl" satisfied condition "Succeeded or Failed"
    Jan  5 09:30:36.495: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-subpath-test-secret-lgfl container test-container-subpath-secret-lgfl: <nil>
    STEP: delete the pod 01/05/23 09:30:36.522
    Jan  5 09:30:36.535: INFO: Waiting for pod pod-subpath-test-secret-lgfl to disappear
    Jan  5 09:30:36.539: INFO: Pod pod-subpath-test-secret-lgfl no longer exists
    STEP: Deleting pod pod-subpath-test-secret-lgfl 01/05/23 09:30:36.539
    Jan  5 09:30:36.539: INFO: Deleting pod "pod-subpath-test-secret-lgfl" in namespace "subpath-943"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  5 09:30:36.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-943" for this suite. 01/05/23 09:30:36.553
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:30:36.56
Jan  5 09:30:36.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 09:30:36.56
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:30:36.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:30:36.589
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-e53d5608-e61f-413b-8d6d-822eb5a60b43 01/05/23 09:30:36.597
STEP: Creating a pod to test consume configMaps 01/05/23 09:30:36.604
Jan  5 09:30:36.615: INFO: Waiting up to 5m0s for pod "pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27" in namespace "configmap-6124" to be "Succeeded or Failed"
Jan  5 09:30:36.622: INFO: Pod "pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27": Phase="Pending", Reason="", readiness=false. Elapsed: 7.1323ms
Jan  5 09:30:38.628: INFO: Pod "pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013186795s
Jan  5 09:30:40.630: INFO: Pod "pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01478241s
STEP: Saw pod success 01/05/23 09:30:40.63
Jan  5 09:30:40.630: INFO: Pod "pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27" satisfied condition "Succeeded or Failed"
Jan  5 09:30:40.635: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 09:30:40.677
Jan  5 09:30:40.689: INFO: Waiting for pod pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27 to disappear
Jan  5 09:30:40.693: INFO: Pod pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 09:30:40.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6124" for this suite. 01/05/23 09:30:40.702
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":104,"skipped":1859,"failed":0}
------------------------------
â€¢ [4.149 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:30:36.56
    Jan  5 09:30:36.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 09:30:36.56
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:30:36.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:30:36.589
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-e53d5608-e61f-413b-8d6d-822eb5a60b43 01/05/23 09:30:36.597
    STEP: Creating a pod to test consume configMaps 01/05/23 09:30:36.604
    Jan  5 09:30:36.615: INFO: Waiting up to 5m0s for pod "pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27" in namespace "configmap-6124" to be "Succeeded or Failed"
    Jan  5 09:30:36.622: INFO: Pod "pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27": Phase="Pending", Reason="", readiness=false. Elapsed: 7.1323ms
    Jan  5 09:30:38.628: INFO: Pod "pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013186795s
    Jan  5 09:30:40.630: INFO: Pod "pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01478241s
    STEP: Saw pod success 01/05/23 09:30:40.63
    Jan  5 09:30:40.630: INFO: Pod "pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27" satisfied condition "Succeeded or Failed"
    Jan  5 09:30:40.635: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 09:30:40.677
    Jan  5 09:30:40.689: INFO: Waiting for pod pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27 to disappear
    Jan  5 09:30:40.693: INFO: Pod pod-configmaps-591876ad-ba0d-4edf-86ac-96c3eea67c27 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 09:30:40.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6124" for this suite. 01/05/23 09:30:40.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:30:40.709
Jan  5 09:30:40.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 09:30:40.71
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:30:40.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:30:40.738
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 01/05/23 09:30:40.744
Jan  5 09:30:40.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7953 cluster-info'
Jan  5 09:30:40.826: INFO: stderr: ""
Jan  5 09:30:40.826: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 09:30:40.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7953" for this suite. 01/05/23 09:30:40.835
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":105,"skipped":1880,"failed":0}
------------------------------
â€¢ [0.133 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:30:40.709
    Jan  5 09:30:40.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 09:30:40.71
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:30:40.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:30:40.738
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 01/05/23 09:30:40.744
    Jan  5 09:30:40.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-7953 cluster-info'
    Jan  5 09:30:40.826: INFO: stderr: ""
    Jan  5 09:30:40.826: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 09:30:40.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7953" for this suite. 01/05/23 09:30:40.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:30:40.843
Jan  5 09:30:40.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:30:40.844
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:30:40.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:30:40.867
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 01/05/23 09:30:40.874
Jan  5 09:30:40.886: INFO: Waiting up to 5m0s for pod "downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33" in namespace "downward-api-8180" to be "Succeeded or Failed"
Jan  5 09:30:40.938: INFO: Pod "downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33": Phase="Pending", Reason="", readiness=false. Elapsed: 52.54078ms
Jan  5 09:30:42.946: INFO: Pod "downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06010744s
Jan  5 09:30:44.946: INFO: Pod "downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060808101s
STEP: Saw pod success 01/05/23 09:30:44.947
Jan  5 09:30:44.947: INFO: Pod "downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33" satisfied condition "Succeeded or Failed"
Jan  5 09:30:44.952: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33 container dapi-container: <nil>
STEP: delete the pod 01/05/23 09:30:44.965
Jan  5 09:30:44.977: INFO: Waiting for pod downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33 to disappear
Jan  5 09:30:44.981: INFO: Pod downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  5 09:30:44.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8180" for this suite. 01/05/23 09:30:44.99
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":106,"skipped":1895,"failed":0}
------------------------------
â€¢ [4.154 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:30:40.843
    Jan  5 09:30:40.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:30:40.844
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:30:40.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:30:40.867
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 01/05/23 09:30:40.874
    Jan  5 09:30:40.886: INFO: Waiting up to 5m0s for pod "downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33" in namespace "downward-api-8180" to be "Succeeded or Failed"
    Jan  5 09:30:40.938: INFO: Pod "downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33": Phase="Pending", Reason="", readiness=false. Elapsed: 52.54078ms
    Jan  5 09:30:42.946: INFO: Pod "downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06010744s
    Jan  5 09:30:44.946: INFO: Pod "downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060808101s
    STEP: Saw pod success 01/05/23 09:30:44.947
    Jan  5 09:30:44.947: INFO: Pod "downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33" satisfied condition "Succeeded or Failed"
    Jan  5 09:30:44.952: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 09:30:44.965
    Jan  5 09:30:44.977: INFO: Waiting for pod downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33 to disappear
    Jan  5 09:30:44.981: INFO: Pod downward-api-48b40fb5-1ba6-475f-9f0c-58bb3bbc1f33 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  5 09:30:44.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8180" for this suite. 01/05/23 09:30:44.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:30:45
Jan  5 09:30:45.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename dns 01/05/23 09:30:45.001
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:30:45.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:30:45.027
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/05/23 09:30:45.034
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local;sleep 1; done
 01/05/23 09:30:45.041
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-487.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local;sleep 1; done
 01/05/23 09:30:45.041
STEP: creating a pod to probe DNS 01/05/23 09:30:45.041
STEP: submitting the pod to kubernetes 01/05/23 09:30:45.042
Jan  5 09:30:45.056: INFO: Waiting up to 15m0s for pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3" in namespace "dns-487" to be "running"
Jan  5 09:30:45.064: INFO: Pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.07081ms
Jan  5 09:30:47.074: INFO: Pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01798748s
Jan  5 09:30:49.081: INFO: Pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025345616s
Jan  5 09:30:51.070: INFO: Pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014256438s
Jan  5 09:30:53.071: INFO: Pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3": Phase="Running", Reason="", readiness=true. Elapsed: 8.014848782s
Jan  5 09:30:53.071: INFO: Pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3" satisfied condition "running"
STEP: retrieving the pod 01/05/23 09:30:53.071
STEP: looking for the results for each expected name from probers 01/05/23 09:30:53.077
Jan  5 09:30:53.189: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:53.238: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:53.251: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:53.261: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:53.270: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:53.280: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:53.291: INFO: Unable to read jessie_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:53.304: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:53.304: INFO: Lookups using dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local]

Jan  5 09:30:58.318: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:58.370: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:58.381: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:58.391: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:58.400: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:58.409: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:58.418: INFO: Unable to read jessie_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:58.427: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:30:58.427: INFO: Lookups using dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local]

Jan  5 09:31:03.315: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:03.362: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:03.373: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:03.383: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:03.395: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:03.405: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:03.414: INFO: Unable to read jessie_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:03.424: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:03.424: INFO: Lookups using dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local]

Jan  5 09:31:08.318: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:08.366: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:08.376: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:08.386: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:08.395: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:08.404: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:08.414: INFO: Unable to read jessie_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:08.424: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:08.424: INFO: Lookups using dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local]

Jan  5 09:31:13.316: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:13.362: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:13.376: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:13.393: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:13.405: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:13.414: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:13.466: INFO: Unable to read jessie_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:13.476: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
Jan  5 09:31:13.476: INFO: Lookups using dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local]

Jan  5 09:31:18.490: INFO: DNS probes using dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3 succeeded

STEP: deleting the pod 01/05/23 09:31:18.49
STEP: deleting the test headless service 01/05/23 09:31:18.503
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 09:31:18.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-487" for this suite. 01/05/23 09:31:18.522
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":107,"skipped":1904,"failed":0}
------------------------------
â€¢ [SLOW TEST] [33.530 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:30:45
    Jan  5 09:30:45.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename dns 01/05/23 09:30:45.001
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:30:45.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:30:45.027
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/05/23 09:30:45.034
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local;sleep 1; done
     01/05/23 09:30:45.041
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-487.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-487.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local;sleep 1; done
     01/05/23 09:30:45.041
    STEP: creating a pod to probe DNS 01/05/23 09:30:45.041
    STEP: submitting the pod to kubernetes 01/05/23 09:30:45.042
    Jan  5 09:30:45.056: INFO: Waiting up to 15m0s for pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3" in namespace "dns-487" to be "running"
    Jan  5 09:30:45.064: INFO: Pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.07081ms
    Jan  5 09:30:47.074: INFO: Pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01798748s
    Jan  5 09:30:49.081: INFO: Pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025345616s
    Jan  5 09:30:51.070: INFO: Pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014256438s
    Jan  5 09:30:53.071: INFO: Pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3": Phase="Running", Reason="", readiness=true. Elapsed: 8.014848782s
    Jan  5 09:30:53.071: INFO: Pod "dns-test-c2b8540e-27e4-4544-b112-06585a3492c3" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 09:30:53.071
    STEP: looking for the results for each expected name from probers 01/05/23 09:30:53.077
    Jan  5 09:30:53.189: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:53.238: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:53.251: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:53.261: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:53.270: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:53.280: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:53.291: INFO: Unable to read jessie_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:53.304: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:53.304: INFO: Lookups using dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local]

    Jan  5 09:30:58.318: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:58.370: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:58.381: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:58.391: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:58.400: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:58.409: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:58.418: INFO: Unable to read jessie_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:58.427: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:30:58.427: INFO: Lookups using dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local]

    Jan  5 09:31:03.315: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:03.362: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:03.373: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:03.383: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:03.395: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:03.405: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:03.414: INFO: Unable to read jessie_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:03.424: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:03.424: INFO: Lookups using dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local]

    Jan  5 09:31:08.318: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:08.366: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:08.376: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:08.386: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:08.395: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:08.404: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:08.414: INFO: Unable to read jessie_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:08.424: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:08.424: INFO: Lookups using dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local]

    Jan  5 09:31:13.316: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:13.362: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:13.376: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:13.393: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:13.405: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:13.414: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:13.466: INFO: Unable to read jessie_udp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:13.476: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local from pod dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3: the server could not find the requested resource (get pods dns-test-c2b8540e-27e4-4544-b112-06585a3492c3)
    Jan  5 09:31:13.476: INFO: Lookups using dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local wheezy_udp@dns-test-service-2.dns-487.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-487.svc.cluster.local jessie_udp@dns-test-service-2.dns-487.svc.cluster.local jessie_tcp@dns-test-service-2.dns-487.svc.cluster.local]

    Jan  5 09:31:18.490: INFO: DNS probes using dns-487/dns-test-c2b8540e-27e4-4544-b112-06585a3492c3 succeeded

    STEP: deleting the pod 01/05/23 09:31:18.49
    STEP: deleting the test headless service 01/05/23 09:31:18.503
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 09:31:18.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-487" for this suite. 01/05/23 09:31:18.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:31:18.531
Jan  5 09:31:18.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename gc 01/05/23 09:31:18.532
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:31:18.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:31:18.561
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/05/23 09:31:18.568
STEP: delete the rc 01/05/23 09:31:23.58
STEP: wait for all pods to be garbage collected 01/05/23 09:31:23.587
STEP: Gathering metrics 01/05/23 09:31:28.597
W0105 09:31:28.613503      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 09:31:28.613: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 09:31:28.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9695" for this suite. 01/05/23 09:31:28.624
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":108,"skipped":1926,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.100 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:31:18.531
    Jan  5 09:31:18.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename gc 01/05/23 09:31:18.532
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:31:18.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:31:18.561
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/05/23 09:31:18.568
    STEP: delete the rc 01/05/23 09:31:23.58
    STEP: wait for all pods to be garbage collected 01/05/23 09:31:23.587
    STEP: Gathering metrics 01/05/23 09:31:28.597
    W0105 09:31:28.613503      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 09:31:28.613: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 09:31:28.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9695" for this suite. 01/05/23 09:31:28.624
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:31:28.632
Jan  5 09:31:28.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename podtemplate 01/05/23 09:31:28.633
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:31:28.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:31:28.659
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/05/23 09:31:28.667
Jan  5 09:31:28.674: INFO: created test-podtemplate-1
Jan  5 09:31:28.680: INFO: created test-podtemplate-2
Jan  5 09:31:28.689: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/05/23 09:31:28.689
STEP: delete collection of pod templates 01/05/23 09:31:28.693
Jan  5 09:31:28.693: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/05/23 09:31:28.705
Jan  5 09:31:28.705: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan  5 09:31:28.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-533" for this suite. 01/05/23 09:31:28.719
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":109,"skipped":1927,"failed":0}
------------------------------
â€¢ [0.093 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:31:28.632
    Jan  5 09:31:28.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename podtemplate 01/05/23 09:31:28.633
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:31:28.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:31:28.659
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/05/23 09:31:28.667
    Jan  5 09:31:28.674: INFO: created test-podtemplate-1
    Jan  5 09:31:28.680: INFO: created test-podtemplate-2
    Jan  5 09:31:28.689: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/05/23 09:31:28.689
    STEP: delete collection of pod templates 01/05/23 09:31:28.693
    Jan  5 09:31:28.693: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/05/23 09:31:28.705
    Jan  5 09:31:28.705: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan  5 09:31:28.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-533" for this suite. 01/05/23 09:31:28.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:31:28.726
Jan  5 09:31:28.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename dns 01/05/23 09:31:28.727
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:31:28.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:31:28.749
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/05/23 09:31:28.756
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1485 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1485;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1485 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1485;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1485.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1485.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1485.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1485.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1485.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1485.svc;check="$$(dig +notcp +noall +answer +search 119.176.116.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.116.176.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.176.116.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.116.176.119_tcp@PTR;sleep 1; done
 01/05/23 09:31:28.784
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1485 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1485;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1485 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1485;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1485.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1485.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1485.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1485.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1485.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1485.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1485.svc;check="$$(dig +notcp +noall +answer +search 119.176.116.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.116.176.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.176.116.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.116.176.119_tcp@PTR;sleep 1; done
 01/05/23 09:31:28.784
STEP: creating a pod to probe DNS 01/05/23 09:31:28.784
STEP: submitting the pod to kubernetes 01/05/23 09:31:28.784
Jan  5 09:31:28.818: INFO: Waiting up to 15m0s for pod "dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb" in namespace "dns-1485" to be "running"
Jan  5 09:31:28.826: INFO: Pod "dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.10124ms
Jan  5 09:31:30.839: INFO: Pod "dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb": Phase="Running", Reason="", readiness=true. Elapsed: 2.020510887s
Jan  5 09:31:30.839: INFO: Pod "dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb" satisfied condition "running"
STEP: retrieving the pod 01/05/23 09:31:30.839
STEP: looking for the results for each expected name from probers 01/05/23 09:31:30.844
Jan  5 09:31:30.957: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.005: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.016: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.026: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.035: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.044: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.053: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.062: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.114: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.125: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.133: INFO: Unable to read jessie_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.142: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.153: INFO: Unable to read jessie_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.164: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.175: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.186: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:31.228: INFO: Lookups using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1485 wheezy_tcp@dns-test-service.dns-1485 wheezy_udp@dns-test-service.dns-1485.svc wheezy_tcp@dns-test-service.dns-1485.svc wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1485 jessie_tcp@dns-test-service.dns-1485 jessie_udp@dns-test-service.dns-1485.svc jessie_tcp@dns-test-service.dns-1485.svc jessie_udp@_http._tcp.dns-test-service.dns-1485.svc jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc]

Jan  5 09:31:36.239: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.285: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.295: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.303: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.312: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.322: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.333: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.343: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.389: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.399: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.415: INFO: Unable to read jessie_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.425: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.479: INFO: Unable to read jessie_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.491: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.547: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:36.585: INFO: Lookups using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1485 wheezy_tcp@dns-test-service.dns-1485 wheezy_udp@dns-test-service.dns-1485.svc wheezy_tcp@dns-test-service.dns-1485.svc wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1485 jessie_tcp@dns-test-service.dns-1485 jessie_udp@dns-test-service.dns-1485.svc jessie_tcp@dns-test-service.dns-1485.svc jessie_udp@_http._tcp.dns-test-service.dns-1485.svc jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc]

Jan  5 09:31:41.239: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.285: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.294: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.302: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.312: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.321: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.331: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.339: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.390: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.407: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.425: INFO: Unable to read jessie_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.436: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.446: INFO: Unable to read jessie_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.455: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.464: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.472: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:41.507: INFO: Lookups using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1485 wheezy_tcp@dns-test-service.dns-1485 wheezy_udp@dns-test-service.dns-1485.svc wheezy_tcp@dns-test-service.dns-1485.svc wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1485 jessie_tcp@dns-test-service.dns-1485 jessie_udp@dns-test-service.dns-1485.svc jessie_tcp@dns-test-service.dns-1485.svc jessie_udp@_http._tcp.dns-test-service.dns-1485.svc jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc]

Jan  5 09:31:46.240: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.285: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.296: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.319: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.329: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.338: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.353: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.404: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.414: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.424: INFO: Unable to read jessie_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.435: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.446: INFO: Unable to read jessie_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.456: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.465: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.475: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:46.530: INFO: Lookups using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1485 wheezy_tcp@dns-test-service.dns-1485 wheezy_udp@dns-test-service.dns-1485.svc wheezy_tcp@dns-test-service.dns-1485.svc wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1485 jessie_tcp@dns-test-service.dns-1485 jessie_udp@dns-test-service.dns-1485.svc jessie_tcp@dns-test-service.dns-1485.svc jessie_udp@_http._tcp.dns-test-service.dns-1485.svc jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc]

Jan  5 09:31:51.239: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.287: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.301: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.345: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.355: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.364: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.375: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.385: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.430: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.437: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.456: INFO: Unable to read jessie_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.501: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.512: INFO: Unable to read jessie_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.521: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.531: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.542: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:51.581: INFO: Lookups using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1485 wheezy_tcp@dns-test-service.dns-1485 wheezy_udp@dns-test-service.dns-1485.svc wheezy_tcp@dns-test-service.dns-1485.svc wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1485 jessie_tcp@dns-test-service.dns-1485 jessie_udp@dns-test-service.dns-1485.svc jessie_tcp@dns-test-service.dns-1485.svc jessie_udp@_http._tcp.dns-test-service.dns-1485.svc jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc]

Jan  5 09:31:56.241: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.292: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.341: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.351: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.363: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.373: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.385: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.395: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.441: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.450: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.459: INFO: Unable to read jessie_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.479: INFO: Unable to read jessie_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.532: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.541: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.549: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
Jan  5 09:31:56.586: INFO: Lookups using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1485 wheezy_tcp@dns-test-service.dns-1485 wheezy_udp@dns-test-service.dns-1485.svc wheezy_tcp@dns-test-service.dns-1485.svc wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1485 jessie_tcp@dns-test-service.dns-1485 jessie_udp@dns-test-service.dns-1485.svc jessie_tcp@dns-test-service.dns-1485.svc jessie_udp@_http._tcp.dns-test-service.dns-1485.svc jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc]

Jan  5 09:32:01.488: INFO: DNS probes using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb succeeded

STEP: deleting the pod 01/05/23 09:32:01.488
STEP: deleting the test service 01/05/23 09:32:01.504
STEP: deleting the test headless service 01/05/23 09:32:01.523
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 09:32:01.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1485" for this suite. 01/05/23 09:32:01.546
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":110,"skipped":1962,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.827 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:31:28.726
    Jan  5 09:31:28.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename dns 01/05/23 09:31:28.727
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:31:28.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:31:28.749
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/05/23 09:31:28.756
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1485 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1485;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1485 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1485;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1485.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1485.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1485.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1485.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1485.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1485.svc;check="$$(dig +notcp +noall +answer +search 119.176.116.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.116.176.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.176.116.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.116.176.119_tcp@PTR;sleep 1; done
     01/05/23 09:31:28.784
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1485 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1485;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1485 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1485;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1485.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1485.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1485.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1485.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1485.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1485.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1485.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1485.svc;check="$$(dig +notcp +noall +answer +search 119.176.116.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.116.176.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.176.116.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.116.176.119_tcp@PTR;sleep 1; done
     01/05/23 09:31:28.784
    STEP: creating a pod to probe DNS 01/05/23 09:31:28.784
    STEP: submitting the pod to kubernetes 01/05/23 09:31:28.784
    Jan  5 09:31:28.818: INFO: Waiting up to 15m0s for pod "dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb" in namespace "dns-1485" to be "running"
    Jan  5 09:31:28.826: INFO: Pod "dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.10124ms
    Jan  5 09:31:30.839: INFO: Pod "dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb": Phase="Running", Reason="", readiness=true. Elapsed: 2.020510887s
    Jan  5 09:31:30.839: INFO: Pod "dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 09:31:30.839
    STEP: looking for the results for each expected name from probers 01/05/23 09:31:30.844
    Jan  5 09:31:30.957: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.005: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.016: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.026: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.035: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.044: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.053: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.062: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.114: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.125: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.133: INFO: Unable to read jessie_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.142: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.153: INFO: Unable to read jessie_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.164: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.175: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.186: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:31.228: INFO: Lookups using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1485 wheezy_tcp@dns-test-service.dns-1485 wheezy_udp@dns-test-service.dns-1485.svc wheezy_tcp@dns-test-service.dns-1485.svc wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1485 jessie_tcp@dns-test-service.dns-1485 jessie_udp@dns-test-service.dns-1485.svc jessie_tcp@dns-test-service.dns-1485.svc jessie_udp@_http._tcp.dns-test-service.dns-1485.svc jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc]

    Jan  5 09:31:36.239: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.285: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.295: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.303: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.312: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.322: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.333: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.343: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.389: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.399: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.415: INFO: Unable to read jessie_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.425: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.479: INFO: Unable to read jessie_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.491: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.547: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:36.585: INFO: Lookups using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1485 wheezy_tcp@dns-test-service.dns-1485 wheezy_udp@dns-test-service.dns-1485.svc wheezy_tcp@dns-test-service.dns-1485.svc wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1485 jessie_tcp@dns-test-service.dns-1485 jessie_udp@dns-test-service.dns-1485.svc jessie_tcp@dns-test-service.dns-1485.svc jessie_udp@_http._tcp.dns-test-service.dns-1485.svc jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc]

    Jan  5 09:31:41.239: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.285: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.294: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.302: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.312: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.321: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.331: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.339: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.390: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.407: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.425: INFO: Unable to read jessie_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.436: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.446: INFO: Unable to read jessie_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.455: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.464: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.472: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:41.507: INFO: Lookups using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1485 wheezy_tcp@dns-test-service.dns-1485 wheezy_udp@dns-test-service.dns-1485.svc wheezy_tcp@dns-test-service.dns-1485.svc wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1485 jessie_tcp@dns-test-service.dns-1485 jessie_udp@dns-test-service.dns-1485.svc jessie_tcp@dns-test-service.dns-1485.svc jessie_udp@_http._tcp.dns-test-service.dns-1485.svc jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc]

    Jan  5 09:31:46.240: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.285: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.296: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.319: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.329: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.338: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.353: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.404: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.414: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.424: INFO: Unable to read jessie_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.435: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.446: INFO: Unable to read jessie_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.456: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.465: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.475: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:46.530: INFO: Lookups using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1485 wheezy_tcp@dns-test-service.dns-1485 wheezy_udp@dns-test-service.dns-1485.svc wheezy_tcp@dns-test-service.dns-1485.svc wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1485 jessie_tcp@dns-test-service.dns-1485 jessie_udp@dns-test-service.dns-1485.svc jessie_tcp@dns-test-service.dns-1485.svc jessie_udp@_http._tcp.dns-test-service.dns-1485.svc jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc]

    Jan  5 09:31:51.239: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.287: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.301: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.345: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.355: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.364: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.375: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.385: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.430: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.437: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.456: INFO: Unable to read jessie_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.501: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.512: INFO: Unable to read jessie_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.521: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.531: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.542: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:51.581: INFO: Lookups using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1485 wheezy_tcp@dns-test-service.dns-1485 wheezy_udp@dns-test-service.dns-1485.svc wheezy_tcp@dns-test-service.dns-1485.svc wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1485 jessie_tcp@dns-test-service.dns-1485 jessie_udp@dns-test-service.dns-1485.svc jessie_tcp@dns-test-service.dns-1485.svc jessie_udp@_http._tcp.dns-test-service.dns-1485.svc jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc]

    Jan  5 09:31:56.241: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.292: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.341: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.351: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.363: INFO: Unable to read wheezy_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.373: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.385: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.395: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.441: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.450: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.459: INFO: Unable to read jessie_udp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485 from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.479: INFO: Unable to read jessie_udp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.532: INFO: Unable to read jessie_tcp@dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.541: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.549: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc from pod dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb: the server could not find the requested resource (get pods dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb)
    Jan  5 09:31:56.586: INFO: Lookups using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1485 wheezy_tcp@dns-test-service.dns-1485 wheezy_udp@dns-test-service.dns-1485.svc wheezy_tcp@dns-test-service.dns-1485.svc wheezy_udp@_http._tcp.dns-test-service.dns-1485.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1485.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1485 jessie_tcp@dns-test-service.dns-1485 jessie_udp@dns-test-service.dns-1485.svc jessie_tcp@dns-test-service.dns-1485.svc jessie_udp@_http._tcp.dns-test-service.dns-1485.svc jessie_tcp@_http._tcp.dns-test-service.dns-1485.svc]

    Jan  5 09:32:01.488: INFO: DNS probes using dns-1485/dns-test-44567eec-03a2-46f1-aa0c-b5e994181ccb succeeded

    STEP: deleting the pod 01/05/23 09:32:01.488
    STEP: deleting the test service 01/05/23 09:32:01.504
    STEP: deleting the test headless service 01/05/23 09:32:01.523
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 09:32:01.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1485" for this suite. 01/05/23 09:32:01.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:32:01.555
Jan  5 09:32:01.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename runtimeclass 01/05/23 09:32:01.556
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:32:01.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:32:01.577
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-9294-delete-me 01/05/23 09:32:01.59
STEP: Waiting for the RuntimeClass to disappear 01/05/23 09:32:01.596
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  5 09:32:01.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9294" for this suite. 01/05/23 09:32:01.616
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":111,"skipped":1990,"failed":0}
------------------------------
â€¢ [0.068 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:32:01.555
    Jan  5 09:32:01.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 09:32:01.556
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:32:01.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:32:01.577
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-9294-delete-me 01/05/23 09:32:01.59
    STEP: Waiting for the RuntimeClass to disappear 01/05/23 09:32:01.596
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  5 09:32:01.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9294" for this suite. 01/05/23 09:32:01.616
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:32:01.624
Jan  5 09:32:01.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename job 01/05/23 09:32:01.625
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:32:01.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:32:01.649
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 01/05/23 09:32:01.657
STEP: Ensuring job reaches completions 01/05/23 09:32:01.665
STEP: Ensuring pods with index for job exist 01/05/23 09:32:11.672
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  5 09:32:11.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1410" for this suite. 01/05/23 09:32:11.687
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":112,"skipped":1990,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.070 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:32:01.624
    Jan  5 09:32:01.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename job 01/05/23 09:32:01.625
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:32:01.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:32:01.649
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 01/05/23 09:32:01.657
    STEP: Ensuring job reaches completions 01/05/23 09:32:01.665
    STEP: Ensuring pods with index for job exist 01/05/23 09:32:11.672
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  5 09:32:11.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1410" for this suite. 01/05/23 09:32:11.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:32:11.695
Jan  5 09:32:11.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 09:32:11.696
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:32:11.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:32:11.721
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 01/05/23 09:32:11.729
Jan  5 09:32:11.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: rename a version 01/05/23 09:32:18.716
STEP: check the new version name is served 01/05/23 09:32:18.74
STEP: check the old version name is removed 01/05/23 09:32:21.513
STEP: check the other version is not changed 01/05/23 09:32:23.278
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:32:28.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1091" for this suite. 01/05/23 09:32:28.809
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":113,"skipped":1998,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.119 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:32:11.695
    Jan  5 09:32:11.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 09:32:11.696
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:32:11.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:32:11.721
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 01/05/23 09:32:11.729
    Jan  5 09:32:11.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: rename a version 01/05/23 09:32:18.716
    STEP: check the new version name is served 01/05/23 09:32:18.74
    STEP: check the old version name is removed 01/05/23 09:32:21.513
    STEP: check the other version is not changed 01/05/23 09:32:23.278
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:32:28.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1091" for this suite. 01/05/23 09:32:28.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:32:28.815
Jan  5 09:32:28.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename var-expansion 01/05/23 09:32:28.816
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:32:28.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:32:28.839
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 01/05/23 09:32:28.846
STEP: waiting for pod running 01/05/23 09:32:28.857
Jan  5 09:32:28.857: INFO: Waiting up to 2m0s for pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa" in namespace "var-expansion-848" to be "running"
Jan  5 09:32:28.863: INFO: Pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.365247ms
Jan  5 09:32:30.870: INFO: Pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa": Phase="Running", Reason="", readiness=true. Elapsed: 2.012871465s
Jan  5 09:32:30.870: INFO: Pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa" satisfied condition "running"
STEP: creating a file in subpath 01/05/23 09:32:30.87
Jan  5 09:32:30.874: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-848 PodName:var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:32:30.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:32:30.875: INFO: ExecWithOptions: Clientset creation
Jan  5 09:32:30.875: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/var-expansion-848/pods/var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/05/23 09:32:31.325
Jan  5 09:32:31.331: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-848 PodName:var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:32:31.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:32:31.332: INFO: ExecWithOptions: Clientset creation
Jan  5 09:32:31.332: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/var-expansion-848/pods/var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/05/23 09:32:31.653
Jan  5 09:32:32.170: INFO: Successfully updated pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa"
STEP: waiting for annotated pod running 01/05/23 09:32:32.17
Jan  5 09:32:32.170: INFO: Waiting up to 2m0s for pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa" in namespace "var-expansion-848" to be "running"
Jan  5 09:32:32.175: INFO: Pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa": Phase="Running", Reason="", readiness=true. Elapsed: 4.749285ms
Jan  5 09:32:32.175: INFO: Pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa" satisfied condition "running"
STEP: deleting the pod gracefully 01/05/23 09:32:32.175
Jan  5 09:32:32.175: INFO: Deleting pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa" in namespace "var-expansion-848"
Jan  5 09:32:32.182: INFO: Wait up to 5m0s for pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 09:33:06.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-848" for this suite. 01/05/23 09:33:06.202
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":114,"skipped":2009,"failed":0}
------------------------------
â€¢ [SLOW TEST] [37.394 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:32:28.815
    Jan  5 09:32:28.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename var-expansion 01/05/23 09:32:28.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:32:28.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:32:28.839
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 01/05/23 09:32:28.846
    STEP: waiting for pod running 01/05/23 09:32:28.857
    Jan  5 09:32:28.857: INFO: Waiting up to 2m0s for pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa" in namespace "var-expansion-848" to be "running"
    Jan  5 09:32:28.863: INFO: Pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.365247ms
    Jan  5 09:32:30.870: INFO: Pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa": Phase="Running", Reason="", readiness=true. Elapsed: 2.012871465s
    Jan  5 09:32:30.870: INFO: Pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa" satisfied condition "running"
    STEP: creating a file in subpath 01/05/23 09:32:30.87
    Jan  5 09:32:30.874: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-848 PodName:var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:32:30.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:32:30.875: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:32:30.875: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/var-expansion-848/pods/var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/05/23 09:32:31.325
    Jan  5 09:32:31.331: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-848 PodName:var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:32:31.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:32:31.332: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:32:31.332: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/var-expansion-848/pods/var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/05/23 09:32:31.653
    Jan  5 09:32:32.170: INFO: Successfully updated pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa"
    STEP: waiting for annotated pod running 01/05/23 09:32:32.17
    Jan  5 09:32:32.170: INFO: Waiting up to 2m0s for pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa" in namespace "var-expansion-848" to be "running"
    Jan  5 09:32:32.175: INFO: Pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa": Phase="Running", Reason="", readiness=true. Elapsed: 4.749285ms
    Jan  5 09:32:32.175: INFO: Pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa" satisfied condition "running"
    STEP: deleting the pod gracefully 01/05/23 09:32:32.175
    Jan  5 09:32:32.175: INFO: Deleting pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa" in namespace "var-expansion-848"
    Jan  5 09:32:32.182: INFO: Wait up to 5m0s for pod "var-expansion-f1c98afe-ad8c-4843-aeef-b8ad790fa8aa" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 09:33:06.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-848" for this suite. 01/05/23 09:33:06.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:33:06.21
Jan  5 09:33:06.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:33:06.21
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:06.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:06.235
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:33:06.243
Jan  5 09:33:06.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29" in namespace "downward-api-5945" to be "Succeeded or Failed"
Jan  5 09:33:06.259: INFO: Pod "downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29": Phase="Pending", Reason="", readiness=false. Elapsed: 6.193688ms
Jan  5 09:33:08.266: INFO: Pod "downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012984567s
Jan  5 09:33:10.267: INFO: Pod "downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01403744s
STEP: Saw pod success 01/05/23 09:33:10.267
Jan  5 09:33:10.267: INFO: Pod "downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29" satisfied condition "Succeeded or Failed"
Jan  5 09:33:10.273: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29 container client-container: <nil>
STEP: delete the pod 01/05/23 09:33:10.328
Jan  5 09:33:10.341: INFO: Waiting for pod downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29 to disappear
Jan  5 09:33:10.346: INFO: Pod downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 09:33:10.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5945" for this suite. 01/05/23 09:33:10.356
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":115,"skipped":2015,"failed":0}
------------------------------
â€¢ [4.153 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:33:06.21
    Jan  5 09:33:06.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:33:06.21
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:06.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:06.235
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:33:06.243
    Jan  5 09:33:06.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29" in namespace "downward-api-5945" to be "Succeeded or Failed"
    Jan  5 09:33:06.259: INFO: Pod "downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29": Phase="Pending", Reason="", readiness=false. Elapsed: 6.193688ms
    Jan  5 09:33:08.266: INFO: Pod "downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012984567s
    Jan  5 09:33:10.267: INFO: Pod "downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01403744s
    STEP: Saw pod success 01/05/23 09:33:10.267
    Jan  5 09:33:10.267: INFO: Pod "downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29" satisfied condition "Succeeded or Failed"
    Jan  5 09:33:10.273: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29 container client-container: <nil>
    STEP: delete the pod 01/05/23 09:33:10.328
    Jan  5 09:33:10.341: INFO: Waiting for pod downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29 to disappear
    Jan  5 09:33:10.346: INFO: Pod downwardapi-volume-2e0c9de9-71c1-4810-b600-235255b93a29 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 09:33:10.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5945" for this suite. 01/05/23 09:33:10.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:33:10.365
Jan  5 09:33:10.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/05/23 09:33:10.366
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:10.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:10.388
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/05/23 09:33:10.395
STEP: Creating hostNetwork=false pod 01/05/23 09:33:10.396
Jan  5 09:33:10.414: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-21" to be "running and ready"
Jan  5 09:33:10.421: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.942805ms
Jan  5 09:33:10.421: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:33:12.435: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021032202s
Jan  5 09:33:12.435: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan  5 09:33:12.435: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/05/23 09:33:12.441
Jan  5 09:33:12.449: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-21" to be "running and ready"
Jan  5 09:33:12.455: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605071ms
Jan  5 09:33:12.455: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:33:14.462: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013250176s
Jan  5 09:33:14.462: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan  5 09:33:14.462: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/05/23 09:33:14.469
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/05/23 09:33:14.469
Jan  5 09:33:14.469: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:33:14.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:33:14.470: INFO: ExecWithOptions: Clientset creation
Jan  5 09:33:14.470: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  5 09:33:14.874: INFO: Exec stderr: ""
Jan  5 09:33:14.874: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:33:14.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:33:14.875: INFO: ExecWithOptions: Clientset creation
Jan  5 09:33:14.875: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  5 09:33:15.363: INFO: Exec stderr: ""
Jan  5 09:33:15.363: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:33:15.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:33:15.364: INFO: ExecWithOptions: Clientset creation
Jan  5 09:33:15.364: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  5 09:33:15.774: INFO: Exec stderr: ""
Jan  5 09:33:15.775: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:33:15.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:33:15.775: INFO: ExecWithOptions: Clientset creation
Jan  5 09:33:15.776: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  5 09:33:16.252: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/05/23 09:33:16.252
Jan  5 09:33:16.253: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:33:16.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:33:16.253: INFO: ExecWithOptions: Clientset creation
Jan  5 09:33:16.253: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan  5 09:33:16.746: INFO: Exec stderr: ""
Jan  5 09:33:16.746: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:33:16.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:33:16.747: INFO: ExecWithOptions: Clientset creation
Jan  5 09:33:16.747: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan  5 09:33:17.192: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/05/23 09:33:17.192
Jan  5 09:33:17.192: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:33:17.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:33:17.193: INFO: ExecWithOptions: Clientset creation
Jan  5 09:33:17.193: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  5 09:33:17.689: INFO: Exec stderr: ""
Jan  5 09:33:17.689: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:33:17.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:33:17.690: INFO: ExecWithOptions: Clientset creation
Jan  5 09:33:17.690: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  5 09:33:18.112: INFO: Exec stderr: ""
Jan  5 09:33:18.112: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:33:18.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:33:18.112: INFO: ExecWithOptions: Clientset creation
Jan  5 09:33:18.112: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  5 09:33:18.535: INFO: Exec stderr: ""
Jan  5 09:33:18.535: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:33:18.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:33:18.536: INFO: ExecWithOptions: Clientset creation
Jan  5 09:33:18.536: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  5 09:33:19.040: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jan  5 09:33:19.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-21" for this suite. 01/05/23 09:33:19.051
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":116,"skipped":2026,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.694 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:33:10.365
    Jan  5 09:33:10.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/05/23 09:33:10.366
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:10.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:10.388
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/05/23 09:33:10.395
    STEP: Creating hostNetwork=false pod 01/05/23 09:33:10.396
    Jan  5 09:33:10.414: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-21" to be "running and ready"
    Jan  5 09:33:10.421: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.942805ms
    Jan  5 09:33:10.421: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:33:12.435: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021032202s
    Jan  5 09:33:12.435: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan  5 09:33:12.435: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/05/23 09:33:12.441
    Jan  5 09:33:12.449: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-21" to be "running and ready"
    Jan  5 09:33:12.455: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605071ms
    Jan  5 09:33:12.455: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:33:14.462: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013250176s
    Jan  5 09:33:14.462: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan  5 09:33:14.462: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/05/23 09:33:14.469
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/05/23 09:33:14.469
    Jan  5 09:33:14.469: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:33:14.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:33:14.470: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:33:14.470: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  5 09:33:14.874: INFO: Exec stderr: ""
    Jan  5 09:33:14.874: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:33:14.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:33:14.875: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:33:14.875: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  5 09:33:15.363: INFO: Exec stderr: ""
    Jan  5 09:33:15.363: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:33:15.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:33:15.364: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:33:15.364: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  5 09:33:15.774: INFO: Exec stderr: ""
    Jan  5 09:33:15.775: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:33:15.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:33:15.775: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:33:15.776: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  5 09:33:16.252: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/05/23 09:33:16.252
    Jan  5 09:33:16.253: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:33:16.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:33:16.253: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:33:16.253: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan  5 09:33:16.746: INFO: Exec stderr: ""
    Jan  5 09:33:16.746: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:33:16.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:33:16.747: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:33:16.747: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan  5 09:33:17.192: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/05/23 09:33:17.192
    Jan  5 09:33:17.192: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:33:17.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:33:17.193: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:33:17.193: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  5 09:33:17.689: INFO: Exec stderr: ""
    Jan  5 09:33:17.689: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:33:17.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:33:17.690: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:33:17.690: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  5 09:33:18.112: INFO: Exec stderr: ""
    Jan  5 09:33:18.112: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:33:18.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:33:18.112: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:33:18.112: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  5 09:33:18.535: INFO: Exec stderr: ""
    Jan  5 09:33:18.535: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-21 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:33:18.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:33:18.536: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:33:18.536: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/e2e-kubelet-etc-hosts-21/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  5 09:33:19.040: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jan  5 09:33:19.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-21" for this suite. 01/05/23 09:33:19.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:33:19.062
Jan  5 09:33:19.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 09:33:19.063
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:19.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:19.09
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 09:33:19.115
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:33:19.678
STEP: Deploying the webhook pod 01/05/23 09:33:19.686
STEP: Wait for the deployment to be ready 01/05/23 09:33:19.711
Jan  5 09:33:19.724: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 09:33:21.743
STEP: Verifying the service has paired with the endpoint 01/05/23 09:33:21.759
Jan  5 09:33:22.760: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 01/05/23 09:33:22.766
STEP: Creating a custom resource definition that should be denied by the webhook 01/05/23 09:33:22.889
Jan  5 09:33:22.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:33:23.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-58" for this suite. 01/05/23 09:33:23.023
STEP: Destroying namespace "webhook-58-markers" for this suite. 01/05/23 09:33:23.03
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":117,"skipped":2063,"failed":0}
------------------------------
â€¢ [4.019 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:33:19.062
    Jan  5 09:33:19.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 09:33:19.063
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:19.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:19.09
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 09:33:19.115
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:33:19.678
    STEP: Deploying the webhook pod 01/05/23 09:33:19.686
    STEP: Wait for the deployment to be ready 01/05/23 09:33:19.711
    Jan  5 09:33:19.724: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 09:33:21.743
    STEP: Verifying the service has paired with the endpoint 01/05/23 09:33:21.759
    Jan  5 09:33:22.760: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/05/23 09:33:22.766
    STEP: Creating a custom resource definition that should be denied by the webhook 01/05/23 09:33:22.889
    Jan  5 09:33:22.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:33:23.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-58" for this suite. 01/05/23 09:33:23.023
    STEP: Destroying namespace "webhook-58-markers" for this suite. 01/05/23 09:33:23.03
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:33:23.082
Jan  5 09:33:23.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 09:33:23.083
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:23.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:23.107
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-3944 01/05/23 09:33:23.116
STEP: creating service affinity-clusterip in namespace services-3944 01/05/23 09:33:23.116
STEP: creating replication controller affinity-clusterip in namespace services-3944 01/05/23 09:33:23.132
I0105 09:33:23.140311      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3944, replica count: 3
I0105 09:33:26.190910      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 09:33:26.202: INFO: Creating new exec pod
Jan  5 09:33:26.211: INFO: Waiting up to 5m0s for pod "execpod-affinitykf8ms" in namespace "services-3944" to be "running"
Jan  5 09:33:26.218: INFO: Pod "execpod-affinitykf8ms": Phase="Pending", Reason="", readiness=false. Elapsed: 6.793949ms
Jan  5 09:33:28.223: INFO: Pod "execpod-affinitykf8ms": Phase="Running", Reason="", readiness=true. Elapsed: 2.011972722s
Jan  5 09:33:28.223: INFO: Pod "execpod-affinitykf8ms" satisfied condition "running"
Jan  5 09:33:29.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3944 exec execpod-affinitykf8ms -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan  5 09:33:29.598: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan  5 09:33:29.599: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:33:29.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3944 exec execpod-affinitykf8ms -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.122.122.106 80'
Jan  5 09:33:30.069: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.122.122.106 80\nConnection to 10.122.122.106 80 port [tcp/http] succeeded!\n"
Jan  5 09:33:30.069: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:33:30.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3944 exec execpod-affinitykf8ms -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.122.122.106:80/ ; done'
Jan  5 09:33:30.670: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n"
Jan  5 09:33:30.670: INFO: stdout: "\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6"
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
Jan  5 09:33:30.670: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3944, will wait for the garbage collector to delete the pods 01/05/23 09:33:30.679
Jan  5 09:33:30.742: INFO: Deleting ReplicationController affinity-clusterip took: 8.262898ms
Jan  5 09:33:30.843: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.026387ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:33:32.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3944" for this suite. 01/05/23 09:33:32.976
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":118,"skipped":2068,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.901 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:33:23.082
    Jan  5 09:33:23.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 09:33:23.083
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:23.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:23.107
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-3944 01/05/23 09:33:23.116
    STEP: creating service affinity-clusterip in namespace services-3944 01/05/23 09:33:23.116
    STEP: creating replication controller affinity-clusterip in namespace services-3944 01/05/23 09:33:23.132
    I0105 09:33:23.140311      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3944, replica count: 3
    I0105 09:33:26.190910      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 09:33:26.202: INFO: Creating new exec pod
    Jan  5 09:33:26.211: INFO: Waiting up to 5m0s for pod "execpod-affinitykf8ms" in namespace "services-3944" to be "running"
    Jan  5 09:33:26.218: INFO: Pod "execpod-affinitykf8ms": Phase="Pending", Reason="", readiness=false. Elapsed: 6.793949ms
    Jan  5 09:33:28.223: INFO: Pod "execpod-affinitykf8ms": Phase="Running", Reason="", readiness=true. Elapsed: 2.011972722s
    Jan  5 09:33:28.223: INFO: Pod "execpod-affinitykf8ms" satisfied condition "running"
    Jan  5 09:33:29.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3944 exec execpod-affinitykf8ms -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jan  5 09:33:29.598: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan  5 09:33:29.599: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:33:29.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3944 exec execpod-affinitykf8ms -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.122.122.106 80'
    Jan  5 09:33:30.069: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.122.122.106 80\nConnection to 10.122.122.106 80 port [tcp/http] succeeded!\n"
    Jan  5 09:33:30.069: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:33:30.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3944 exec execpod-affinitykf8ms -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.122.122.106:80/ ; done'
    Jan  5 09:33:30.670: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.122.122.106:80/\n"
    Jan  5 09:33:30.670: INFO: stdout: "\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6\naffinity-clusterip-g49l6"
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Received response from host: affinity-clusterip-g49l6
    Jan  5 09:33:30.670: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3944, will wait for the garbage collector to delete the pods 01/05/23 09:33:30.679
    Jan  5 09:33:30.742: INFO: Deleting ReplicationController affinity-clusterip took: 8.262898ms
    Jan  5 09:33:30.843: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.026387ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:33:32.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3944" for this suite. 01/05/23 09:33:32.976
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:33:32.983
Jan  5 09:33:32.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 09:33:32.984
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:33.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:33.009
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 01/05/23 09:33:33.017
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:33:33.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-172" for this suite. 01/05/23 09:33:33.038
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":119,"skipped":2071,"failed":0}
------------------------------
â€¢ [0.062 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:33:32.983
    Jan  5 09:33:32.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 09:33:32.984
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:33.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:33.009
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 01/05/23 09:33:33.017
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:33:33.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-172" for this suite. 01/05/23 09:33:33.038
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:33:33.048
Jan  5 09:33:33.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 09:33:33.049
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:33.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:33.071
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 01/05/23 09:33:33.079
Jan  5 09:33:33.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: mark a version not serverd 01/05/23 09:33:40.123
STEP: check the unserved version gets removed 01/05/23 09:33:40.15
STEP: check the other version is not changed 01/05/23 09:33:43.19
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:33:48.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8858" for this suite. 01/05/23 09:33:48.84
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":120,"skipped":2097,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.798 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:33:33.048
    Jan  5 09:33:33.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 09:33:33.049
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:33.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:33.071
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 01/05/23 09:33:33.079
    Jan  5 09:33:33.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: mark a version not serverd 01/05/23 09:33:40.123
    STEP: check the unserved version gets removed 01/05/23 09:33:40.15
    STEP: check the other version is not changed 01/05/23 09:33:43.19
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:33:48.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8858" for this suite. 01/05/23 09:33:48.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:33:48.846
Jan  5 09:33:48.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 09:33:48.847
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:48.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:48.865
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-3354 01/05/23 09:33:48.872
Jan  5 09:33:48.882: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3354" to be "running and ready"
Jan  5 09:33:48.887: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107982ms
Jan  5 09:33:48.887: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:33:50.894: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.011258074s
Jan  5 09:33:50.894: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan  5 09:33:50.894: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan  5 09:33:50.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3354 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan  5 09:33:51.320: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan  5 09:33:51.320: INFO: stdout: "iptables"
Jan  5 09:33:51.320: INFO: proxyMode: iptables
Jan  5 09:33:51.334: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan  5 09:33:51.340: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-3354 01/05/23 09:33:51.34
STEP: creating replication controller affinity-clusterip-timeout in namespace services-3354 01/05/23 09:33:51.352
I0105 09:33:51.363732      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3354, replica count: 3
I0105 09:33:54.414926      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 09:33:54.422: INFO: Creating new exec pod
Jan  5 09:33:54.437: INFO: Waiting up to 5m0s for pod "execpod-affinity5lz52" in namespace "services-3354" to be "running"
Jan  5 09:33:54.440: INFO: Pod "execpod-affinity5lz52": Phase="Pending", Reason="", readiness=false. Elapsed: 3.17978ms
Jan  5 09:33:56.446: INFO: Pod "execpod-affinity5lz52": Phase="Running", Reason="", readiness=true. Elapsed: 2.008880296s
Jan  5 09:33:56.446: INFO: Pod "execpod-affinity5lz52" satisfied condition "running"
Jan  5 09:33:57.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3354 exec execpod-affinity5lz52 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan  5 09:33:57.914: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan  5 09:33:57.914: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:33:57.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3354 exec execpod-affinity5lz52 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.120.164.107 80'
Jan  5 09:33:58.389: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.120.164.107 80\nConnection to 10.120.164.107 80 port [tcp/http] succeeded!\n"
Jan  5 09:33:58.389: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:33:58.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3354 exec execpod-affinity5lz52 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.120.164.107:80/ ; done'
Jan  5 09:33:58.973: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n"
Jan  5 09:33:58.973: INFO: stdout: "\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn"
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
Jan  5 09:33:58.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3354 exec execpod-affinity5lz52 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.120.164.107:80/'
Jan  5 09:33:59.410: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n"
Jan  5 09:33:59.410: INFO: stdout: "affinity-clusterip-timeout-vmscn"
Jan  5 09:34:19.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3354 exec execpod-affinity5lz52 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.120.164.107:80/'
Jan  5 09:34:20.045: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n"
Jan  5 09:34:20.045: INFO: stdout: "affinity-clusterip-timeout-jxzjq"
Jan  5 09:34:20.045: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3354, will wait for the garbage collector to delete the pods 01/05/23 09:34:20.057
Jan  5 09:34:20.123: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.91113ms
Jan  5 09:34:20.223: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.412802ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:34:22.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3354" for this suite. 01/05/23 09:34:22.357
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":121,"skipped":2107,"failed":0}
------------------------------
â€¢ [SLOW TEST] [33.521 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:33:48.846
    Jan  5 09:33:48.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 09:33:48.847
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:33:48.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:33:48.865
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-3354 01/05/23 09:33:48.872
    Jan  5 09:33:48.882: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3354" to be "running and ready"
    Jan  5 09:33:48.887: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107982ms
    Jan  5 09:33:48.887: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:33:50.894: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.011258074s
    Jan  5 09:33:50.894: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan  5 09:33:50.894: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan  5 09:33:50.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3354 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan  5 09:33:51.320: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan  5 09:33:51.320: INFO: stdout: "iptables"
    Jan  5 09:33:51.320: INFO: proxyMode: iptables
    Jan  5 09:33:51.334: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan  5 09:33:51.340: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-3354 01/05/23 09:33:51.34
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-3354 01/05/23 09:33:51.352
    I0105 09:33:51.363732      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3354, replica count: 3
    I0105 09:33:54.414926      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 09:33:54.422: INFO: Creating new exec pod
    Jan  5 09:33:54.437: INFO: Waiting up to 5m0s for pod "execpod-affinity5lz52" in namespace "services-3354" to be "running"
    Jan  5 09:33:54.440: INFO: Pod "execpod-affinity5lz52": Phase="Pending", Reason="", readiness=false. Elapsed: 3.17978ms
    Jan  5 09:33:56.446: INFO: Pod "execpod-affinity5lz52": Phase="Running", Reason="", readiness=true. Elapsed: 2.008880296s
    Jan  5 09:33:56.446: INFO: Pod "execpod-affinity5lz52" satisfied condition "running"
    Jan  5 09:33:57.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3354 exec execpod-affinity5lz52 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Jan  5 09:33:57.914: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Jan  5 09:33:57.914: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:33:57.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3354 exec execpod-affinity5lz52 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.120.164.107 80'
    Jan  5 09:33:58.389: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.120.164.107 80\nConnection to 10.120.164.107 80 port [tcp/http] succeeded!\n"
    Jan  5 09:33:58.389: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:33:58.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3354 exec execpod-affinity5lz52 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.120.164.107:80/ ; done'
    Jan  5 09:33:58.973: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n"
    Jan  5 09:33:58.973: INFO: stdout: "\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn\naffinity-clusterip-timeout-vmscn"
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Received response from host: affinity-clusterip-timeout-vmscn
    Jan  5 09:33:58.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3354 exec execpod-affinity5lz52 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.120.164.107:80/'
    Jan  5 09:33:59.410: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n"
    Jan  5 09:33:59.410: INFO: stdout: "affinity-clusterip-timeout-vmscn"
    Jan  5 09:34:19.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3354 exec execpod-affinity5lz52 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.120.164.107:80/'
    Jan  5 09:34:20.045: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.120.164.107:80/\n"
    Jan  5 09:34:20.045: INFO: stdout: "affinity-clusterip-timeout-jxzjq"
    Jan  5 09:34:20.045: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3354, will wait for the garbage collector to delete the pods 01/05/23 09:34:20.057
    Jan  5 09:34:20.123: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.91113ms
    Jan  5 09:34:20.223: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.412802ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:34:22.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3354" for this suite. 01/05/23 09:34:22.357
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:34:22.37
Jan  5 09:34:22.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename watch 01/05/23 09:34:22.37
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:34:22.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:34:22.394
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/05/23 09:34:22.401
STEP: starting a background goroutine to produce watch events 01/05/23 09:34:22.405
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/05/23 09:34:22.405
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  5 09:34:25.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1776" for this suite. 01/05/23 09:34:25.229
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":122,"skipped":2124,"failed":0}
------------------------------
â€¢ [2.909 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:34:22.37
    Jan  5 09:34:22.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename watch 01/05/23 09:34:22.37
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:34:22.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:34:22.394
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/05/23 09:34:22.401
    STEP: starting a background goroutine to produce watch events 01/05/23 09:34:22.405
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/05/23 09:34:22.405
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  5 09:34:25.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1776" for this suite. 01/05/23 09:34:25.229
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:34:25.281
Jan  5 09:34:25.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:34:25.282
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:34:25.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:34:25.308
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:34:25.314
Jan  5 09:34:25.323: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68" in namespace "downward-api-9337" to be "Succeeded or Failed"
Jan  5 09:34:25.328: INFO: Pod "downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68": Phase="Pending", Reason="", readiness=false. Elapsed: 5.354595ms
Jan  5 09:34:27.334: INFO: Pod "downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010921883s
Jan  5 09:34:29.335: INFO: Pod "downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012324417s
STEP: Saw pod success 01/05/23 09:34:29.335
Jan  5 09:34:29.335: INFO: Pod "downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68" satisfied condition "Succeeded or Failed"
Jan  5 09:34:29.339: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68 container client-container: <nil>
STEP: delete the pod 01/05/23 09:34:29.356
Jan  5 09:34:29.368: INFO: Waiting for pod downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68 to disappear
Jan  5 09:34:29.373: INFO: Pod downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 09:34:29.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9337" for this suite. 01/05/23 09:34:29.383
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":123,"skipped":2125,"failed":0}
------------------------------
â€¢ [4.108 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:34:25.281
    Jan  5 09:34:25.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:34:25.282
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:34:25.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:34:25.308
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:34:25.314
    Jan  5 09:34:25.323: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68" in namespace "downward-api-9337" to be "Succeeded or Failed"
    Jan  5 09:34:25.328: INFO: Pod "downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68": Phase="Pending", Reason="", readiness=false. Elapsed: 5.354595ms
    Jan  5 09:34:27.334: INFO: Pod "downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010921883s
    Jan  5 09:34:29.335: INFO: Pod "downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012324417s
    STEP: Saw pod success 01/05/23 09:34:29.335
    Jan  5 09:34:29.335: INFO: Pod "downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68" satisfied condition "Succeeded or Failed"
    Jan  5 09:34:29.339: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68 container client-container: <nil>
    STEP: delete the pod 01/05/23 09:34:29.356
    Jan  5 09:34:29.368: INFO: Waiting for pod downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68 to disappear
    Jan  5 09:34:29.373: INFO: Pod downwardapi-volume-85034de7-42bb-4d90-a04c-d0d88bd5af68 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 09:34:29.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9337" for this suite. 01/05/23 09:34:29.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:34:29.39
Jan  5 09:34:29.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 09:34:29.391
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:34:29.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:34:29.413
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/05/23 09:34:29.418
Jan  5 09:34:29.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:34:31.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:34:44.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5185" for this suite. 01/05/23 09:34:44.245
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":124,"skipped":2138,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.862 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:34:29.39
    Jan  5 09:34:29.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 09:34:29.391
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:34:29.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:34:29.413
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/05/23 09:34:29.418
    Jan  5 09:34:29.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:34:31.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:34:44.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5185" for this suite. 01/05/23 09:34:44.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:34:44.252
Jan  5 09:34:44.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename daemonsets 01/05/23 09:34:44.253
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:34:44.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:34:44.277
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jan  5 09:34:44.330: INFO: Create a RollingUpdate DaemonSet
Jan  5 09:34:44.342: INFO: Check that daemon pods launch on every node of the cluster
Jan  5 09:34:44.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:34:44.363: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:34:45.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 09:34:45.380: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r is running 0 daemon pod, expected 1
Jan  5 09:34:46.381: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  5 09:34:46.381: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
Jan  5 09:34:46.381: INFO: Update the DaemonSet to trigger a rollout
Jan  5 09:34:46.391: INFO: Updating DaemonSet daemon-set
Jan  5 09:34:49.430: INFO: Roll back the DaemonSet before rollout is complete
Jan  5 09:34:49.441: INFO: Updating DaemonSet daemon-set
Jan  5 09:34:49.441: INFO: Make sure DaemonSet rollback is complete
Jan  5 09:34:49.447: INFO: Wrong image for pod: daemon-set-n9bbn. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan  5 09:34:49.447: INFO: Pod daemon-set-n9bbn is not available
Jan  5 09:34:52.467: INFO: Pod daemon-set-zqpvj is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/05/23 09:34:52.486
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4929, will wait for the garbage collector to delete the pods 01/05/23 09:34:52.486
Jan  5 09:34:52.547: INFO: Deleting DaemonSet.extensions daemon-set took: 6.737457ms
Jan  5 09:34:52.648: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.161646ms
Jan  5 09:34:54.054: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:34:54.054: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 09:34:54.059: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18190"},"items":null}

Jan  5 09:34:54.063: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18190"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:34:54.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4929" for this suite. 01/05/23 09:34:54.105
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":125,"skipped":2147,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.859 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:34:44.252
    Jan  5 09:34:44.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename daemonsets 01/05/23 09:34:44.253
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:34:44.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:34:44.277
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jan  5 09:34:44.330: INFO: Create a RollingUpdate DaemonSet
    Jan  5 09:34:44.342: INFO: Check that daemon pods launch on every node of the cluster
    Jan  5 09:34:44.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:34:44.363: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:34:45.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 09:34:45.380: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r is running 0 daemon pod, expected 1
    Jan  5 09:34:46.381: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  5 09:34:46.381: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    Jan  5 09:34:46.381: INFO: Update the DaemonSet to trigger a rollout
    Jan  5 09:34:46.391: INFO: Updating DaemonSet daemon-set
    Jan  5 09:34:49.430: INFO: Roll back the DaemonSet before rollout is complete
    Jan  5 09:34:49.441: INFO: Updating DaemonSet daemon-set
    Jan  5 09:34:49.441: INFO: Make sure DaemonSet rollback is complete
    Jan  5 09:34:49.447: INFO: Wrong image for pod: daemon-set-n9bbn. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jan  5 09:34:49.447: INFO: Pod daemon-set-n9bbn is not available
    Jan  5 09:34:52.467: INFO: Pod daemon-set-zqpvj is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 09:34:52.486
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4929, will wait for the garbage collector to delete the pods 01/05/23 09:34:52.486
    Jan  5 09:34:52.547: INFO: Deleting DaemonSet.extensions daemon-set took: 6.737457ms
    Jan  5 09:34:52.648: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.161646ms
    Jan  5 09:34:54.054: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:34:54.054: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 09:34:54.059: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18190"},"items":null}

    Jan  5 09:34:54.063: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18190"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:34:54.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4929" for this suite. 01/05/23 09:34:54.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:34:54.112
Jan  5 09:34:54.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename var-expansion 01/05/23 09:34:54.113
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:34:54.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:34:54.135
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 01/05/23 09:34:54.142
Jan  5 09:34:54.151: INFO: Waiting up to 5m0s for pod "var-expansion-e421de87-774e-4bde-a457-4d6aaa561565" in namespace "var-expansion-1252" to be "Succeeded or Failed"
Jan  5 09:34:54.155: INFO: Pod "var-expansion-e421de87-774e-4bde-a457-4d6aaa561565": Phase="Pending", Reason="", readiness=false. Elapsed: 3.752338ms
Jan  5 09:34:56.161: INFO: Pod "var-expansion-e421de87-774e-4bde-a457-4d6aaa561565": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00993292s
Jan  5 09:34:58.160: INFO: Pod "var-expansion-e421de87-774e-4bde-a457-4d6aaa561565": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009427289s
STEP: Saw pod success 01/05/23 09:34:58.16
Jan  5 09:34:58.161: INFO: Pod "var-expansion-e421de87-774e-4bde-a457-4d6aaa561565" satisfied condition "Succeeded or Failed"
Jan  5 09:34:58.165: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod var-expansion-e421de87-774e-4bde-a457-4d6aaa561565 container dapi-container: <nil>
STEP: delete the pod 01/05/23 09:34:58.179
Jan  5 09:34:58.189: INFO: Waiting for pod var-expansion-e421de87-774e-4bde-a457-4d6aaa561565 to disappear
Jan  5 09:34:58.193: INFO: Pod var-expansion-e421de87-774e-4bde-a457-4d6aaa561565 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 09:34:58.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1252" for this suite. 01/05/23 09:34:58.211
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":126,"skipped":2189,"failed":0}
------------------------------
â€¢ [4.105 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:34:54.112
    Jan  5 09:34:54.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename var-expansion 01/05/23 09:34:54.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:34:54.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:34:54.135
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 01/05/23 09:34:54.142
    Jan  5 09:34:54.151: INFO: Waiting up to 5m0s for pod "var-expansion-e421de87-774e-4bde-a457-4d6aaa561565" in namespace "var-expansion-1252" to be "Succeeded or Failed"
    Jan  5 09:34:54.155: INFO: Pod "var-expansion-e421de87-774e-4bde-a457-4d6aaa561565": Phase="Pending", Reason="", readiness=false. Elapsed: 3.752338ms
    Jan  5 09:34:56.161: INFO: Pod "var-expansion-e421de87-774e-4bde-a457-4d6aaa561565": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00993292s
    Jan  5 09:34:58.160: INFO: Pod "var-expansion-e421de87-774e-4bde-a457-4d6aaa561565": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009427289s
    STEP: Saw pod success 01/05/23 09:34:58.16
    Jan  5 09:34:58.161: INFO: Pod "var-expansion-e421de87-774e-4bde-a457-4d6aaa561565" satisfied condition "Succeeded or Failed"
    Jan  5 09:34:58.165: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod var-expansion-e421de87-774e-4bde-a457-4d6aaa561565 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 09:34:58.179
    Jan  5 09:34:58.189: INFO: Waiting for pod var-expansion-e421de87-774e-4bde-a457-4d6aaa561565 to disappear
    Jan  5 09:34:58.193: INFO: Pod var-expansion-e421de87-774e-4bde-a457-4d6aaa561565 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 09:34:58.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1252" for this suite. 01/05/23 09:34:58.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:34:58.22
Jan  5 09:34:58.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pods 01/05/23 09:34:58.221
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:34:58.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:34:58.242
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 01/05/23 09:34:58.248
Jan  5 09:34:58.259: INFO: Waiting up to 5m0s for pod "pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365" in namespace "pods-7199" to be "running and ready"
Jan  5 09:34:58.269: INFO: Pod "pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365": Phase="Pending", Reason="", readiness=false. Elapsed: 10.321863ms
Jan  5 09:34:58.269: INFO: The phase of Pod pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:35:00.276: INFO: Pod "pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365": Phase="Running", Reason="", readiness=true. Elapsed: 2.017047492s
Jan  5 09:35:00.276: INFO: The phase of Pod pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365 is Running (Ready = true)
Jan  5 09:35:00.276: INFO: Pod "pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365" satisfied condition "running and ready"
Jan  5 09:35:00.284: INFO: Pod pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365 has hostIP: 10.250.2.138
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 09:35:00.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7199" for this suite. 01/05/23 09:35:00.293
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":127,"skipped":2211,"failed":0}
------------------------------
â€¢ [2.080 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:34:58.22
    Jan  5 09:34:58.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pods 01/05/23 09:34:58.221
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:34:58.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:34:58.242
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 01/05/23 09:34:58.248
    Jan  5 09:34:58.259: INFO: Waiting up to 5m0s for pod "pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365" in namespace "pods-7199" to be "running and ready"
    Jan  5 09:34:58.269: INFO: Pod "pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365": Phase="Pending", Reason="", readiness=false. Elapsed: 10.321863ms
    Jan  5 09:34:58.269: INFO: The phase of Pod pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:35:00.276: INFO: Pod "pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365": Phase="Running", Reason="", readiness=true. Elapsed: 2.017047492s
    Jan  5 09:35:00.276: INFO: The phase of Pod pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365 is Running (Ready = true)
    Jan  5 09:35:00.276: INFO: Pod "pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365" satisfied condition "running and ready"
    Jan  5 09:35:00.284: INFO: Pod pod-hostip-a61e94aa-99fd-4515-a792-8bd2c5425365 has hostIP: 10.250.2.138
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 09:35:00.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7199" for this suite. 01/05/23 09:35:00.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:35:00.301
Jan  5 09:35:00.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 09:35:00.302
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:35:00.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:35:00.325
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/05/23 09:35:00.334
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/05/23 09:35:00.338
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/05/23 09:35:00.338
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/05/23 09:35:00.338
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/05/23 09:35:00.341
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/05/23 09:35:00.341
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/05/23 09:35:00.345
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:35:00.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-986" for this suite. 01/05/23 09:35:00.355
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":128,"skipped":2218,"failed":0}
------------------------------
â€¢ [0.060 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:35:00.301
    Jan  5 09:35:00.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 09:35:00.302
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:35:00.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:35:00.325
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/05/23 09:35:00.334
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/05/23 09:35:00.338
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/05/23 09:35:00.338
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/05/23 09:35:00.338
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/05/23 09:35:00.341
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/05/23 09:35:00.341
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/05/23 09:35:00.345
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:35:00.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-986" for this suite. 01/05/23 09:35:00.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:35:00.361
Jan  5 09:35:00.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:35:00.362
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:35:00.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:35:00.381
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-8c2e53a4-576a-4077-b902-e9555eaedb47 01/05/23 09:35:00.388
STEP: Creating a pod to test consume secrets 01/05/23 09:35:00.394
Jan  5 09:35:00.405: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c" in namespace "projected-890" to be "Succeeded or Failed"
Jan  5 09:35:00.412: INFO: Pod "pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.199769ms
Jan  5 09:35:02.420: INFO: Pod "pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015241848s
Jan  5 09:35:04.419: INFO: Pod "pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013856582s
STEP: Saw pod success 01/05/23 09:35:04.419
Jan  5 09:35:04.419: INFO: Pod "pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c" satisfied condition "Succeeded or Failed"
Jan  5 09:35:04.425: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 09:35:04.436
Jan  5 09:35:04.449: INFO: Waiting for pod pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c to disappear
Jan  5 09:35:04.454: INFO: Pod pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 09:35:04.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-890" for this suite. 01/05/23 09:35:04.463
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":129,"skipped":2228,"failed":0}
------------------------------
â€¢ [4.110 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:35:00.361
    Jan  5 09:35:00.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:35:00.362
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:35:00.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:35:00.381
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-8c2e53a4-576a-4077-b902-e9555eaedb47 01/05/23 09:35:00.388
    STEP: Creating a pod to test consume secrets 01/05/23 09:35:00.394
    Jan  5 09:35:00.405: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c" in namespace "projected-890" to be "Succeeded or Failed"
    Jan  5 09:35:00.412: INFO: Pod "pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.199769ms
    Jan  5 09:35:02.420: INFO: Pod "pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015241848s
    Jan  5 09:35:04.419: INFO: Pod "pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013856582s
    STEP: Saw pod success 01/05/23 09:35:04.419
    Jan  5 09:35:04.419: INFO: Pod "pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c" satisfied condition "Succeeded or Failed"
    Jan  5 09:35:04.425: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 09:35:04.436
    Jan  5 09:35:04.449: INFO: Waiting for pod pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c to disappear
    Jan  5 09:35:04.454: INFO: Pod pod-projected-secrets-da544363-8345-47ae-b02f-ac8d7858fb9c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 09:35:04.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-890" for this suite. 01/05/23 09:35:04.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:35:04.473
Jan  5 09:35:04.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename var-expansion 01/05/23 09:35:04.474
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:35:04.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:35:04.498
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 01/05/23 09:35:04.504
Jan  5 09:35:04.515: INFO: Waiting up to 5m0s for pod "var-expansion-c99b5416-a7cd-4b95-b920-44930b475832" in namespace "var-expansion-3436" to be "Succeeded or Failed"
Jan  5 09:35:04.528: INFO: Pod "var-expansion-c99b5416-a7cd-4b95-b920-44930b475832": Phase="Pending", Reason="", readiness=false. Elapsed: 12.479907ms
Jan  5 09:35:06.534: INFO: Pod "var-expansion-c99b5416-a7cd-4b95-b920-44930b475832": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018251464s
Jan  5 09:35:08.534: INFO: Pod "var-expansion-c99b5416-a7cd-4b95-b920-44930b475832": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018776446s
STEP: Saw pod success 01/05/23 09:35:08.534
Jan  5 09:35:08.534: INFO: Pod "var-expansion-c99b5416-a7cd-4b95-b920-44930b475832" satisfied condition "Succeeded or Failed"
Jan  5 09:35:08.538: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod var-expansion-c99b5416-a7cd-4b95-b920-44930b475832 container dapi-container: <nil>
STEP: delete the pod 01/05/23 09:35:08.55
Jan  5 09:35:08.558: INFO: Waiting for pod var-expansion-c99b5416-a7cd-4b95-b920-44930b475832 to disappear
Jan  5 09:35:08.565: INFO: Pod var-expansion-c99b5416-a7cd-4b95-b920-44930b475832 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 09:35:08.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3436" for this suite. 01/05/23 09:35:08.574
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":130,"skipped":2238,"failed":0}
------------------------------
â€¢ [4.107 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:35:04.473
    Jan  5 09:35:04.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename var-expansion 01/05/23 09:35:04.474
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:35:04.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:35:04.498
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 01/05/23 09:35:04.504
    Jan  5 09:35:04.515: INFO: Waiting up to 5m0s for pod "var-expansion-c99b5416-a7cd-4b95-b920-44930b475832" in namespace "var-expansion-3436" to be "Succeeded or Failed"
    Jan  5 09:35:04.528: INFO: Pod "var-expansion-c99b5416-a7cd-4b95-b920-44930b475832": Phase="Pending", Reason="", readiness=false. Elapsed: 12.479907ms
    Jan  5 09:35:06.534: INFO: Pod "var-expansion-c99b5416-a7cd-4b95-b920-44930b475832": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018251464s
    Jan  5 09:35:08.534: INFO: Pod "var-expansion-c99b5416-a7cd-4b95-b920-44930b475832": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018776446s
    STEP: Saw pod success 01/05/23 09:35:08.534
    Jan  5 09:35:08.534: INFO: Pod "var-expansion-c99b5416-a7cd-4b95-b920-44930b475832" satisfied condition "Succeeded or Failed"
    Jan  5 09:35:08.538: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod var-expansion-c99b5416-a7cd-4b95-b920-44930b475832 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 09:35:08.55
    Jan  5 09:35:08.558: INFO: Waiting for pod var-expansion-c99b5416-a7cd-4b95-b920-44930b475832 to disappear
    Jan  5 09:35:08.565: INFO: Pod var-expansion-c99b5416-a7cd-4b95-b920-44930b475832 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 09:35:08.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3436" for this suite. 01/05/23 09:35:08.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:35:08.582
Jan  5 09:35:08.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename sched-preemption 01/05/23 09:35:08.582
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:35:08.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:35:08.603
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan  5 09:35:08.628: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 09:36:08.701: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 01/05/23 09:36:08.705
Jan  5 09:36:08.730: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan  5 09:36:08.738: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan  5 09:36:08.766: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan  5 09:36:08.774: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan  5 09:36:08.799: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan  5 09:36:08.809: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jan  5 09:36:08.833: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jan  5 09:36:08.841: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/05/23 09:36:08.841
Jan  5 09:36:08.841: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1274" to be "running"
Jan  5 09:36:08.849: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.568831ms
Jan  5 09:36:10.857: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016445375s
Jan  5 09:36:12.857: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.015787553s
Jan  5 09:36:12.857: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan  5 09:36:12.857: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
Jan  5 09:36:12.862: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.372903ms
Jan  5 09:36:12.862: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 09:36:12.862: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
Jan  5 09:36:12.867: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.416941ms
Jan  5 09:36:14.874: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011503473s
Jan  5 09:36:16.873: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010457221s
Jan  5 09:36:18.876: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013724602s
Jan  5 09:36:20.874: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.011836452s
Jan  5 09:36:20.874: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 09:36:20.874: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
Jan  5 09:36:20.880: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.523334ms
Jan  5 09:36:20.880: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 09:36:20.880: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
Jan  5 09:36:20.885: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.077724ms
Jan  5 09:36:20.885: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 09:36:20.886: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
Jan  5 09:36:20.891: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.976504ms
Jan  5 09:36:20.891: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 09:36:20.891: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
Jan  5 09:36:20.896: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.040643ms
Jan  5 09:36:20.896: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 09:36:20.896: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
Jan  5 09:36:20.901: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.718352ms
Jan  5 09:36:20.901: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/05/23 09:36:20.901
Jan  5 09:36:20.926: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan  5 09:36:20.929: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.786113ms
Jan  5 09:36:22.945: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018927324s
Jan  5 09:36:24.936: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010376944s
Jan  5 09:36:24.936: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:36:24.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1274" for this suite. 01/05/23 09:36:25.001
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":131,"skipped":2268,"failed":0}
------------------------------
â€¢ [SLOW TEST] [76.500 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:35:08.582
    Jan  5 09:35:08.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename sched-preemption 01/05/23 09:35:08.582
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:35:08.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:35:08.603
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan  5 09:35:08.628: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 09:36:08.701: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 01/05/23 09:36:08.705
    Jan  5 09:36:08.730: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan  5 09:36:08.738: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan  5 09:36:08.766: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan  5 09:36:08.774: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan  5 09:36:08.799: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan  5 09:36:08.809: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Jan  5 09:36:08.833: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Jan  5 09:36:08.841: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/05/23 09:36:08.841
    Jan  5 09:36:08.841: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1274" to be "running"
    Jan  5 09:36:08.849: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.568831ms
    Jan  5 09:36:10.857: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016445375s
    Jan  5 09:36:12.857: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.015787553s
    Jan  5 09:36:12.857: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan  5 09:36:12.857: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
    Jan  5 09:36:12.862: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.372903ms
    Jan  5 09:36:12.862: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 09:36:12.862: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
    Jan  5 09:36:12.867: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.416941ms
    Jan  5 09:36:14.874: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011503473s
    Jan  5 09:36:16.873: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010457221s
    Jan  5 09:36:18.876: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013724602s
    Jan  5 09:36:20.874: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.011836452s
    Jan  5 09:36:20.874: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 09:36:20.874: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
    Jan  5 09:36:20.880: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.523334ms
    Jan  5 09:36:20.880: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 09:36:20.880: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
    Jan  5 09:36:20.885: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.077724ms
    Jan  5 09:36:20.885: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 09:36:20.886: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
    Jan  5 09:36:20.891: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.976504ms
    Jan  5 09:36:20.891: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 09:36:20.891: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
    Jan  5 09:36:20.896: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.040643ms
    Jan  5 09:36:20.896: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 09:36:20.896: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-1274" to be "running"
    Jan  5 09:36:20.901: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.718352ms
    Jan  5 09:36:20.901: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/05/23 09:36:20.901
    Jan  5 09:36:20.926: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan  5 09:36:20.929: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.786113ms
    Jan  5 09:36:22.945: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018927324s
    Jan  5 09:36:24.936: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010376944s
    Jan  5 09:36:24.936: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:36:24.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-1274" for this suite. 01/05/23 09:36:25.001
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:36:25.083
Jan  5 09:36:25.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename statefulset 01/05/23 09:36:25.084
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:36:25.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:36:25.107
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-428 01/05/23 09:36:25.114
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-428 01/05/23 09:36:25.121
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-428 01/05/23 09:36:25.129
Jan  5 09:36:25.134: INFO: Found 0 stateful pods, waiting for 1
Jan  5 09:36:35.141: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/05/23 09:36:35.141
Jan  5 09:36:35.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 09:36:35.640: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 09:36:35.640: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 09:36:35.640: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 09:36:35.646: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  5 09:36:45.657: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 09:36:45.657: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 09:36:45.676: INFO: POD   NODE                                                        PHASE    GRACE  CONDITIONS
Jan  5 09:36:45.676: INFO: ss-0  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:25 +0000 UTC  }]
Jan  5 09:36:45.676: INFO: 
Jan  5 09:36:45.676: INFO: StatefulSet ss has not reached scale 3, at 1
Jan  5 09:36:46.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994957396s
Jan  5 09:36:47.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988079585s
Jan  5 09:36:48.696: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98147017s
Jan  5 09:36:49.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975511496s
Jan  5 09:36:50.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968387322s
Jan  5 09:36:51.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962322661s
Jan  5 09:36:52.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955757581s
Jan  5 09:36:53.731: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.948921641s
Jan  5 09:36:54.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.712578ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-428 01/05/23 09:36:55.737
Jan  5 09:36:55.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:36:56.303: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 09:36:56.303: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 09:36:56.303: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 09:36:56.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:36:56.828: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  5 09:36:56.829: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 09:36:56.829: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 09:36:56.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:36:57.338: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  5 09:36:57.338: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 09:36:57.338: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 09:36:57.345: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 09:36:57.345: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 09:36:57.345: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/05/23 09:36:57.345
Jan  5 09:36:57.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 09:36:57.866: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 09:36:57.866: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 09:36:57.866: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 09:36:57.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 09:36:58.363: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 09:36:58.363: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 09:36:58.363: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 09:36:58.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 09:36:58.877: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 09:36:58.877: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 09:36:58.877: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 09:36:58.877: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 09:36:58.882: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan  5 09:37:08.896: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 09:37:08.896: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 09:37:08.896: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 09:37:08.921: INFO: POD   NODE                                                        PHASE    GRACE  CONDITIONS
Jan  5 09:37:08.921: INFO: ss-0  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:25 +0000 UTC  }]
Jan  5 09:37:08.921: INFO: ss-1  shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:45 +0000 UTC  }]
Jan  5 09:37:08.921: INFO: ss-2  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:45 +0000 UTC  }]
Jan  5 09:37:08.921: INFO: 
Jan  5 09:37:08.921: INFO: StatefulSet ss has not reached scale 0, at 3
Jan  5 09:37:09.929: INFO: POD   NODE                                                        PHASE    GRACE  CONDITIONS
Jan  5 09:37:09.929: INFO: ss-0  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:25 +0000 UTC  }]
Jan  5 09:37:09.929: INFO: ss-1  shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:45 +0000 UTC  }]
Jan  5 09:37:09.929: INFO: 
Jan  5 09:37:09.929: INFO: StatefulSet ss has not reached scale 0, at 2
Jan  5 09:37:10.935: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.98155947s
Jan  5 09:37:11.941: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.975718933s
Jan  5 09:37:12.948: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.969568618s
Jan  5 09:37:13.954: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.963526388s
Jan  5 09:37:14.959: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.957351655s
Jan  5 09:37:15.965: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.95200192s
Jan  5 09:37:16.971: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.946072248s
Jan  5 09:37:17.976: INFO: Verifying statefulset ss doesn't scale past 0 for another 940.556984ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-428 01/05/23 09:37:18.976
Jan  5 09:37:18.985: INFO: Scaling statefulset ss to 0
Jan  5 09:37:19.011: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 09:37:19.023: INFO: Deleting all statefulset in ns statefulset-428
Jan  5 09:37:19.036: INFO: Scaling statefulset ss to 0
Jan  5 09:37:19.060: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 09:37:19.064: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 09:37:19.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-428" for this suite. 01/05/23 09:37:19.086
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":132,"skipped":2289,"failed":0}
------------------------------
â€¢ [SLOW TEST] [54.009 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:36:25.083
    Jan  5 09:36:25.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename statefulset 01/05/23 09:36:25.084
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:36:25.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:36:25.107
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-428 01/05/23 09:36:25.114
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-428 01/05/23 09:36:25.121
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-428 01/05/23 09:36:25.129
    Jan  5 09:36:25.134: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 09:36:35.141: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/05/23 09:36:35.141
    Jan  5 09:36:35.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 09:36:35.640: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 09:36:35.640: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 09:36:35.640: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 09:36:35.646: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan  5 09:36:45.657: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 09:36:45.657: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 09:36:45.676: INFO: POD   NODE                                                        PHASE    GRACE  CONDITIONS
    Jan  5 09:36:45.676: INFO: ss-0  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:25 +0000 UTC  }]
    Jan  5 09:36:45.676: INFO: 
    Jan  5 09:36:45.676: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan  5 09:36:46.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994957396s
    Jan  5 09:36:47.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988079585s
    Jan  5 09:36:48.696: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98147017s
    Jan  5 09:36:49.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975511496s
    Jan  5 09:36:50.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968387322s
    Jan  5 09:36:51.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962322661s
    Jan  5 09:36:52.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955757581s
    Jan  5 09:36:53.731: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.948921641s
    Jan  5 09:36:54.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.712578ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-428 01/05/23 09:36:55.737
    Jan  5 09:36:55.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:36:56.303: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 09:36:56.303: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 09:36:56.303: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 09:36:56.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:36:56.828: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan  5 09:36:56.829: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 09:36:56.829: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 09:36:56.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:36:57.338: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan  5 09:36:57.338: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 09:36:57.338: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 09:36:57.345: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 09:36:57.345: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 09:36:57.345: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/05/23 09:36:57.345
    Jan  5 09:36:57.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 09:36:57.866: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 09:36:57.866: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 09:36:57.866: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 09:36:57.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 09:36:58.363: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 09:36:58.363: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 09:36:58.363: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 09:36:58.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-428 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 09:36:58.877: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 09:36:58.877: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 09:36:58.877: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 09:36:58.877: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 09:36:58.882: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan  5 09:37:08.896: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 09:37:08.896: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 09:37:08.896: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 09:37:08.921: INFO: POD   NODE                                                        PHASE    GRACE  CONDITIONS
    Jan  5 09:37:08.921: INFO: ss-0  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:25 +0000 UTC  }]
    Jan  5 09:37:08.921: INFO: ss-1  shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:45 +0000 UTC  }]
    Jan  5 09:37:08.921: INFO: ss-2  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:45 +0000 UTC  }]
    Jan  5 09:37:08.921: INFO: 
    Jan  5 09:37:08.921: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan  5 09:37:09.929: INFO: POD   NODE                                                        PHASE    GRACE  CONDITIONS
    Jan  5 09:37:09.929: INFO: ss-0  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:25 +0000 UTC  }]
    Jan  5 09:37:09.929: INFO: ss-1  shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 09:36:45 +0000 UTC  }]
    Jan  5 09:37:09.929: INFO: 
    Jan  5 09:37:09.929: INFO: StatefulSet ss has not reached scale 0, at 2
    Jan  5 09:37:10.935: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.98155947s
    Jan  5 09:37:11.941: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.975718933s
    Jan  5 09:37:12.948: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.969568618s
    Jan  5 09:37:13.954: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.963526388s
    Jan  5 09:37:14.959: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.957351655s
    Jan  5 09:37:15.965: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.95200192s
    Jan  5 09:37:16.971: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.946072248s
    Jan  5 09:37:17.976: INFO: Verifying statefulset ss doesn't scale past 0 for another 940.556984ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-428 01/05/23 09:37:18.976
    Jan  5 09:37:18.985: INFO: Scaling statefulset ss to 0
    Jan  5 09:37:19.011: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 09:37:19.023: INFO: Deleting all statefulset in ns statefulset-428
    Jan  5 09:37:19.036: INFO: Scaling statefulset ss to 0
    Jan  5 09:37:19.060: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 09:37:19.064: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 09:37:19.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-428" for this suite. 01/05/23 09:37:19.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:37:19.095
Jan  5 09:37:19.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename deployment 01/05/23 09:37:19.095
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:37:19.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:37:19.131
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan  5 09:37:19.149: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan  5 09:37:24.211: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 09:37:24.211
Jan  5 09:37:24.212: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan  5 09:37:26.220: INFO: Creating deployment "test-rollover-deployment"
Jan  5 09:37:26.234: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan  5 09:37:28.251: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan  5 09:37:28.263: INFO: Ensure that both replica sets have 1 created replica
Jan  5 09:37:28.273: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan  5 09:37:28.286: INFO: Updating deployment test-rollover-deployment
Jan  5 09:37:28.286: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan  5 09:37:30.302: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan  5 09:37:30.317: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan  5 09:37:30.330: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 09:37:30.330: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:37:32.342: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 09:37:32.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:37:34.344: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 09:37:34.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:37:36.341: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 09:37:36.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:37:38.343: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 09:37:38.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:37:40.346: INFO: 
Jan  5 09:37:40.346: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 09:37:40.361: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1450  3b85733c-a67b-4eb5-a7e4-0b187daebdcd 19331 2 2023-01-05 09:37:26 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 09:37:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:37:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b65088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 09:37:26 +0000 UTC,LastTransitionTime:2023-01-05 09:37:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-05 09:37:39 +0000 UTC,LastTransitionTime:2023-01-05 09:37:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  5 09:37:40.366: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-1450  16a83e13-a0c6-413c-91b1-351075a6df59 19324 2 2023-01-05 09:37:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3b85733c-a67b-4eb5-a7e4-0b187daebdcd 0xc001b657d7 0xc001b657d8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 09:37:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b85733c-a67b-4eb5-a7e4-0b187daebdcd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:37:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b65888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 09:37:40.366: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan  5 09:37:40.366: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1450  de575092-f8b6-481f-b75d-afff96b997c2 19330 2 2023-01-05 09:37:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3b85733c-a67b-4eb5-a7e4-0b187daebdcd 0xc001b65587 0xc001b65588}] [] [{e2e.test Update apps/v1 2023-01-05 09:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:37:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b85733c-a67b-4eb5-a7e4-0b187daebdcd\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:37:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001b65648 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 09:37:40.366: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-1450  a00c8c63-713a-48d1-991a-b4380daccfe6 19268 2 2023-01-05 09:37:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3b85733c-a67b-4eb5-a7e4-0b187daebdcd 0xc001b656b7 0xc001b656b8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 09:37:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b85733c-a67b-4eb5-a7e4-0b187daebdcd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:37:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b65768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 09:37:40.372: INFO: Pod "test-rollover-deployment-6d45fd857b-cbg98" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-cbg98 test-rollover-deployment-6d45fd857b- deployment-1450  e40362c5-1ff2-4619-9cf8-bd98e454edf3 19275 0 2023-01-05 09:37:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 16a83e13-a0c6-413c-91b1-351075a6df59 0xc001b65dc7 0xc001b65dc8}] [] [{kube-controller-manager Update v1 2023-01-05 09:37:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16a83e13-a0c6-413c-91b1-351075a6df59\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 09:37:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kh5c6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kh5c6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:37:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:37:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:37:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:37:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.90,StartTime:2023-01-05 09:37:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 09:37:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://d5afecfe640dd242c43e6f9ff02f1f5bb8f4cfb29f8e06b368842e5ebd39a188,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 09:37:40.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1450" for this suite. 01/05/23 09:37:40.381
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":133,"skipped":2326,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.293 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:37:19.095
    Jan  5 09:37:19.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename deployment 01/05/23 09:37:19.095
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:37:19.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:37:19.131
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan  5 09:37:19.149: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan  5 09:37:24.211: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 09:37:24.211
    Jan  5 09:37:24.212: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan  5 09:37:26.220: INFO: Creating deployment "test-rollover-deployment"
    Jan  5 09:37:26.234: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan  5 09:37:28.251: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan  5 09:37:28.263: INFO: Ensure that both replica sets have 1 created replica
    Jan  5 09:37:28.273: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan  5 09:37:28.286: INFO: Updating deployment test-rollover-deployment
    Jan  5 09:37:28.286: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan  5 09:37:30.302: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan  5 09:37:30.317: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan  5 09:37:30.330: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 09:37:30.330: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:37:32.342: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 09:37:32.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:37:34.344: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 09:37:34.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:37:36.341: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 09:37:36.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:37:38.343: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 09:37:38.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 37, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 37, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:37:40.346: INFO: 
    Jan  5 09:37:40.346: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 09:37:40.361: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-1450  3b85733c-a67b-4eb5-a7e4-0b187daebdcd 19331 2 2023-01-05 09:37:26 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 09:37:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:37:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b65088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 09:37:26 +0000 UTC,LastTransitionTime:2023-01-05 09:37:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-05 09:37:39 +0000 UTC,LastTransitionTime:2023-01-05 09:37:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  5 09:37:40.366: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-1450  16a83e13-a0c6-413c-91b1-351075a6df59 19324 2 2023-01-05 09:37:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3b85733c-a67b-4eb5-a7e4-0b187daebdcd 0xc001b657d7 0xc001b657d8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 09:37:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b85733c-a67b-4eb5-a7e4-0b187daebdcd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:37:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b65888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 09:37:40.366: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan  5 09:37:40.366: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1450  de575092-f8b6-481f-b75d-afff96b997c2 19330 2 2023-01-05 09:37:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3b85733c-a67b-4eb5-a7e4-0b187daebdcd 0xc001b65587 0xc001b65588}] [] [{e2e.test Update apps/v1 2023-01-05 09:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:37:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b85733c-a67b-4eb5-a7e4-0b187daebdcd\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:37:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001b65648 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 09:37:40.366: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-1450  a00c8c63-713a-48d1-991a-b4380daccfe6 19268 2 2023-01-05 09:37:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3b85733c-a67b-4eb5-a7e4-0b187daebdcd 0xc001b656b7 0xc001b656b8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 09:37:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b85733c-a67b-4eb5-a7e4-0b187daebdcd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 09:37:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b65768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 09:37:40.372: INFO: Pod "test-rollover-deployment-6d45fd857b-cbg98" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-cbg98 test-rollover-deployment-6d45fd857b- deployment-1450  e40362c5-1ff2-4619-9cf8-bd98e454edf3 19275 0 2023-01-05 09:37:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 16a83e13-a0c6-413c-91b1-351075a6df59 0xc001b65dc7 0xc001b65dc8}] [] [{kube-controller-manager Update v1 2023-01-05 09:37:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16a83e13-a0c6-413c-91b1-351075a6df59\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 09:37:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kh5c6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kh5c6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:37:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:37:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:37:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 09:37:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.90,StartTime:2023-01-05 09:37:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 09:37:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://d5afecfe640dd242c43e6f9ff02f1f5bb8f4cfb29f8e06b368842e5ebd39a188,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 09:37:40.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1450" for this suite. 01/05/23 09:37:40.381
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:37:40.388
Jan  5 09:37:40.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename replicaset 01/05/23 09:37:40.389
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:37:40.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:37:40.415
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan  5 09:37:40.421: INFO: Creating ReplicaSet my-hostname-basic-830458ba-6445-4382-a906-4a948139979e
Jan  5 09:37:40.434: INFO: Pod name my-hostname-basic-830458ba-6445-4382-a906-4a948139979e: Found 0 pods out of 1
Jan  5 09:37:45.447: INFO: Pod name my-hostname-basic-830458ba-6445-4382-a906-4a948139979e: Found 1 pods out of 1
Jan  5 09:37:45.447: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-830458ba-6445-4382-a906-4a948139979e" is running
Jan  5 09:37:45.447: INFO: Waiting up to 5m0s for pod "my-hostname-basic-830458ba-6445-4382-a906-4a948139979e-sxdj4" in namespace "replicaset-7935" to be "running"
Jan  5 09:37:45.451: INFO: Pod "my-hostname-basic-830458ba-6445-4382-a906-4a948139979e-sxdj4": Phase="Running", Reason="", readiness=true. Elapsed: 4.501049ms
Jan  5 09:37:45.451: INFO: Pod "my-hostname-basic-830458ba-6445-4382-a906-4a948139979e-sxdj4" satisfied condition "running"
Jan  5 09:37:45.451: INFO: Pod "my-hostname-basic-830458ba-6445-4382-a906-4a948139979e-sxdj4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:37:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:37:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:37:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:37:40 +0000 UTC Reason: Message:}])
Jan  5 09:37:45.452: INFO: Trying to dial the pod
Jan  5 09:37:50.584: INFO: Controller my-hostname-basic-830458ba-6445-4382-a906-4a948139979e: Got expected result from replica 1 [my-hostname-basic-830458ba-6445-4382-a906-4a948139979e-sxdj4]: "my-hostname-basic-830458ba-6445-4382-a906-4a948139979e-sxdj4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  5 09:37:50.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7935" for this suite. 01/05/23 09:37:50.596
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":134,"skipped":2329,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.214 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:37:40.388
    Jan  5 09:37:40.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename replicaset 01/05/23 09:37:40.389
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:37:40.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:37:40.415
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan  5 09:37:40.421: INFO: Creating ReplicaSet my-hostname-basic-830458ba-6445-4382-a906-4a948139979e
    Jan  5 09:37:40.434: INFO: Pod name my-hostname-basic-830458ba-6445-4382-a906-4a948139979e: Found 0 pods out of 1
    Jan  5 09:37:45.447: INFO: Pod name my-hostname-basic-830458ba-6445-4382-a906-4a948139979e: Found 1 pods out of 1
    Jan  5 09:37:45.447: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-830458ba-6445-4382-a906-4a948139979e" is running
    Jan  5 09:37:45.447: INFO: Waiting up to 5m0s for pod "my-hostname-basic-830458ba-6445-4382-a906-4a948139979e-sxdj4" in namespace "replicaset-7935" to be "running"
    Jan  5 09:37:45.451: INFO: Pod "my-hostname-basic-830458ba-6445-4382-a906-4a948139979e-sxdj4": Phase="Running", Reason="", readiness=true. Elapsed: 4.501049ms
    Jan  5 09:37:45.451: INFO: Pod "my-hostname-basic-830458ba-6445-4382-a906-4a948139979e-sxdj4" satisfied condition "running"
    Jan  5 09:37:45.451: INFO: Pod "my-hostname-basic-830458ba-6445-4382-a906-4a948139979e-sxdj4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:37:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:37:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:37:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 09:37:40 +0000 UTC Reason: Message:}])
    Jan  5 09:37:45.452: INFO: Trying to dial the pod
    Jan  5 09:37:50.584: INFO: Controller my-hostname-basic-830458ba-6445-4382-a906-4a948139979e: Got expected result from replica 1 [my-hostname-basic-830458ba-6445-4382-a906-4a948139979e-sxdj4]: "my-hostname-basic-830458ba-6445-4382-a906-4a948139979e-sxdj4", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  5 09:37:50.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7935" for this suite. 01/05/23 09:37:50.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:37:50.604
Jan  5 09:37:50.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename gc 01/05/23 09:37:50.606
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:37:50.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:37:50.678
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/05/23 09:37:50.685
STEP: Wait for the Deployment to create new ReplicaSet 01/05/23 09:37:50.69
STEP: delete the deployment 01/05/23 09:37:51.206
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/05/23 09:37:51.215
STEP: Gathering metrics 01/05/23 09:37:51.749
W0105 09:37:51.769853      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 09:37:51.769: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 09:37:51.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4493" for this suite. 01/05/23 09:37:51.778
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":135,"skipped":2339,"failed":0}
------------------------------
â€¢ [1.180 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:37:50.604
    Jan  5 09:37:50.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename gc 01/05/23 09:37:50.606
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:37:50.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:37:50.678
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/05/23 09:37:50.685
    STEP: Wait for the Deployment to create new ReplicaSet 01/05/23 09:37:50.69
    STEP: delete the deployment 01/05/23 09:37:51.206
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/05/23 09:37:51.215
    STEP: Gathering metrics 01/05/23 09:37:51.749
    W0105 09:37:51.769853      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 09:37:51.769: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 09:37:51.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4493" for this suite. 01/05/23 09:37:51.778
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:37:51.784
Jan  5 09:37:51.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename deployment 01/05/23 09:37:51.785
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:37:51.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:37:51.807
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/05/23 09:37:51.821
STEP: waiting for Deployment to be created 01/05/23 09:37:51.828
STEP: waiting for all Replicas to be Ready 01/05/23 09:37:51.832
Jan  5 09:37:51.838: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 09:37:51.838: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 09:37:51.839: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 09:37:51.839: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 09:37:51.851: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 09:37:51.851: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 09:37:51.869: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 09:37:51.869: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 09:37:53.185: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan  5 09:37:53.185: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan  5 09:37:53.367: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/05/23 09:37:53.367
W0105 09:37:53.378016      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  5 09:37:53.382: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/05/23 09:37:53.382
Jan  5 09:37:53.385: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
Jan  5 09:37:53.385: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
Jan  5 09:37:53.385: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
Jan  5 09:37:53.385: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:53.388: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:53.388: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:53.413: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:53.413: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:53.425: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
Jan  5 09:37:53.425: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
Jan  5 09:37:53.432: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
Jan  5 09:37:53.432: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
Jan  5 09:37:54.375: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:54.375: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:54.392: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
STEP: listing Deployments 01/05/23 09:37:54.392
Jan  5 09:37:54.397: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/05/23 09:37:54.397
Jan  5 09:37:54.417: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/05/23 09:37:54.417
Jan  5 09:37:54.428: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 09:37:54.428: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 09:37:54.438: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 09:37:54.453: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 09:37:54.467: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 09:37:55.190: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 09:37:55.666: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 09:37:55.684: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 09:37:55.700: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 09:37:57.391: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/05/23 09:37:57.412
STEP: fetching the DeploymentStatus 01/05/23 09:37:57.423
Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 3
Jan  5 09:37:57.435: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:57.435: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
Jan  5 09:37:57.435: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 3
STEP: deleting the Deployment 01/05/23 09:37:57.435
Jan  5 09:37:57.448: INFO: observed event type MODIFIED
Jan  5 09:37:57.449: INFO: observed event type MODIFIED
Jan  5 09:37:57.449: INFO: observed event type MODIFIED
Jan  5 09:37:57.449: INFO: observed event type MODIFIED
Jan  5 09:37:57.449: INFO: observed event type MODIFIED
Jan  5 09:37:57.449: INFO: observed event type MODIFIED
Jan  5 09:37:57.449: INFO: observed event type MODIFIED
Jan  5 09:37:57.450: INFO: observed event type MODIFIED
Jan  5 09:37:57.450: INFO: observed event type MODIFIED
Jan  5 09:37:57.450: INFO: observed event type MODIFIED
Jan  5 09:37:57.450: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 09:37:57.457: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 09:37:57.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-472" for this suite. 01/05/23 09:37:57.474
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":136,"skipped":2341,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.696 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:37:51.784
    Jan  5 09:37:51.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename deployment 01/05/23 09:37:51.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:37:51.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:37:51.807
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/05/23 09:37:51.821
    STEP: waiting for Deployment to be created 01/05/23 09:37:51.828
    STEP: waiting for all Replicas to be Ready 01/05/23 09:37:51.832
    Jan  5 09:37:51.838: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 09:37:51.838: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 09:37:51.839: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 09:37:51.839: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 09:37:51.851: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 09:37:51.851: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 09:37:51.869: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 09:37:51.869: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 09:37:53.185: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan  5 09:37:53.185: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan  5 09:37:53.367: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/05/23 09:37:53.367
    W0105 09:37:53.378016      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  5 09:37:53.382: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/05/23 09:37:53.382
    Jan  5 09:37:53.385: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
    Jan  5 09:37:53.385: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
    Jan  5 09:37:53.385: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
    Jan  5 09:37:53.385: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
    Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
    Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
    Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
    Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 0
    Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:53.386: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:53.388: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:53.388: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:53.413: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:53.413: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:53.425: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    Jan  5 09:37:53.425: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    Jan  5 09:37:53.432: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    Jan  5 09:37:53.432: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    Jan  5 09:37:54.375: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:54.375: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:54.392: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    STEP: listing Deployments 01/05/23 09:37:54.392
    Jan  5 09:37:54.397: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/05/23 09:37:54.397
    Jan  5 09:37:54.417: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/05/23 09:37:54.417
    Jan  5 09:37:54.428: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 09:37:54.428: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 09:37:54.438: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 09:37:54.453: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 09:37:54.467: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 09:37:55.190: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 09:37:55.666: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 09:37:55.684: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 09:37:55.700: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 09:37:57.391: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/05/23 09:37:57.412
    STEP: fetching the DeploymentStatus 01/05/23 09:37:57.423
    Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 1
    Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:57.433: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 3
    Jan  5 09:37:57.435: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:57.435: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 2
    Jan  5 09:37:57.435: INFO: observed Deployment test-deployment in namespace deployment-472 with ReadyReplicas 3
    STEP: deleting the Deployment 01/05/23 09:37:57.435
    Jan  5 09:37:57.448: INFO: observed event type MODIFIED
    Jan  5 09:37:57.449: INFO: observed event type MODIFIED
    Jan  5 09:37:57.449: INFO: observed event type MODIFIED
    Jan  5 09:37:57.449: INFO: observed event type MODIFIED
    Jan  5 09:37:57.449: INFO: observed event type MODIFIED
    Jan  5 09:37:57.449: INFO: observed event type MODIFIED
    Jan  5 09:37:57.449: INFO: observed event type MODIFIED
    Jan  5 09:37:57.450: INFO: observed event type MODIFIED
    Jan  5 09:37:57.450: INFO: observed event type MODIFIED
    Jan  5 09:37:57.450: INFO: observed event type MODIFIED
    Jan  5 09:37:57.450: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 09:37:57.457: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 09:37:57.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-472" for this suite. 01/05/23 09:37:57.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:37:57.482
Jan  5 09:37:57.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename ephemeral-containers-test 01/05/23 09:37:57.483
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:37:57.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:37:57.507
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/05/23 09:37:57.514
Jan  5 09:37:57.527: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5774" to be "running and ready"
Jan  5 09:37:57.536: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.417466ms
Jan  5 09:37:57.536: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:37:59.543: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015557766s
Jan  5 09:37:59.543: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan  5 09:37:59.543: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/05/23 09:37:59.549
Jan  5 09:37:59.562: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5774" to be "container debugger running"
Jan  5 09:37:59.566: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.948094ms
Jan  5 09:38:01.573: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011518894s
Jan  5 09:38:03.573: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011604319s
Jan  5 09:38:03.573: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/05/23 09:38:03.573
Jan  5 09:38:03.573: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5774 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 09:38:03.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:38:03.574: INFO: ExecWithOptions: Clientset creation
Jan  5 09:38:03.574: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/ephemeral-containers-test-5774/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan  5 09:38:03.956: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 09:38:04.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-5774" for this suite. 01/05/23 09:38:04.057
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":137,"skipped":2367,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.580 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:37:57.482
    Jan  5 09:37:57.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/05/23 09:37:57.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:37:57.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:37:57.507
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/05/23 09:37:57.514
    Jan  5 09:37:57.527: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5774" to be "running and ready"
    Jan  5 09:37:57.536: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.417466ms
    Jan  5 09:37:57.536: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:37:59.543: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015557766s
    Jan  5 09:37:59.543: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan  5 09:37:59.543: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/05/23 09:37:59.549
    Jan  5 09:37:59.562: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5774" to be "container debugger running"
    Jan  5 09:37:59.566: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.948094ms
    Jan  5 09:38:01.573: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011518894s
    Jan  5 09:38:03.573: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011604319s
    Jan  5 09:38:03.573: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/05/23 09:38:03.573
    Jan  5 09:38:03.573: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5774 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 09:38:03.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:38:03.574: INFO: ExecWithOptions: Clientset creation
    Jan  5 09:38:03.574: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/ephemeral-containers-test-5774/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan  5 09:38:03.956: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 09:38:04.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-5774" for this suite. 01/05/23 09:38:04.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:38:04.065
Jan  5 09:38:04.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename security-context 01/05/23 09:38:04.066
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:38:04.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:38:04.09
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/05/23 09:38:04.096
Jan  5 09:38:04.108: INFO: Waiting up to 5m0s for pod "security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f" in namespace "security-context-8333" to be "Succeeded or Failed"
Jan  5 09:38:04.114: INFO: Pod "security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.576506ms
Jan  5 09:38:06.118: INFO: Pod "security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01046646s
Jan  5 09:38:08.122: INFO: Pod "security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014014126s
STEP: Saw pod success 01/05/23 09:38:08.122
Jan  5 09:38:08.122: INFO: Pod "security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f" satisfied condition "Succeeded or Failed"
Jan  5 09:38:08.126: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f container test-container: <nil>
STEP: delete the pod 01/05/23 09:38:08.172
Jan  5 09:38:08.201: INFO: Waiting for pod security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f to disappear
Jan  5 09:38:08.205: INFO: Pod security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  5 09:38:08.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8333" for this suite. 01/05/23 09:38:08.214
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":138,"skipped":2372,"failed":0}
------------------------------
â€¢ [4.155 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:38:04.065
    Jan  5 09:38:04.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename security-context 01/05/23 09:38:04.066
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:38:04.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:38:04.09
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/05/23 09:38:04.096
    Jan  5 09:38:04.108: INFO: Waiting up to 5m0s for pod "security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f" in namespace "security-context-8333" to be "Succeeded or Failed"
    Jan  5 09:38:04.114: INFO: Pod "security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.576506ms
    Jan  5 09:38:06.118: INFO: Pod "security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01046646s
    Jan  5 09:38:08.122: INFO: Pod "security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014014126s
    STEP: Saw pod success 01/05/23 09:38:08.122
    Jan  5 09:38:08.122: INFO: Pod "security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f" satisfied condition "Succeeded or Failed"
    Jan  5 09:38:08.126: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f container test-container: <nil>
    STEP: delete the pod 01/05/23 09:38:08.172
    Jan  5 09:38:08.201: INFO: Waiting for pod security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f to disappear
    Jan  5 09:38:08.205: INFO: Pod security-context-bcb1d6f3-1d66-423d-985c-1d4345d6212f no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  5 09:38:08.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-8333" for this suite. 01/05/23 09:38:08.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:38:08.221
Jan  5 09:38:08.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 09:38:08.222
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:38:08.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:38:08.25
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/05/23 09:38:08.258
Jan  5 09:38:08.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:38:11.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:38:23.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-642" for this suite. 01/05/23 09:38:23.149
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":139,"skipped":2387,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.936 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:38:08.221
    Jan  5 09:38:08.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 09:38:08.222
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:38:08.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:38:08.25
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/05/23 09:38:08.258
    Jan  5 09:38:08.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:38:11.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:38:23.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-642" for this suite. 01/05/23 09:38:23.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:38:23.158
Jan  5 09:38:23.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename statefulset 01/05/23 09:38:23.159
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:38:23.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:38:23.182
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8448 01/05/23 09:38:23.188
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 01/05/23 09:38:23.196
Jan  5 09:38:23.208: INFO: Found 0 stateful pods, waiting for 3
Jan  5 09:38:33.217: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 09:38:33.217: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 09:38:33.217: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/05/23 09:38:33.233
Jan  5 09:38:33.257: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/05/23 09:38:33.257
STEP: Not applying an update when the partition is greater than the number of replicas 01/05/23 09:38:43.325
STEP: Performing a canary update 01/05/23 09:38:43.325
Jan  5 09:38:43.348: INFO: Updating stateful set ss2
Jan  5 09:38:43.407: INFO: Waiting for Pod statefulset-8448/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 01/05/23 09:38:53.423
Jan  5 09:38:53.470: INFO: Found 2 stateful pods, waiting for 3
Jan  5 09:39:03.479: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 09:39:03.479: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 09:39:03.479: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/05/23 09:39:03.492
Jan  5 09:39:03.516: INFO: Updating stateful set ss2
Jan  5 09:39:03.531: INFO: Waiting for Pod statefulset-8448/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan  5 09:39:13.570: INFO: Updating stateful set ss2
Jan  5 09:39:13.586: INFO: Waiting for StatefulSet statefulset-8448/ss2 to complete update
Jan  5 09:39:13.586: INFO: Waiting for Pod statefulset-8448/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 09:39:23.597: INFO: Deleting all statefulset in ns statefulset-8448
Jan  5 09:39:23.603: INFO: Scaling statefulset ss2 to 0
Jan  5 09:39:33.634: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 09:39:33.639: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 09:39:33.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8448" for this suite. 01/05/23 09:39:33.666
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":140,"skipped":2403,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.515 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:38:23.158
    Jan  5 09:38:23.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename statefulset 01/05/23 09:38:23.159
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:38:23.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:38:23.182
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8448 01/05/23 09:38:23.188
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 01/05/23 09:38:23.196
    Jan  5 09:38:23.208: INFO: Found 0 stateful pods, waiting for 3
    Jan  5 09:38:33.217: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 09:38:33.217: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 09:38:33.217: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/05/23 09:38:33.233
    Jan  5 09:38:33.257: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/05/23 09:38:33.257
    STEP: Not applying an update when the partition is greater than the number of replicas 01/05/23 09:38:43.325
    STEP: Performing a canary update 01/05/23 09:38:43.325
    Jan  5 09:38:43.348: INFO: Updating stateful set ss2
    Jan  5 09:38:43.407: INFO: Waiting for Pod statefulset-8448/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 01/05/23 09:38:53.423
    Jan  5 09:38:53.470: INFO: Found 2 stateful pods, waiting for 3
    Jan  5 09:39:03.479: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 09:39:03.479: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 09:39:03.479: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/05/23 09:39:03.492
    Jan  5 09:39:03.516: INFO: Updating stateful set ss2
    Jan  5 09:39:03.531: INFO: Waiting for Pod statefulset-8448/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan  5 09:39:13.570: INFO: Updating stateful set ss2
    Jan  5 09:39:13.586: INFO: Waiting for StatefulSet statefulset-8448/ss2 to complete update
    Jan  5 09:39:13.586: INFO: Waiting for Pod statefulset-8448/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 09:39:23.597: INFO: Deleting all statefulset in ns statefulset-8448
    Jan  5 09:39:23.603: INFO: Scaling statefulset ss2 to 0
    Jan  5 09:39:33.634: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 09:39:33.639: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 09:39:33.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8448" for this suite. 01/05/23 09:39:33.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:39:33.674
Jan  5 09:39:33.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 09:39:33.675
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:39:33.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:39:33.718
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-164bb8a0-0041-4ab4-b178-5a24c162e17b 01/05/23 09:39:33.735
STEP: Creating configMap with name cm-test-opt-upd-ce5d9203-231a-42f3-9198-8bf852bb1c3c 01/05/23 09:39:33.74
STEP: Creating the pod 01/05/23 09:39:33.746
Jan  5 09:39:33.760: INFO: Waiting up to 5m0s for pod "pod-configmaps-97f0885b-1adf-477f-89ec-5fc589979c87" in namespace "configmap-3678" to be "running and ready"
Jan  5 09:39:33.769: INFO: Pod "pod-configmaps-97f0885b-1adf-477f-89ec-5fc589979c87": Phase="Pending", Reason="", readiness=false. Elapsed: 8.654221ms
Jan  5 09:39:33.769: INFO: The phase of Pod pod-configmaps-97f0885b-1adf-477f-89ec-5fc589979c87 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:39:35.776: INFO: Pod "pod-configmaps-97f0885b-1adf-477f-89ec-5fc589979c87": Phase="Running", Reason="", readiness=true. Elapsed: 2.016340617s
Jan  5 09:39:35.777: INFO: The phase of Pod pod-configmaps-97f0885b-1adf-477f-89ec-5fc589979c87 is Running (Ready = true)
Jan  5 09:39:35.777: INFO: Pod "pod-configmaps-97f0885b-1adf-477f-89ec-5fc589979c87" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-164bb8a0-0041-4ab4-b178-5a24c162e17b 01/05/23 09:39:35.988
STEP: Updating configmap cm-test-opt-upd-ce5d9203-231a-42f3-9198-8bf852bb1c3c 01/05/23 09:39:35.997
STEP: Creating configMap with name cm-test-opt-create-94a92bae-2864-4921-9088-4cd516659cb7 01/05/23 09:39:36.002
STEP: waiting to observe update in volume 01/05/23 09:39:36.01
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 09:39:38.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3678" for this suite. 01/05/23 09:39:38.231
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":141,"skipped":2444,"failed":0}
------------------------------
â€¢ [4.563 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:39:33.674
    Jan  5 09:39:33.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 09:39:33.675
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:39:33.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:39:33.718
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-164bb8a0-0041-4ab4-b178-5a24c162e17b 01/05/23 09:39:33.735
    STEP: Creating configMap with name cm-test-opt-upd-ce5d9203-231a-42f3-9198-8bf852bb1c3c 01/05/23 09:39:33.74
    STEP: Creating the pod 01/05/23 09:39:33.746
    Jan  5 09:39:33.760: INFO: Waiting up to 5m0s for pod "pod-configmaps-97f0885b-1adf-477f-89ec-5fc589979c87" in namespace "configmap-3678" to be "running and ready"
    Jan  5 09:39:33.769: INFO: Pod "pod-configmaps-97f0885b-1adf-477f-89ec-5fc589979c87": Phase="Pending", Reason="", readiness=false. Elapsed: 8.654221ms
    Jan  5 09:39:33.769: INFO: The phase of Pod pod-configmaps-97f0885b-1adf-477f-89ec-5fc589979c87 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:39:35.776: INFO: Pod "pod-configmaps-97f0885b-1adf-477f-89ec-5fc589979c87": Phase="Running", Reason="", readiness=true. Elapsed: 2.016340617s
    Jan  5 09:39:35.777: INFO: The phase of Pod pod-configmaps-97f0885b-1adf-477f-89ec-5fc589979c87 is Running (Ready = true)
    Jan  5 09:39:35.777: INFO: Pod "pod-configmaps-97f0885b-1adf-477f-89ec-5fc589979c87" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-164bb8a0-0041-4ab4-b178-5a24c162e17b 01/05/23 09:39:35.988
    STEP: Updating configmap cm-test-opt-upd-ce5d9203-231a-42f3-9198-8bf852bb1c3c 01/05/23 09:39:35.997
    STEP: Creating configMap with name cm-test-opt-create-94a92bae-2864-4921-9088-4cd516659cb7 01/05/23 09:39:36.002
    STEP: waiting to observe update in volume 01/05/23 09:39:36.01
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 09:39:38.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3678" for this suite. 01/05/23 09:39:38.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:39:38.239
Jan  5 09:39:38.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename podtemplate 01/05/23 09:39:38.24
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:39:38.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:39:38.26
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/05/23 09:39:38.267
STEP: Replace a pod template 01/05/23 09:39:38.273
Jan  5 09:39:38.287: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan  5 09:39:38.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3835" for this suite. 01/05/23 09:39:38.298
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":142,"skipped":2484,"failed":0}
------------------------------
â€¢ [0.066 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:39:38.239
    Jan  5 09:39:38.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename podtemplate 01/05/23 09:39:38.24
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:39:38.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:39:38.26
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/05/23 09:39:38.267
    STEP: Replace a pod template 01/05/23 09:39:38.273
    Jan  5 09:39:38.287: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan  5 09:39:38.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3835" for this suite. 01/05/23 09:39:38.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:39:38.308
Jan  5 09:39:38.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 09:39:38.309
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:39:38.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:39:38.33
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 09:39:38.349
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:39:38.99
STEP: Deploying the webhook pod 01/05/23 09:39:38.996
STEP: Wait for the deployment to be ready 01/05/23 09:39:39.011
Jan  5 09:39:39.025: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/05/23 09:39:41.044
STEP: Verifying the service has paired with the endpoint 01/05/23 09:39:41.069
Jan  5 09:39:42.069: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 01/05/23 09:39:42.079
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/05/23 09:39:42.215
STEP: Creating a configMap that should not be mutated 01/05/23 09:39:42.227
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/05/23 09:39:42.243
STEP: Creating a configMap that should be mutated 01/05/23 09:39:42.254
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:39:42.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7252" for this suite. 01/05/23 09:39:42.441
STEP: Destroying namespace "webhook-7252-markers" for this suite. 01/05/23 09:39:42.456
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":143,"skipped":2514,"failed":0}
------------------------------
â€¢ [4.231 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:39:38.308
    Jan  5 09:39:38.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 09:39:38.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:39:38.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:39:38.33
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 09:39:38.349
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:39:38.99
    STEP: Deploying the webhook pod 01/05/23 09:39:38.996
    STEP: Wait for the deployment to be ready 01/05/23 09:39:39.011
    Jan  5 09:39:39.025: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/05/23 09:39:41.044
    STEP: Verifying the service has paired with the endpoint 01/05/23 09:39:41.069
    Jan  5 09:39:42.069: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 01/05/23 09:39:42.079
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/05/23 09:39:42.215
    STEP: Creating a configMap that should not be mutated 01/05/23 09:39:42.227
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/05/23 09:39:42.243
    STEP: Creating a configMap that should be mutated 01/05/23 09:39:42.254
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:39:42.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7252" for this suite. 01/05/23 09:39:42.441
    STEP: Destroying namespace "webhook-7252-markers" for this suite. 01/05/23 09:39:42.456
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:39:42.542
Jan  5 09:39:42.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename prestop 01/05/23 09:39:42.543
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:39:42.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:39:42.567
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-6186 01/05/23 09:39:42.574
STEP: Waiting for pods to come up. 01/05/23 09:39:42.585
Jan  5 09:39:42.585: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6186" to be "running"
Jan  5 09:39:42.598: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 12.360289ms
Jan  5 09:39:44.605: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.019577052s
Jan  5 09:39:44.605: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-6186 01/05/23 09:39:44.61
Jan  5 09:39:44.619: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6186" to be "running"
Jan  5 09:39:44.626: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.708861ms
Jan  5 09:39:46.632: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.0128607s
Jan  5 09:39:46.632: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/05/23 09:39:46.632
Jan  5 09:39:51.751: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/05/23 09:39:51.751
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jan  5 09:39:51.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6186" for this suite. 01/05/23 09:39:51.78
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":144,"skipped":2586,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.245 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:39:42.542
    Jan  5 09:39:42.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename prestop 01/05/23 09:39:42.543
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:39:42.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:39:42.567
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-6186 01/05/23 09:39:42.574
    STEP: Waiting for pods to come up. 01/05/23 09:39:42.585
    Jan  5 09:39:42.585: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6186" to be "running"
    Jan  5 09:39:42.598: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 12.360289ms
    Jan  5 09:39:44.605: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.019577052s
    Jan  5 09:39:44.605: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-6186 01/05/23 09:39:44.61
    Jan  5 09:39:44.619: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6186" to be "running"
    Jan  5 09:39:44.626: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.708861ms
    Jan  5 09:39:46.632: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.0128607s
    Jan  5 09:39:46.632: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/05/23 09:39:46.632
    Jan  5 09:39:51.751: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/05/23 09:39:51.751
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jan  5 09:39:51.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-6186" for this suite. 01/05/23 09:39:51.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:39:51.791
Jan  5 09:39:51.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:39:51.792
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:39:51.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:39:51.814
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-c1f343bd-b172-44e8-8c4f-6ff02bd290a2 01/05/23 09:39:51.821
STEP: Creating a pod to test consume configMaps 01/05/23 09:39:51.827
Jan  5 09:39:51.839: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec" in namespace "projected-1057" to be "Succeeded or Failed"
Jan  5 09:39:51.847: INFO: Pod "pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.471754ms
Jan  5 09:39:53.855: INFO: Pod "pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015716253s
Jan  5 09:39:55.853: INFO: Pod "pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014028812s
STEP: Saw pod success 01/05/23 09:39:55.853
Jan  5 09:39:55.853: INFO: Pod "pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec" satisfied condition "Succeeded or Failed"
Jan  5 09:39:55.858: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec container agnhost-container: <nil>
STEP: delete the pod 01/05/23 09:39:55.914
Jan  5 09:39:55.929: INFO: Waiting for pod pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec to disappear
Jan  5 09:39:55.934: INFO: Pod pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 09:39:55.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1057" for this suite. 01/05/23 09:39:55.944
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":145,"skipped":2644,"failed":0}
------------------------------
â€¢ [4.159 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:39:51.791
    Jan  5 09:39:51.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:39:51.792
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:39:51.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:39:51.814
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-c1f343bd-b172-44e8-8c4f-6ff02bd290a2 01/05/23 09:39:51.821
    STEP: Creating a pod to test consume configMaps 01/05/23 09:39:51.827
    Jan  5 09:39:51.839: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec" in namespace "projected-1057" to be "Succeeded or Failed"
    Jan  5 09:39:51.847: INFO: Pod "pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.471754ms
    Jan  5 09:39:53.855: INFO: Pod "pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015716253s
    Jan  5 09:39:55.853: INFO: Pod "pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014028812s
    STEP: Saw pod success 01/05/23 09:39:55.853
    Jan  5 09:39:55.853: INFO: Pod "pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec" satisfied condition "Succeeded or Failed"
    Jan  5 09:39:55.858: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 09:39:55.914
    Jan  5 09:39:55.929: INFO: Waiting for pod pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec to disappear
    Jan  5 09:39:55.934: INFO: Pod pod-projected-configmaps-5d38e9fc-ce1c-4df4-85df-7f1f37920cec no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 09:39:55.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1057" for this suite. 01/05/23 09:39:55.944
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:39:55.95
Jan  5 09:39:55.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 09:39:55.951
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:39:55.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:39:55.973
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/05/23 09:39:55.989
Jan  5 09:39:56.000: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-604" to be "running and ready"
Jan  5 09:39:56.007: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.218593ms
Jan  5 09:39:56.007: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:39:58.015: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014879371s
Jan  5 09:39:58.015: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  5 09:39:58.015: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 01/05/23 09:39:58.02
Jan  5 09:39:58.031: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-604" to be "running and ready"
Jan  5 09:39:58.039: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.81401ms
Jan  5 09:39:58.039: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:40:00.046: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015696398s
Jan  5 09:40:00.046: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan  5 09:40:00.046: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/05/23 09:40:00.052
STEP: delete the pod with lifecycle hook 01/05/23 09:40:00.073
Jan  5 09:40:00.082: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  5 09:40:00.090: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  5 09:40:02.090: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  5 09:40:02.097: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  5 09:40:04.091: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  5 09:40:04.097: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan  5 09:40:04.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-604" for this suite. 01/05/23 09:40:04.109
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":146,"skipped":2647,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.171 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:39:55.95
    Jan  5 09:39:55.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 09:39:55.951
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:39:55.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:39:55.973
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/05/23 09:39:55.989
    Jan  5 09:39:56.000: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-604" to be "running and ready"
    Jan  5 09:39:56.007: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.218593ms
    Jan  5 09:39:56.007: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:39:58.015: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014879371s
    Jan  5 09:39:58.015: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  5 09:39:58.015: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 01/05/23 09:39:58.02
    Jan  5 09:39:58.031: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-604" to be "running and ready"
    Jan  5 09:39:58.039: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.81401ms
    Jan  5 09:39:58.039: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:40:00.046: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015696398s
    Jan  5 09:40:00.046: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan  5 09:40:00.046: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/05/23 09:40:00.052
    STEP: delete the pod with lifecycle hook 01/05/23 09:40:00.073
    Jan  5 09:40:00.082: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  5 09:40:00.090: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan  5 09:40:02.090: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  5 09:40:02.097: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan  5 09:40:04.091: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  5 09:40:04.097: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan  5 09:40:04.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-604" for this suite. 01/05/23 09:40:04.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:40:04.123
Jan  5 09:40:04.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename subpath 01/05/23 09:40:04.124
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:04.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:04.146
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 09:40:04.152
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-q99z 01/05/23 09:40:04.163
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 09:40:04.164
Jan  5 09:40:04.177: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-q99z" in namespace "subpath-7224" to be "Succeeded or Failed"
Jan  5 09:40:04.186: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Pending", Reason="", readiness=false. Elapsed: 8.845549ms
Jan  5 09:40:06.194: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 2.016821223s
Jan  5 09:40:08.195: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 4.017886823s
Jan  5 09:40:10.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 6.015851814s
Jan  5 09:40:12.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 8.016281602s
Jan  5 09:40:14.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 10.015663853s
Jan  5 09:40:16.192: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 12.014969401s
Jan  5 09:40:18.195: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 14.01749536s
Jan  5 09:40:20.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 16.016164666s
Jan  5 09:40:22.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 18.015538855s
Jan  5 09:40:24.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 20.016056881s
Jan  5 09:40:26.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=false. Elapsed: 22.016341185s
Jan  5 09:40:28.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016339344s
STEP: Saw pod success 01/05/23 09:40:28.193
Jan  5 09:40:28.194: INFO: Pod "pod-subpath-test-downwardapi-q99z" satisfied condition "Succeeded or Failed"
Jan  5 09:40:28.199: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-subpath-test-downwardapi-q99z container test-container-subpath-downwardapi-q99z: <nil>
STEP: delete the pod 01/05/23 09:40:28.227
Jan  5 09:40:28.239: INFO: Waiting for pod pod-subpath-test-downwardapi-q99z to disappear
Jan  5 09:40:28.243: INFO: Pod pod-subpath-test-downwardapi-q99z no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-q99z 01/05/23 09:40:28.243
Jan  5 09:40:28.243: INFO: Deleting pod "pod-subpath-test-downwardapi-q99z" in namespace "subpath-7224"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  5 09:40:28.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7224" for this suite. 01/05/23 09:40:28.257
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":147,"skipped":2654,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.143 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:40:04.123
    Jan  5 09:40:04.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename subpath 01/05/23 09:40:04.124
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:04.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:04.146
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 09:40:04.152
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-q99z 01/05/23 09:40:04.163
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 09:40:04.164
    Jan  5 09:40:04.177: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-q99z" in namespace "subpath-7224" to be "Succeeded or Failed"
    Jan  5 09:40:04.186: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Pending", Reason="", readiness=false. Elapsed: 8.845549ms
    Jan  5 09:40:06.194: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 2.016821223s
    Jan  5 09:40:08.195: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 4.017886823s
    Jan  5 09:40:10.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 6.015851814s
    Jan  5 09:40:12.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 8.016281602s
    Jan  5 09:40:14.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 10.015663853s
    Jan  5 09:40:16.192: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 12.014969401s
    Jan  5 09:40:18.195: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 14.01749536s
    Jan  5 09:40:20.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 16.016164666s
    Jan  5 09:40:22.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 18.015538855s
    Jan  5 09:40:24.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=true. Elapsed: 20.016056881s
    Jan  5 09:40:26.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Running", Reason="", readiness=false. Elapsed: 22.016341185s
    Jan  5 09:40:28.193: INFO: Pod "pod-subpath-test-downwardapi-q99z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016339344s
    STEP: Saw pod success 01/05/23 09:40:28.193
    Jan  5 09:40:28.194: INFO: Pod "pod-subpath-test-downwardapi-q99z" satisfied condition "Succeeded or Failed"
    Jan  5 09:40:28.199: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-subpath-test-downwardapi-q99z container test-container-subpath-downwardapi-q99z: <nil>
    STEP: delete the pod 01/05/23 09:40:28.227
    Jan  5 09:40:28.239: INFO: Waiting for pod pod-subpath-test-downwardapi-q99z to disappear
    Jan  5 09:40:28.243: INFO: Pod pod-subpath-test-downwardapi-q99z no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-q99z 01/05/23 09:40:28.243
    Jan  5 09:40:28.243: INFO: Deleting pod "pod-subpath-test-downwardapi-q99z" in namespace "subpath-7224"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  5 09:40:28.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7224" for this suite. 01/05/23 09:40:28.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:40:28.267
Jan  5 09:40:28.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 09:40:28.268
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:28.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:28.29
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 01/05/23 09:40:28.298
Jan  5 09:40:28.311: INFO: Waiting up to 5m0s for pod "pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd" in namespace "emptydir-7949" to be "Succeeded or Failed"
Jan  5 09:40:28.317: INFO: Pod "pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.948995ms
Jan  5 09:40:30.324: INFO: Pod "pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013192668s
Jan  5 09:40:32.324: INFO: Pod "pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013267331s
STEP: Saw pod success 01/05/23 09:40:32.324
Jan  5 09:40:32.324: INFO: Pod "pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd" satisfied condition "Succeeded or Failed"
Jan  5 09:40:32.329: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd container test-container: <nil>
STEP: delete the pod 01/05/23 09:40:32.34
Jan  5 09:40:32.352: INFO: Waiting for pod pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd to disappear
Jan  5 09:40:32.357: INFO: Pod pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 09:40:32.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7949" for this suite. 01/05/23 09:40:32.367
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":148,"skipped":2659,"failed":0}
------------------------------
â€¢ [4.107 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:40:28.267
    Jan  5 09:40:28.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 09:40:28.268
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:28.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:28.29
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/05/23 09:40:28.298
    Jan  5 09:40:28.311: INFO: Waiting up to 5m0s for pod "pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd" in namespace "emptydir-7949" to be "Succeeded or Failed"
    Jan  5 09:40:28.317: INFO: Pod "pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.948995ms
    Jan  5 09:40:30.324: INFO: Pod "pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013192668s
    Jan  5 09:40:32.324: INFO: Pod "pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013267331s
    STEP: Saw pod success 01/05/23 09:40:32.324
    Jan  5 09:40:32.324: INFO: Pod "pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd" satisfied condition "Succeeded or Failed"
    Jan  5 09:40:32.329: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd container test-container: <nil>
    STEP: delete the pod 01/05/23 09:40:32.34
    Jan  5 09:40:32.352: INFO: Waiting for pod pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd to disappear
    Jan  5 09:40:32.357: INFO: Pod pod-3507841a-41f6-4996-82b3-e16ee5b6ebcd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 09:40:32.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7949" for this suite. 01/05/23 09:40:32.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:40:32.377
Jan  5 09:40:32.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename daemonsets 01/05/23 09:40:32.378
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:32.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:32.4
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 01/05/23 09:40:32.453
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:40:32.459
Jan  5 09:40:32.473: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:40:32.473: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:40:33.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:40:33.493: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
Jan  5 09:40:34.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  5 09:40:34.500: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/05/23 09:40:34.506
Jan  5 09:40:34.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 09:40:34.540: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv is running 0 daemon pod, expected 1
Jan  5 09:40:35.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 09:40:35.562: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv is running 0 daemon pod, expected 1
Jan  5 09:40:36.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan  5 09:40:36.561: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/05/23 09:40:36.561
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/05/23 09:40:36.57
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5946, will wait for the garbage collector to delete the pods 01/05/23 09:40:36.57
Jan  5 09:40:36.634: INFO: Deleting DaemonSet.extensions daemon-set took: 8.16536ms
Jan  5 09:40:36.734: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.124171ms
Jan  5 09:40:38.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:40:38.740: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 09:40:38.745: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20936"},"items":null}

Jan  5 09:40:38.750: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20936"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:40:38.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5946" for this suite. 01/05/23 09:40:38.796
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":149,"skipped":2772,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.430 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:40:32.377
    Jan  5 09:40:32.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename daemonsets 01/05/23 09:40:32.378
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:32.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:32.4
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 01/05/23 09:40:32.453
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:40:32.459
    Jan  5 09:40:32.473: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:40:32.473: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:40:33.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:40:33.493: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t is running 0 daemon pod, expected 1
    Jan  5 09:40:34.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  5 09:40:34.500: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/05/23 09:40:34.506
    Jan  5 09:40:34.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 09:40:34.540: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv is running 0 daemon pod, expected 1
    Jan  5 09:40:35.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 09:40:35.562: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv is running 0 daemon pod, expected 1
    Jan  5 09:40:36.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan  5 09:40:36.561: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/05/23 09:40:36.561
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 09:40:36.57
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5946, will wait for the garbage collector to delete the pods 01/05/23 09:40:36.57
    Jan  5 09:40:36.634: INFO: Deleting DaemonSet.extensions daemon-set took: 8.16536ms
    Jan  5 09:40:36.734: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.124171ms
    Jan  5 09:40:38.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:40:38.740: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 09:40:38.745: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20936"},"items":null}

    Jan  5 09:40:38.750: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20936"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:40:38.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5946" for this suite. 01/05/23 09:40:38.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:40:38.809
Jan  5 09:40:38.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pods 01/05/23 09:40:38.81
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:38.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:38.831
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jan  5 09:40:38.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: creating the pod 01/05/23 09:40:38.839
STEP: submitting the pod to kubernetes 01/05/23 09:40:38.839
Jan  5 09:40:38.850: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-6465288e-f8ac-4897-804e-4fc53602c2ab" in namespace "pods-5984" to be "running and ready"
Jan  5 09:40:38.858: INFO: Pod "pod-logs-websocket-6465288e-f8ac-4897-804e-4fc53602c2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.898231ms
Jan  5 09:40:38.858: INFO: The phase of Pod pod-logs-websocket-6465288e-f8ac-4897-804e-4fc53602c2ab is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:40:40.865: INFO: Pod "pod-logs-websocket-6465288e-f8ac-4897-804e-4fc53602c2ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.015175745s
Jan  5 09:40:40.865: INFO: The phase of Pod pod-logs-websocket-6465288e-f8ac-4897-804e-4fc53602c2ab is Running (Ready = true)
Jan  5 09:40:40.865: INFO: Pod "pod-logs-websocket-6465288e-f8ac-4897-804e-4fc53602c2ab" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 09:40:40.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5984" for this suite. 01/05/23 09:40:40.922
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":150,"skipped":2798,"failed":0}
------------------------------
â€¢ [2.122 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:40:38.809
    Jan  5 09:40:38.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pods 01/05/23 09:40:38.81
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:38.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:38.831
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jan  5 09:40:38.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: creating the pod 01/05/23 09:40:38.839
    STEP: submitting the pod to kubernetes 01/05/23 09:40:38.839
    Jan  5 09:40:38.850: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-6465288e-f8ac-4897-804e-4fc53602c2ab" in namespace "pods-5984" to be "running and ready"
    Jan  5 09:40:38.858: INFO: Pod "pod-logs-websocket-6465288e-f8ac-4897-804e-4fc53602c2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.898231ms
    Jan  5 09:40:38.858: INFO: The phase of Pod pod-logs-websocket-6465288e-f8ac-4897-804e-4fc53602c2ab is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:40:40.865: INFO: Pod "pod-logs-websocket-6465288e-f8ac-4897-804e-4fc53602c2ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.015175745s
    Jan  5 09:40:40.865: INFO: The phase of Pod pod-logs-websocket-6465288e-f8ac-4897-804e-4fc53602c2ab is Running (Ready = true)
    Jan  5 09:40:40.865: INFO: Pod "pod-logs-websocket-6465288e-f8ac-4897-804e-4fc53602c2ab" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 09:40:40.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5984" for this suite. 01/05/23 09:40:40.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:40:40.931
Jan  5 09:40:40.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename namespaces 01/05/23 09:40:40.932
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:40.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:40.955
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 01/05/23 09:40:40.964
Jan  5 09:40:40.973: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/05/23 09:40:40.973
Jan  5 09:40:40.981: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/05/23 09:40:40.981
Jan  5 09:40:40.992: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:40:40.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9122" for this suite. 01/05/23 09:40:41.005
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":151,"skipped":2803,"failed":0}
------------------------------
â€¢ [0.080 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:40:40.931
    Jan  5 09:40:40.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename namespaces 01/05/23 09:40:40.932
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:40.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:40.955
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 01/05/23 09:40:40.964
    Jan  5 09:40:40.973: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/05/23 09:40:40.973
    Jan  5 09:40:40.981: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/05/23 09:40:40.981
    Jan  5 09:40:40.992: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:40:40.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9122" for this suite. 01/05/23 09:40:41.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:40:41.012
Jan  5 09:40:41.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename var-expansion 01/05/23 09:40:41.013
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:41.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:41.034
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jan  5 09:40:41.055: INFO: Waiting up to 2m0s for pod "var-expansion-b153b5de-3ad3-4be8-ac72-0f51cf7493ed" in namespace "var-expansion-3034" to be "container 0 failed with reason CreateContainerConfigError"
Jan  5 09:40:41.064: INFO: Pod "var-expansion-b153b5de-3ad3-4be8-ac72-0f51cf7493ed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.742778ms
Jan  5 09:40:43.072: INFO: Pod "var-expansion-b153b5de-3ad3-4be8-ac72-0f51cf7493ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016804446s
Jan  5 09:40:43.072: INFO: Pod "var-expansion-b153b5de-3ad3-4be8-ac72-0f51cf7493ed" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan  5 09:40:43.072: INFO: Deleting pod "var-expansion-b153b5de-3ad3-4be8-ac72-0f51cf7493ed" in namespace "var-expansion-3034"
Jan  5 09:40:43.084: INFO: Wait up to 5m0s for pod "var-expansion-b153b5de-3ad3-4be8-ac72-0f51cf7493ed" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 09:40:45.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3034" for this suite. 01/05/23 09:40:45.124
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":152,"skipped":2809,"failed":0}
------------------------------
â€¢ [4.119 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:40:41.012
    Jan  5 09:40:41.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename var-expansion 01/05/23 09:40:41.013
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:41.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:41.034
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jan  5 09:40:41.055: INFO: Waiting up to 2m0s for pod "var-expansion-b153b5de-3ad3-4be8-ac72-0f51cf7493ed" in namespace "var-expansion-3034" to be "container 0 failed with reason CreateContainerConfigError"
    Jan  5 09:40:41.064: INFO: Pod "var-expansion-b153b5de-3ad3-4be8-ac72-0f51cf7493ed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.742778ms
    Jan  5 09:40:43.072: INFO: Pod "var-expansion-b153b5de-3ad3-4be8-ac72-0f51cf7493ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016804446s
    Jan  5 09:40:43.072: INFO: Pod "var-expansion-b153b5de-3ad3-4be8-ac72-0f51cf7493ed" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan  5 09:40:43.072: INFO: Deleting pod "var-expansion-b153b5de-3ad3-4be8-ac72-0f51cf7493ed" in namespace "var-expansion-3034"
    Jan  5 09:40:43.084: INFO: Wait up to 5m0s for pod "var-expansion-b153b5de-3ad3-4be8-ac72-0f51cf7493ed" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 09:40:45.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3034" for this suite. 01/05/23 09:40:45.124
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:40:45.132
Jan  5 09:40:45.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:40:45.133
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:45.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:45.161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:40:45.168
Jan  5 09:40:45.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996" in namespace "projected-6259" to be "Succeeded or Failed"
Jan  5 09:40:45.186: INFO: Pod "downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101299ms
Jan  5 09:40:47.196: INFO: Pod "downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015573833s
Jan  5 09:40:49.193: INFO: Pod "downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013125354s
STEP: Saw pod success 01/05/23 09:40:49.193
Jan  5 09:40:49.193: INFO: Pod "downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996" satisfied condition "Succeeded or Failed"
Jan  5 09:40:49.200: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996 container client-container: <nil>
STEP: delete the pod 01/05/23 09:40:49.22
Jan  5 09:40:49.234: INFO: Waiting for pod downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996 to disappear
Jan  5 09:40:49.239: INFO: Pod downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 09:40:49.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6259" for this suite. 01/05/23 09:40:49.25
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":153,"skipped":2810,"failed":0}
------------------------------
â€¢ [4.126 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:40:45.132
    Jan  5 09:40:45.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:40:45.133
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:45.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:45.161
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:40:45.168
    Jan  5 09:40:45.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996" in namespace "projected-6259" to be "Succeeded or Failed"
    Jan  5 09:40:45.186: INFO: Pod "downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101299ms
    Jan  5 09:40:47.196: INFO: Pod "downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015573833s
    Jan  5 09:40:49.193: INFO: Pod "downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013125354s
    STEP: Saw pod success 01/05/23 09:40:49.193
    Jan  5 09:40:49.193: INFO: Pod "downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996" satisfied condition "Succeeded or Failed"
    Jan  5 09:40:49.200: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996 container client-container: <nil>
    STEP: delete the pod 01/05/23 09:40:49.22
    Jan  5 09:40:49.234: INFO: Waiting for pod downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996 to disappear
    Jan  5 09:40:49.239: INFO: Pod downwardapi-volume-e3c11fdc-b757-413a-8bd3-73de1db59996 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 09:40:49.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6259" for this suite. 01/05/23 09:40:49.25
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:40:49.259
Jan  5 09:40:49.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 09:40:49.259
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:49.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:49.28
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-9392 01/05/23 09:40:49.287
STEP: creating service affinity-nodeport in namespace services-9392 01/05/23 09:40:49.287
STEP: creating replication controller affinity-nodeport in namespace services-9392 01/05/23 09:40:49.313
I0105 09:40:49.320782      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9392, replica count: 3
I0105 09:40:52.372308      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 09:40:52.396: INFO: Creating new exec pod
Jan  5 09:40:52.406: INFO: Waiting up to 5m0s for pod "execpod-affinityjh6h2" in namespace "services-9392" to be "running"
Jan  5 09:40:52.413: INFO: Pod "execpod-affinityjh6h2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.626535ms
Jan  5 09:40:54.420: INFO: Pod "execpod-affinityjh6h2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014129522s
Jan  5 09:40:54.420: INFO: Pod "execpod-affinityjh6h2" satisfied condition "running"
Jan  5 09:40:55.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9392 exec execpod-affinityjh6h2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan  5 09:40:55.960: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan  5 09:40:55.960: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:40:55.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9392 exec execpod-affinityjh6h2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.116.79.229 80'
Jan  5 09:40:56.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.116.79.229 80\nConnection to 10.116.79.229 80 port [tcp/http] succeeded!\n"
Jan  5 09:40:56.372: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:40:56.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9392 exec execpod-affinityjh6h2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.138 32688'
Jan  5 09:40:56.840: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.138 32688\nConnection to 10.250.2.138 32688 port [tcp/*] succeeded!\n"
Jan  5 09:40:56.840: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:40:56.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9392 exec execpod-affinityjh6h2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.128 32688'
Jan  5 09:40:57.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.128 32688\nConnection to 10.250.0.128 32688 port [tcp/*] succeeded!\n"
Jan  5 09:40:57.364: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:40:57.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9392 exec execpod-affinityjh6h2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.128:32688/ ; done'
Jan  5 09:40:57.971: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n"
Jan  5 09:40:57.971: INFO: stdout: "\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn"
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
Jan  5 09:40:57.971: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9392, will wait for the garbage collector to delete the pods 01/05/23 09:40:57.989
Jan  5 09:40:58.055: INFO: Deleting ReplicationController affinity-nodeport took: 8.611854ms
Jan  5 09:40:58.155: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.215068ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:41:00.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9392" for this suite. 01/05/23 09:41:00.109
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":154,"skipped":2810,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.861 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:40:49.259
    Jan  5 09:40:49.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 09:40:49.259
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:40:49.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:40:49.28
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-9392 01/05/23 09:40:49.287
    STEP: creating service affinity-nodeport in namespace services-9392 01/05/23 09:40:49.287
    STEP: creating replication controller affinity-nodeport in namespace services-9392 01/05/23 09:40:49.313
    I0105 09:40:49.320782      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9392, replica count: 3
    I0105 09:40:52.372308      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 09:40:52.396: INFO: Creating new exec pod
    Jan  5 09:40:52.406: INFO: Waiting up to 5m0s for pod "execpod-affinityjh6h2" in namespace "services-9392" to be "running"
    Jan  5 09:40:52.413: INFO: Pod "execpod-affinityjh6h2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.626535ms
    Jan  5 09:40:54.420: INFO: Pod "execpod-affinityjh6h2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014129522s
    Jan  5 09:40:54.420: INFO: Pod "execpod-affinityjh6h2" satisfied condition "running"
    Jan  5 09:40:55.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9392 exec execpod-affinityjh6h2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jan  5 09:40:55.960: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan  5 09:40:55.960: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:40:55.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9392 exec execpod-affinityjh6h2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.116.79.229 80'
    Jan  5 09:40:56.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.116.79.229 80\nConnection to 10.116.79.229 80 port [tcp/http] succeeded!\n"
    Jan  5 09:40:56.372: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:40:56.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9392 exec execpod-affinityjh6h2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.138 32688'
    Jan  5 09:40:56.840: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.138 32688\nConnection to 10.250.2.138 32688 port [tcp/*] succeeded!\n"
    Jan  5 09:40:56.840: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:40:56.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9392 exec execpod-affinityjh6h2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.128 32688'
    Jan  5 09:40:57.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.128 32688\nConnection to 10.250.0.128 32688 port [tcp/*] succeeded!\n"
    Jan  5 09:40:57.364: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:40:57.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9392 exec execpod-affinityjh6h2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.128:32688/ ; done'
    Jan  5 09:40:57.971: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:32688/\n"
    Jan  5 09:40:57.971: INFO: stdout: "\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn\naffinity-nodeport-26fgn"
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Received response from host: affinity-nodeport-26fgn
    Jan  5 09:40:57.971: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-9392, will wait for the garbage collector to delete the pods 01/05/23 09:40:57.989
    Jan  5 09:40:58.055: INFO: Deleting ReplicationController affinity-nodeport took: 8.611854ms
    Jan  5 09:40:58.155: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.215068ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:41:00.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9392" for this suite. 01/05/23 09:41:00.109
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:41:00.12
Jan  5 09:41:00.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 09:41:00.121
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:41:00.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:41:00.151
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 01/05/23 09:41:00.158
Jan  5 09:41:00.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 create -f -'
Jan  5 09:41:00.699: INFO: stderr: ""
Jan  5 09:41:00.699: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 09:41:00.699
Jan  5 09:41:00.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 09:41:00.770: INFO: stderr: ""
Jan  5 09:41:00.770: INFO: stdout: "update-demo-nautilus-6m8zg update-demo-nautilus-zkdlb "
Jan  5 09:41:00.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods update-demo-nautilus-6m8zg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 09:41:00.839: INFO: stderr: ""
Jan  5 09:41:00.839: INFO: stdout: ""
Jan  5 09:41:00.839: INFO: update-demo-nautilus-6m8zg is created but not running
Jan  5 09:41:05.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 09:41:05.904: INFO: stderr: ""
Jan  5 09:41:05.904: INFO: stdout: "update-demo-nautilus-6m8zg update-demo-nautilus-zkdlb "
Jan  5 09:41:05.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods update-demo-nautilus-6m8zg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 09:41:05.970: INFO: stderr: ""
Jan  5 09:41:05.970: INFO: stdout: "true"
Jan  5 09:41:05.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods update-demo-nautilus-6m8zg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 09:41:06.030: INFO: stderr: ""
Jan  5 09:41:06.030: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 09:41:06.030: INFO: validating pod update-demo-nautilus-6m8zg
Jan  5 09:41:06.148: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 09:41:06.148: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 09:41:06.148: INFO: update-demo-nautilus-6m8zg is verified up and running
Jan  5 09:41:06.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods update-demo-nautilus-zkdlb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 09:41:06.213: INFO: stderr: ""
Jan  5 09:41:06.213: INFO: stdout: "true"
Jan  5 09:41:06.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods update-demo-nautilus-zkdlb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 09:41:06.282: INFO: stderr: ""
Jan  5 09:41:06.282: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 09:41:06.282: INFO: validating pod update-demo-nautilus-zkdlb
Jan  5 09:41:06.398: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 09:41:06.398: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 09:41:06.398: INFO: update-demo-nautilus-zkdlb is verified up and running
STEP: using delete to clean up resources 01/05/23 09:41:06.398
Jan  5 09:41:06.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 delete --grace-period=0 --force -f -'
Jan  5 09:41:06.476: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 09:41:06.476: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  5 09:41:06.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get rc,svc -l name=update-demo --no-headers'
Jan  5 09:41:06.545: INFO: stderr: "No resources found in kubectl-345 namespace.\n"
Jan  5 09:41:06.545: INFO: stdout: ""
Jan  5 09:41:06.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  5 09:41:06.609: INFO: stderr: ""
Jan  5 09:41:06.609: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 09:41:06.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-345" for this suite. 01/05/23 09:41:06.623
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":155,"skipped":2810,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.512 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:41:00.12
    Jan  5 09:41:00.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 09:41:00.121
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:41:00.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:41:00.151
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 01/05/23 09:41:00.158
    Jan  5 09:41:00.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 create -f -'
    Jan  5 09:41:00.699: INFO: stderr: ""
    Jan  5 09:41:00.699: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 09:41:00.699
    Jan  5 09:41:00.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 09:41:00.770: INFO: stderr: ""
    Jan  5 09:41:00.770: INFO: stdout: "update-demo-nautilus-6m8zg update-demo-nautilus-zkdlb "
    Jan  5 09:41:00.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods update-demo-nautilus-6m8zg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 09:41:00.839: INFO: stderr: ""
    Jan  5 09:41:00.839: INFO: stdout: ""
    Jan  5 09:41:00.839: INFO: update-demo-nautilus-6m8zg is created but not running
    Jan  5 09:41:05.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 09:41:05.904: INFO: stderr: ""
    Jan  5 09:41:05.904: INFO: stdout: "update-demo-nautilus-6m8zg update-demo-nautilus-zkdlb "
    Jan  5 09:41:05.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods update-demo-nautilus-6m8zg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 09:41:05.970: INFO: stderr: ""
    Jan  5 09:41:05.970: INFO: stdout: "true"
    Jan  5 09:41:05.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods update-demo-nautilus-6m8zg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 09:41:06.030: INFO: stderr: ""
    Jan  5 09:41:06.030: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 09:41:06.030: INFO: validating pod update-demo-nautilus-6m8zg
    Jan  5 09:41:06.148: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 09:41:06.148: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 09:41:06.148: INFO: update-demo-nautilus-6m8zg is verified up and running
    Jan  5 09:41:06.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods update-demo-nautilus-zkdlb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 09:41:06.213: INFO: stderr: ""
    Jan  5 09:41:06.213: INFO: stdout: "true"
    Jan  5 09:41:06.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods update-demo-nautilus-zkdlb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 09:41:06.282: INFO: stderr: ""
    Jan  5 09:41:06.282: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 09:41:06.282: INFO: validating pod update-demo-nautilus-zkdlb
    Jan  5 09:41:06.398: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 09:41:06.398: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 09:41:06.398: INFO: update-demo-nautilus-zkdlb is verified up and running
    STEP: using delete to clean up resources 01/05/23 09:41:06.398
    Jan  5 09:41:06.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 delete --grace-period=0 --force -f -'
    Jan  5 09:41:06.476: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 09:41:06.476: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan  5 09:41:06.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get rc,svc -l name=update-demo --no-headers'
    Jan  5 09:41:06.545: INFO: stderr: "No resources found in kubectl-345 namespace.\n"
    Jan  5 09:41:06.545: INFO: stdout: ""
    Jan  5 09:41:06.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-345 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  5 09:41:06.609: INFO: stderr: ""
    Jan  5 09:41:06.609: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 09:41:06.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-345" for this suite. 01/05/23 09:41:06.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:41:06.634
Jan  5 09:41:06.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename tables 01/05/23 09:41:06.635
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:41:06.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:41:06.659
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jan  5 09:41:06.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7288" for this suite. 01/05/23 09:41:06.687
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":156,"skipped":2847,"failed":0}
------------------------------
â€¢ [0.062 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:41:06.634
    Jan  5 09:41:06.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename tables 01/05/23 09:41:06.635
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:41:06.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:41:06.659
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jan  5 09:41:06.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-7288" for this suite. 01/05/23 09:41:06.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:41:06.697
Jan  5 09:41:06.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-probe 01/05/23 09:41:06.698
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:41:06.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:41:06.72
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff in namespace container-probe-4747 01/05/23 09:41:06.726
Jan  5 09:41:06.739: INFO: Waiting up to 5m0s for pod "liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff" in namespace "container-probe-4747" to be "not pending"
Jan  5 09:41:06.744: INFO: Pod "liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.856536ms
Jan  5 09:41:08.751: INFO: Pod "liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff": Phase="Running", Reason="", readiness=true. Elapsed: 2.012086484s
Jan  5 09:41:08.751: INFO: Pod "liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff" satisfied condition "not pending"
Jan  5 09:41:08.751: INFO: Started pod liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff in namespace container-probe-4747
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 09:41:08.751
Jan  5 09:41:08.756: INFO: Initial restart count of pod liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff is 0
STEP: deleting the pod 01/05/23 09:45:09.776
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 09:45:09.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4747" for this suite. 01/05/23 09:45:09.808
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":157,"skipped":2856,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.119 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:41:06.697
    Jan  5 09:41:06.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-probe 01/05/23 09:41:06.698
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:41:06.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:41:06.72
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff in namespace container-probe-4747 01/05/23 09:41:06.726
    Jan  5 09:41:06.739: INFO: Waiting up to 5m0s for pod "liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff" in namespace "container-probe-4747" to be "not pending"
    Jan  5 09:41:06.744: INFO: Pod "liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.856536ms
    Jan  5 09:41:08.751: INFO: Pod "liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff": Phase="Running", Reason="", readiness=true. Elapsed: 2.012086484s
    Jan  5 09:41:08.751: INFO: Pod "liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff" satisfied condition "not pending"
    Jan  5 09:41:08.751: INFO: Started pod liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff in namespace container-probe-4747
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 09:41:08.751
    Jan  5 09:41:08.756: INFO: Initial restart count of pod liveness-1ef22d29-81cc-48f4-8885-b3b3bd5c9aff is 0
    STEP: deleting the pod 01/05/23 09:45:09.776
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 09:45:09.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4747" for this suite. 01/05/23 09:45:09.808
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:45:09.816
Jan  5 09:45:09.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-runtime 01/05/23 09:45:09.817
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:45:09.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:45:09.841
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 01/05/23 09:45:09.848
STEP: wait for the container to reach Succeeded 01/05/23 09:45:09.863
STEP: get the container status 01/05/23 09:45:13.91
STEP: the container should be terminated 01/05/23 09:45:13.917
STEP: the termination message should be set 01/05/23 09:45:13.917
Jan  5 09:45:13.918: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/05/23 09:45:13.918
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  5 09:45:13.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1805" for this suite. 01/05/23 09:45:13.951
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":158,"skipped":2859,"failed":0}
------------------------------
â€¢ [4.143 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:45:09.816
    Jan  5 09:45:09.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-runtime 01/05/23 09:45:09.817
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:45:09.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:45:09.841
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 01/05/23 09:45:09.848
    STEP: wait for the container to reach Succeeded 01/05/23 09:45:09.863
    STEP: get the container status 01/05/23 09:45:13.91
    STEP: the container should be terminated 01/05/23 09:45:13.917
    STEP: the termination message should be set 01/05/23 09:45:13.917
    Jan  5 09:45:13.918: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/05/23 09:45:13.918
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  5 09:45:13.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1805" for this suite. 01/05/23 09:45:13.951
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:45:13.959
Jan  5 09:45:13.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:45:13.96
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:45:13.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:45:13.983
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:45:13.989
Jan  5 09:45:14.000: INFO: Waiting up to 5m0s for pod "downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446" in namespace "projected-2684" to be "Succeeded or Failed"
Jan  5 09:45:14.007: INFO: Pod "downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446": Phase="Pending", Reason="", readiness=false. Elapsed: 7.032851ms
Jan  5 09:45:16.026: INFO: Pod "downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025816637s
Jan  5 09:45:18.015: INFO: Pod "downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015466573s
STEP: Saw pod success 01/05/23 09:45:18.015
Jan  5 09:45:18.015: INFO: Pod "downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446" satisfied condition "Succeeded or Failed"
Jan  5 09:45:18.023: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446 container client-container: <nil>
STEP: delete the pod 01/05/23 09:45:18.042
Jan  5 09:45:18.057: INFO: Waiting for pod downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446 to disappear
Jan  5 09:45:18.062: INFO: Pod downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 09:45:18.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2684" for this suite. 01/05/23 09:45:18.074
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":159,"skipped":2860,"failed":0}
------------------------------
â€¢ [4.124 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:45:13.959
    Jan  5 09:45:13.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:45:13.96
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:45:13.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:45:13.983
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:45:13.989
    Jan  5 09:45:14.000: INFO: Waiting up to 5m0s for pod "downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446" in namespace "projected-2684" to be "Succeeded or Failed"
    Jan  5 09:45:14.007: INFO: Pod "downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446": Phase="Pending", Reason="", readiness=false. Elapsed: 7.032851ms
    Jan  5 09:45:16.026: INFO: Pod "downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025816637s
    Jan  5 09:45:18.015: INFO: Pod "downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015466573s
    STEP: Saw pod success 01/05/23 09:45:18.015
    Jan  5 09:45:18.015: INFO: Pod "downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446" satisfied condition "Succeeded or Failed"
    Jan  5 09:45:18.023: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446 container client-container: <nil>
    STEP: delete the pod 01/05/23 09:45:18.042
    Jan  5 09:45:18.057: INFO: Waiting for pod downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446 to disappear
    Jan  5 09:45:18.062: INFO: Pod downwardapi-volume-116ece4d-7724-4992-8aae-810e75de2446 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 09:45:18.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2684" for this suite. 01/05/23 09:45:18.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:45:18.084
Jan  5 09:45:18.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename containers 01/05/23 09:45:18.085
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:45:18.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:45:18.114
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jan  5 09:45:18.134: INFO: Waiting up to 5m0s for pod "client-containers-99bebb06-27a9-4d9b-9d6b-202c15ca90a1" in namespace "containers-5788" to be "running"
Jan  5 09:45:18.144: INFO: Pod "client-containers-99bebb06-27a9-4d9b-9d6b-202c15ca90a1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.960307ms
Jan  5 09:45:20.151: INFO: Pod "client-containers-99bebb06-27a9-4d9b-9d6b-202c15ca90a1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017338931s
Jan  5 09:45:20.152: INFO: Pod "client-containers-99bebb06-27a9-4d9b-9d6b-202c15ca90a1" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan  5 09:45:20.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5788" for this suite. 01/05/23 09:45:20.178
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":160,"skipped":2868,"failed":0}
------------------------------
â€¢ [2.103 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:45:18.084
    Jan  5 09:45:18.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename containers 01/05/23 09:45:18.085
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:45:18.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:45:18.114
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jan  5 09:45:18.134: INFO: Waiting up to 5m0s for pod "client-containers-99bebb06-27a9-4d9b-9d6b-202c15ca90a1" in namespace "containers-5788" to be "running"
    Jan  5 09:45:18.144: INFO: Pod "client-containers-99bebb06-27a9-4d9b-9d6b-202c15ca90a1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.960307ms
    Jan  5 09:45:20.151: INFO: Pod "client-containers-99bebb06-27a9-4d9b-9d6b-202c15ca90a1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017338931s
    Jan  5 09:45:20.152: INFO: Pod "client-containers-99bebb06-27a9-4d9b-9d6b-202c15ca90a1" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan  5 09:45:20.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5788" for this suite. 01/05/23 09:45:20.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:45:20.188
Jan  5 09:45:20.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 09:45:20.189
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:45:20.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:45:20.215
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-3192 01/05/23 09:45:20.223
Jan  5 09:45:20.235: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3192" to be "running and ready"
Jan  5 09:45:20.243: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094001ms
Jan  5 09:45:20.243: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:45:22.253: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.01742489s
Jan  5 09:45:22.253: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan  5 09:45:22.253: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan  5 09:45:22.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan  5 09:45:22.694: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan  5 09:45:22.694: INFO: stdout: "iptables"
Jan  5 09:45:22.694: INFO: proxyMode: iptables
Jan  5 09:45:22.708: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan  5 09:45:22.718: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3192 01/05/23 09:45:22.718
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3192 01/05/23 09:45:22.754
I0105 09:45:22.762697      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3192, replica count: 3
I0105 09:45:25.814491      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 09:45:25.833: INFO: Creating new exec pod
Jan  5 09:45:25.842: INFO: Waiting up to 5m0s for pod "execpod-affinityzk9jb" in namespace "services-3192" to be "running"
Jan  5 09:45:25.849: INFO: Pod "execpod-affinityzk9jb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.981035ms
Jan  5 09:45:27.857: INFO: Pod "execpod-affinityzk9jb": Phase="Running", Reason="", readiness=true. Elapsed: 2.01482425s
Jan  5 09:45:27.857: INFO: Pod "execpod-affinityzk9jb" satisfied condition "running"
Jan  5 09:45:28.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan  5 09:45:29.339: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan  5 09:45:29.339: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:45:29.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.33.78 80'
Jan  5 09:45:29.791: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.33.78 80\nConnection to 10.123.33.78 80 port [tcp/http] succeeded!\n"
Jan  5 09:45:29.791: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:45:29.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.138 30831'
Jan  5 09:45:30.340: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.138 30831\nConnection to 10.250.2.138 30831 port [tcp/*] succeeded!\n"
Jan  5 09:45:30.340: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:45:30.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.174 30831'
Jan  5 09:45:30.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.174 30831\nConnection to 10.250.0.174 30831 port [tcp/*] succeeded!\n"
Jan  5 09:45:30.860: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:45:30.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.128:30831/ ; done'
Jan  5 09:45:31.420: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n"
Jan  5 09:45:31.420: INFO: stdout: "\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg"
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
Jan  5 09:45:31.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.0.128:30831/'
Jan  5 09:45:31.942: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n"
Jan  5 09:45:31.942: INFO: stdout: "affinity-nodeport-timeout-pv8kg"
Jan  5 09:45:51.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.0.128:30831/'
Jan  5 09:45:52.350: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n"
Jan  5 09:45:52.350: INFO: stdout: "affinity-nodeport-timeout-ffv29"
Jan  5 09:45:52.350: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3192, will wait for the garbage collector to delete the pods 01/05/23 09:45:52.363
Jan  5 09:45:52.428: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.495098ms
Jan  5 09:45:52.529: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.469791ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:45:54.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3192" for this suite. 01/05/23 09:45:54.679
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":161,"skipped":2883,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.497 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:45:20.188
    Jan  5 09:45:20.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 09:45:20.189
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:45:20.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:45:20.215
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-3192 01/05/23 09:45:20.223
    Jan  5 09:45:20.235: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3192" to be "running and ready"
    Jan  5 09:45:20.243: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094001ms
    Jan  5 09:45:20.243: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:45:22.253: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.01742489s
    Jan  5 09:45:22.253: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan  5 09:45:22.253: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan  5 09:45:22.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan  5 09:45:22.694: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan  5 09:45:22.694: INFO: stdout: "iptables"
    Jan  5 09:45:22.694: INFO: proxyMode: iptables
    Jan  5 09:45:22.708: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan  5 09:45:22.718: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-3192 01/05/23 09:45:22.718
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-3192 01/05/23 09:45:22.754
    I0105 09:45:22.762697      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3192, replica count: 3
    I0105 09:45:25.814491      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 09:45:25.833: INFO: Creating new exec pod
    Jan  5 09:45:25.842: INFO: Waiting up to 5m0s for pod "execpod-affinityzk9jb" in namespace "services-3192" to be "running"
    Jan  5 09:45:25.849: INFO: Pod "execpod-affinityzk9jb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.981035ms
    Jan  5 09:45:27.857: INFO: Pod "execpod-affinityzk9jb": Phase="Running", Reason="", readiness=true. Elapsed: 2.01482425s
    Jan  5 09:45:27.857: INFO: Pod "execpod-affinityzk9jb" satisfied condition "running"
    Jan  5 09:45:28.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Jan  5 09:45:29.339: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Jan  5 09:45:29.339: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:45:29.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.33.78 80'
    Jan  5 09:45:29.791: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.33.78 80\nConnection to 10.123.33.78 80 port [tcp/http] succeeded!\n"
    Jan  5 09:45:29.791: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:45:29.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.138 30831'
    Jan  5 09:45:30.340: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.138 30831\nConnection to 10.250.2.138 30831 port [tcp/*] succeeded!\n"
    Jan  5 09:45:30.340: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:45:30.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.174 30831'
    Jan  5 09:45:30.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.174 30831\nConnection to 10.250.0.174 30831 port [tcp/*] succeeded!\n"
    Jan  5 09:45:30.860: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:45:30.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.128:30831/ ; done'
    Jan  5 09:45:31.420: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n"
    Jan  5 09:45:31.420: INFO: stdout: "\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg\naffinity-nodeport-timeout-pv8kg"
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Received response from host: affinity-nodeport-timeout-pv8kg
    Jan  5 09:45:31.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.0.128:30831/'
    Jan  5 09:45:31.942: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n"
    Jan  5 09:45:31.942: INFO: stdout: "affinity-nodeport-timeout-pv8kg"
    Jan  5 09:45:51.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-3192 exec execpod-affinityzk9jb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.0.128:30831/'
    Jan  5 09:45:52.350: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.0.128:30831/\n"
    Jan  5 09:45:52.350: INFO: stdout: "affinity-nodeport-timeout-ffv29"
    Jan  5 09:45:52.350: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3192, will wait for the garbage collector to delete the pods 01/05/23 09:45:52.363
    Jan  5 09:45:52.428: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.495098ms
    Jan  5 09:45:52.529: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.469791ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:45:54.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3192" for this suite. 01/05/23 09:45:54.679
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:45:54.686
Jan  5 09:45:54.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename subpath 01/05/23 09:45:54.686
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:45:54.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:45:54.707
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 09:45:54.713
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-v2gh 01/05/23 09:45:54.725
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 09:45:54.725
Jan  5 09:45:54.738: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-v2gh" in namespace "subpath-738" to be "Succeeded or Failed"
Jan  5 09:45:54.745: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.109317ms
Jan  5 09:45:56.750: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 2.011894435s
Jan  5 09:45:58.753: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 4.014103373s
Jan  5 09:46:00.751: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 6.012222186s
Jan  5 09:46:02.752: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 8.013509996s
Jan  5 09:46:04.752: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 10.013605092s
Jan  5 09:46:06.753: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 12.014756698s
Jan  5 09:46:08.750: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 14.011978019s
Jan  5 09:46:10.751: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 16.012203406s
Jan  5 09:46:12.754: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 18.015091892s
Jan  5 09:46:14.755: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 20.016096063s
Jan  5 09:46:16.752: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=false. Elapsed: 22.013234512s
Jan  5 09:46:18.753: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014373809s
STEP: Saw pod success 01/05/23 09:46:18.753
Jan  5 09:46:18.753: INFO: Pod "pod-subpath-test-projected-v2gh" satisfied condition "Succeeded or Failed"
Jan  5 09:46:18.759: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-subpath-test-projected-v2gh container test-container-subpath-projected-v2gh: <nil>
STEP: delete the pod 01/05/23 09:46:18.772
Jan  5 09:46:18.783: INFO: Waiting for pod pod-subpath-test-projected-v2gh to disappear
Jan  5 09:46:18.788: INFO: Pod pod-subpath-test-projected-v2gh no longer exists
STEP: Deleting pod pod-subpath-test-projected-v2gh 01/05/23 09:46:18.788
Jan  5 09:46:18.788: INFO: Deleting pod "pod-subpath-test-projected-v2gh" in namespace "subpath-738"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  5 09:46:18.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-738" for this suite. 01/05/23 09:46:18.804
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":162,"skipped":2889,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.127 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:45:54.686
    Jan  5 09:45:54.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename subpath 01/05/23 09:45:54.686
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:45:54.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:45:54.707
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 09:45:54.713
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-v2gh 01/05/23 09:45:54.725
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 09:45:54.725
    Jan  5 09:45:54.738: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-v2gh" in namespace "subpath-738" to be "Succeeded or Failed"
    Jan  5 09:45:54.745: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.109317ms
    Jan  5 09:45:56.750: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 2.011894435s
    Jan  5 09:45:58.753: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 4.014103373s
    Jan  5 09:46:00.751: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 6.012222186s
    Jan  5 09:46:02.752: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 8.013509996s
    Jan  5 09:46:04.752: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 10.013605092s
    Jan  5 09:46:06.753: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 12.014756698s
    Jan  5 09:46:08.750: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 14.011978019s
    Jan  5 09:46:10.751: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 16.012203406s
    Jan  5 09:46:12.754: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 18.015091892s
    Jan  5 09:46:14.755: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=true. Elapsed: 20.016096063s
    Jan  5 09:46:16.752: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Running", Reason="", readiness=false. Elapsed: 22.013234512s
    Jan  5 09:46:18.753: INFO: Pod "pod-subpath-test-projected-v2gh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014373809s
    STEP: Saw pod success 01/05/23 09:46:18.753
    Jan  5 09:46:18.753: INFO: Pod "pod-subpath-test-projected-v2gh" satisfied condition "Succeeded or Failed"
    Jan  5 09:46:18.759: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-subpath-test-projected-v2gh container test-container-subpath-projected-v2gh: <nil>
    STEP: delete the pod 01/05/23 09:46:18.772
    Jan  5 09:46:18.783: INFO: Waiting for pod pod-subpath-test-projected-v2gh to disappear
    Jan  5 09:46:18.788: INFO: Pod pod-subpath-test-projected-v2gh no longer exists
    STEP: Deleting pod pod-subpath-test-projected-v2gh 01/05/23 09:46:18.788
    Jan  5 09:46:18.788: INFO: Deleting pod "pod-subpath-test-projected-v2gh" in namespace "subpath-738"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  5 09:46:18.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-738" for this suite. 01/05/23 09:46:18.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:46:18.814
Jan  5 09:46:18.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 09:46:18.815
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:46:18.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:46:18.848
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/05/23 09:46:18.865
Jan  5 09:46:18.877: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5957" to be "running and ready"
Jan  5 09:46:18.885: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.775799ms
Jan  5 09:46:18.885: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:46:20.893: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015280806s
Jan  5 09:46:20.893: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  5 09:46:20.893: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 01/05/23 09:46:20.898
Jan  5 09:46:20.907: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5957" to be "running and ready"
Jan  5 09:46:20.913: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874689ms
Jan  5 09:46:20.913: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:46:22.923: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015192607s
Jan  5 09:46:22.923: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan  5 09:46:22.923: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/05/23 09:46:22.928
Jan  5 09:46:22.937: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  5 09:46:22.942: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  5 09:46:24.943: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  5 09:46:24.950: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  5 09:46:26.942: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  5 09:46:26.949: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/05/23 09:46:26.949
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan  5 09:46:26.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5957" for this suite. 01/05/23 09:46:26.986
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":163,"skipped":2903,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.180 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:46:18.814
    Jan  5 09:46:18.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 09:46:18.815
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:46:18.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:46:18.848
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/05/23 09:46:18.865
    Jan  5 09:46:18.877: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5957" to be "running and ready"
    Jan  5 09:46:18.885: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.775799ms
    Jan  5 09:46:18.885: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:46:20.893: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015280806s
    Jan  5 09:46:20.893: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  5 09:46:20.893: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 01/05/23 09:46:20.898
    Jan  5 09:46:20.907: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5957" to be "running and ready"
    Jan  5 09:46:20.913: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874689ms
    Jan  5 09:46:20.913: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:46:22.923: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015192607s
    Jan  5 09:46:22.923: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan  5 09:46:22.923: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/05/23 09:46:22.928
    Jan  5 09:46:22.937: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  5 09:46:22.942: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan  5 09:46:24.943: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  5 09:46:24.950: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan  5 09:46:26.942: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  5 09:46:26.949: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/05/23 09:46:26.949
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan  5 09:46:26.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5957" for this suite. 01/05/23 09:46:26.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:46:26.997
Jan  5 09:46:26.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-runtime 01/05/23 09:46:26.998
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:46:27.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:46:27.021
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/05/23 09:46:27.041
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/05/23 09:46:47.196
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/05/23 09:46:47.2
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/05/23 09:46:47.209
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/05/23 09:46:47.209
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/05/23 09:46:47.243
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/05/23 09:46:50.273
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/05/23 09:46:52.292
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/05/23 09:46:52.302
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/05/23 09:46:52.302
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/05/23 09:46:52.344
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/05/23 09:46:53.359
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/05/23 09:46:55.377
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/05/23 09:46:55.388
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/05/23 09:46:55.388
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  5 09:46:55.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2231" for this suite. 01/05/23 09:46:55.443
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":164,"skipped":2909,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.455 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:46:26.997
    Jan  5 09:46:26.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-runtime 01/05/23 09:46:26.998
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:46:27.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:46:27.021
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/05/23 09:46:27.041
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/05/23 09:46:47.196
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/05/23 09:46:47.2
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/05/23 09:46:47.209
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/05/23 09:46:47.209
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/05/23 09:46:47.243
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/05/23 09:46:50.273
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/05/23 09:46:52.292
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/05/23 09:46:52.302
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/05/23 09:46:52.302
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/05/23 09:46:52.344
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/05/23 09:46:53.359
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/05/23 09:46:55.377
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/05/23 09:46:55.388
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/05/23 09:46:55.388
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  5 09:46:55.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2231" for this suite. 01/05/23 09:46:55.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:46:55.452
Jan  5 09:46:55.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename watch 01/05/23 09:46:55.453
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:46:55.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:46:55.477
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/05/23 09:46:55.483
STEP: creating a watch on configmaps with label B 01/05/23 09:46:55.486
STEP: creating a watch on configmaps with label A or B 01/05/23 09:46:55.489
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/05/23 09:46:55.492
Jan  5 09:46:55.498: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23136 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:46:55.498: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23136 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/05/23 09:46:55.498
Jan  5 09:46:55.510: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23138 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:46:55.511: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23138 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/05/23 09:46:55.511
Jan  5 09:46:55.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23139 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:46:55.527: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23139 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/05/23 09:46:55.527
Jan  5 09:46:55.535: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23140 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:46:55.535: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23140 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/05/23 09:46:55.535
Jan  5 09:46:55.541: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1195  e41ae0b0-b494-451e-b770-ad49bcf7b508 23141 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:46:55.541: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1195  e41ae0b0-b494-451e-b770-ad49bcf7b508 23141 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/05/23 09:47:05.542
Jan  5 09:47:05.550: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1195  e41ae0b0-b494-451e-b770-ad49bcf7b508 23193 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:47:05.550: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1195  e41ae0b0-b494-451e-b770-ad49bcf7b508 23193 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  5 09:47:15.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1195" for this suite. 01/05/23 09:47:15.568
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":165,"skipped":2926,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.123 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:46:55.452
    Jan  5 09:46:55.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename watch 01/05/23 09:46:55.453
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:46:55.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:46:55.477
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/05/23 09:46:55.483
    STEP: creating a watch on configmaps with label B 01/05/23 09:46:55.486
    STEP: creating a watch on configmaps with label A or B 01/05/23 09:46:55.489
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/05/23 09:46:55.492
    Jan  5 09:46:55.498: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23136 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:46:55.498: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23136 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/05/23 09:46:55.498
    Jan  5 09:46:55.510: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23138 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:46:55.511: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23138 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/05/23 09:46:55.511
    Jan  5 09:46:55.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23139 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:46:55.527: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23139 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/05/23 09:46:55.527
    Jan  5 09:46:55.535: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23140 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:46:55.535: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1195  0cc154ab-0d23-4430-8a5f-566dcdc6e8f0 23140 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/05/23 09:46:55.535
    Jan  5 09:46:55.541: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1195  e41ae0b0-b494-451e-b770-ad49bcf7b508 23141 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:46:55.541: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1195  e41ae0b0-b494-451e-b770-ad49bcf7b508 23141 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/05/23 09:47:05.542
    Jan  5 09:47:05.550: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1195  e41ae0b0-b494-451e-b770-ad49bcf7b508 23193 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:47:05.550: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1195  e41ae0b0-b494-451e-b770-ad49bcf7b508 23193 0 2023-01-05 09:46:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 09:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  5 09:47:15.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1195" for this suite. 01/05/23 09:47:15.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:15.576
Jan  5 09:47:15.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 09:47:15.577
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:15.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:15.601
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 09:47:15.621
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:47:15.971
STEP: Deploying the webhook pod 01/05/23 09:47:15.979
STEP: Wait for the deployment to be ready 01/05/23 09:47:16.008
Jan  5 09:47:16.029: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 09:47:18.053
STEP: Verifying the service has paired with the endpoint 01/05/23 09:47:18.078
Jan  5 09:47:19.078: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jan  5 09:47:19.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/05/23 09:47:19.598
STEP: Creating a custom resource that should be denied by the webhook 01/05/23 09:47:19.734
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/05/23 09:47:21.879
STEP: Updating the custom resource with disallowed data should be denied 01/05/23 09:47:21.982
STEP: Deleting the custom resource should be denied 01/05/23 09:47:22.084
STEP: Remove the offending key and value from the custom resource data 01/05/23 09:47:22.166
STEP: Deleting the updated custom resource should be successful 01/05/23 09:47:22.188
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:47:22.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3972" for this suite. 01/05/23 09:47:22.831
STEP: Destroying namespace "webhook-3972-markers" for this suite. 01/05/23 09:47:22.846
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":166,"skipped":2945,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.361 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:15.576
    Jan  5 09:47:15.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 09:47:15.577
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:15.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:15.601
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 09:47:15.621
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:47:15.971
    STEP: Deploying the webhook pod 01/05/23 09:47:15.979
    STEP: Wait for the deployment to be ready 01/05/23 09:47:16.008
    Jan  5 09:47:16.029: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 09:47:18.053
    STEP: Verifying the service has paired with the endpoint 01/05/23 09:47:18.078
    Jan  5 09:47:19.078: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jan  5 09:47:19.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/05/23 09:47:19.598
    STEP: Creating a custom resource that should be denied by the webhook 01/05/23 09:47:19.734
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/05/23 09:47:21.879
    STEP: Updating the custom resource with disallowed data should be denied 01/05/23 09:47:21.982
    STEP: Deleting the custom resource should be denied 01/05/23 09:47:22.084
    STEP: Remove the offending key and value from the custom resource data 01/05/23 09:47:22.166
    STEP: Deleting the updated custom resource should be successful 01/05/23 09:47:22.188
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:47:22.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3972" for this suite. 01/05/23 09:47:22.831
    STEP: Destroying namespace "webhook-3972-markers" for this suite. 01/05/23 09:47:22.846
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:22.938
Jan  5 09:47:22.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 09:47:22.939
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:22.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:22.985
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 09:47:22.992
Jan  5 09:47:22.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-650 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan  5 09:47:23.077: INFO: stderr: ""
Jan  5 09:47:23.077: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/05/23 09:47:23.077
Jan  5 09:47:23.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-650 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan  5 09:47:23.943: INFO: stderr: ""
Jan  5 09:47:23.943: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 09:47:23.943
Jan  5 09:47:23.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-650 delete pods e2e-test-httpd-pod'
Jan  5 09:47:25.430: INFO: stderr: ""
Jan  5 09:47:25.430: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 09:47:25.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-650" for this suite. 01/05/23 09:47:25.441
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":167,"skipped":2947,"failed":0}
------------------------------
â€¢ [2.515 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:22.938
    Jan  5 09:47:22.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 09:47:22.939
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:22.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:22.985
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 09:47:22.992
    Jan  5 09:47:22.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-650 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan  5 09:47:23.077: INFO: stderr: ""
    Jan  5 09:47:23.077: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/05/23 09:47:23.077
    Jan  5 09:47:23.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-650 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jan  5 09:47:23.943: INFO: stderr: ""
    Jan  5 09:47:23.943: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 09:47:23.943
    Jan  5 09:47:23.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-650 delete pods e2e-test-httpd-pod'
    Jan  5 09:47:25.430: INFO: stderr: ""
    Jan  5 09:47:25.430: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 09:47:25.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-650" for this suite. 01/05/23 09:47:25.441
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:25.453
Jan  5 09:47:25.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 09:47:25.454
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:25.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:25.479
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:47:25.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-708" for this suite. 01/05/23 09:47:25.502
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":168,"skipped":2947,"failed":0}
------------------------------
â€¢ [0.057 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:25.453
    Jan  5 09:47:25.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 09:47:25.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:25.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:25.479
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:47:25.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-708" for this suite. 01/05/23 09:47:25.502
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:25.51
Jan  5 09:47:25.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pods 01/05/23 09:47:25.511
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:25.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:25.537
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/05/23 09:47:25.545
STEP: submitting the pod to kubernetes 01/05/23 09:47:25.545
STEP: verifying QOS class is set on the pod 01/05/23 09:47:25.561
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jan  5 09:47:25.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3591" for this suite. 01/05/23 09:47:25.581
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":169,"skipped":2950,"failed":0}
------------------------------
â€¢ [0.079 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:25.51
    Jan  5 09:47:25.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pods 01/05/23 09:47:25.511
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:25.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:25.537
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/05/23 09:47:25.545
    STEP: submitting the pod to kubernetes 01/05/23 09:47:25.545
    STEP: verifying QOS class is set on the pod 01/05/23 09:47:25.561
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jan  5 09:47:25.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3591" for this suite. 01/05/23 09:47:25.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:25.589
Jan  5 09:47:25.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename endpointslice 01/05/23 09:47:25.59
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:25.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:25.614
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 01/05/23 09:47:25.62
STEP: getting /apis/discovery.k8s.io 01/05/23 09:47:25.626
STEP: getting /apis/discovery.k8s.iov1 01/05/23 09:47:25.629
STEP: creating 01/05/23 09:47:25.632
STEP: getting 01/05/23 09:47:25.648
STEP: listing 01/05/23 09:47:25.653
STEP: watching 01/05/23 09:47:25.66
Jan  5 09:47:25.660: INFO: starting watch
STEP: cluster-wide listing 01/05/23 09:47:25.663
STEP: cluster-wide watching 01/05/23 09:47:25.669
Jan  5 09:47:25.669: INFO: starting watch
STEP: patching 01/05/23 09:47:25.672
STEP: updating 01/05/23 09:47:25.68
Jan  5 09:47:25.692: INFO: waiting for watch events with expected annotations
Jan  5 09:47:25.692: INFO: saw patched and updated annotations
STEP: deleting 01/05/23 09:47:25.692
STEP: deleting a collection 01/05/23 09:47:25.708
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan  5 09:47:25.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5903" for this suite. 01/05/23 09:47:25.744
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":170,"skipped":2957,"failed":0}
------------------------------
â€¢ [0.161 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:25.589
    Jan  5 09:47:25.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename endpointslice 01/05/23 09:47:25.59
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:25.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:25.614
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 01/05/23 09:47:25.62
    STEP: getting /apis/discovery.k8s.io 01/05/23 09:47:25.626
    STEP: getting /apis/discovery.k8s.iov1 01/05/23 09:47:25.629
    STEP: creating 01/05/23 09:47:25.632
    STEP: getting 01/05/23 09:47:25.648
    STEP: listing 01/05/23 09:47:25.653
    STEP: watching 01/05/23 09:47:25.66
    Jan  5 09:47:25.660: INFO: starting watch
    STEP: cluster-wide listing 01/05/23 09:47:25.663
    STEP: cluster-wide watching 01/05/23 09:47:25.669
    Jan  5 09:47:25.669: INFO: starting watch
    STEP: patching 01/05/23 09:47:25.672
    STEP: updating 01/05/23 09:47:25.68
    Jan  5 09:47:25.692: INFO: waiting for watch events with expected annotations
    Jan  5 09:47:25.692: INFO: saw patched and updated annotations
    STEP: deleting 01/05/23 09:47:25.692
    STEP: deleting a collection 01/05/23 09:47:25.708
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan  5 09:47:25.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5903" for this suite. 01/05/23 09:47:25.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:25.756
Jan  5 09:47:25.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:47:25.756
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:25.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:25.777
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:47:25.784
Jan  5 09:47:25.797: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0" in namespace "downward-api-9154" to be "Succeeded or Failed"
Jan  5 09:47:25.804: INFO: Pod "downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.376774ms
Jan  5 09:47:27.811: INFO: Pod "downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013598444s
Jan  5 09:47:29.813: INFO: Pod "downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015592206s
STEP: Saw pod success 01/05/23 09:47:29.813
Jan  5 09:47:29.813: INFO: Pod "downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0" satisfied condition "Succeeded or Failed"
Jan  5 09:47:29.818: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0 container client-container: <nil>
STEP: delete the pod 01/05/23 09:47:29.836
Jan  5 09:47:29.848: INFO: Waiting for pod downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0 to disappear
Jan  5 09:47:29.852: INFO: Pod downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 09:47:29.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9154" for this suite. 01/05/23 09:47:29.866
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":171,"skipped":3003,"failed":0}
------------------------------
â€¢ [4.117 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:25.756
    Jan  5 09:47:25.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:47:25.756
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:25.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:25.777
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:47:25.784
    Jan  5 09:47:25.797: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0" in namespace "downward-api-9154" to be "Succeeded or Failed"
    Jan  5 09:47:25.804: INFO: Pod "downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.376774ms
    Jan  5 09:47:27.811: INFO: Pod "downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013598444s
    Jan  5 09:47:29.813: INFO: Pod "downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015592206s
    STEP: Saw pod success 01/05/23 09:47:29.813
    Jan  5 09:47:29.813: INFO: Pod "downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0" satisfied condition "Succeeded or Failed"
    Jan  5 09:47:29.818: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0 container client-container: <nil>
    STEP: delete the pod 01/05/23 09:47:29.836
    Jan  5 09:47:29.848: INFO: Waiting for pod downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0 to disappear
    Jan  5 09:47:29.852: INFO: Pod downwardapi-volume-cbbe6224-5bc5-46b9-87ed-ffd8652272b0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 09:47:29.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9154" for this suite. 01/05/23 09:47:29.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:29.883
Jan  5 09:47:29.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 09:47:29.883
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:29.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:29.907
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 09:47:29.932
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:47:30.297
STEP: Deploying the webhook pod 01/05/23 09:47:30.304
STEP: Wait for the deployment to be ready 01/05/23 09:47:30.319
Jan  5 09:47:30.361: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 09:47:32.387
STEP: Verifying the service has paired with the endpoint 01/05/23 09:47:32.418
Jan  5 09:47:33.420: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 01/05/23 09:47:33.427
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 09:47:33.575
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/05/23 09:47:33.71
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 09:47:33.727
STEP: Patching a validating webhook configuration's rules to include the create operation 01/05/23 09:47:33.741
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 09:47:33.753
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:47:33.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3500" for this suite. 01/05/23 09:47:33.794
STEP: Destroying namespace "webhook-3500-markers" for this suite. 01/05/23 09:47:33.802
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":172,"skipped":3088,"failed":0}
------------------------------
â€¢ [3.984 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:29.883
    Jan  5 09:47:29.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 09:47:29.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:29.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:29.907
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 09:47:29.932
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:47:30.297
    STEP: Deploying the webhook pod 01/05/23 09:47:30.304
    STEP: Wait for the deployment to be ready 01/05/23 09:47:30.319
    Jan  5 09:47:30.361: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 09:47:32.387
    STEP: Verifying the service has paired with the endpoint 01/05/23 09:47:32.418
    Jan  5 09:47:33.420: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 01/05/23 09:47:33.427
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 09:47:33.575
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/05/23 09:47:33.71
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 09:47:33.727
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/05/23 09:47:33.741
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 09:47:33.753
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:47:33.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3500" for this suite. 01/05/23 09:47:33.794
    STEP: Destroying namespace "webhook-3500-markers" for this suite. 01/05/23 09:47:33.802
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:33.867
Jan  5 09:47:33.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 09:47:33.868
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:33.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:33.909
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 01/05/23 09:47:33.919
Jan  5 09:47:33.932: INFO: Waiting up to 5m0s for pod "pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b" in namespace "emptydir-9442" to be "Succeeded or Failed"
Jan  5 09:47:33.939: INFO: Pod "pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.261618ms
Jan  5 09:47:35.950: INFO: Pod "pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018026243s
Jan  5 09:47:37.948: INFO: Pod "pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015629716s
STEP: Saw pod success 01/05/23 09:47:37.948
Jan  5 09:47:37.948: INFO: Pod "pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b" satisfied condition "Succeeded or Failed"
Jan  5 09:47:37.953: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b container test-container: <nil>
STEP: delete the pod 01/05/23 09:47:37.969
Jan  5 09:47:37.986: INFO: Waiting for pod pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b to disappear
Jan  5 09:47:37.996: INFO: Pod pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 09:47:37.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9442" for this suite. 01/05/23 09:47:38.008
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":173,"skipped":3093,"failed":0}
------------------------------
â€¢ [4.149 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:33.867
    Jan  5 09:47:33.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 09:47:33.868
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:33.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:33.909
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/05/23 09:47:33.919
    Jan  5 09:47:33.932: INFO: Waiting up to 5m0s for pod "pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b" in namespace "emptydir-9442" to be "Succeeded or Failed"
    Jan  5 09:47:33.939: INFO: Pod "pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.261618ms
    Jan  5 09:47:35.950: INFO: Pod "pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018026243s
    Jan  5 09:47:37.948: INFO: Pod "pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015629716s
    STEP: Saw pod success 01/05/23 09:47:37.948
    Jan  5 09:47:37.948: INFO: Pod "pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b" satisfied condition "Succeeded or Failed"
    Jan  5 09:47:37.953: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b container test-container: <nil>
    STEP: delete the pod 01/05/23 09:47:37.969
    Jan  5 09:47:37.986: INFO: Waiting for pod pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b to disappear
    Jan  5 09:47:37.996: INFO: Pod pod-10f8c784-ded9-4c59-996e-b5cd76b2ad6b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 09:47:37.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9442" for this suite. 01/05/23 09:47:38.008
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:38.017
Jan  5 09:47:38.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename disruption 01/05/23 09:47:38.018
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:38.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:38.041
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 01/05/23 09:47:38.054
STEP: Updating PodDisruptionBudget status 01/05/23 09:47:40.068
STEP: Waiting for all pods to be running 01/05/23 09:47:40.079
Jan  5 09:47:40.086: INFO: running pods: 0 < 1
STEP: locating a running pod 01/05/23 09:47:42.092
STEP: Waiting for the pdb to be processed 01/05/23 09:47:42.11
STEP: Patching PodDisruptionBudget status 01/05/23 09:47:42.122
STEP: Waiting for the pdb to be processed 01/05/23 09:47:42.137
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  5 09:47:42.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1477" for this suite. 01/05/23 09:47:42.155
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":174,"skipped":3094,"failed":0}
------------------------------
â€¢ [4.147 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:38.017
    Jan  5 09:47:38.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename disruption 01/05/23 09:47:38.018
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:38.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:38.041
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 01/05/23 09:47:38.054
    STEP: Updating PodDisruptionBudget status 01/05/23 09:47:40.068
    STEP: Waiting for all pods to be running 01/05/23 09:47:40.079
    Jan  5 09:47:40.086: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/05/23 09:47:42.092
    STEP: Waiting for the pdb to be processed 01/05/23 09:47:42.11
    STEP: Patching PodDisruptionBudget status 01/05/23 09:47:42.122
    STEP: Waiting for the pdb to be processed 01/05/23 09:47:42.137
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  5 09:47:42.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1477" for this suite. 01/05/23 09:47:42.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:42.165
Jan  5 09:47:42.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:47:42.166
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:42.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:42.194
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:47:42.202
Jan  5 09:47:42.219: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800" in namespace "downward-api-999" to be "Succeeded or Failed"
Jan  5 09:47:42.226: INFO: Pod "downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800": Phase="Pending", Reason="", readiness=false. Elapsed: 6.504866ms
Jan  5 09:47:44.232: INFO: Pod "downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013259904s
Jan  5 09:47:46.234: INFO: Pod "downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015172224s
STEP: Saw pod success 01/05/23 09:47:46.234
Jan  5 09:47:46.234: INFO: Pod "downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800" satisfied condition "Succeeded or Failed"
Jan  5 09:47:46.240: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800 container client-container: <nil>
STEP: delete the pod 01/05/23 09:47:46.253
Jan  5 09:47:46.264: INFO: Waiting for pod downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800 to disappear
Jan  5 09:47:46.268: INFO: Pod downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 09:47:46.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-999" for this suite. 01/05/23 09:47:46.278
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":175,"skipped":3102,"failed":0}
------------------------------
â€¢ [4.121 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:42.165
    Jan  5 09:47:42.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:47:42.166
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:42.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:42.194
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:47:42.202
    Jan  5 09:47:42.219: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800" in namespace "downward-api-999" to be "Succeeded or Failed"
    Jan  5 09:47:42.226: INFO: Pod "downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800": Phase="Pending", Reason="", readiness=false. Elapsed: 6.504866ms
    Jan  5 09:47:44.232: INFO: Pod "downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013259904s
    Jan  5 09:47:46.234: INFO: Pod "downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015172224s
    STEP: Saw pod success 01/05/23 09:47:46.234
    Jan  5 09:47:46.234: INFO: Pod "downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800" satisfied condition "Succeeded or Failed"
    Jan  5 09:47:46.240: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800 container client-container: <nil>
    STEP: delete the pod 01/05/23 09:47:46.253
    Jan  5 09:47:46.264: INFO: Waiting for pod downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800 to disappear
    Jan  5 09:47:46.268: INFO: Pod downwardapi-volume-4d37bbf6-4db3-410e-bc0a-465532e6d800 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 09:47:46.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-999" for this suite. 01/05/23 09:47:46.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:46.288
Jan  5 09:47:46.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pods 01/05/23 09:47:46.288
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:46.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:46.311
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 01/05/23 09:47:46.319
STEP: submitting the pod to kubernetes 01/05/23 09:47:46.319
Jan  5 09:47:46.331: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431" in namespace "pods-2729" to be "running and ready"
Jan  5 09:47:46.336: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431": Phase="Pending", Reason="", readiness=false. Elapsed: 5.27525ms
Jan  5 09:47:46.336: INFO: The phase of Pod pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:47:48.344: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431": Phase="Running", Reason="", readiness=true. Elapsed: 2.013762079s
Jan  5 09:47:48.345: INFO: The phase of Pod pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431 is Running (Ready = true)
Jan  5 09:47:48.345: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/05/23 09:47:48.349
STEP: updating the pod 01/05/23 09:47:48.357
Jan  5 09:47:48.874: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431"
Jan  5 09:47:48.874: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431" in namespace "pods-2729" to be "terminated with reason DeadlineExceeded"
Jan  5 09:47:48.879: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431": Phase="Running", Reason="", readiness=true. Elapsed: 5.454126ms
Jan  5 09:47:50.888: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431": Phase="Running", Reason="", readiness=true. Elapsed: 2.013758872s
Jan  5 09:47:52.887: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431": Phase="Running", Reason="", readiness=false. Elapsed: 4.01269027s
Jan  5 09:47:54.888: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.01399292s
Jan  5 09:47:54.888: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 09:47:54.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2729" for this suite. 01/05/23 09:47:54.901
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":176,"skipped":3114,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.621 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:46.288
    Jan  5 09:47:46.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pods 01/05/23 09:47:46.288
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:46.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:46.311
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 01/05/23 09:47:46.319
    STEP: submitting the pod to kubernetes 01/05/23 09:47:46.319
    Jan  5 09:47:46.331: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431" in namespace "pods-2729" to be "running and ready"
    Jan  5 09:47:46.336: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431": Phase="Pending", Reason="", readiness=false. Elapsed: 5.27525ms
    Jan  5 09:47:46.336: INFO: The phase of Pod pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:47:48.344: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431": Phase="Running", Reason="", readiness=true. Elapsed: 2.013762079s
    Jan  5 09:47:48.345: INFO: The phase of Pod pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431 is Running (Ready = true)
    Jan  5 09:47:48.345: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/05/23 09:47:48.349
    STEP: updating the pod 01/05/23 09:47:48.357
    Jan  5 09:47:48.874: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431"
    Jan  5 09:47:48.874: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431" in namespace "pods-2729" to be "terminated with reason DeadlineExceeded"
    Jan  5 09:47:48.879: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431": Phase="Running", Reason="", readiness=true. Elapsed: 5.454126ms
    Jan  5 09:47:50.888: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431": Phase="Running", Reason="", readiness=true. Elapsed: 2.013758872s
    Jan  5 09:47:52.887: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431": Phase="Running", Reason="", readiness=false. Elapsed: 4.01269027s
    Jan  5 09:47:54.888: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.01399292s
    Jan  5 09:47:54.888: INFO: Pod "pod-update-activedeadlineseconds-b3b34518-1389-4859-9c30-75775084f431" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 09:47:54.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2729" for this suite. 01/05/23 09:47:54.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:54.914
Jan  5 09:47:54.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:47:54.915
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:54.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:54.939
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-34321ff3-4a63-4718-a309-a97adde31ff3 01/05/23 09:47:54.946
STEP: Creating a pod to test consume secrets 01/05/23 09:47:54.953
Jan  5 09:47:54.966: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6" in namespace "projected-2346" to be "Succeeded or Failed"
Jan  5 09:47:54.972: INFO: Pod "pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.261964ms
Jan  5 09:47:56.981: INFO: Pod "pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014631364s
Jan  5 09:47:58.980: INFO: Pod "pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014221766s
STEP: Saw pod success 01/05/23 09:47:58.98
Jan  5 09:47:58.980: INFO: Pod "pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6" satisfied condition "Succeeded or Failed"
Jan  5 09:47:58.985: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 09:47:58.999
Jan  5 09:47:59.013: INFO: Waiting for pod pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6 to disappear
Jan  5 09:47:59.018: INFO: Pod pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 09:47:59.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2346" for this suite. 01/05/23 09:47:59.029
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":177,"skipped":3151,"failed":0}
------------------------------
â€¢ [4.121 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:54.914
    Jan  5 09:47:54.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:47:54.915
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:54.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:54.939
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-34321ff3-4a63-4718-a309-a97adde31ff3 01/05/23 09:47:54.946
    STEP: Creating a pod to test consume secrets 01/05/23 09:47:54.953
    Jan  5 09:47:54.966: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6" in namespace "projected-2346" to be "Succeeded or Failed"
    Jan  5 09:47:54.972: INFO: Pod "pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.261964ms
    Jan  5 09:47:56.981: INFO: Pod "pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014631364s
    Jan  5 09:47:58.980: INFO: Pod "pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014221766s
    STEP: Saw pod success 01/05/23 09:47:58.98
    Jan  5 09:47:58.980: INFO: Pod "pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6" satisfied condition "Succeeded or Failed"
    Jan  5 09:47:58.985: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 09:47:58.999
    Jan  5 09:47:59.013: INFO: Waiting for pod pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6 to disappear
    Jan  5 09:47:59.018: INFO: Pod pod-projected-secrets-6fc070fa-9fe7-414e-a50b-092ef3284eb6 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 09:47:59.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2346" for this suite. 01/05/23 09:47:59.029
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:59.036
Jan  5 09:47:59.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename discovery 01/05/23 09:47:59.037
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:59.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:59.056
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/05/23 09:47:59.065
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan  5 09:47:59.514: INFO: Checking APIGroup: apiregistration.k8s.io
Jan  5 09:47:59.517: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan  5 09:47:59.518: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan  5 09:47:59.518: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan  5 09:47:59.518: INFO: Checking APIGroup: apps
Jan  5 09:47:59.521: INFO: PreferredVersion.GroupVersion: apps/v1
Jan  5 09:47:59.521: INFO: Versions found [{apps/v1 v1}]
Jan  5 09:47:59.521: INFO: apps/v1 matches apps/v1
Jan  5 09:47:59.521: INFO: Checking APIGroup: events.k8s.io
Jan  5 09:47:59.524: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan  5 09:47:59.524: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan  5 09:47:59.524: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan  5 09:47:59.524: INFO: Checking APIGroup: authentication.k8s.io
Jan  5 09:47:59.527: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan  5 09:47:59.527: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan  5 09:47:59.527: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan  5 09:47:59.527: INFO: Checking APIGroup: authorization.k8s.io
Jan  5 09:47:59.531: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan  5 09:47:59.531: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan  5 09:47:59.531: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan  5 09:47:59.531: INFO: Checking APIGroup: autoscaling
Jan  5 09:47:59.533: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan  5 09:47:59.533: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jan  5 09:47:59.533: INFO: autoscaling/v2 matches autoscaling/v2
Jan  5 09:47:59.533: INFO: Checking APIGroup: batch
Jan  5 09:47:59.536: INFO: PreferredVersion.GroupVersion: batch/v1
Jan  5 09:47:59.536: INFO: Versions found [{batch/v1 v1}]
Jan  5 09:47:59.536: INFO: batch/v1 matches batch/v1
Jan  5 09:47:59.536: INFO: Checking APIGroup: certificates.k8s.io
Jan  5 09:47:59.539: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan  5 09:47:59.539: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan  5 09:47:59.539: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan  5 09:47:59.540: INFO: Checking APIGroup: networking.k8s.io
Jan  5 09:47:59.543: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan  5 09:47:59.543: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan  5 09:47:59.543: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan  5 09:47:59.543: INFO: Checking APIGroup: policy
Jan  5 09:47:59.547: INFO: PreferredVersion.GroupVersion: policy/v1
Jan  5 09:47:59.547: INFO: Versions found [{policy/v1 v1}]
Jan  5 09:47:59.547: INFO: policy/v1 matches policy/v1
Jan  5 09:47:59.547: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan  5 09:47:59.551: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan  5 09:47:59.551: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan  5 09:47:59.551: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan  5 09:47:59.551: INFO: Checking APIGroup: storage.k8s.io
Jan  5 09:47:59.556: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan  5 09:47:59.556: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan  5 09:47:59.556: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan  5 09:47:59.556: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan  5 09:47:59.561: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan  5 09:47:59.561: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan  5 09:47:59.561: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan  5 09:47:59.561: INFO: Checking APIGroup: apiextensions.k8s.io
Jan  5 09:47:59.565: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan  5 09:47:59.565: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan  5 09:47:59.565: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan  5 09:47:59.565: INFO: Checking APIGroup: scheduling.k8s.io
Jan  5 09:47:59.568: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan  5 09:47:59.568: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan  5 09:47:59.568: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan  5 09:47:59.568: INFO: Checking APIGroup: coordination.k8s.io
Jan  5 09:47:59.572: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan  5 09:47:59.572: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan  5 09:47:59.572: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan  5 09:47:59.572: INFO: Checking APIGroup: node.k8s.io
Jan  5 09:47:59.576: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan  5 09:47:59.576: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan  5 09:47:59.576: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan  5 09:47:59.576: INFO: Checking APIGroup: discovery.k8s.io
Jan  5 09:47:59.579: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan  5 09:47:59.579: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan  5 09:47:59.579: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan  5 09:47:59.579: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan  5 09:47:59.583: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan  5 09:47:59.583: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan  5 09:47:59.583: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan  5 09:47:59.583: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan  5 09:47:59.586: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan  5 09:47:59.586: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jan  5 09:47:59.586: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan  5 09:47:59.586: INFO: Checking APIGroup: cilium.io
Jan  5 09:47:59.589: INFO: PreferredVersion.GroupVersion: cilium.io/v2
Jan  5 09:47:59.589: INFO: Versions found [{cilium.io/v2 v2}]
Jan  5 09:47:59.589: INFO: cilium.io/v2 matches cilium.io/v2
Jan  5 09:47:59.589: INFO: Checking APIGroup: metrics.k8s.io
Jan  5 09:47:59.594: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan  5 09:47:59.594: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan  5 09:47:59.594: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jan  5 09:47:59.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1915" for this suite. 01/05/23 09:47:59.605
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":178,"skipped":3154,"failed":0}
------------------------------
â€¢ [0.578 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:59.036
    Jan  5 09:47:59.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename discovery 01/05/23 09:47:59.037
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:59.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:59.056
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/05/23 09:47:59.065
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan  5 09:47:59.514: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan  5 09:47:59.517: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan  5 09:47:59.518: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan  5 09:47:59.518: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan  5 09:47:59.518: INFO: Checking APIGroup: apps
    Jan  5 09:47:59.521: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan  5 09:47:59.521: INFO: Versions found [{apps/v1 v1}]
    Jan  5 09:47:59.521: INFO: apps/v1 matches apps/v1
    Jan  5 09:47:59.521: INFO: Checking APIGroup: events.k8s.io
    Jan  5 09:47:59.524: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan  5 09:47:59.524: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan  5 09:47:59.524: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan  5 09:47:59.524: INFO: Checking APIGroup: authentication.k8s.io
    Jan  5 09:47:59.527: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan  5 09:47:59.527: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan  5 09:47:59.527: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan  5 09:47:59.527: INFO: Checking APIGroup: authorization.k8s.io
    Jan  5 09:47:59.531: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan  5 09:47:59.531: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan  5 09:47:59.531: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan  5 09:47:59.531: INFO: Checking APIGroup: autoscaling
    Jan  5 09:47:59.533: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan  5 09:47:59.533: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jan  5 09:47:59.533: INFO: autoscaling/v2 matches autoscaling/v2
    Jan  5 09:47:59.533: INFO: Checking APIGroup: batch
    Jan  5 09:47:59.536: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan  5 09:47:59.536: INFO: Versions found [{batch/v1 v1}]
    Jan  5 09:47:59.536: INFO: batch/v1 matches batch/v1
    Jan  5 09:47:59.536: INFO: Checking APIGroup: certificates.k8s.io
    Jan  5 09:47:59.539: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan  5 09:47:59.539: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan  5 09:47:59.539: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan  5 09:47:59.540: INFO: Checking APIGroup: networking.k8s.io
    Jan  5 09:47:59.543: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan  5 09:47:59.543: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan  5 09:47:59.543: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan  5 09:47:59.543: INFO: Checking APIGroup: policy
    Jan  5 09:47:59.547: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan  5 09:47:59.547: INFO: Versions found [{policy/v1 v1}]
    Jan  5 09:47:59.547: INFO: policy/v1 matches policy/v1
    Jan  5 09:47:59.547: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan  5 09:47:59.551: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan  5 09:47:59.551: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan  5 09:47:59.551: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan  5 09:47:59.551: INFO: Checking APIGroup: storage.k8s.io
    Jan  5 09:47:59.556: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan  5 09:47:59.556: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan  5 09:47:59.556: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan  5 09:47:59.556: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan  5 09:47:59.561: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan  5 09:47:59.561: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan  5 09:47:59.561: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan  5 09:47:59.561: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan  5 09:47:59.565: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan  5 09:47:59.565: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan  5 09:47:59.565: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan  5 09:47:59.565: INFO: Checking APIGroup: scheduling.k8s.io
    Jan  5 09:47:59.568: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan  5 09:47:59.568: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan  5 09:47:59.568: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan  5 09:47:59.568: INFO: Checking APIGroup: coordination.k8s.io
    Jan  5 09:47:59.572: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan  5 09:47:59.572: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan  5 09:47:59.572: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan  5 09:47:59.572: INFO: Checking APIGroup: node.k8s.io
    Jan  5 09:47:59.576: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan  5 09:47:59.576: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan  5 09:47:59.576: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan  5 09:47:59.576: INFO: Checking APIGroup: discovery.k8s.io
    Jan  5 09:47:59.579: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan  5 09:47:59.579: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan  5 09:47:59.579: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan  5 09:47:59.579: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan  5 09:47:59.583: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jan  5 09:47:59.583: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jan  5 09:47:59.583: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jan  5 09:47:59.583: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jan  5 09:47:59.586: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jan  5 09:47:59.586: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Jan  5 09:47:59.586: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jan  5 09:47:59.586: INFO: Checking APIGroup: cilium.io
    Jan  5 09:47:59.589: INFO: PreferredVersion.GroupVersion: cilium.io/v2
    Jan  5 09:47:59.589: INFO: Versions found [{cilium.io/v2 v2}]
    Jan  5 09:47:59.589: INFO: cilium.io/v2 matches cilium.io/v2
    Jan  5 09:47:59.589: INFO: Checking APIGroup: metrics.k8s.io
    Jan  5 09:47:59.594: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan  5 09:47:59.594: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan  5 09:47:59.594: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jan  5 09:47:59.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-1915" for this suite. 01/05/23 09:47:59.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:47:59.618
Jan  5 09:47:59.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 09:47:59.618
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:59.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:59.648
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-c0341842-8e0a-4384-9af6-459deedc6ce5 01/05/23 09:47:59.653
STEP: Creating a pod to test consume secrets 01/05/23 09:47:59.659
Jan  5 09:47:59.671: INFO: Waiting up to 5m0s for pod "pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442" in namespace "secrets-442" to be "Succeeded or Failed"
Jan  5 09:47:59.693: INFO: Pod "pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442": Phase="Pending", Reason="", readiness=false. Elapsed: 21.821037ms
Jan  5 09:48:01.700: INFO: Pod "pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442": Phase="Running", Reason="", readiness=false. Elapsed: 2.029128485s
Jan  5 09:48:03.701: INFO: Pod "pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029703418s
STEP: Saw pod success 01/05/23 09:48:03.701
Jan  5 09:48:03.701: INFO: Pod "pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442" satisfied condition "Succeeded or Failed"
Jan  5 09:48:03.707: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 09:48:03.724
Jan  5 09:48:03.739: INFO: Waiting for pod pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442 to disappear
Jan  5 09:48:03.745: INFO: Pod pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 09:48:03.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-442" for this suite. 01/05/23 09:48:03.757
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":179,"skipped":3180,"failed":0}
------------------------------
â€¢ [4.148 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:47:59.618
    Jan  5 09:47:59.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 09:47:59.618
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:47:59.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:47:59.648
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-c0341842-8e0a-4384-9af6-459deedc6ce5 01/05/23 09:47:59.653
    STEP: Creating a pod to test consume secrets 01/05/23 09:47:59.659
    Jan  5 09:47:59.671: INFO: Waiting up to 5m0s for pod "pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442" in namespace "secrets-442" to be "Succeeded or Failed"
    Jan  5 09:47:59.693: INFO: Pod "pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442": Phase="Pending", Reason="", readiness=false. Elapsed: 21.821037ms
    Jan  5 09:48:01.700: INFO: Pod "pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442": Phase="Running", Reason="", readiness=false. Elapsed: 2.029128485s
    Jan  5 09:48:03.701: INFO: Pod "pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029703418s
    STEP: Saw pod success 01/05/23 09:48:03.701
    Jan  5 09:48:03.701: INFO: Pod "pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442" satisfied condition "Succeeded or Failed"
    Jan  5 09:48:03.707: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 09:48:03.724
    Jan  5 09:48:03.739: INFO: Waiting for pod pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442 to disappear
    Jan  5 09:48:03.745: INFO: Pod pod-secrets-da747a69-d696-4d88-af5d-95f0513fc442 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 09:48:03.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-442" for this suite. 01/05/23 09:48:03.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:48:03.768
Jan  5 09:48:03.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename sched-pred 01/05/23 09:48:03.769
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:03.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:03.793
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan  5 09:48:03.803: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  5 09:48:03.825: INFO: Waiting for terminating namespaces to be deleted...
Jan  5 09:48:03.830: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t before test
Jan  5 09:48:03.859: INFO: apiserver-proxy-m4wgr from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.859: INFO: 	Container proxy ready: true, restart count 0
Jan  5 09:48:03.859: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 09:48:03.859: INFO: cilium-5qvr2 from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.859: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 09:48:03.859: INFO: csi-driver-node-nt6bm from kube-system started at 2023-01-05 08:52:50 +0000 UTC (3 container statuses recorded)
Jan  5 09:48:03.859: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 09:48:03.859: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 09:48:03.859: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 09:48:03.859: INFO: kube-proxy-worker-omyby-v1.25.5-xvlqq from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.859: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 09:48:03.859: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 09:48:03.859: INFO: node-exporter-hckzs from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.859: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 09:48:03.859: INFO: node-problem-detector-5ln6w from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.859: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 09:48:03.859: INFO: sonobuoy-e2e-job-2d2d23fb3e814b3d from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.859: INFO: 	Container e2e ready: true, restart count 0
Jan  5 09:48:03.860: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 09:48:03.860: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-m6xzl from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.860: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 09:48:03.860: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 09:48:03.860: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r before test
Jan  5 09:48:03.906: INFO: apiserver-proxy-4w6xb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.907: INFO: 	Container proxy ready: true, restart count 0
Jan  5 09:48:03.907: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 09:48:03.907: INFO: cilium-cjv9l from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.907: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 09:48:03.907: INFO: cilium-operator-6bf67c77c6-r9hbq from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.907: INFO: 	Container cilium-operator ready: true, restart count 0
Jan  5 09:48:03.907: INFO: csi-driver-node-r2l8x from kube-system started at 2023-01-05 08:52:30 +0000 UTC (3 container statuses recorded)
Jan  5 09:48:03.907: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 09:48:03.907: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 09:48:03.907: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 09:48:03.907: INFO: kube-proxy-worker-omyby-v1.25.5-fz64v from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.907: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 09:48:03.907: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 09:48:03.907: INFO: node-exporter-7hcrb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.907: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 09:48:03.907: INFO: node-problem-detector-l85nm from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.907: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 09:48:03.907: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-xghsj from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.907: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 09:48:03.907: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 09:48:03.907: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 before test
Jan  5 09:48:03.937: INFO: apiserver-proxy-fnbwn from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container proxy ready: true, restart count 0
Jan  5 09:48:03.937: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 09:48:03.937: INFO: blackbox-exporter-c866d5696-wkcmf from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan  5 09:48:03.937: INFO: blackbox-exporter-c866d5696-z5dx5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan  5 09:48:03.937: INFO: cilium-operator-6bf67c77c6-mmgs4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container cilium-operator ready: true, restart count 0
Jan  5 09:48:03.937: INFO: cilium-x98hw from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 09:48:03.937: INFO: coredns-7594945774-g79vz from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container coredns ready: true, restart count 0
Jan  5 09:48:03.937: INFO: coredns-7594945774-wghlc from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container coredns ready: true, restart count 0
Jan  5 09:48:03.937: INFO: csi-driver-node-wcklx from kube-system started at 2023-01-05 08:52:24 +0000 UTC (3 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 09:48:03.937: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 09:48:03.937: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 09:48:03.937: INFO: kube-proxy-worker-vot5k-v1.25.5-86pmg from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 09:48:03.937: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 09:48:03.937: INFO: metrics-server-b58b76d9c-jzq89 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container metrics-server ready: true, restart count 0
Jan  5 09:48:03.937: INFO: metrics-server-b58b76d9c-kwfd5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container metrics-server ready: true, restart count 0
Jan  5 09:48:03.937: INFO: node-exporter-xlnb4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 09:48:03.937: INFO: node-problem-detector-bzpk7 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 09:48:03.937: INFO: vpn-shoot-69957f7fdd-rmrkr from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container vpn-shoot ready: true, restart count 0
Jan  5 09:48:03.937: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-mw7c6 from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.937: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 09:48:03.937: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 09:48:03.937: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv before test
Jan  5 09:48:03.961: INFO: apiserver-proxy-4j65b from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.961: INFO: 	Container proxy ready: true, restart count 0
Jan  5 09:48:03.961: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 09:48:03.961: INFO: cilium-57wqw from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.961: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 09:48:03.961: INFO: csi-driver-node-rj5cq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (3 container statuses recorded)
Jan  5 09:48:03.961: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 09:48:03.961: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 09:48:03.961: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 09:48:03.961: INFO: kube-proxy-worker-vot5k-v1.25.5-pg2pl from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.961: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 09:48:03.961: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 09:48:03.961: INFO: node-exporter-25qsq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.961: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 09:48:03.961: INFO: node-problem-detector-7xm42 from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.961: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 09:48:03.961: INFO: pod-qos-class-90d9752b-8885-48d7-877d-0a436ced1da0 from pods-3591 started at 2023-01-05 09:47:25 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.961: INFO: 	Container agnhost ready: false, restart count 0
Jan  5 09:48:03.961: INFO: sonobuoy from sonobuoy started at 2023-01-05 09:06:38 +0000 UTC (1 container statuses recorded)
Jan  5 09:48:03.961: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  5 09:48:03.961: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-qvcts from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 09:48:03.961: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 09:48:03.961: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/05/23 09:48:03.961
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173760ad4b88552c], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling.] 01/05/23 09:48:04.016
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:48:05.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9960" for this suite. 01/05/23 09:48:05.029
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":180,"skipped":3258,"failed":0}
------------------------------
â€¢ [1.269 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:48:03.768
    Jan  5 09:48:03.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename sched-pred 01/05/23 09:48:03.769
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:03.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:03.793
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan  5 09:48:03.803: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  5 09:48:03.825: INFO: Waiting for terminating namespaces to be deleted...
    Jan  5 09:48:03.830: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t before test
    Jan  5 09:48:03.859: INFO: apiserver-proxy-m4wgr from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.859: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 09:48:03.859: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 09:48:03.859: INFO: cilium-5qvr2 from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.859: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 09:48:03.859: INFO: csi-driver-node-nt6bm from kube-system started at 2023-01-05 08:52:50 +0000 UTC (3 container statuses recorded)
    Jan  5 09:48:03.859: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 09:48:03.859: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 09:48:03.859: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 09:48:03.859: INFO: kube-proxy-worker-omyby-v1.25.5-xvlqq from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.859: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 09:48:03.859: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 09:48:03.859: INFO: node-exporter-hckzs from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.859: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 09:48:03.859: INFO: node-problem-detector-5ln6w from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.859: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 09:48:03.859: INFO: sonobuoy-e2e-job-2d2d23fb3e814b3d from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.859: INFO: 	Container e2e ready: true, restart count 0
    Jan  5 09:48:03.860: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 09:48:03.860: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-m6xzl from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.860: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 09:48:03.860: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 09:48:03.860: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r before test
    Jan  5 09:48:03.906: INFO: apiserver-proxy-4w6xb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.907: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: cilium-cjv9l from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.907: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: cilium-operator-6bf67c77c6-r9hbq from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.907: INFO: 	Container cilium-operator ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: csi-driver-node-r2l8x from kube-system started at 2023-01-05 08:52:30 +0000 UTC (3 container statuses recorded)
    Jan  5 09:48:03.907: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: kube-proxy-worker-omyby-v1.25.5-fz64v from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.907: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: node-exporter-7hcrb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.907: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: node-problem-detector-l85nm from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.907: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-xghsj from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.907: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 09:48:03.907: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 before test
    Jan  5 09:48:03.937: INFO: apiserver-proxy-fnbwn from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: blackbox-exporter-c866d5696-wkcmf from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: blackbox-exporter-c866d5696-z5dx5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: cilium-operator-6bf67c77c6-mmgs4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container cilium-operator ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: cilium-x98hw from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: coredns-7594945774-g79vz from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container coredns ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: coredns-7594945774-wghlc from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container coredns ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: csi-driver-node-wcklx from kube-system started at 2023-01-05 08:52:24 +0000 UTC (3 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: kube-proxy-worker-vot5k-v1.25.5-86pmg from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: metrics-server-b58b76d9c-jzq89 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: metrics-server-b58b76d9c-kwfd5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: node-exporter-xlnb4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: node-problem-detector-bzpk7 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: vpn-shoot-69957f7fdd-rmrkr from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container vpn-shoot ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-mw7c6 from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.937: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 09:48:03.937: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv before test
    Jan  5 09:48:03.961: INFO: apiserver-proxy-4j65b from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.961: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 09:48:03.961: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 09:48:03.961: INFO: cilium-57wqw from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.961: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 09:48:03.961: INFO: csi-driver-node-rj5cq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (3 container statuses recorded)
    Jan  5 09:48:03.961: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 09:48:03.961: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 09:48:03.961: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 09:48:03.961: INFO: kube-proxy-worker-vot5k-v1.25.5-pg2pl from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.961: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 09:48:03.961: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 09:48:03.961: INFO: node-exporter-25qsq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.961: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 09:48:03.961: INFO: node-problem-detector-7xm42 from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.961: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 09:48:03.961: INFO: pod-qos-class-90d9752b-8885-48d7-877d-0a436ced1da0 from pods-3591 started at 2023-01-05 09:47:25 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.961: INFO: 	Container agnhost ready: false, restart count 0
    Jan  5 09:48:03.961: INFO: sonobuoy from sonobuoy started at 2023-01-05 09:06:38 +0000 UTC (1 container statuses recorded)
    Jan  5 09:48:03.961: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  5 09:48:03.961: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-qvcts from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 09:48:03.961: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 09:48:03.961: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/05/23 09:48:03.961
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173760ad4b88552c], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling.] 01/05/23 09:48:04.016
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:48:05.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9960" for this suite. 01/05/23 09:48:05.029
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:48:05.038
Jan  5 09:48:05.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename svcaccounts 01/05/23 09:48:05.039
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:05.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:05.068
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 01/05/23 09:48:05.074
STEP: watching for the ServiceAccount to be added 01/05/23 09:48:05.084
STEP: patching the ServiceAccount 01/05/23 09:48:05.088
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/05/23 09:48:05.094
STEP: deleting the ServiceAccount 01/05/23 09:48:05.102
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  5 09:48:05.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9032" for this suite. 01/05/23 09:48:05.133
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":181,"skipped":3278,"failed":0}
------------------------------
â€¢ [0.102 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:48:05.038
    Jan  5 09:48:05.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 09:48:05.039
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:05.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:05.068
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 01/05/23 09:48:05.074
    STEP: watching for the ServiceAccount to be added 01/05/23 09:48:05.084
    STEP: patching the ServiceAccount 01/05/23 09:48:05.088
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/05/23 09:48:05.094
    STEP: deleting the ServiceAccount 01/05/23 09:48:05.102
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  5 09:48:05.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9032" for this suite. 01/05/23 09:48:05.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:48:05.142
Jan  5 09:48:05.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename security-context-test 01/05/23 09:48:05.143
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:05.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:05.163
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jan  5 09:48:05.181: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-87549fb1-dc89-454b-9e9b-e25b49e69eb6" in namespace "security-context-test-2275" to be "Succeeded or Failed"
Jan  5 09:48:05.186: INFO: Pod "alpine-nnp-false-87549fb1-dc89-454b-9e9b-e25b49e69eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.378072ms
Jan  5 09:48:07.195: INFO: Pod "alpine-nnp-false-87549fb1-dc89-454b-9e9b-e25b49e69eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014383756s
Jan  5 09:48:09.193: INFO: Pod "alpine-nnp-false-87549fb1-dc89-454b-9e9b-e25b49e69eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011941885s
Jan  5 09:48:11.194: INFO: Pod "alpine-nnp-false-87549fb1-dc89-454b-9e9b-e25b49e69eb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013005808s
Jan  5 09:48:11.194: INFO: Pod "alpine-nnp-false-87549fb1-dc89-454b-9e9b-e25b49e69eb6" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  5 09:48:11.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2275" for this suite. 01/05/23 09:48:11.221
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":182,"skipped":3283,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.086 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:48:05.142
    Jan  5 09:48:05.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename security-context-test 01/05/23 09:48:05.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:05.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:05.163
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jan  5 09:48:05.181: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-87549fb1-dc89-454b-9e9b-e25b49e69eb6" in namespace "security-context-test-2275" to be "Succeeded or Failed"
    Jan  5 09:48:05.186: INFO: Pod "alpine-nnp-false-87549fb1-dc89-454b-9e9b-e25b49e69eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.378072ms
    Jan  5 09:48:07.195: INFO: Pod "alpine-nnp-false-87549fb1-dc89-454b-9e9b-e25b49e69eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014383756s
    Jan  5 09:48:09.193: INFO: Pod "alpine-nnp-false-87549fb1-dc89-454b-9e9b-e25b49e69eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011941885s
    Jan  5 09:48:11.194: INFO: Pod "alpine-nnp-false-87549fb1-dc89-454b-9e9b-e25b49e69eb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013005808s
    Jan  5 09:48:11.194: INFO: Pod "alpine-nnp-false-87549fb1-dc89-454b-9e9b-e25b49e69eb6" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  5 09:48:11.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2275" for this suite. 01/05/23 09:48:11.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:48:11.228
Jan  5 09:48:11.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename cronjob 01/05/23 09:48:11.229
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:11.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:11.249
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/05/23 09:48:11.254
STEP: creating 01/05/23 09:48:11.255
STEP: getting 01/05/23 09:48:11.261
STEP: listing 01/05/23 09:48:11.268
STEP: watching 01/05/23 09:48:11.272
Jan  5 09:48:11.273: INFO: starting watch
STEP: cluster-wide listing 01/05/23 09:48:11.275
STEP: cluster-wide watching 01/05/23 09:48:11.28
Jan  5 09:48:11.280: INFO: starting watch
STEP: patching 01/05/23 09:48:11.283
STEP: updating 01/05/23 09:48:11.29
Jan  5 09:48:11.303: INFO: waiting for watch events with expected annotations
Jan  5 09:48:11.303: INFO: saw patched and updated annotations
STEP: patching /status 01/05/23 09:48:11.303
STEP: updating /status 01/05/23 09:48:11.311
STEP: get /status 01/05/23 09:48:11.324
STEP: deleting 01/05/23 09:48:11.329
STEP: deleting a collection 01/05/23 09:48:11.347
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  5 09:48:11.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-322" for this suite. 01/05/23 09:48:11.37
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":183,"skipped":3289,"failed":0}
------------------------------
â€¢ [0.149 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:48:11.228
    Jan  5 09:48:11.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename cronjob 01/05/23 09:48:11.229
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:11.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:11.249
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/05/23 09:48:11.254
    STEP: creating 01/05/23 09:48:11.255
    STEP: getting 01/05/23 09:48:11.261
    STEP: listing 01/05/23 09:48:11.268
    STEP: watching 01/05/23 09:48:11.272
    Jan  5 09:48:11.273: INFO: starting watch
    STEP: cluster-wide listing 01/05/23 09:48:11.275
    STEP: cluster-wide watching 01/05/23 09:48:11.28
    Jan  5 09:48:11.280: INFO: starting watch
    STEP: patching 01/05/23 09:48:11.283
    STEP: updating 01/05/23 09:48:11.29
    Jan  5 09:48:11.303: INFO: waiting for watch events with expected annotations
    Jan  5 09:48:11.303: INFO: saw patched and updated annotations
    STEP: patching /status 01/05/23 09:48:11.303
    STEP: updating /status 01/05/23 09:48:11.311
    STEP: get /status 01/05/23 09:48:11.324
    STEP: deleting 01/05/23 09:48:11.329
    STEP: deleting a collection 01/05/23 09:48:11.347
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  5 09:48:11.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-322" for this suite. 01/05/23 09:48:11.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:48:11.379
Jan  5 09:48:11.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename containers 01/05/23 09:48:11.38
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:11.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:11.401
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 01/05/23 09:48:11.408
Jan  5 09:48:11.425: INFO: Waiting up to 5m0s for pod "client-containers-0904b82f-655b-4209-a751-6a97db9f260b" in namespace "containers-6274" to be "Succeeded or Failed"
Jan  5 09:48:11.432: INFO: Pod "client-containers-0904b82f-655b-4209-a751-6a97db9f260b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.200676ms
Jan  5 09:48:13.441: INFO: Pod "client-containers-0904b82f-655b-4209-a751-6a97db9f260b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015250649s
Jan  5 09:48:15.441: INFO: Pod "client-containers-0904b82f-655b-4209-a751-6a97db9f260b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015413767s
STEP: Saw pod success 01/05/23 09:48:15.441
Jan  5 09:48:15.441: INFO: Pod "client-containers-0904b82f-655b-4209-a751-6a97db9f260b" satisfied condition "Succeeded or Failed"
Jan  5 09:48:15.446: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod client-containers-0904b82f-655b-4209-a751-6a97db9f260b container agnhost-container: <nil>
STEP: delete the pod 01/05/23 09:48:15.47
Jan  5 09:48:15.483: INFO: Waiting for pod client-containers-0904b82f-655b-4209-a751-6a97db9f260b to disappear
Jan  5 09:48:15.488: INFO: Pod client-containers-0904b82f-655b-4209-a751-6a97db9f260b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan  5 09:48:15.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6274" for this suite. 01/05/23 09:48:15.5
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":184,"skipped":3322,"failed":0}
------------------------------
â€¢ [4.128 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:48:11.379
    Jan  5 09:48:11.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename containers 01/05/23 09:48:11.38
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:11.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:11.401
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 01/05/23 09:48:11.408
    Jan  5 09:48:11.425: INFO: Waiting up to 5m0s for pod "client-containers-0904b82f-655b-4209-a751-6a97db9f260b" in namespace "containers-6274" to be "Succeeded or Failed"
    Jan  5 09:48:11.432: INFO: Pod "client-containers-0904b82f-655b-4209-a751-6a97db9f260b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.200676ms
    Jan  5 09:48:13.441: INFO: Pod "client-containers-0904b82f-655b-4209-a751-6a97db9f260b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015250649s
    Jan  5 09:48:15.441: INFO: Pod "client-containers-0904b82f-655b-4209-a751-6a97db9f260b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015413767s
    STEP: Saw pod success 01/05/23 09:48:15.441
    Jan  5 09:48:15.441: INFO: Pod "client-containers-0904b82f-655b-4209-a751-6a97db9f260b" satisfied condition "Succeeded or Failed"
    Jan  5 09:48:15.446: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod client-containers-0904b82f-655b-4209-a751-6a97db9f260b container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 09:48:15.47
    Jan  5 09:48:15.483: INFO: Waiting for pod client-containers-0904b82f-655b-4209-a751-6a97db9f260b to disappear
    Jan  5 09:48:15.488: INFO: Pod client-containers-0904b82f-655b-4209-a751-6a97db9f260b no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan  5 09:48:15.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-6274" for this suite. 01/05/23 09:48:15.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:48:15.509
Jan  5 09:48:15.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 09:48:15.51
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:15.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:15.533
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 01/05/23 09:48:15.544
Jan  5 09:48:15.558: INFO: Waiting up to 5m0s for pod "pod-80aa938b-576a-491e-a37b-8300c4cf9a8a" in namespace "emptydir-4408" to be "Succeeded or Failed"
Jan  5 09:48:15.565: INFO: Pod "pod-80aa938b-576a-491e-a37b-8300c4cf9a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.909472ms
Jan  5 09:48:17.572: INFO: Pod "pod-80aa938b-576a-491e-a37b-8300c4cf9a8a": Phase="Running", Reason="", readiness=false. Elapsed: 2.013541565s
Jan  5 09:48:19.572: INFO: Pod "pod-80aa938b-576a-491e-a37b-8300c4cf9a8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013638821s
STEP: Saw pod success 01/05/23 09:48:19.572
Jan  5 09:48:19.572: INFO: Pod "pod-80aa938b-576a-491e-a37b-8300c4cf9a8a" satisfied condition "Succeeded or Failed"
Jan  5 09:48:19.579: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-80aa938b-576a-491e-a37b-8300c4cf9a8a container test-container: <nil>
STEP: delete the pod 01/05/23 09:48:19.593
Jan  5 09:48:19.604: INFO: Waiting for pod pod-80aa938b-576a-491e-a37b-8300c4cf9a8a to disappear
Jan  5 09:48:19.609: INFO: Pod pod-80aa938b-576a-491e-a37b-8300c4cf9a8a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 09:48:19.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4408" for this suite. 01/05/23 09:48:19.622
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":185,"skipped":3385,"failed":0}
------------------------------
â€¢ [4.122 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:48:15.509
    Jan  5 09:48:15.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 09:48:15.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:15.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:15.533
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/05/23 09:48:15.544
    Jan  5 09:48:15.558: INFO: Waiting up to 5m0s for pod "pod-80aa938b-576a-491e-a37b-8300c4cf9a8a" in namespace "emptydir-4408" to be "Succeeded or Failed"
    Jan  5 09:48:15.565: INFO: Pod "pod-80aa938b-576a-491e-a37b-8300c4cf9a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.909472ms
    Jan  5 09:48:17.572: INFO: Pod "pod-80aa938b-576a-491e-a37b-8300c4cf9a8a": Phase="Running", Reason="", readiness=false. Elapsed: 2.013541565s
    Jan  5 09:48:19.572: INFO: Pod "pod-80aa938b-576a-491e-a37b-8300c4cf9a8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013638821s
    STEP: Saw pod success 01/05/23 09:48:19.572
    Jan  5 09:48:19.572: INFO: Pod "pod-80aa938b-576a-491e-a37b-8300c4cf9a8a" satisfied condition "Succeeded or Failed"
    Jan  5 09:48:19.579: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-80aa938b-576a-491e-a37b-8300c4cf9a8a container test-container: <nil>
    STEP: delete the pod 01/05/23 09:48:19.593
    Jan  5 09:48:19.604: INFO: Waiting for pod pod-80aa938b-576a-491e-a37b-8300c4cf9a8a to disappear
    Jan  5 09:48:19.609: INFO: Pod pod-80aa938b-576a-491e-a37b-8300c4cf9a8a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 09:48:19.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4408" for this suite. 01/05/23 09:48:19.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:48:19.632
Jan  5 09:48:19.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename events 01/05/23 09:48:19.632
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:19.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:19.655
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/05/23 09:48:19.661
Jan  5 09:48:19.666: INFO: created test-event-1
Jan  5 09:48:19.671: INFO: created test-event-2
Jan  5 09:48:19.676: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/05/23 09:48:19.676
STEP: delete collection of events 01/05/23 09:48:19.68
Jan  5 09:48:19.680: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/05/23 09:48:19.696
Jan  5 09:48:19.696: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan  5 09:48:19.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4353" for this suite. 01/05/23 09:48:19.712
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":186,"skipped":3392,"failed":0}
------------------------------
â€¢ [0.088 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:48:19.632
    Jan  5 09:48:19.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename events 01/05/23 09:48:19.632
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:19.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:19.655
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/05/23 09:48:19.661
    Jan  5 09:48:19.666: INFO: created test-event-1
    Jan  5 09:48:19.671: INFO: created test-event-2
    Jan  5 09:48:19.676: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/05/23 09:48:19.676
    STEP: delete collection of events 01/05/23 09:48:19.68
    Jan  5 09:48:19.680: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/05/23 09:48:19.696
    Jan  5 09:48:19.696: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan  5 09:48:19.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4353" for this suite. 01/05/23 09:48:19.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:48:19.72
Jan  5 09:48:19.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename statefulset 01/05/23 09:48:19.721
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:19.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:19.747
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4007 01/05/23 09:48:19.755
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 01/05/23 09:48:19.762
Jan  5 09:48:19.774: INFO: Found 0 stateful pods, waiting for 3
Jan  5 09:48:29.784: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 09:48:29.784: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 09:48:29.784: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 09:48:29.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4007 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 09:48:30.276: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 09:48:30.276: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 09:48:30.276: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/05/23 09:48:40.305
Jan  5 09:48:40.329: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/05/23 09:48:40.329
STEP: Updating Pods in reverse ordinal order 01/05/23 09:48:50.358
Jan  5 09:48:50.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4007 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:48:50.724: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 09:48:50.724: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 09:48:50.724: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 01/05/23 09:49:00.805
Jan  5 09:49:00.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4007 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 09:49:01.434: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 09:49:01.434: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 09:49:01.434: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 09:49:11.494: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/05/23 09:49:21.525
Jan  5 09:49:21.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4007 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 09:49:22.085: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 09:49:22.085: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 09:49:22.085: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 09:49:32.127: INFO: Deleting all statefulset in ns statefulset-4007
Jan  5 09:49:32.132: INFO: Scaling statefulset ss2 to 0
Jan  5 09:49:42.157: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 09:49:42.163: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 09:49:42.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4007" for this suite. 01/05/23 09:49:42.213
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":187,"skipped":3422,"failed":0}
------------------------------
â€¢ [SLOW TEST] [82.501 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:48:19.72
    Jan  5 09:48:19.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename statefulset 01/05/23 09:48:19.721
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:48:19.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:48:19.747
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4007 01/05/23 09:48:19.755
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 01/05/23 09:48:19.762
    Jan  5 09:48:19.774: INFO: Found 0 stateful pods, waiting for 3
    Jan  5 09:48:29.784: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 09:48:29.784: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 09:48:29.784: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 09:48:29.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4007 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 09:48:30.276: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 09:48:30.276: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 09:48:30.276: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/05/23 09:48:40.305
    Jan  5 09:48:40.329: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/05/23 09:48:40.329
    STEP: Updating Pods in reverse ordinal order 01/05/23 09:48:50.358
    Jan  5 09:48:50.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4007 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:48:50.724: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 09:48:50.724: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 09:48:50.724: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 01/05/23 09:49:00.805
    Jan  5 09:49:00.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4007 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 09:49:01.434: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 09:49:01.434: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 09:49:01.434: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 09:49:11.494: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/05/23 09:49:21.525
    Jan  5 09:49:21.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=statefulset-4007 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 09:49:22.085: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 09:49:22.085: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 09:49:22.085: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 09:49:32.127: INFO: Deleting all statefulset in ns statefulset-4007
    Jan  5 09:49:32.132: INFO: Scaling statefulset ss2 to 0
    Jan  5 09:49:42.157: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 09:49:42.163: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 09:49:42.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4007" for this suite. 01/05/23 09:49:42.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:49:42.222
Jan  5 09:49:42.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 09:49:42.223
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:49:42.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:49:42.248
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/05/23 09:49:42.255
Jan  5 09:49:42.269: INFO: Waiting up to 5m0s for pod "pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82" in namespace "emptydir-2567" to be "Succeeded or Failed"
Jan  5 09:49:42.281: INFO: Pod "pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82": Phase="Pending", Reason="", readiness=false. Elapsed: 11.713053ms
Jan  5 09:49:44.289: INFO: Pod "pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019321622s
Jan  5 09:49:46.289: INFO: Pod "pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019523374s
STEP: Saw pod success 01/05/23 09:49:46.289
Jan  5 09:49:46.289: INFO: Pod "pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82" satisfied condition "Succeeded or Failed"
Jan  5 09:49:46.295: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82 container test-container: <nil>
STEP: delete the pod 01/05/23 09:49:46.31
Jan  5 09:49:46.326: INFO: Waiting for pod pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82 to disappear
Jan  5 09:49:46.331: INFO: Pod pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 09:49:46.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2567" for this suite. 01/05/23 09:49:46.343
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":188,"skipped":3428,"failed":0}
------------------------------
â€¢ [4.129 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:49:42.222
    Jan  5 09:49:42.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 09:49:42.223
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:49:42.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:49:42.248
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/05/23 09:49:42.255
    Jan  5 09:49:42.269: INFO: Waiting up to 5m0s for pod "pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82" in namespace "emptydir-2567" to be "Succeeded or Failed"
    Jan  5 09:49:42.281: INFO: Pod "pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82": Phase="Pending", Reason="", readiness=false. Elapsed: 11.713053ms
    Jan  5 09:49:44.289: INFO: Pod "pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019321622s
    Jan  5 09:49:46.289: INFO: Pod "pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019523374s
    STEP: Saw pod success 01/05/23 09:49:46.289
    Jan  5 09:49:46.289: INFO: Pod "pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82" satisfied condition "Succeeded or Failed"
    Jan  5 09:49:46.295: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82 container test-container: <nil>
    STEP: delete the pod 01/05/23 09:49:46.31
    Jan  5 09:49:46.326: INFO: Waiting for pod pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82 to disappear
    Jan  5 09:49:46.331: INFO: Pod pod-3e0abf61-3912-4b81-b3a9-c4dd646b8e82 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 09:49:46.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2567" for this suite. 01/05/23 09:49:46.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:49:46.352
Jan  5 09:49:46.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename crd-webhook 01/05/23 09:49:46.353
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:49:46.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:49:46.372
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/05/23 09:49:46.379
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/05/23 09:49:46.947
STEP: Deploying the custom resource conversion webhook pod 01/05/23 09:49:46.955
STEP: Wait for the deployment to be ready 01/05/23 09:49:46.969
Jan  5 09:49:46.981: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 09:49:49.001
STEP: Verifying the service has paired with the endpoint 01/05/23 09:49:49.029
Jan  5 09:49:50.029: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan  5 09:49:50.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Creating a v1 custom resource 01/05/23 09:49:52.866
STEP: Create a v2 custom resource 01/05/23 09:49:52.887
STEP: List CRs in v1 01/05/23 09:49:52.959
STEP: List CRs in v2 01/05/23 09:49:53.013
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:49:53.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7418" for this suite. 01/05/23 09:49:53.562
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":189,"skipped":3440,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.264 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:49:46.352
    Jan  5 09:49:46.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename crd-webhook 01/05/23 09:49:46.353
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:49:46.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:49:46.372
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/05/23 09:49:46.379
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/05/23 09:49:46.947
    STEP: Deploying the custom resource conversion webhook pod 01/05/23 09:49:46.955
    STEP: Wait for the deployment to be ready 01/05/23 09:49:46.969
    Jan  5 09:49:46.981: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 09:49:49.001
    STEP: Verifying the service has paired with the endpoint 01/05/23 09:49:49.029
    Jan  5 09:49:50.029: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan  5 09:49:50.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Creating a v1 custom resource 01/05/23 09:49:52.866
    STEP: Create a v2 custom resource 01/05/23 09:49:52.887
    STEP: List CRs in v1 01/05/23 09:49:52.959
    STEP: List CRs in v2 01/05/23 09:49:53.013
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:49:53.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-7418" for this suite. 01/05/23 09:49:53.562
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:49:53.617
Jan  5 09:49:53.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename endpointslicemirroring 01/05/23 09:49:53.618
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:49:53.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:49:53.651
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/05/23 09:49:53.697
Jan  5 09:49:53.710: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/05/23 09:49:55.716
STEP: mirroring deletion of a custom Endpoint 01/05/23 09:49:55.729
Jan  5 09:49:55.743: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jan  5 09:49:57.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9603" for this suite. 01/05/23 09:49:57.761
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":190,"skipped":3449,"failed":0}
------------------------------
â€¢ [4.152 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:49:53.617
    Jan  5 09:49:53.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename endpointslicemirroring 01/05/23 09:49:53.618
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:49:53.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:49:53.651
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/05/23 09:49:53.697
    Jan  5 09:49:53.710: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/05/23 09:49:55.716
    STEP: mirroring deletion of a custom Endpoint 01/05/23 09:49:55.729
    Jan  5 09:49:55.743: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jan  5 09:49:57.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-9603" for this suite. 01/05/23 09:49:57.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:49:57.773
Jan  5 09:49:57.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename dns 01/05/23 09:49:57.773
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:49:57.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:49:57.797
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/05/23 09:49:57.803
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 230.159.124.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.124.159.230_udp@PTR;check="$$(dig +tcp +noall +answer +search 230.159.124.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.124.159.230_tcp@PTR;sleep 1; done
 01/05/23 09:49:57.842
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 230.159.124.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.124.159.230_udp@PTR;check="$$(dig +tcp +noall +answer +search 230.159.124.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.124.159.230_tcp@PTR;sleep 1; done
 01/05/23 09:49:57.842
STEP: creating a pod to probe DNS 01/05/23 09:49:57.842
STEP: submitting the pod to kubernetes 01/05/23 09:49:57.843
Jan  5 09:49:57.861: INFO: Waiting up to 15m0s for pod "dns-test-e8913770-a7ad-442a-a792-0e910bfaf420" in namespace "dns-3996" to be "running"
Jan  5 09:49:57.868: INFO: Pod "dns-test-e8913770-a7ad-442a-a792-0e910bfaf420": Phase="Pending", Reason="", readiness=false. Elapsed: 7.805596ms
Jan  5 09:49:59.877: INFO: Pod "dns-test-e8913770-a7ad-442a-a792-0e910bfaf420": Phase="Running", Reason="", readiness=true. Elapsed: 2.016235641s
Jan  5 09:49:59.877: INFO: Pod "dns-test-e8913770-a7ad-442a-a792-0e910bfaf420" satisfied condition "running"
STEP: retrieving the pod 01/05/23 09:49:59.877
STEP: looking for the results for each expected name from probers 01/05/23 09:49:59.883
Jan  5 09:49:59.997: INFO: Unable to read wheezy_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:00.042: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:00.054: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:00.067: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:00.146: INFO: Unable to read jessie_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:00.155: INFO: Unable to read jessie_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:00.164: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:00.176: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:00.218: INFO: Lookups using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 failed for: [wheezy_udp@dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_udp@dns-test-service.dns-3996.svc.cluster.local jessie_tcp@dns-test-service.dns-3996.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local]

Jan  5 09:50:05.234: INFO: Unable to read wheezy_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:05.281: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:05.292: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:05.302: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:05.361: INFO: Unable to read jessie_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:05.373: INFO: Unable to read jessie_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:05.386: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:05.401: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:05.454: INFO: Lookups using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 failed for: [wheezy_udp@dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_udp@dns-test-service.dns-3996.svc.cluster.local jessie_tcp@dns-test-service.dns-3996.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local]

Jan  5 09:50:10.255: INFO: Unable to read wheezy_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:10.300: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:10.309: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:10.319: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:10.370: INFO: Unable to read jessie_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:10.380: INFO: Unable to read jessie_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:10.388: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:10.398: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:10.443: INFO: Lookups using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 failed for: [wheezy_udp@dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_udp@dns-test-service.dns-3996.svc.cluster.local jessie_tcp@dns-test-service.dns-3996.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local]

Jan  5 09:50:15.236: INFO: Unable to read wheezy_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:15.280: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:15.291: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:15.301: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:15.362: INFO: Unable to read jessie_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:15.373: INFO: Unable to read jessie_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:15.384: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:15.396: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:15.445: INFO: Lookups using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 failed for: [wheezy_udp@dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_udp@dns-test-service.dns-3996.svc.cluster.local jessie_tcp@dns-test-service.dns-3996.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local]

Jan  5 09:50:20.229: INFO: Unable to read wheezy_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:20.277: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:20.289: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:20.300: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:20.351: INFO: Unable to read jessie_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:20.363: INFO: Unable to read jessie_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:20.374: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:20.384: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:20.429: INFO: Lookups using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 failed for: [wheezy_udp@dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_udp@dns-test-service.dns-3996.svc.cluster.local jessie_tcp@dns-test-service.dns-3996.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local]

Jan  5 09:50:25.231: INFO: Unable to read wheezy_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:25.276: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:25.289: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:25.298: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:25.352: INFO: Unable to read jessie_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:25.363: INFO: Unable to read jessie_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:25.373: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:25.384: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
Jan  5 09:50:25.425: INFO: Lookups using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 failed for: [wheezy_udp@dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_udp@dns-test-service.dns-3996.svc.cluster.local jessie_tcp@dns-test-service.dns-3996.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local]

Jan  5 09:50:30.461: INFO: DNS probes using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 succeeded

STEP: deleting the pod 01/05/23 09:50:30.461
STEP: deleting the test service 01/05/23 09:50:30.479
STEP: deleting the test headless service 01/05/23 09:50:30.518
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 09:50:30.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3996" for this suite. 01/05/23 09:50:30.554
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":191,"skipped":3485,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.789 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:49:57.773
    Jan  5 09:49:57.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename dns 01/05/23 09:49:57.773
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:49:57.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:49:57.797
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/05/23 09:49:57.803
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 230.159.124.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.124.159.230_udp@PTR;check="$$(dig +tcp +noall +answer +search 230.159.124.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.124.159.230_tcp@PTR;sleep 1; done
     01/05/23 09:49:57.842
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3996.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 230.159.124.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.124.159.230_udp@PTR;check="$$(dig +tcp +noall +answer +search 230.159.124.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.124.159.230_tcp@PTR;sleep 1; done
     01/05/23 09:49:57.842
    STEP: creating a pod to probe DNS 01/05/23 09:49:57.842
    STEP: submitting the pod to kubernetes 01/05/23 09:49:57.843
    Jan  5 09:49:57.861: INFO: Waiting up to 15m0s for pod "dns-test-e8913770-a7ad-442a-a792-0e910bfaf420" in namespace "dns-3996" to be "running"
    Jan  5 09:49:57.868: INFO: Pod "dns-test-e8913770-a7ad-442a-a792-0e910bfaf420": Phase="Pending", Reason="", readiness=false. Elapsed: 7.805596ms
    Jan  5 09:49:59.877: INFO: Pod "dns-test-e8913770-a7ad-442a-a792-0e910bfaf420": Phase="Running", Reason="", readiness=true. Elapsed: 2.016235641s
    Jan  5 09:49:59.877: INFO: Pod "dns-test-e8913770-a7ad-442a-a792-0e910bfaf420" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 09:49:59.877
    STEP: looking for the results for each expected name from probers 01/05/23 09:49:59.883
    Jan  5 09:49:59.997: INFO: Unable to read wheezy_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:00.042: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:00.054: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:00.067: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:00.146: INFO: Unable to read jessie_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:00.155: INFO: Unable to read jessie_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:00.164: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:00.176: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:00.218: INFO: Lookups using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 failed for: [wheezy_udp@dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_udp@dns-test-service.dns-3996.svc.cluster.local jessie_tcp@dns-test-service.dns-3996.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local]

    Jan  5 09:50:05.234: INFO: Unable to read wheezy_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:05.281: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:05.292: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:05.302: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:05.361: INFO: Unable to read jessie_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:05.373: INFO: Unable to read jessie_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:05.386: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:05.401: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:05.454: INFO: Lookups using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 failed for: [wheezy_udp@dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_udp@dns-test-service.dns-3996.svc.cluster.local jessie_tcp@dns-test-service.dns-3996.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local]

    Jan  5 09:50:10.255: INFO: Unable to read wheezy_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:10.300: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:10.309: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:10.319: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:10.370: INFO: Unable to read jessie_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:10.380: INFO: Unable to read jessie_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:10.388: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:10.398: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:10.443: INFO: Lookups using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 failed for: [wheezy_udp@dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_udp@dns-test-service.dns-3996.svc.cluster.local jessie_tcp@dns-test-service.dns-3996.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local]

    Jan  5 09:50:15.236: INFO: Unable to read wheezy_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:15.280: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:15.291: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:15.301: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:15.362: INFO: Unable to read jessie_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:15.373: INFO: Unable to read jessie_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:15.384: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:15.396: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:15.445: INFO: Lookups using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 failed for: [wheezy_udp@dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_udp@dns-test-service.dns-3996.svc.cluster.local jessie_tcp@dns-test-service.dns-3996.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local]

    Jan  5 09:50:20.229: INFO: Unable to read wheezy_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:20.277: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:20.289: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:20.300: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:20.351: INFO: Unable to read jessie_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:20.363: INFO: Unable to read jessie_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:20.374: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:20.384: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:20.429: INFO: Lookups using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 failed for: [wheezy_udp@dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_udp@dns-test-service.dns-3996.svc.cluster.local jessie_tcp@dns-test-service.dns-3996.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local]

    Jan  5 09:50:25.231: INFO: Unable to read wheezy_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:25.276: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:25.289: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:25.298: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:25.352: INFO: Unable to read jessie_udp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:25.363: INFO: Unable to read jessie_tcp@dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:25.373: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:25.384: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local from pod dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420: the server could not find the requested resource (get pods dns-test-e8913770-a7ad-442a-a792-0e910bfaf420)
    Jan  5 09:50:25.425: INFO: Lookups using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 failed for: [wheezy_udp@dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@dns-test-service.dns-3996.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_udp@dns-test-service.dns-3996.svc.cluster.local jessie_tcp@dns-test-service.dns-3996.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3996.svc.cluster.local]

    Jan  5 09:50:30.461: INFO: DNS probes using dns-3996/dns-test-e8913770-a7ad-442a-a792-0e910bfaf420 succeeded

    STEP: deleting the pod 01/05/23 09:50:30.461
    STEP: deleting the test service 01/05/23 09:50:30.479
    STEP: deleting the test headless service 01/05/23 09:50:30.518
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 09:50:30.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3996" for this suite. 01/05/23 09:50:30.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:50:30.563
Jan  5 09:50:30.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename var-expansion 01/05/23 09:50:30.564
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:50:30.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:50:30.587
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jan  5 09:50:30.606: INFO: Waiting up to 2m0s for pod "var-expansion-fa9a0161-e3a9-4401-91f8-276bed4889e7" in namespace "var-expansion-628" to be "container 0 failed with reason CreateContainerConfigError"
Jan  5 09:50:30.612: INFO: Pod "var-expansion-fa9a0161-e3a9-4401-91f8-276bed4889e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.668173ms
Jan  5 09:50:32.620: INFO: Pod "var-expansion-fa9a0161-e3a9-4401-91f8-276bed4889e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014296906s
Jan  5 09:50:32.620: INFO: Pod "var-expansion-fa9a0161-e3a9-4401-91f8-276bed4889e7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan  5 09:50:32.620: INFO: Deleting pod "var-expansion-fa9a0161-e3a9-4401-91f8-276bed4889e7" in namespace "var-expansion-628"
Jan  5 09:50:32.629: INFO: Wait up to 5m0s for pod "var-expansion-fa9a0161-e3a9-4401-91f8-276bed4889e7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 09:50:34.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-628" for this suite. 01/05/23 09:50:34.652
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":192,"skipped":3492,"failed":0}
------------------------------
â€¢ [4.096 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:50:30.563
    Jan  5 09:50:30.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename var-expansion 01/05/23 09:50:30.564
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:50:30.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:50:30.587
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jan  5 09:50:30.606: INFO: Waiting up to 2m0s for pod "var-expansion-fa9a0161-e3a9-4401-91f8-276bed4889e7" in namespace "var-expansion-628" to be "container 0 failed with reason CreateContainerConfigError"
    Jan  5 09:50:30.612: INFO: Pod "var-expansion-fa9a0161-e3a9-4401-91f8-276bed4889e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.668173ms
    Jan  5 09:50:32.620: INFO: Pod "var-expansion-fa9a0161-e3a9-4401-91f8-276bed4889e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014296906s
    Jan  5 09:50:32.620: INFO: Pod "var-expansion-fa9a0161-e3a9-4401-91f8-276bed4889e7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan  5 09:50:32.620: INFO: Deleting pod "var-expansion-fa9a0161-e3a9-4401-91f8-276bed4889e7" in namespace "var-expansion-628"
    Jan  5 09:50:32.629: INFO: Wait up to 5m0s for pod "var-expansion-fa9a0161-e3a9-4401-91f8-276bed4889e7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 09:50:34.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-628" for this suite. 01/05/23 09:50:34.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:50:34.66
Jan  5 09:50:34.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename aggregator 01/05/23 09:50:34.661
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:50:34.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:50:34.686
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan  5 09:50:34.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/05/23 09:50:34.694
Jan  5 09:50:35.360: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan  5 09:50:37.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:50:39.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:50:41.449: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:50:43.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:50:45.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:50:47.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:50:49.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:50:51.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:50:53.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:50:55.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:50:57.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 09:50:59.694: INFO: Waited 237.296121ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/05/23 09:51:00.244
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/05/23 09:51:00.25
STEP: List APIServices 01/05/23 09:51:00.259
Jan  5 09:51:00.276: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jan  5 09:51:00.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-127" for this suite. 01/05/23 09:51:00.599
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":193,"skipped":3500,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.946 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:50:34.66
    Jan  5 09:50:34.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename aggregator 01/05/23 09:50:34.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:50:34.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:50:34.686
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan  5 09:50:34.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/05/23 09:50:34.694
    Jan  5 09:50:35.360: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan  5 09:50:37.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:50:39.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:50:41.449: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:50:43.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:50:45.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:50:47.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:50:49.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:50:51.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:50:53.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:50:55.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:50:57.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 9, 50, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 09:50:59.694: INFO: Waited 237.296121ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/05/23 09:51:00.244
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/05/23 09:51:00.25
    STEP: List APIServices 01/05/23 09:51:00.259
    Jan  5 09:51:00.276: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jan  5 09:51:00.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-127" for this suite. 01/05/23 09:51:00.599
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:51:00.606
Jan  5 09:51:00.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 09:51:00.607
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:51:00.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:51:00.674
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/05/23 09:51:00.681
Jan  5 09:51:00.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/05/23 09:51:13.511
Jan  5 09:51:13.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 09:51:17.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:51:28.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6450" for this suite. 01/05/23 09:51:28.843
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":194,"skipped":3503,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.244 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:51:00.606
    Jan  5 09:51:00.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 09:51:00.607
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:51:00.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:51:00.674
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/05/23 09:51:00.681
    Jan  5 09:51:00.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/05/23 09:51:13.511
    Jan  5 09:51:13.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 09:51:17.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:51:28.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6450" for this suite. 01/05/23 09:51:28.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:51:28.851
Jan  5 09:51:28.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename containers 01/05/23 09:51:28.852
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:51:28.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:51:28.881
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 01/05/23 09:51:28.89
Jan  5 09:51:28.906: INFO: Waiting up to 5m0s for pod "client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a" in namespace "containers-9625" to be "Succeeded or Failed"
Jan  5 09:51:28.921: INFO: Pod "client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.475139ms
Jan  5 09:51:30.928: INFO: Pod "client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022012334s
Jan  5 09:51:32.928: INFO: Pod "client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022123733s
STEP: Saw pod success 01/05/23 09:51:32.928
Jan  5 09:51:32.928: INFO: Pod "client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a" satisfied condition "Succeeded or Failed"
Jan  5 09:51:32.932: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a container agnhost-container: <nil>
STEP: delete the pod 01/05/23 09:51:32.945
Jan  5 09:51:32.954: INFO: Waiting for pod client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a to disappear
Jan  5 09:51:32.958: INFO: Pod client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan  5 09:51:32.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9625" for this suite. 01/05/23 09:51:32.967
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":195,"skipped":3527,"failed":0}
------------------------------
â€¢ [4.122 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:51:28.851
    Jan  5 09:51:28.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename containers 01/05/23 09:51:28.852
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:51:28.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:51:28.881
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 01/05/23 09:51:28.89
    Jan  5 09:51:28.906: INFO: Waiting up to 5m0s for pod "client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a" in namespace "containers-9625" to be "Succeeded or Failed"
    Jan  5 09:51:28.921: INFO: Pod "client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.475139ms
    Jan  5 09:51:30.928: INFO: Pod "client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022012334s
    Jan  5 09:51:32.928: INFO: Pod "client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022123733s
    STEP: Saw pod success 01/05/23 09:51:32.928
    Jan  5 09:51:32.928: INFO: Pod "client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a" satisfied condition "Succeeded or Failed"
    Jan  5 09:51:32.932: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 09:51:32.945
    Jan  5 09:51:32.954: INFO: Waiting for pod client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a to disappear
    Jan  5 09:51:32.958: INFO: Pod client-containers-412dc201-9cca-4509-8db2-e5d93f9ec78a no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan  5 09:51:32.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9625" for this suite. 01/05/23 09:51:32.967
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:51:32.974
Jan  5 09:51:32.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 09:51:32.975
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:51:32.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:51:32.995
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 01/05/23 09:51:33.001
Jan  5 09:51:33.002: INFO: namespace kubectl-3502
Jan  5 09:51:33.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3502 create -f -'
Jan  5 09:51:33.548: INFO: stderr: ""
Jan  5 09:51:33.548: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/05/23 09:51:33.548
Jan  5 09:51:34.556: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 09:51:34.556: INFO: Found 0 / 1
Jan  5 09:51:35.556: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 09:51:35.556: INFO: Found 1 / 1
Jan  5 09:51:35.556: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  5 09:51:35.562: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 09:51:35.562: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  5 09:51:35.562: INFO: wait on agnhost-primary startup in kubectl-3502 
Jan  5 09:51:35.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3502 logs agnhost-primary-spv4n agnhost-primary'
Jan  5 09:51:35.646: INFO: stderr: ""
Jan  5 09:51:35.646: INFO: stdout: "Paused\n"
STEP: exposing RC 01/05/23 09:51:35.646
Jan  5 09:51:35.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3502 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan  5 09:51:35.720: INFO: stderr: ""
Jan  5 09:51:35.720: INFO: stdout: "service/rm2 exposed\n"
Jan  5 09:51:35.741: INFO: Service rm2 in namespace kubectl-3502 found.
STEP: exposing service 01/05/23 09:51:37.75
Jan  5 09:51:37.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3502 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan  5 09:51:37.842: INFO: stderr: ""
Jan  5 09:51:37.842: INFO: stdout: "service/rm3 exposed\n"
Jan  5 09:51:37.849: INFO: Service rm3 in namespace kubectl-3502 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 09:51:39.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3502" for this suite. 01/05/23 09:51:39.866
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":196,"skipped":3530,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.897 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:51:32.974
    Jan  5 09:51:32.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 09:51:32.975
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:51:32.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:51:32.995
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 01/05/23 09:51:33.001
    Jan  5 09:51:33.002: INFO: namespace kubectl-3502
    Jan  5 09:51:33.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3502 create -f -'
    Jan  5 09:51:33.548: INFO: stderr: ""
    Jan  5 09:51:33.548: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/05/23 09:51:33.548
    Jan  5 09:51:34.556: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 09:51:34.556: INFO: Found 0 / 1
    Jan  5 09:51:35.556: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 09:51:35.556: INFO: Found 1 / 1
    Jan  5 09:51:35.556: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan  5 09:51:35.562: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 09:51:35.562: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  5 09:51:35.562: INFO: wait on agnhost-primary startup in kubectl-3502 
    Jan  5 09:51:35.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3502 logs agnhost-primary-spv4n agnhost-primary'
    Jan  5 09:51:35.646: INFO: stderr: ""
    Jan  5 09:51:35.646: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/05/23 09:51:35.646
    Jan  5 09:51:35.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3502 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan  5 09:51:35.720: INFO: stderr: ""
    Jan  5 09:51:35.720: INFO: stdout: "service/rm2 exposed\n"
    Jan  5 09:51:35.741: INFO: Service rm2 in namespace kubectl-3502 found.
    STEP: exposing service 01/05/23 09:51:37.75
    Jan  5 09:51:37.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3502 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan  5 09:51:37.842: INFO: stderr: ""
    Jan  5 09:51:37.842: INFO: stdout: "service/rm3 exposed\n"
    Jan  5 09:51:37.849: INFO: Service rm3 in namespace kubectl-3502 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 09:51:39.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3502" for this suite. 01/05/23 09:51:39.866
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:51:39.872
Jan  5 09:51:39.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename sched-preemption 01/05/23 09:51:39.872
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:51:39.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:51:39.892
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan  5 09:51:39.921: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 09:52:39.992: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 01/05/23 09:52:39.996
Jan  5 09:52:40.035: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan  5 09:52:40.046: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan  5 09:52:40.070: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan  5 09:52:40.081: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan  5 09:52:40.106: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan  5 09:52:40.115: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jan  5 09:52:40.144: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jan  5 09:52:40.154: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/05/23 09:52:40.154
Jan  5 09:52:40.154: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3565" to be "running"
Jan  5 09:52:40.161: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.430106ms
Jan  5 09:52:42.167: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.012865002s
Jan  5 09:52:42.167: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan  5 09:52:42.167: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
Jan  5 09:52:42.172: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.250936ms
Jan  5 09:52:42.172: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 09:52:42.172: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
Jan  5 09:52:42.177: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.181367ms
Jan  5 09:52:42.177: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 09:52:42.177: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
Jan  5 09:52:42.181: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.061275ms
Jan  5 09:52:42.181: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 09:52:42.181: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
Jan  5 09:52:42.187: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.093051ms
Jan  5 09:52:42.187: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 09:52:42.187: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
Jan  5 09:52:42.195: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.694707ms
Jan  5 09:52:42.195: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 09:52:42.195: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
Jan  5 09:52:42.202: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.353375ms
Jan  5 09:52:42.202: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 09:52:42.202: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
Jan  5 09:52:42.209: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.688859ms
Jan  5 09:52:42.209: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/05/23 09:52:42.209
Jan  5 09:52:42.222: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3565" to be "running"
Jan  5 09:52:42.231: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.273529ms
Jan  5 09:52:44.243: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021836058s
Jan  5 09:52:46.243: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.021331918s
Jan  5 09:52:46.243: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:52:46.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3565" for this suite. 01/05/23 09:52:46.338
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":197,"skipped":3532,"failed":0}
------------------------------
â€¢ [SLOW TEST] [66.605 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:51:39.872
    Jan  5 09:51:39.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename sched-preemption 01/05/23 09:51:39.872
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:51:39.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:51:39.892
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan  5 09:51:39.921: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 09:52:39.992: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 01/05/23 09:52:39.996
    Jan  5 09:52:40.035: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan  5 09:52:40.046: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan  5 09:52:40.070: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan  5 09:52:40.081: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan  5 09:52:40.106: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan  5 09:52:40.115: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Jan  5 09:52:40.144: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Jan  5 09:52:40.154: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/05/23 09:52:40.154
    Jan  5 09:52:40.154: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3565" to be "running"
    Jan  5 09:52:40.161: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.430106ms
    Jan  5 09:52:42.167: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.012865002s
    Jan  5 09:52:42.167: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan  5 09:52:42.167: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
    Jan  5 09:52:42.172: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.250936ms
    Jan  5 09:52:42.172: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 09:52:42.172: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
    Jan  5 09:52:42.177: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.181367ms
    Jan  5 09:52:42.177: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 09:52:42.177: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
    Jan  5 09:52:42.181: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.061275ms
    Jan  5 09:52:42.181: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 09:52:42.181: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
    Jan  5 09:52:42.187: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.093051ms
    Jan  5 09:52:42.187: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 09:52:42.187: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
    Jan  5 09:52:42.195: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.694707ms
    Jan  5 09:52:42.195: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 09:52:42.195: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
    Jan  5 09:52:42.202: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.353375ms
    Jan  5 09:52:42.202: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 09:52:42.202: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-3565" to be "running"
    Jan  5 09:52:42.209: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.688859ms
    Jan  5 09:52:42.209: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/05/23 09:52:42.209
    Jan  5 09:52:42.222: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3565" to be "running"
    Jan  5 09:52:42.231: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.273529ms
    Jan  5 09:52:44.243: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021836058s
    Jan  5 09:52:46.243: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.021331918s
    Jan  5 09:52:46.243: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:52:46.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-3565" for this suite. 01/05/23 09:52:46.338
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:52:46.479
Jan  5 09:52:46.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename svc-latency 01/05/23 09:52:46.48
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:52:46.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:52:46.527
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan  5 09:52:46.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1 01/05/23 09:52:46.54
I0105 09:52:46.549670      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1, replica count: 1
I0105 09:52:47.600488      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0105 09:52:48.601623      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 09:52:48.715: INFO: Created: latency-svc-jdmkt
Jan  5 09:52:48.718: INFO: Got endpoints: latency-svc-jdmkt [16.155721ms]
Jan  5 09:52:48.730: INFO: Created: latency-svc-7vwpd
Jan  5 09:52:48.738: INFO: Got endpoints: latency-svc-7vwpd [20.505876ms]
Jan  5 09:52:48.741: INFO: Created: latency-svc-snc59
Jan  5 09:52:48.746: INFO: Created: latency-svc-z7nkw
Jan  5 09:52:48.748: INFO: Got endpoints: latency-svc-snc59 [29.84677ms]
Jan  5 09:52:48.757: INFO: Got endpoints: latency-svc-z7nkw [38.522461ms]
Jan  5 09:52:48.757: INFO: Created: latency-svc-v5khr
Jan  5 09:52:48.765: INFO: Got endpoints: latency-svc-v5khr [47.156766ms]
Jan  5 09:52:48.766: INFO: Created: latency-svc-xh2r2
Jan  5 09:52:48.772: INFO: Created: latency-svc-qm8lr
Jan  5 09:52:48.772: INFO: Got endpoints: latency-svc-xh2r2 [53.968568ms]
Jan  5 09:52:48.778: INFO: Got endpoints: latency-svc-qm8lr [59.992868ms]
Jan  5 09:52:48.780: INFO: Created: latency-svc-xtkcv
Jan  5 09:52:48.784: INFO: Got endpoints: latency-svc-xtkcv [65.746634ms]
Jan  5 09:52:48.788: INFO: Created: latency-svc-jlfcf
Jan  5 09:52:48.802: INFO: Created: latency-svc-wfqzq
Jan  5 09:52:48.803: INFO: Got endpoints: latency-svc-jlfcf [84.425297ms]
Jan  5 09:52:48.807: INFO: Created: latency-svc-9rldd
Jan  5 09:52:48.808: INFO: Got endpoints: latency-svc-wfqzq [89.450301ms]
Jan  5 09:52:48.810: INFO: Got endpoints: latency-svc-9rldd [91.694272ms]
Jan  5 09:52:48.817: INFO: Created: latency-svc-zsrgz
Jan  5 09:52:48.825: INFO: Got endpoints: latency-svc-zsrgz [107.449084ms]
Jan  5 09:52:48.828: INFO: Created: latency-svc-dqdqs
Jan  5 09:52:48.837: INFO: Created: latency-svc-bz4sk
Jan  5 09:52:48.837: INFO: Got endpoints: latency-svc-dqdqs [118.930036ms]
Jan  5 09:52:48.849: INFO: Created: latency-svc-5t8x2
Jan  5 09:52:48.849: INFO: Got endpoints: latency-svc-bz4sk [131.063856ms]
Jan  5 09:52:48.855: INFO: Created: latency-svc-fw2sg
Jan  5 09:52:48.857: INFO: Got endpoints: latency-svc-5t8x2 [138.782231ms]
Jan  5 09:52:48.863: INFO: Got endpoints: latency-svc-fw2sg [144.840204ms]
Jan  5 09:52:48.863: INFO: Created: latency-svc-k2z5s
Jan  5 09:52:48.870: INFO: Created: latency-svc-cxw2t
Jan  5 09:52:48.870: INFO: Got endpoints: latency-svc-k2z5s [131.889557ms]
Jan  5 09:52:48.878: INFO: Created: latency-svc-gqfs7
Jan  5 09:52:48.878: INFO: Got endpoints: latency-svc-cxw2t [130.554314ms]
Jan  5 09:52:48.884: INFO: Created: latency-svc-qfwcv
Jan  5 09:52:48.884: INFO: Got endpoints: latency-svc-gqfs7 [127.406106ms]
Jan  5 09:52:48.893: INFO: Created: latency-svc-qjr8p
Jan  5 09:52:48.900: INFO: Got endpoints: latency-svc-qfwcv [134.503139ms]
Jan  5 09:52:48.900: INFO: Got endpoints: latency-svc-qjr8p [127.678895ms]
Jan  5 09:52:48.919: INFO: Created: latency-svc-4pp7g
Jan  5 09:52:48.925: INFO: Created: latency-svc-xkjm8
Jan  5 09:52:48.925: INFO: Got endpoints: latency-svc-4pp7g [146.937532ms]
Jan  5 09:52:48.937: INFO: Created: latency-svc-p5dj5
Jan  5 09:52:48.937: INFO: Got endpoints: latency-svc-xkjm8 [153.167346ms]
Jan  5 09:52:48.943: INFO: Got endpoints: latency-svc-p5dj5 [140.653916ms]
Jan  5 09:52:48.944: INFO: Created: latency-svc-wtwdx
Jan  5 09:52:48.949: INFO: Got endpoints: latency-svc-wtwdx [141.177915ms]
Jan  5 09:52:48.950: INFO: Created: latency-svc-9txv6
Jan  5 09:52:48.956: INFO: Created: latency-svc-8bvdd
Jan  5 09:52:48.957: INFO: Got endpoints: latency-svc-9txv6 [147.063317ms]
Jan  5 09:52:48.964: INFO: Got endpoints: latency-svc-8bvdd [139.011499ms]
Jan  5 09:52:48.965: INFO: Created: latency-svc-cvnr9
Jan  5 09:52:48.974: INFO: Got endpoints: latency-svc-cvnr9 [136.920403ms]
Jan  5 09:52:48.979: INFO: Created: latency-svc-9kclm
Jan  5 09:52:48.980: INFO: Created: latency-svc-j4kz6
Jan  5 09:52:48.987: INFO: Got endpoints: latency-svc-9kclm [137.686013ms]
Jan  5 09:52:48.987: INFO: Got endpoints: latency-svc-j4kz6 [129.890706ms]
Jan  5 09:52:48.989: INFO: Created: latency-svc-xqrkz
Jan  5 09:52:48.996: INFO: Got endpoints: latency-svc-xqrkz [132.387929ms]
Jan  5 09:52:48.997: INFO: Created: latency-svc-fgtb8
Jan  5 09:52:49.004: INFO: Got endpoints: latency-svc-fgtb8 [133.958603ms]
Jan  5 09:52:49.006: INFO: Created: latency-svc-mhhpk
Jan  5 09:52:49.012: INFO: Created: latency-svc-vg6c4
Jan  5 09:52:49.013: INFO: Got endpoints: latency-svc-mhhpk [134.565175ms]
Jan  5 09:52:49.044: INFO: Got endpoints: latency-svc-vg6c4 [159.731383ms]
Jan  5 09:52:49.048: INFO: Created: latency-svc-5cnmn
Jan  5 09:52:49.052: INFO: Got endpoints: latency-svc-5cnmn [152.118547ms]
Jan  5 09:52:49.055: INFO: Created: latency-svc-p9kt7
Jan  5 09:52:49.062: INFO: Got endpoints: latency-svc-p9kt7 [161.575848ms]
Jan  5 09:52:49.063: INFO: Created: latency-svc-4xs2z
Jan  5 09:52:49.068: INFO: Created: latency-svc-s9fg2
Jan  5 09:52:49.073: INFO: Got endpoints: latency-svc-4xs2z [147.68144ms]
Jan  5 09:52:49.077: INFO: Created: latency-svc-rwdtp
Jan  5 09:52:49.083: INFO: Created: latency-svc-9fd6h
Jan  5 09:52:49.087: INFO: Created: latency-svc-nlcfb
Jan  5 09:52:49.097: INFO: Created: latency-svc-qbfv6
Jan  5 09:52:49.107: INFO: Created: latency-svc-hj96l
Jan  5 09:52:49.115: INFO: Created: latency-svc-gwgjp
Jan  5 09:52:49.121: INFO: Got endpoints: latency-svc-s9fg2 [183.463084ms]
Jan  5 09:52:49.126: INFO: Created: latency-svc-b68gv
Jan  5 09:52:49.133: INFO: Created: latency-svc-rhblf
Jan  5 09:52:49.156: INFO: Created: latency-svc-kzx56
Jan  5 09:52:49.161: INFO: Created: latency-svc-5bjk4
Jan  5 09:52:49.169: INFO: Created: latency-svc-q7r8r
Jan  5 09:52:49.170: INFO: Got endpoints: latency-svc-rwdtp [226.182064ms]
Jan  5 09:52:49.182: INFO: Created: latency-svc-z5bt4
Jan  5 09:52:49.191: INFO: Created: latency-svc-jd7q4
Jan  5 09:52:49.195: INFO: Created: latency-svc-pwqj9
Jan  5 09:52:49.208: INFO: Created: latency-svc-wwlns
Jan  5 09:52:49.212: INFO: Created: latency-svc-86g7v
Jan  5 09:52:49.221: INFO: Got endpoints: latency-svc-9fd6h [272.202988ms]
Jan  5 09:52:49.232: INFO: Created: latency-svc-czx6j
Jan  5 09:52:49.269: INFO: Got endpoints: latency-svc-nlcfb [311.728875ms]
Jan  5 09:52:49.285: INFO: Created: latency-svc-5797f
Jan  5 09:52:49.321: INFO: Got endpoints: latency-svc-qbfv6 [356.412233ms]
Jan  5 09:52:49.335: INFO: Created: latency-svc-hs4n9
Jan  5 09:52:49.376: INFO: Got endpoints: latency-svc-hj96l [401.690332ms]
Jan  5 09:52:49.388: INFO: Created: latency-svc-kqnls
Jan  5 09:52:49.419: INFO: Got endpoints: latency-svc-gwgjp [431.908766ms]
Jan  5 09:52:49.432: INFO: Created: latency-svc-h6hmd
Jan  5 09:52:49.469: INFO: Got endpoints: latency-svc-b68gv [481.470825ms]
Jan  5 09:52:49.494: INFO: Created: latency-svc-p655g
Jan  5 09:52:49.519: INFO: Got endpoints: latency-svc-rhblf [523.548958ms]
Jan  5 09:52:49.557: INFO: Created: latency-svc-n5tbh
Jan  5 09:52:49.570: INFO: Got endpoints: latency-svc-kzx56 [565.885102ms]
Jan  5 09:52:49.584: INFO: Created: latency-svc-tjh5q
Jan  5 09:52:49.620: INFO: Got endpoints: latency-svc-5bjk4 [606.630987ms]
Jan  5 09:52:49.633: INFO: Created: latency-svc-dn9tj
Jan  5 09:52:49.671: INFO: Got endpoints: latency-svc-q7r8r [627.532251ms]
Jan  5 09:52:49.684: INFO: Created: latency-svc-cb4bv
Jan  5 09:52:49.723: INFO: Got endpoints: latency-svc-z5bt4 [671.382313ms]
Jan  5 09:52:49.738: INFO: Created: latency-svc-8pq6s
Jan  5 09:52:49.771: INFO: Got endpoints: latency-svc-jd7q4 [709.213434ms]
Jan  5 09:52:49.786: INFO: Created: latency-svc-hqljb
Jan  5 09:52:49.819: INFO: Got endpoints: latency-svc-pwqj9 [746.521338ms]
Jan  5 09:52:49.833: INFO: Created: latency-svc-nf8f7
Jan  5 09:52:49.869: INFO: Got endpoints: latency-svc-wwlns [748.59467ms]
Jan  5 09:52:49.882: INFO: Created: latency-svc-9jd2j
Jan  5 09:52:49.921: INFO: Got endpoints: latency-svc-86g7v [751.069031ms]
Jan  5 09:52:49.931: INFO: Created: latency-svc-8lg99
Jan  5 09:52:49.970: INFO: Got endpoints: latency-svc-czx6j [748.924705ms]
Jan  5 09:52:49.983: INFO: Created: latency-svc-6t78d
Jan  5 09:52:50.022: INFO: Got endpoints: latency-svc-5797f [753.147662ms]
Jan  5 09:52:50.034: INFO: Created: latency-svc-4mnq4
Jan  5 09:52:50.068: INFO: Got endpoints: latency-svc-hs4n9 [746.953676ms]
Jan  5 09:52:50.079: INFO: Created: latency-svc-dl9f4
Jan  5 09:52:50.125: INFO: Got endpoints: latency-svc-kqnls [749.276513ms]
Jan  5 09:52:50.137: INFO: Created: latency-svc-hs28d
Jan  5 09:52:50.169: INFO: Got endpoints: latency-svc-h6hmd [750.078361ms]
Jan  5 09:52:50.181: INFO: Created: latency-svc-dfccs
Jan  5 09:52:50.221: INFO: Got endpoints: latency-svc-p655g [752.024415ms]
Jan  5 09:52:50.260: INFO: Created: latency-svc-q9nww
Jan  5 09:52:50.272: INFO: Got endpoints: latency-svc-n5tbh [752.670371ms]
Jan  5 09:52:50.285: INFO: Created: latency-svc-tjxsg
Jan  5 09:52:50.320: INFO: Got endpoints: latency-svc-tjh5q [749.479501ms]
Jan  5 09:52:50.332: INFO: Created: latency-svc-ql2gg
Jan  5 09:52:50.369: INFO: Got endpoints: latency-svc-dn9tj [749.614173ms]
Jan  5 09:52:50.384: INFO: Created: latency-svc-sq8t8
Jan  5 09:52:50.420: INFO: Got endpoints: latency-svc-cb4bv [748.188481ms]
Jan  5 09:52:50.433: INFO: Created: latency-svc-ft46d
Jan  5 09:52:50.470: INFO: Got endpoints: latency-svc-8pq6s [746.976167ms]
Jan  5 09:52:50.482: INFO: Created: latency-svc-n5kts
Jan  5 09:52:50.520: INFO: Got endpoints: latency-svc-hqljb [749.426843ms]
Jan  5 09:52:50.536: INFO: Created: latency-svc-lsnq7
Jan  5 09:52:50.590: INFO: Got endpoints: latency-svc-nf8f7 [770.465054ms]
Jan  5 09:52:50.613: INFO: Created: latency-svc-hqqht
Jan  5 09:52:50.619: INFO: Got endpoints: latency-svc-9jd2j [749.147182ms]
Jan  5 09:52:50.637: INFO: Created: latency-svc-7d5kj
Jan  5 09:52:50.669: INFO: Got endpoints: latency-svc-8lg99 [747.835161ms]
Jan  5 09:52:50.684: INFO: Created: latency-svc-b6m5s
Jan  5 09:52:50.719: INFO: Got endpoints: latency-svc-6t78d [748.569493ms]
Jan  5 09:52:50.733: INFO: Created: latency-svc-grssw
Jan  5 09:52:50.770: INFO: Got endpoints: latency-svc-4mnq4 [747.345698ms]
Jan  5 09:52:50.781: INFO: Created: latency-svc-sbctg
Jan  5 09:52:50.822: INFO: Got endpoints: latency-svc-dl9f4 [753.898153ms]
Jan  5 09:52:50.835: INFO: Created: latency-svc-knl4r
Jan  5 09:52:50.870: INFO: Got endpoints: latency-svc-hs28d [745.277737ms]
Jan  5 09:52:50.883: INFO: Created: latency-svc-zlnqg
Jan  5 09:52:50.926: INFO: Got endpoints: latency-svc-dfccs [756.603135ms]
Jan  5 09:52:50.938: INFO: Created: latency-svc-5qhnv
Jan  5 09:52:50.974: INFO: Got endpoints: latency-svc-q9nww [753.616209ms]
Jan  5 09:52:50.987: INFO: Created: latency-svc-hd94g
Jan  5 09:52:51.020: INFO: Got endpoints: latency-svc-tjxsg [748.383674ms]
Jan  5 09:52:51.034: INFO: Created: latency-svc-9md6d
Jan  5 09:52:51.071: INFO: Got endpoints: latency-svc-ql2gg [750.788327ms]
Jan  5 09:52:51.084: INFO: Created: latency-svc-wgjxd
Jan  5 09:52:51.124: INFO: Got endpoints: latency-svc-sq8t8 [755.116338ms]
Jan  5 09:52:51.137: INFO: Created: latency-svc-tqzm7
Jan  5 09:52:51.171: INFO: Got endpoints: latency-svc-ft46d [750.847728ms]
Jan  5 09:52:51.182: INFO: Created: latency-svc-4pzlz
Jan  5 09:52:51.220: INFO: Got endpoints: latency-svc-n5kts [749.492958ms]
Jan  5 09:52:51.238: INFO: Created: latency-svc-tpwml
Jan  5 09:52:51.272: INFO: Got endpoints: latency-svc-lsnq7 [751.734574ms]
Jan  5 09:52:51.285: INFO: Created: latency-svc-v5hts
Jan  5 09:52:51.319: INFO: Got endpoints: latency-svc-hqqht [728.643179ms]
Jan  5 09:52:51.337: INFO: Created: latency-svc-m2v98
Jan  5 09:52:51.369: INFO: Got endpoints: latency-svc-7d5kj [750.376487ms]
Jan  5 09:52:51.383: INFO: Created: latency-svc-7gbd4
Jan  5 09:52:51.424: INFO: Got endpoints: latency-svc-b6m5s [755.645307ms]
Jan  5 09:52:51.448: INFO: Created: latency-svc-bgvzr
Jan  5 09:52:51.468: INFO: Got endpoints: latency-svc-grssw [748.995439ms]
Jan  5 09:52:51.480: INFO: Created: latency-svc-2ppsl
Jan  5 09:52:51.523: INFO: Got endpoints: latency-svc-sbctg [753.615657ms]
Jan  5 09:52:51.536: INFO: Created: latency-svc-b8fbs
Jan  5 09:52:51.569: INFO: Got endpoints: latency-svc-knl4r [747.081374ms]
Jan  5 09:52:51.582: INFO: Created: latency-svc-pptsg
Jan  5 09:52:51.621: INFO: Got endpoints: latency-svc-zlnqg [750.017266ms]
Jan  5 09:52:51.636: INFO: Created: latency-svc-bnwdl
Jan  5 09:52:51.670: INFO: Got endpoints: latency-svc-5qhnv [744.062596ms]
Jan  5 09:52:51.684: INFO: Created: latency-svc-pmrlc
Jan  5 09:52:51.720: INFO: Got endpoints: latency-svc-hd94g [745.331466ms]
Jan  5 09:52:51.735: INFO: Created: latency-svc-ql7x8
Jan  5 09:52:51.769: INFO: Got endpoints: latency-svc-9md6d [748.458405ms]
Jan  5 09:52:51.784: INFO: Created: latency-svc-jbjsf
Jan  5 09:52:51.821: INFO: Got endpoints: latency-svc-wgjxd [750.590718ms]
Jan  5 09:52:51.835: INFO: Created: latency-svc-fjjhh
Jan  5 09:52:51.897: INFO: Got endpoints: latency-svc-tqzm7 [772.464619ms]
Jan  5 09:52:51.909: INFO: Created: latency-svc-fkrx5
Jan  5 09:52:51.920: INFO: Got endpoints: latency-svc-4pzlz [748.595281ms]
Jan  5 09:52:51.935: INFO: Created: latency-svc-7phlw
Jan  5 09:52:51.970: INFO: Got endpoints: latency-svc-tpwml [749.535317ms]
Jan  5 09:52:51.986: INFO: Created: latency-svc-4vkw8
Jan  5 09:52:52.021: INFO: Got endpoints: latency-svc-v5hts [748.709405ms]
Jan  5 09:52:52.044: INFO: Created: latency-svc-fmbkt
Jan  5 09:52:52.071: INFO: Got endpoints: latency-svc-m2v98 [751.919961ms]
Jan  5 09:52:52.084: INFO: Created: latency-svc-fzd5h
Jan  5 09:52:52.123: INFO: Got endpoints: latency-svc-7gbd4 [753.440299ms]
Jan  5 09:52:52.137: INFO: Created: latency-svc-9zl87
Jan  5 09:52:52.168: INFO: Got endpoints: latency-svc-bgvzr [743.93101ms]
Jan  5 09:52:52.183: INFO: Created: latency-svc-9b75f
Jan  5 09:52:52.223: INFO: Got endpoints: latency-svc-2ppsl [754.825196ms]
Jan  5 09:52:52.234: INFO: Created: latency-svc-b49qc
Jan  5 09:52:52.268: INFO: Got endpoints: latency-svc-b8fbs [744.740673ms]
Jan  5 09:52:52.280: INFO: Created: latency-svc-n2mqn
Jan  5 09:52:52.323: INFO: Got endpoints: latency-svc-pptsg [754.062722ms]
Jan  5 09:52:52.339: INFO: Created: latency-svc-4lns7
Jan  5 09:52:52.371: INFO: Got endpoints: latency-svc-bnwdl [750.457099ms]
Jan  5 09:52:52.424: INFO: Got endpoints: latency-svc-pmrlc [753.773743ms]
Jan  5 09:52:52.424: INFO: Created: latency-svc-jxkbg
Jan  5 09:52:52.443: INFO: Created: latency-svc-dxm6h
Jan  5 09:52:52.470: INFO: Got endpoints: latency-svc-ql7x8 [749.428047ms]
Jan  5 09:52:52.482: INFO: Created: latency-svc-d4tgq
Jan  5 09:52:52.518: INFO: Got endpoints: latency-svc-jbjsf [749.179032ms]
Jan  5 09:52:52.531: INFO: Created: latency-svc-n9ds5
Jan  5 09:52:52.568: INFO: Got endpoints: latency-svc-fjjhh [746.536237ms]
Jan  5 09:52:52.598: INFO: Created: latency-svc-p9zhs
Jan  5 09:52:52.622: INFO: Got endpoints: latency-svc-fkrx5 [725.309224ms]
Jan  5 09:52:52.636: INFO: Created: latency-svc-tdp8s
Jan  5 09:52:52.670: INFO: Got endpoints: latency-svc-7phlw [750.643828ms]
Jan  5 09:52:52.685: INFO: Created: latency-svc-d8xrc
Jan  5 09:52:52.718: INFO: Got endpoints: latency-svc-4vkw8 [748.589071ms]
Jan  5 09:52:52.731: INFO: Created: latency-svc-xnp2z
Jan  5 09:52:52.771: INFO: Got endpoints: latency-svc-fmbkt [750.418207ms]
Jan  5 09:52:52.793: INFO: Created: latency-svc-v8fc2
Jan  5 09:52:52.832: INFO: Got endpoints: latency-svc-fzd5h [761.574931ms]
Jan  5 09:52:52.845: INFO: Created: latency-svc-9qg8k
Jan  5 09:52:52.876: INFO: Got endpoints: latency-svc-9zl87 [753.781818ms]
Jan  5 09:52:52.889: INFO: Created: latency-svc-6z6m4
Jan  5 09:52:52.920: INFO: Got endpoints: latency-svc-9b75f [751.320842ms]
Jan  5 09:52:52.933: INFO: Created: latency-svc-gx9lr
Jan  5 09:52:52.984: INFO: Got endpoints: latency-svc-b49qc [760.622282ms]
Jan  5 09:52:52.996: INFO: Created: latency-svc-6p6rf
Jan  5 09:52:53.019: INFO: Got endpoints: latency-svc-n2mqn [750.672361ms]
Jan  5 09:52:53.032: INFO: Created: latency-svc-74n5r
Jan  5 09:52:53.068: INFO: Got endpoints: latency-svc-4lns7 [744.914368ms]
Jan  5 09:52:53.082: INFO: Created: latency-svc-rz9sx
Jan  5 09:52:53.118: INFO: Got endpoints: latency-svc-jxkbg [746.904034ms]
Jan  5 09:52:53.132: INFO: Created: latency-svc-x4gnj
Jan  5 09:52:53.170: INFO: Got endpoints: latency-svc-dxm6h [745.495323ms]
Jan  5 09:52:53.183: INFO: Created: latency-svc-kbf8l
Jan  5 09:52:53.218: INFO: Got endpoints: latency-svc-d4tgq [748.787611ms]
Jan  5 09:52:53.233: INFO: Created: latency-svc-hz8m4
Jan  5 09:52:53.271: INFO: Got endpoints: latency-svc-n9ds5 [752.79256ms]
Jan  5 09:52:53.284: INFO: Created: latency-svc-5xsnp
Jan  5 09:52:53.322: INFO: Got endpoints: latency-svc-p9zhs [753.350893ms]
Jan  5 09:52:53.333: INFO: Created: latency-svc-p5wjd
Jan  5 09:52:53.370: INFO: Got endpoints: latency-svc-tdp8s [747.256781ms]
Jan  5 09:52:53.381: INFO: Created: latency-svc-qlkjc
Jan  5 09:52:53.420: INFO: Got endpoints: latency-svc-d8xrc [749.736963ms]
Jan  5 09:52:53.432: INFO: Created: latency-svc-4cl88
Jan  5 09:52:53.471: INFO: Got endpoints: latency-svc-xnp2z [752.511767ms]
Jan  5 09:52:53.485: INFO: Created: latency-svc-wwgxg
Jan  5 09:52:53.533: INFO: Got endpoints: latency-svc-v8fc2 [758.39805ms]
Jan  5 09:52:53.546: INFO: Created: latency-svc-rsmx8
Jan  5 09:52:53.568: INFO: Got endpoints: latency-svc-9qg8k [735.150053ms]
Jan  5 09:52:53.605: INFO: Created: latency-svc-7856w
Jan  5 09:52:53.619: INFO: Got endpoints: latency-svc-6z6m4 [742.134497ms]
Jan  5 09:52:53.631: INFO: Created: latency-svc-qkp9j
Jan  5 09:52:53.670: INFO: Got endpoints: latency-svc-gx9lr [749.731244ms]
Jan  5 09:52:53.683: INFO: Created: latency-svc-hvtr5
Jan  5 09:52:53.719: INFO: Got endpoints: latency-svc-6p6rf [735.437019ms]
Jan  5 09:52:53.731: INFO: Created: latency-svc-tpxnp
Jan  5 09:52:53.770: INFO: Got endpoints: latency-svc-74n5r [751.450084ms]
Jan  5 09:52:53.782: INFO: Created: latency-svc-pnrfq
Jan  5 09:52:53.819: INFO: Got endpoints: latency-svc-rz9sx [750.608252ms]
Jan  5 09:52:53.831: INFO: Created: latency-svc-55vvd
Jan  5 09:52:53.869: INFO: Got endpoints: latency-svc-x4gnj [750.623498ms]
Jan  5 09:52:53.883: INFO: Created: latency-svc-7k4qg
Jan  5 09:52:53.919: INFO: Got endpoints: latency-svc-kbf8l [749.788609ms]
Jan  5 09:52:53.931: INFO: Created: latency-svc-stdx5
Jan  5 09:52:53.999: INFO: Got endpoints: latency-svc-hz8m4 [780.672569ms]
Jan  5 09:52:54.013: INFO: Created: latency-svc-86bk4
Jan  5 09:52:54.022: INFO: Got endpoints: latency-svc-5xsnp [750.937287ms]
Jan  5 09:52:54.033: INFO: Created: latency-svc-hwhsm
Jan  5 09:52:54.069: INFO: Got endpoints: latency-svc-p5wjd [747.59278ms]
Jan  5 09:52:54.086: INFO: Created: latency-svc-jdjlz
Jan  5 09:52:54.120: INFO: Got endpoints: latency-svc-qlkjc [749.849044ms]
Jan  5 09:52:54.136: INFO: Created: latency-svc-s8dkm
Jan  5 09:52:54.169: INFO: Got endpoints: latency-svc-4cl88 [749.384796ms]
Jan  5 09:52:54.182: INFO: Created: latency-svc-ktf4f
Jan  5 09:52:54.220: INFO: Got endpoints: latency-svc-wwgxg [749.238494ms]
Jan  5 09:52:54.238: INFO: Created: latency-svc-xmt9t
Jan  5 09:52:54.272: INFO: Got endpoints: latency-svc-rsmx8 [738.544533ms]
Jan  5 09:52:54.285: INFO: Created: latency-svc-wnst5
Jan  5 09:52:54.323: INFO: Got endpoints: latency-svc-7856w [754.770475ms]
Jan  5 09:52:54.335: INFO: Created: latency-svc-f2b5q
Jan  5 09:52:54.368: INFO: Got endpoints: latency-svc-qkp9j [749.823477ms]
Jan  5 09:52:54.382: INFO: Created: latency-svc-hpkmm
Jan  5 09:52:54.422: INFO: Got endpoints: latency-svc-hvtr5 [752.174797ms]
Jan  5 09:52:54.437: INFO: Created: latency-svc-pxk4p
Jan  5 09:52:54.470: INFO: Got endpoints: latency-svc-tpxnp [750.196923ms]
Jan  5 09:52:54.483: INFO: Created: latency-svc-vrvrf
Jan  5 09:52:54.523: INFO: Got endpoints: latency-svc-pnrfq [752.345456ms]
Jan  5 09:52:54.542: INFO: Created: latency-svc-vvf62
Jan  5 09:52:54.573: INFO: Got endpoints: latency-svc-55vvd [753.714714ms]
Jan  5 09:52:54.586: INFO: Created: latency-svc-9gbxg
Jan  5 09:52:54.619: INFO: Got endpoints: latency-svc-7k4qg [750.302592ms]
Jan  5 09:52:54.633: INFO: Created: latency-svc-fnxzt
Jan  5 09:52:54.672: INFO: Got endpoints: latency-svc-stdx5 [752.409476ms]
Jan  5 09:52:54.684: INFO: Created: latency-svc-79hx4
Jan  5 09:52:54.722: INFO: Got endpoints: latency-svc-86bk4 [722.711635ms]
Jan  5 09:52:54.736: INFO: Created: latency-svc-rplwn
Jan  5 09:52:54.771: INFO: Got endpoints: latency-svc-hwhsm [748.505405ms]
Jan  5 09:52:54.783: INFO: Created: latency-svc-bl4lx
Jan  5 09:52:54.821: INFO: Got endpoints: latency-svc-jdjlz [751.451958ms]
Jan  5 09:52:54.833: INFO: Created: latency-svc-rng42
Jan  5 09:52:54.871: INFO: Got endpoints: latency-svc-s8dkm [751.543058ms]
Jan  5 09:52:54.890: INFO: Created: latency-svc-hlw2k
Jan  5 09:52:54.920: INFO: Got endpoints: latency-svc-ktf4f [750.658186ms]
Jan  5 09:52:54.932: INFO: Created: latency-svc-r5hrg
Jan  5 09:52:54.973: INFO: Got endpoints: latency-svc-xmt9t [752.913688ms]
Jan  5 09:52:54.991: INFO: Created: latency-svc-xvd8p
Jan  5 09:52:55.020: INFO: Got endpoints: latency-svc-wnst5 [747.633476ms]
Jan  5 09:52:55.034: INFO: Created: latency-svc-5ppcd
Jan  5 09:52:55.070: INFO: Got endpoints: latency-svc-f2b5q [746.867136ms]
Jan  5 09:52:55.085: INFO: Created: latency-svc-89z2r
Jan  5 09:52:55.119: INFO: Got endpoints: latency-svc-hpkmm [750.905047ms]
Jan  5 09:52:55.132: INFO: Created: latency-svc-7l8p6
Jan  5 09:52:55.170: INFO: Got endpoints: latency-svc-pxk4p [747.766365ms]
Jan  5 09:52:55.189: INFO: Created: latency-svc-kj2lp
Jan  5 09:52:55.221: INFO: Got endpoints: latency-svc-vrvrf [750.845886ms]
Jan  5 09:52:55.234: INFO: Created: latency-svc-wwzpf
Jan  5 09:52:55.269: INFO: Got endpoints: latency-svc-vvf62 [746.723158ms]
Jan  5 09:52:55.281: INFO: Created: latency-svc-4vqvx
Jan  5 09:52:55.320: INFO: Got endpoints: latency-svc-9gbxg [746.87501ms]
Jan  5 09:52:55.336: INFO: Created: latency-svc-t9cd5
Jan  5 09:52:55.369: INFO: Got endpoints: latency-svc-fnxzt [750.047895ms]
Jan  5 09:52:55.385: INFO: Created: latency-svc-jgc42
Jan  5 09:52:55.425: INFO: Got endpoints: latency-svc-79hx4 [753.368466ms]
Jan  5 09:52:55.438: INFO: Created: latency-svc-z64fz
Jan  5 09:52:55.470: INFO: Got endpoints: latency-svc-rplwn [747.782073ms]
Jan  5 09:52:55.484: INFO: Created: latency-svc-qxwnt
Jan  5 09:52:55.521: INFO: Got endpoints: latency-svc-bl4lx [750.33352ms]
Jan  5 09:52:55.538: INFO: Created: latency-svc-7nfwb
Jan  5 09:52:55.571: INFO: Got endpoints: latency-svc-rng42 [750.342577ms]
Jan  5 09:52:55.584: INFO: Created: latency-svc-jhldw
Jan  5 09:52:55.622: INFO: Got endpoints: latency-svc-hlw2k [750.246047ms]
Jan  5 09:52:55.641: INFO: Created: latency-svc-m486g
Jan  5 09:52:55.668: INFO: Got endpoints: latency-svc-r5hrg [747.962533ms]
Jan  5 09:52:55.692: INFO: Created: latency-svc-9n5vk
Jan  5 09:52:55.718: INFO: Got endpoints: latency-svc-xvd8p [745.170868ms]
Jan  5 09:52:55.730: INFO: Created: latency-svc-pl4r8
Jan  5 09:52:55.772: INFO: Got endpoints: latency-svc-5ppcd [751.573956ms]
Jan  5 09:52:55.787: INFO: Created: latency-svc-swglw
Jan  5 09:52:55.821: INFO: Got endpoints: latency-svc-89z2r [751.446758ms]
Jan  5 09:52:55.833: INFO: Created: latency-svc-59pns
Jan  5 09:52:55.873: INFO: Got endpoints: latency-svc-7l8p6 [753.660635ms]
Jan  5 09:52:55.888: INFO: Created: latency-svc-pcr8c
Jan  5 09:52:55.920: INFO: Got endpoints: latency-svc-kj2lp [750.337286ms]
Jan  5 09:52:55.933: INFO: Created: latency-svc-pmxc2
Jan  5 09:52:55.968: INFO: Got endpoints: latency-svc-wwzpf [747.776194ms]
Jan  5 09:52:55.981: INFO: Created: latency-svc-nn72c
Jan  5 09:52:56.021: INFO: Got endpoints: latency-svc-4vqvx [751.344015ms]
Jan  5 09:52:56.033: INFO: Created: latency-svc-5rnkw
Jan  5 09:52:56.071: INFO: Got endpoints: latency-svc-t9cd5 [750.91742ms]
Jan  5 09:52:56.086: INFO: Created: latency-svc-89dsz
Jan  5 09:52:56.119: INFO: Got endpoints: latency-svc-jgc42 [749.991952ms]
Jan  5 09:52:56.132: INFO: Created: latency-svc-llwhq
Jan  5 09:52:56.174: INFO: Got endpoints: latency-svc-z64fz [748.646108ms]
Jan  5 09:52:56.188: INFO: Created: latency-svc-jhg27
Jan  5 09:52:56.219: INFO: Got endpoints: latency-svc-qxwnt [749.394336ms]
Jan  5 09:52:56.231: INFO: Created: latency-svc-8xrgm
Jan  5 09:52:56.272: INFO: Got endpoints: latency-svc-7nfwb [751.163259ms]
Jan  5 09:52:56.285: INFO: Created: latency-svc-rkghc
Jan  5 09:52:56.321: INFO: Got endpoints: latency-svc-jhldw [749.988405ms]
Jan  5 09:52:56.332: INFO: Created: latency-svc-mjc9h
Jan  5 09:52:56.372: INFO: Got endpoints: latency-svc-m486g [749.961665ms]
Jan  5 09:52:56.389: INFO: Created: latency-svc-hg4bz
Jan  5 09:52:56.419: INFO: Got endpoints: latency-svc-9n5vk [751.104688ms]
Jan  5 09:52:56.439: INFO: Created: latency-svc-9x6kb
Jan  5 09:52:56.469: INFO: Got endpoints: latency-svc-pl4r8 [750.609584ms]
Jan  5 09:52:56.488: INFO: Created: latency-svc-nfgv6
Jan  5 09:52:56.518: INFO: Got endpoints: latency-svc-swglw [746.270302ms]
Jan  5 09:52:56.533: INFO: Created: latency-svc-z2wv6
Jan  5 09:52:56.571: INFO: Got endpoints: latency-svc-59pns [749.66964ms]
Jan  5 09:52:56.620: INFO: Got endpoints: latency-svc-pcr8c [747.08336ms]
Jan  5 09:52:56.669: INFO: Got endpoints: latency-svc-pmxc2 [748.907507ms]
Jan  5 09:52:56.719: INFO: Got endpoints: latency-svc-nn72c [750.834164ms]
Jan  5 09:52:56.769: INFO: Got endpoints: latency-svc-5rnkw [747.678562ms]
Jan  5 09:52:56.821: INFO: Got endpoints: latency-svc-89dsz [750.703211ms]
Jan  5 09:52:56.869: INFO: Got endpoints: latency-svc-llwhq [749.034393ms]
Jan  5 09:52:56.920: INFO: Got endpoints: latency-svc-jhg27 [746.281284ms]
Jan  5 09:52:56.971: INFO: Got endpoints: latency-svc-8xrgm [751.608462ms]
Jan  5 09:52:57.020: INFO: Got endpoints: latency-svc-rkghc [747.295527ms]
Jan  5 09:52:57.071: INFO: Got endpoints: latency-svc-mjc9h [750.024654ms]
Jan  5 09:52:57.122: INFO: Got endpoints: latency-svc-hg4bz [749.973218ms]
Jan  5 09:52:57.169: INFO: Got endpoints: latency-svc-9x6kb [749.902887ms]
Jan  5 09:52:57.222: INFO: Got endpoints: latency-svc-nfgv6 [752.816988ms]
Jan  5 09:52:57.268: INFO: Got endpoints: latency-svc-z2wv6 [750.16623ms]
Jan  5 09:52:57.268: INFO: Latencies: [20.505876ms 29.84677ms 38.522461ms 47.156766ms 53.968568ms 59.992868ms 65.746634ms 84.425297ms 89.450301ms 91.694272ms 107.449084ms 118.930036ms 127.406106ms 127.678895ms 129.890706ms 130.554314ms 131.063856ms 131.889557ms 132.387929ms 133.958603ms 134.503139ms 134.565175ms 136.920403ms 137.686013ms 138.782231ms 139.011499ms 140.653916ms 141.177915ms 144.840204ms 146.937532ms 147.063317ms 147.68144ms 152.118547ms 153.167346ms 159.731383ms 161.575848ms 183.463084ms 226.182064ms 272.202988ms 311.728875ms 356.412233ms 401.690332ms 431.908766ms 481.470825ms 523.548958ms 565.885102ms 606.630987ms 627.532251ms 671.382313ms 709.213434ms 722.711635ms 725.309224ms 728.643179ms 735.150053ms 735.437019ms 738.544533ms 742.134497ms 743.93101ms 744.062596ms 744.740673ms 744.914368ms 745.170868ms 745.277737ms 745.331466ms 745.495323ms 746.270302ms 746.281284ms 746.521338ms 746.536237ms 746.723158ms 746.867136ms 746.87501ms 746.904034ms 746.953676ms 746.976167ms 747.081374ms 747.08336ms 747.256781ms 747.295527ms 747.345698ms 747.59278ms 747.633476ms 747.678562ms 747.766365ms 747.776194ms 747.782073ms 747.835161ms 747.962533ms 748.188481ms 748.383674ms 748.458405ms 748.505405ms 748.569493ms 748.589071ms 748.59467ms 748.595281ms 748.646108ms 748.709405ms 748.787611ms 748.907507ms 748.924705ms 748.995439ms 749.034393ms 749.147182ms 749.179032ms 749.238494ms 749.276513ms 749.384796ms 749.394336ms 749.426843ms 749.428047ms 749.479501ms 749.492958ms 749.535317ms 749.614173ms 749.66964ms 749.731244ms 749.736963ms 749.788609ms 749.823477ms 749.849044ms 749.902887ms 749.961665ms 749.973218ms 749.988405ms 749.991952ms 750.017266ms 750.024654ms 750.047895ms 750.078361ms 750.16623ms 750.196923ms 750.246047ms 750.302592ms 750.33352ms 750.337286ms 750.342577ms 750.376487ms 750.418207ms 750.457099ms 750.590718ms 750.608252ms 750.609584ms 750.623498ms 750.643828ms 750.658186ms 750.672361ms 750.703211ms 750.788327ms 750.834164ms 750.845886ms 750.847728ms 750.905047ms 750.91742ms 750.937287ms 751.069031ms 751.104688ms 751.163259ms 751.320842ms 751.344015ms 751.446758ms 751.450084ms 751.451958ms 751.543058ms 751.573956ms 751.608462ms 751.734574ms 751.919961ms 752.024415ms 752.174797ms 752.345456ms 752.409476ms 752.511767ms 752.670371ms 752.79256ms 752.816988ms 752.913688ms 753.147662ms 753.350893ms 753.368466ms 753.440299ms 753.615657ms 753.616209ms 753.660635ms 753.714714ms 753.773743ms 753.781818ms 753.898153ms 754.062722ms 754.770475ms 754.825196ms 755.116338ms 755.645307ms 756.603135ms 758.39805ms 760.622282ms 761.574931ms 770.465054ms 772.464619ms 780.672569ms]
Jan  5 09:52:57.269: INFO: 50 %ile: 748.924705ms
Jan  5 09:52:57.269: INFO: 90 %ile: 753.440299ms
Jan  5 09:52:57.269: INFO: 99 %ile: 772.464619ms
Jan  5 09:52:57.269: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jan  5 09:52:57.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1" for this suite. 01/05/23 09:52:57.281
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":198,"skipped":3561,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.808 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:52:46.479
    Jan  5 09:52:46.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename svc-latency 01/05/23 09:52:46.48
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:52:46.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:52:46.527
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan  5 09:52:46.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-1 01/05/23 09:52:46.54
    I0105 09:52:46.549670      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1, replica count: 1
    I0105 09:52:47.600488      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0105 09:52:48.601623      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 09:52:48.715: INFO: Created: latency-svc-jdmkt
    Jan  5 09:52:48.718: INFO: Got endpoints: latency-svc-jdmkt [16.155721ms]
    Jan  5 09:52:48.730: INFO: Created: latency-svc-7vwpd
    Jan  5 09:52:48.738: INFO: Got endpoints: latency-svc-7vwpd [20.505876ms]
    Jan  5 09:52:48.741: INFO: Created: latency-svc-snc59
    Jan  5 09:52:48.746: INFO: Created: latency-svc-z7nkw
    Jan  5 09:52:48.748: INFO: Got endpoints: latency-svc-snc59 [29.84677ms]
    Jan  5 09:52:48.757: INFO: Got endpoints: latency-svc-z7nkw [38.522461ms]
    Jan  5 09:52:48.757: INFO: Created: latency-svc-v5khr
    Jan  5 09:52:48.765: INFO: Got endpoints: latency-svc-v5khr [47.156766ms]
    Jan  5 09:52:48.766: INFO: Created: latency-svc-xh2r2
    Jan  5 09:52:48.772: INFO: Created: latency-svc-qm8lr
    Jan  5 09:52:48.772: INFO: Got endpoints: latency-svc-xh2r2 [53.968568ms]
    Jan  5 09:52:48.778: INFO: Got endpoints: latency-svc-qm8lr [59.992868ms]
    Jan  5 09:52:48.780: INFO: Created: latency-svc-xtkcv
    Jan  5 09:52:48.784: INFO: Got endpoints: latency-svc-xtkcv [65.746634ms]
    Jan  5 09:52:48.788: INFO: Created: latency-svc-jlfcf
    Jan  5 09:52:48.802: INFO: Created: latency-svc-wfqzq
    Jan  5 09:52:48.803: INFO: Got endpoints: latency-svc-jlfcf [84.425297ms]
    Jan  5 09:52:48.807: INFO: Created: latency-svc-9rldd
    Jan  5 09:52:48.808: INFO: Got endpoints: latency-svc-wfqzq [89.450301ms]
    Jan  5 09:52:48.810: INFO: Got endpoints: latency-svc-9rldd [91.694272ms]
    Jan  5 09:52:48.817: INFO: Created: latency-svc-zsrgz
    Jan  5 09:52:48.825: INFO: Got endpoints: latency-svc-zsrgz [107.449084ms]
    Jan  5 09:52:48.828: INFO: Created: latency-svc-dqdqs
    Jan  5 09:52:48.837: INFO: Created: latency-svc-bz4sk
    Jan  5 09:52:48.837: INFO: Got endpoints: latency-svc-dqdqs [118.930036ms]
    Jan  5 09:52:48.849: INFO: Created: latency-svc-5t8x2
    Jan  5 09:52:48.849: INFO: Got endpoints: latency-svc-bz4sk [131.063856ms]
    Jan  5 09:52:48.855: INFO: Created: latency-svc-fw2sg
    Jan  5 09:52:48.857: INFO: Got endpoints: latency-svc-5t8x2 [138.782231ms]
    Jan  5 09:52:48.863: INFO: Got endpoints: latency-svc-fw2sg [144.840204ms]
    Jan  5 09:52:48.863: INFO: Created: latency-svc-k2z5s
    Jan  5 09:52:48.870: INFO: Created: latency-svc-cxw2t
    Jan  5 09:52:48.870: INFO: Got endpoints: latency-svc-k2z5s [131.889557ms]
    Jan  5 09:52:48.878: INFO: Created: latency-svc-gqfs7
    Jan  5 09:52:48.878: INFO: Got endpoints: latency-svc-cxw2t [130.554314ms]
    Jan  5 09:52:48.884: INFO: Created: latency-svc-qfwcv
    Jan  5 09:52:48.884: INFO: Got endpoints: latency-svc-gqfs7 [127.406106ms]
    Jan  5 09:52:48.893: INFO: Created: latency-svc-qjr8p
    Jan  5 09:52:48.900: INFO: Got endpoints: latency-svc-qfwcv [134.503139ms]
    Jan  5 09:52:48.900: INFO: Got endpoints: latency-svc-qjr8p [127.678895ms]
    Jan  5 09:52:48.919: INFO: Created: latency-svc-4pp7g
    Jan  5 09:52:48.925: INFO: Created: latency-svc-xkjm8
    Jan  5 09:52:48.925: INFO: Got endpoints: latency-svc-4pp7g [146.937532ms]
    Jan  5 09:52:48.937: INFO: Created: latency-svc-p5dj5
    Jan  5 09:52:48.937: INFO: Got endpoints: latency-svc-xkjm8 [153.167346ms]
    Jan  5 09:52:48.943: INFO: Got endpoints: latency-svc-p5dj5 [140.653916ms]
    Jan  5 09:52:48.944: INFO: Created: latency-svc-wtwdx
    Jan  5 09:52:48.949: INFO: Got endpoints: latency-svc-wtwdx [141.177915ms]
    Jan  5 09:52:48.950: INFO: Created: latency-svc-9txv6
    Jan  5 09:52:48.956: INFO: Created: latency-svc-8bvdd
    Jan  5 09:52:48.957: INFO: Got endpoints: latency-svc-9txv6 [147.063317ms]
    Jan  5 09:52:48.964: INFO: Got endpoints: latency-svc-8bvdd [139.011499ms]
    Jan  5 09:52:48.965: INFO: Created: latency-svc-cvnr9
    Jan  5 09:52:48.974: INFO: Got endpoints: latency-svc-cvnr9 [136.920403ms]
    Jan  5 09:52:48.979: INFO: Created: latency-svc-9kclm
    Jan  5 09:52:48.980: INFO: Created: latency-svc-j4kz6
    Jan  5 09:52:48.987: INFO: Got endpoints: latency-svc-9kclm [137.686013ms]
    Jan  5 09:52:48.987: INFO: Got endpoints: latency-svc-j4kz6 [129.890706ms]
    Jan  5 09:52:48.989: INFO: Created: latency-svc-xqrkz
    Jan  5 09:52:48.996: INFO: Got endpoints: latency-svc-xqrkz [132.387929ms]
    Jan  5 09:52:48.997: INFO: Created: latency-svc-fgtb8
    Jan  5 09:52:49.004: INFO: Got endpoints: latency-svc-fgtb8 [133.958603ms]
    Jan  5 09:52:49.006: INFO: Created: latency-svc-mhhpk
    Jan  5 09:52:49.012: INFO: Created: latency-svc-vg6c4
    Jan  5 09:52:49.013: INFO: Got endpoints: latency-svc-mhhpk [134.565175ms]
    Jan  5 09:52:49.044: INFO: Got endpoints: latency-svc-vg6c4 [159.731383ms]
    Jan  5 09:52:49.048: INFO: Created: latency-svc-5cnmn
    Jan  5 09:52:49.052: INFO: Got endpoints: latency-svc-5cnmn [152.118547ms]
    Jan  5 09:52:49.055: INFO: Created: latency-svc-p9kt7
    Jan  5 09:52:49.062: INFO: Got endpoints: latency-svc-p9kt7 [161.575848ms]
    Jan  5 09:52:49.063: INFO: Created: latency-svc-4xs2z
    Jan  5 09:52:49.068: INFO: Created: latency-svc-s9fg2
    Jan  5 09:52:49.073: INFO: Got endpoints: latency-svc-4xs2z [147.68144ms]
    Jan  5 09:52:49.077: INFO: Created: latency-svc-rwdtp
    Jan  5 09:52:49.083: INFO: Created: latency-svc-9fd6h
    Jan  5 09:52:49.087: INFO: Created: latency-svc-nlcfb
    Jan  5 09:52:49.097: INFO: Created: latency-svc-qbfv6
    Jan  5 09:52:49.107: INFO: Created: latency-svc-hj96l
    Jan  5 09:52:49.115: INFO: Created: latency-svc-gwgjp
    Jan  5 09:52:49.121: INFO: Got endpoints: latency-svc-s9fg2 [183.463084ms]
    Jan  5 09:52:49.126: INFO: Created: latency-svc-b68gv
    Jan  5 09:52:49.133: INFO: Created: latency-svc-rhblf
    Jan  5 09:52:49.156: INFO: Created: latency-svc-kzx56
    Jan  5 09:52:49.161: INFO: Created: latency-svc-5bjk4
    Jan  5 09:52:49.169: INFO: Created: latency-svc-q7r8r
    Jan  5 09:52:49.170: INFO: Got endpoints: latency-svc-rwdtp [226.182064ms]
    Jan  5 09:52:49.182: INFO: Created: latency-svc-z5bt4
    Jan  5 09:52:49.191: INFO: Created: latency-svc-jd7q4
    Jan  5 09:52:49.195: INFO: Created: latency-svc-pwqj9
    Jan  5 09:52:49.208: INFO: Created: latency-svc-wwlns
    Jan  5 09:52:49.212: INFO: Created: latency-svc-86g7v
    Jan  5 09:52:49.221: INFO: Got endpoints: latency-svc-9fd6h [272.202988ms]
    Jan  5 09:52:49.232: INFO: Created: latency-svc-czx6j
    Jan  5 09:52:49.269: INFO: Got endpoints: latency-svc-nlcfb [311.728875ms]
    Jan  5 09:52:49.285: INFO: Created: latency-svc-5797f
    Jan  5 09:52:49.321: INFO: Got endpoints: latency-svc-qbfv6 [356.412233ms]
    Jan  5 09:52:49.335: INFO: Created: latency-svc-hs4n9
    Jan  5 09:52:49.376: INFO: Got endpoints: latency-svc-hj96l [401.690332ms]
    Jan  5 09:52:49.388: INFO: Created: latency-svc-kqnls
    Jan  5 09:52:49.419: INFO: Got endpoints: latency-svc-gwgjp [431.908766ms]
    Jan  5 09:52:49.432: INFO: Created: latency-svc-h6hmd
    Jan  5 09:52:49.469: INFO: Got endpoints: latency-svc-b68gv [481.470825ms]
    Jan  5 09:52:49.494: INFO: Created: latency-svc-p655g
    Jan  5 09:52:49.519: INFO: Got endpoints: latency-svc-rhblf [523.548958ms]
    Jan  5 09:52:49.557: INFO: Created: latency-svc-n5tbh
    Jan  5 09:52:49.570: INFO: Got endpoints: latency-svc-kzx56 [565.885102ms]
    Jan  5 09:52:49.584: INFO: Created: latency-svc-tjh5q
    Jan  5 09:52:49.620: INFO: Got endpoints: latency-svc-5bjk4 [606.630987ms]
    Jan  5 09:52:49.633: INFO: Created: latency-svc-dn9tj
    Jan  5 09:52:49.671: INFO: Got endpoints: latency-svc-q7r8r [627.532251ms]
    Jan  5 09:52:49.684: INFO: Created: latency-svc-cb4bv
    Jan  5 09:52:49.723: INFO: Got endpoints: latency-svc-z5bt4 [671.382313ms]
    Jan  5 09:52:49.738: INFO: Created: latency-svc-8pq6s
    Jan  5 09:52:49.771: INFO: Got endpoints: latency-svc-jd7q4 [709.213434ms]
    Jan  5 09:52:49.786: INFO: Created: latency-svc-hqljb
    Jan  5 09:52:49.819: INFO: Got endpoints: latency-svc-pwqj9 [746.521338ms]
    Jan  5 09:52:49.833: INFO: Created: latency-svc-nf8f7
    Jan  5 09:52:49.869: INFO: Got endpoints: latency-svc-wwlns [748.59467ms]
    Jan  5 09:52:49.882: INFO: Created: latency-svc-9jd2j
    Jan  5 09:52:49.921: INFO: Got endpoints: latency-svc-86g7v [751.069031ms]
    Jan  5 09:52:49.931: INFO: Created: latency-svc-8lg99
    Jan  5 09:52:49.970: INFO: Got endpoints: latency-svc-czx6j [748.924705ms]
    Jan  5 09:52:49.983: INFO: Created: latency-svc-6t78d
    Jan  5 09:52:50.022: INFO: Got endpoints: latency-svc-5797f [753.147662ms]
    Jan  5 09:52:50.034: INFO: Created: latency-svc-4mnq4
    Jan  5 09:52:50.068: INFO: Got endpoints: latency-svc-hs4n9 [746.953676ms]
    Jan  5 09:52:50.079: INFO: Created: latency-svc-dl9f4
    Jan  5 09:52:50.125: INFO: Got endpoints: latency-svc-kqnls [749.276513ms]
    Jan  5 09:52:50.137: INFO: Created: latency-svc-hs28d
    Jan  5 09:52:50.169: INFO: Got endpoints: latency-svc-h6hmd [750.078361ms]
    Jan  5 09:52:50.181: INFO: Created: latency-svc-dfccs
    Jan  5 09:52:50.221: INFO: Got endpoints: latency-svc-p655g [752.024415ms]
    Jan  5 09:52:50.260: INFO: Created: latency-svc-q9nww
    Jan  5 09:52:50.272: INFO: Got endpoints: latency-svc-n5tbh [752.670371ms]
    Jan  5 09:52:50.285: INFO: Created: latency-svc-tjxsg
    Jan  5 09:52:50.320: INFO: Got endpoints: latency-svc-tjh5q [749.479501ms]
    Jan  5 09:52:50.332: INFO: Created: latency-svc-ql2gg
    Jan  5 09:52:50.369: INFO: Got endpoints: latency-svc-dn9tj [749.614173ms]
    Jan  5 09:52:50.384: INFO: Created: latency-svc-sq8t8
    Jan  5 09:52:50.420: INFO: Got endpoints: latency-svc-cb4bv [748.188481ms]
    Jan  5 09:52:50.433: INFO: Created: latency-svc-ft46d
    Jan  5 09:52:50.470: INFO: Got endpoints: latency-svc-8pq6s [746.976167ms]
    Jan  5 09:52:50.482: INFO: Created: latency-svc-n5kts
    Jan  5 09:52:50.520: INFO: Got endpoints: latency-svc-hqljb [749.426843ms]
    Jan  5 09:52:50.536: INFO: Created: latency-svc-lsnq7
    Jan  5 09:52:50.590: INFO: Got endpoints: latency-svc-nf8f7 [770.465054ms]
    Jan  5 09:52:50.613: INFO: Created: latency-svc-hqqht
    Jan  5 09:52:50.619: INFO: Got endpoints: latency-svc-9jd2j [749.147182ms]
    Jan  5 09:52:50.637: INFO: Created: latency-svc-7d5kj
    Jan  5 09:52:50.669: INFO: Got endpoints: latency-svc-8lg99 [747.835161ms]
    Jan  5 09:52:50.684: INFO: Created: latency-svc-b6m5s
    Jan  5 09:52:50.719: INFO: Got endpoints: latency-svc-6t78d [748.569493ms]
    Jan  5 09:52:50.733: INFO: Created: latency-svc-grssw
    Jan  5 09:52:50.770: INFO: Got endpoints: latency-svc-4mnq4 [747.345698ms]
    Jan  5 09:52:50.781: INFO: Created: latency-svc-sbctg
    Jan  5 09:52:50.822: INFO: Got endpoints: latency-svc-dl9f4 [753.898153ms]
    Jan  5 09:52:50.835: INFO: Created: latency-svc-knl4r
    Jan  5 09:52:50.870: INFO: Got endpoints: latency-svc-hs28d [745.277737ms]
    Jan  5 09:52:50.883: INFO: Created: latency-svc-zlnqg
    Jan  5 09:52:50.926: INFO: Got endpoints: latency-svc-dfccs [756.603135ms]
    Jan  5 09:52:50.938: INFO: Created: latency-svc-5qhnv
    Jan  5 09:52:50.974: INFO: Got endpoints: latency-svc-q9nww [753.616209ms]
    Jan  5 09:52:50.987: INFO: Created: latency-svc-hd94g
    Jan  5 09:52:51.020: INFO: Got endpoints: latency-svc-tjxsg [748.383674ms]
    Jan  5 09:52:51.034: INFO: Created: latency-svc-9md6d
    Jan  5 09:52:51.071: INFO: Got endpoints: latency-svc-ql2gg [750.788327ms]
    Jan  5 09:52:51.084: INFO: Created: latency-svc-wgjxd
    Jan  5 09:52:51.124: INFO: Got endpoints: latency-svc-sq8t8 [755.116338ms]
    Jan  5 09:52:51.137: INFO: Created: latency-svc-tqzm7
    Jan  5 09:52:51.171: INFO: Got endpoints: latency-svc-ft46d [750.847728ms]
    Jan  5 09:52:51.182: INFO: Created: latency-svc-4pzlz
    Jan  5 09:52:51.220: INFO: Got endpoints: latency-svc-n5kts [749.492958ms]
    Jan  5 09:52:51.238: INFO: Created: latency-svc-tpwml
    Jan  5 09:52:51.272: INFO: Got endpoints: latency-svc-lsnq7 [751.734574ms]
    Jan  5 09:52:51.285: INFO: Created: latency-svc-v5hts
    Jan  5 09:52:51.319: INFO: Got endpoints: latency-svc-hqqht [728.643179ms]
    Jan  5 09:52:51.337: INFO: Created: latency-svc-m2v98
    Jan  5 09:52:51.369: INFO: Got endpoints: latency-svc-7d5kj [750.376487ms]
    Jan  5 09:52:51.383: INFO: Created: latency-svc-7gbd4
    Jan  5 09:52:51.424: INFO: Got endpoints: latency-svc-b6m5s [755.645307ms]
    Jan  5 09:52:51.448: INFO: Created: latency-svc-bgvzr
    Jan  5 09:52:51.468: INFO: Got endpoints: latency-svc-grssw [748.995439ms]
    Jan  5 09:52:51.480: INFO: Created: latency-svc-2ppsl
    Jan  5 09:52:51.523: INFO: Got endpoints: latency-svc-sbctg [753.615657ms]
    Jan  5 09:52:51.536: INFO: Created: latency-svc-b8fbs
    Jan  5 09:52:51.569: INFO: Got endpoints: latency-svc-knl4r [747.081374ms]
    Jan  5 09:52:51.582: INFO: Created: latency-svc-pptsg
    Jan  5 09:52:51.621: INFO: Got endpoints: latency-svc-zlnqg [750.017266ms]
    Jan  5 09:52:51.636: INFO: Created: latency-svc-bnwdl
    Jan  5 09:52:51.670: INFO: Got endpoints: latency-svc-5qhnv [744.062596ms]
    Jan  5 09:52:51.684: INFO: Created: latency-svc-pmrlc
    Jan  5 09:52:51.720: INFO: Got endpoints: latency-svc-hd94g [745.331466ms]
    Jan  5 09:52:51.735: INFO: Created: latency-svc-ql7x8
    Jan  5 09:52:51.769: INFO: Got endpoints: latency-svc-9md6d [748.458405ms]
    Jan  5 09:52:51.784: INFO: Created: latency-svc-jbjsf
    Jan  5 09:52:51.821: INFO: Got endpoints: latency-svc-wgjxd [750.590718ms]
    Jan  5 09:52:51.835: INFO: Created: latency-svc-fjjhh
    Jan  5 09:52:51.897: INFO: Got endpoints: latency-svc-tqzm7 [772.464619ms]
    Jan  5 09:52:51.909: INFO: Created: latency-svc-fkrx5
    Jan  5 09:52:51.920: INFO: Got endpoints: latency-svc-4pzlz [748.595281ms]
    Jan  5 09:52:51.935: INFO: Created: latency-svc-7phlw
    Jan  5 09:52:51.970: INFO: Got endpoints: latency-svc-tpwml [749.535317ms]
    Jan  5 09:52:51.986: INFO: Created: latency-svc-4vkw8
    Jan  5 09:52:52.021: INFO: Got endpoints: latency-svc-v5hts [748.709405ms]
    Jan  5 09:52:52.044: INFO: Created: latency-svc-fmbkt
    Jan  5 09:52:52.071: INFO: Got endpoints: latency-svc-m2v98 [751.919961ms]
    Jan  5 09:52:52.084: INFO: Created: latency-svc-fzd5h
    Jan  5 09:52:52.123: INFO: Got endpoints: latency-svc-7gbd4 [753.440299ms]
    Jan  5 09:52:52.137: INFO: Created: latency-svc-9zl87
    Jan  5 09:52:52.168: INFO: Got endpoints: latency-svc-bgvzr [743.93101ms]
    Jan  5 09:52:52.183: INFO: Created: latency-svc-9b75f
    Jan  5 09:52:52.223: INFO: Got endpoints: latency-svc-2ppsl [754.825196ms]
    Jan  5 09:52:52.234: INFO: Created: latency-svc-b49qc
    Jan  5 09:52:52.268: INFO: Got endpoints: latency-svc-b8fbs [744.740673ms]
    Jan  5 09:52:52.280: INFO: Created: latency-svc-n2mqn
    Jan  5 09:52:52.323: INFO: Got endpoints: latency-svc-pptsg [754.062722ms]
    Jan  5 09:52:52.339: INFO: Created: latency-svc-4lns7
    Jan  5 09:52:52.371: INFO: Got endpoints: latency-svc-bnwdl [750.457099ms]
    Jan  5 09:52:52.424: INFO: Got endpoints: latency-svc-pmrlc [753.773743ms]
    Jan  5 09:52:52.424: INFO: Created: latency-svc-jxkbg
    Jan  5 09:52:52.443: INFO: Created: latency-svc-dxm6h
    Jan  5 09:52:52.470: INFO: Got endpoints: latency-svc-ql7x8 [749.428047ms]
    Jan  5 09:52:52.482: INFO: Created: latency-svc-d4tgq
    Jan  5 09:52:52.518: INFO: Got endpoints: latency-svc-jbjsf [749.179032ms]
    Jan  5 09:52:52.531: INFO: Created: latency-svc-n9ds5
    Jan  5 09:52:52.568: INFO: Got endpoints: latency-svc-fjjhh [746.536237ms]
    Jan  5 09:52:52.598: INFO: Created: latency-svc-p9zhs
    Jan  5 09:52:52.622: INFO: Got endpoints: latency-svc-fkrx5 [725.309224ms]
    Jan  5 09:52:52.636: INFO: Created: latency-svc-tdp8s
    Jan  5 09:52:52.670: INFO: Got endpoints: latency-svc-7phlw [750.643828ms]
    Jan  5 09:52:52.685: INFO: Created: latency-svc-d8xrc
    Jan  5 09:52:52.718: INFO: Got endpoints: latency-svc-4vkw8 [748.589071ms]
    Jan  5 09:52:52.731: INFO: Created: latency-svc-xnp2z
    Jan  5 09:52:52.771: INFO: Got endpoints: latency-svc-fmbkt [750.418207ms]
    Jan  5 09:52:52.793: INFO: Created: latency-svc-v8fc2
    Jan  5 09:52:52.832: INFO: Got endpoints: latency-svc-fzd5h [761.574931ms]
    Jan  5 09:52:52.845: INFO: Created: latency-svc-9qg8k
    Jan  5 09:52:52.876: INFO: Got endpoints: latency-svc-9zl87 [753.781818ms]
    Jan  5 09:52:52.889: INFO: Created: latency-svc-6z6m4
    Jan  5 09:52:52.920: INFO: Got endpoints: latency-svc-9b75f [751.320842ms]
    Jan  5 09:52:52.933: INFO: Created: latency-svc-gx9lr
    Jan  5 09:52:52.984: INFO: Got endpoints: latency-svc-b49qc [760.622282ms]
    Jan  5 09:52:52.996: INFO: Created: latency-svc-6p6rf
    Jan  5 09:52:53.019: INFO: Got endpoints: latency-svc-n2mqn [750.672361ms]
    Jan  5 09:52:53.032: INFO: Created: latency-svc-74n5r
    Jan  5 09:52:53.068: INFO: Got endpoints: latency-svc-4lns7 [744.914368ms]
    Jan  5 09:52:53.082: INFO: Created: latency-svc-rz9sx
    Jan  5 09:52:53.118: INFO: Got endpoints: latency-svc-jxkbg [746.904034ms]
    Jan  5 09:52:53.132: INFO: Created: latency-svc-x4gnj
    Jan  5 09:52:53.170: INFO: Got endpoints: latency-svc-dxm6h [745.495323ms]
    Jan  5 09:52:53.183: INFO: Created: latency-svc-kbf8l
    Jan  5 09:52:53.218: INFO: Got endpoints: latency-svc-d4tgq [748.787611ms]
    Jan  5 09:52:53.233: INFO: Created: latency-svc-hz8m4
    Jan  5 09:52:53.271: INFO: Got endpoints: latency-svc-n9ds5 [752.79256ms]
    Jan  5 09:52:53.284: INFO: Created: latency-svc-5xsnp
    Jan  5 09:52:53.322: INFO: Got endpoints: latency-svc-p9zhs [753.350893ms]
    Jan  5 09:52:53.333: INFO: Created: latency-svc-p5wjd
    Jan  5 09:52:53.370: INFO: Got endpoints: latency-svc-tdp8s [747.256781ms]
    Jan  5 09:52:53.381: INFO: Created: latency-svc-qlkjc
    Jan  5 09:52:53.420: INFO: Got endpoints: latency-svc-d8xrc [749.736963ms]
    Jan  5 09:52:53.432: INFO: Created: latency-svc-4cl88
    Jan  5 09:52:53.471: INFO: Got endpoints: latency-svc-xnp2z [752.511767ms]
    Jan  5 09:52:53.485: INFO: Created: latency-svc-wwgxg
    Jan  5 09:52:53.533: INFO: Got endpoints: latency-svc-v8fc2 [758.39805ms]
    Jan  5 09:52:53.546: INFO: Created: latency-svc-rsmx8
    Jan  5 09:52:53.568: INFO: Got endpoints: latency-svc-9qg8k [735.150053ms]
    Jan  5 09:52:53.605: INFO: Created: latency-svc-7856w
    Jan  5 09:52:53.619: INFO: Got endpoints: latency-svc-6z6m4 [742.134497ms]
    Jan  5 09:52:53.631: INFO: Created: latency-svc-qkp9j
    Jan  5 09:52:53.670: INFO: Got endpoints: latency-svc-gx9lr [749.731244ms]
    Jan  5 09:52:53.683: INFO: Created: latency-svc-hvtr5
    Jan  5 09:52:53.719: INFO: Got endpoints: latency-svc-6p6rf [735.437019ms]
    Jan  5 09:52:53.731: INFO: Created: latency-svc-tpxnp
    Jan  5 09:52:53.770: INFO: Got endpoints: latency-svc-74n5r [751.450084ms]
    Jan  5 09:52:53.782: INFO: Created: latency-svc-pnrfq
    Jan  5 09:52:53.819: INFO: Got endpoints: latency-svc-rz9sx [750.608252ms]
    Jan  5 09:52:53.831: INFO: Created: latency-svc-55vvd
    Jan  5 09:52:53.869: INFO: Got endpoints: latency-svc-x4gnj [750.623498ms]
    Jan  5 09:52:53.883: INFO: Created: latency-svc-7k4qg
    Jan  5 09:52:53.919: INFO: Got endpoints: latency-svc-kbf8l [749.788609ms]
    Jan  5 09:52:53.931: INFO: Created: latency-svc-stdx5
    Jan  5 09:52:53.999: INFO: Got endpoints: latency-svc-hz8m4 [780.672569ms]
    Jan  5 09:52:54.013: INFO: Created: latency-svc-86bk4
    Jan  5 09:52:54.022: INFO: Got endpoints: latency-svc-5xsnp [750.937287ms]
    Jan  5 09:52:54.033: INFO: Created: latency-svc-hwhsm
    Jan  5 09:52:54.069: INFO: Got endpoints: latency-svc-p5wjd [747.59278ms]
    Jan  5 09:52:54.086: INFO: Created: latency-svc-jdjlz
    Jan  5 09:52:54.120: INFO: Got endpoints: latency-svc-qlkjc [749.849044ms]
    Jan  5 09:52:54.136: INFO: Created: latency-svc-s8dkm
    Jan  5 09:52:54.169: INFO: Got endpoints: latency-svc-4cl88 [749.384796ms]
    Jan  5 09:52:54.182: INFO: Created: latency-svc-ktf4f
    Jan  5 09:52:54.220: INFO: Got endpoints: latency-svc-wwgxg [749.238494ms]
    Jan  5 09:52:54.238: INFO: Created: latency-svc-xmt9t
    Jan  5 09:52:54.272: INFO: Got endpoints: latency-svc-rsmx8 [738.544533ms]
    Jan  5 09:52:54.285: INFO: Created: latency-svc-wnst5
    Jan  5 09:52:54.323: INFO: Got endpoints: latency-svc-7856w [754.770475ms]
    Jan  5 09:52:54.335: INFO: Created: latency-svc-f2b5q
    Jan  5 09:52:54.368: INFO: Got endpoints: latency-svc-qkp9j [749.823477ms]
    Jan  5 09:52:54.382: INFO: Created: latency-svc-hpkmm
    Jan  5 09:52:54.422: INFO: Got endpoints: latency-svc-hvtr5 [752.174797ms]
    Jan  5 09:52:54.437: INFO: Created: latency-svc-pxk4p
    Jan  5 09:52:54.470: INFO: Got endpoints: latency-svc-tpxnp [750.196923ms]
    Jan  5 09:52:54.483: INFO: Created: latency-svc-vrvrf
    Jan  5 09:52:54.523: INFO: Got endpoints: latency-svc-pnrfq [752.345456ms]
    Jan  5 09:52:54.542: INFO: Created: latency-svc-vvf62
    Jan  5 09:52:54.573: INFO: Got endpoints: latency-svc-55vvd [753.714714ms]
    Jan  5 09:52:54.586: INFO: Created: latency-svc-9gbxg
    Jan  5 09:52:54.619: INFO: Got endpoints: latency-svc-7k4qg [750.302592ms]
    Jan  5 09:52:54.633: INFO: Created: latency-svc-fnxzt
    Jan  5 09:52:54.672: INFO: Got endpoints: latency-svc-stdx5 [752.409476ms]
    Jan  5 09:52:54.684: INFO: Created: latency-svc-79hx4
    Jan  5 09:52:54.722: INFO: Got endpoints: latency-svc-86bk4 [722.711635ms]
    Jan  5 09:52:54.736: INFO: Created: latency-svc-rplwn
    Jan  5 09:52:54.771: INFO: Got endpoints: latency-svc-hwhsm [748.505405ms]
    Jan  5 09:52:54.783: INFO: Created: latency-svc-bl4lx
    Jan  5 09:52:54.821: INFO: Got endpoints: latency-svc-jdjlz [751.451958ms]
    Jan  5 09:52:54.833: INFO: Created: latency-svc-rng42
    Jan  5 09:52:54.871: INFO: Got endpoints: latency-svc-s8dkm [751.543058ms]
    Jan  5 09:52:54.890: INFO: Created: latency-svc-hlw2k
    Jan  5 09:52:54.920: INFO: Got endpoints: latency-svc-ktf4f [750.658186ms]
    Jan  5 09:52:54.932: INFO: Created: latency-svc-r5hrg
    Jan  5 09:52:54.973: INFO: Got endpoints: latency-svc-xmt9t [752.913688ms]
    Jan  5 09:52:54.991: INFO: Created: latency-svc-xvd8p
    Jan  5 09:52:55.020: INFO: Got endpoints: latency-svc-wnst5 [747.633476ms]
    Jan  5 09:52:55.034: INFO: Created: latency-svc-5ppcd
    Jan  5 09:52:55.070: INFO: Got endpoints: latency-svc-f2b5q [746.867136ms]
    Jan  5 09:52:55.085: INFO: Created: latency-svc-89z2r
    Jan  5 09:52:55.119: INFO: Got endpoints: latency-svc-hpkmm [750.905047ms]
    Jan  5 09:52:55.132: INFO: Created: latency-svc-7l8p6
    Jan  5 09:52:55.170: INFO: Got endpoints: latency-svc-pxk4p [747.766365ms]
    Jan  5 09:52:55.189: INFO: Created: latency-svc-kj2lp
    Jan  5 09:52:55.221: INFO: Got endpoints: latency-svc-vrvrf [750.845886ms]
    Jan  5 09:52:55.234: INFO: Created: latency-svc-wwzpf
    Jan  5 09:52:55.269: INFO: Got endpoints: latency-svc-vvf62 [746.723158ms]
    Jan  5 09:52:55.281: INFO: Created: latency-svc-4vqvx
    Jan  5 09:52:55.320: INFO: Got endpoints: latency-svc-9gbxg [746.87501ms]
    Jan  5 09:52:55.336: INFO: Created: latency-svc-t9cd5
    Jan  5 09:52:55.369: INFO: Got endpoints: latency-svc-fnxzt [750.047895ms]
    Jan  5 09:52:55.385: INFO: Created: latency-svc-jgc42
    Jan  5 09:52:55.425: INFO: Got endpoints: latency-svc-79hx4 [753.368466ms]
    Jan  5 09:52:55.438: INFO: Created: latency-svc-z64fz
    Jan  5 09:52:55.470: INFO: Got endpoints: latency-svc-rplwn [747.782073ms]
    Jan  5 09:52:55.484: INFO: Created: latency-svc-qxwnt
    Jan  5 09:52:55.521: INFO: Got endpoints: latency-svc-bl4lx [750.33352ms]
    Jan  5 09:52:55.538: INFO: Created: latency-svc-7nfwb
    Jan  5 09:52:55.571: INFO: Got endpoints: latency-svc-rng42 [750.342577ms]
    Jan  5 09:52:55.584: INFO: Created: latency-svc-jhldw
    Jan  5 09:52:55.622: INFO: Got endpoints: latency-svc-hlw2k [750.246047ms]
    Jan  5 09:52:55.641: INFO: Created: latency-svc-m486g
    Jan  5 09:52:55.668: INFO: Got endpoints: latency-svc-r5hrg [747.962533ms]
    Jan  5 09:52:55.692: INFO: Created: latency-svc-9n5vk
    Jan  5 09:52:55.718: INFO: Got endpoints: latency-svc-xvd8p [745.170868ms]
    Jan  5 09:52:55.730: INFO: Created: latency-svc-pl4r8
    Jan  5 09:52:55.772: INFO: Got endpoints: latency-svc-5ppcd [751.573956ms]
    Jan  5 09:52:55.787: INFO: Created: latency-svc-swglw
    Jan  5 09:52:55.821: INFO: Got endpoints: latency-svc-89z2r [751.446758ms]
    Jan  5 09:52:55.833: INFO: Created: latency-svc-59pns
    Jan  5 09:52:55.873: INFO: Got endpoints: latency-svc-7l8p6 [753.660635ms]
    Jan  5 09:52:55.888: INFO: Created: latency-svc-pcr8c
    Jan  5 09:52:55.920: INFO: Got endpoints: latency-svc-kj2lp [750.337286ms]
    Jan  5 09:52:55.933: INFO: Created: latency-svc-pmxc2
    Jan  5 09:52:55.968: INFO: Got endpoints: latency-svc-wwzpf [747.776194ms]
    Jan  5 09:52:55.981: INFO: Created: latency-svc-nn72c
    Jan  5 09:52:56.021: INFO: Got endpoints: latency-svc-4vqvx [751.344015ms]
    Jan  5 09:52:56.033: INFO: Created: latency-svc-5rnkw
    Jan  5 09:52:56.071: INFO: Got endpoints: latency-svc-t9cd5 [750.91742ms]
    Jan  5 09:52:56.086: INFO: Created: latency-svc-89dsz
    Jan  5 09:52:56.119: INFO: Got endpoints: latency-svc-jgc42 [749.991952ms]
    Jan  5 09:52:56.132: INFO: Created: latency-svc-llwhq
    Jan  5 09:52:56.174: INFO: Got endpoints: latency-svc-z64fz [748.646108ms]
    Jan  5 09:52:56.188: INFO: Created: latency-svc-jhg27
    Jan  5 09:52:56.219: INFO: Got endpoints: latency-svc-qxwnt [749.394336ms]
    Jan  5 09:52:56.231: INFO: Created: latency-svc-8xrgm
    Jan  5 09:52:56.272: INFO: Got endpoints: latency-svc-7nfwb [751.163259ms]
    Jan  5 09:52:56.285: INFO: Created: latency-svc-rkghc
    Jan  5 09:52:56.321: INFO: Got endpoints: latency-svc-jhldw [749.988405ms]
    Jan  5 09:52:56.332: INFO: Created: latency-svc-mjc9h
    Jan  5 09:52:56.372: INFO: Got endpoints: latency-svc-m486g [749.961665ms]
    Jan  5 09:52:56.389: INFO: Created: latency-svc-hg4bz
    Jan  5 09:52:56.419: INFO: Got endpoints: latency-svc-9n5vk [751.104688ms]
    Jan  5 09:52:56.439: INFO: Created: latency-svc-9x6kb
    Jan  5 09:52:56.469: INFO: Got endpoints: latency-svc-pl4r8 [750.609584ms]
    Jan  5 09:52:56.488: INFO: Created: latency-svc-nfgv6
    Jan  5 09:52:56.518: INFO: Got endpoints: latency-svc-swglw [746.270302ms]
    Jan  5 09:52:56.533: INFO: Created: latency-svc-z2wv6
    Jan  5 09:52:56.571: INFO: Got endpoints: latency-svc-59pns [749.66964ms]
    Jan  5 09:52:56.620: INFO: Got endpoints: latency-svc-pcr8c [747.08336ms]
    Jan  5 09:52:56.669: INFO: Got endpoints: latency-svc-pmxc2 [748.907507ms]
    Jan  5 09:52:56.719: INFO: Got endpoints: latency-svc-nn72c [750.834164ms]
    Jan  5 09:52:56.769: INFO: Got endpoints: latency-svc-5rnkw [747.678562ms]
    Jan  5 09:52:56.821: INFO: Got endpoints: latency-svc-89dsz [750.703211ms]
    Jan  5 09:52:56.869: INFO: Got endpoints: latency-svc-llwhq [749.034393ms]
    Jan  5 09:52:56.920: INFO: Got endpoints: latency-svc-jhg27 [746.281284ms]
    Jan  5 09:52:56.971: INFO: Got endpoints: latency-svc-8xrgm [751.608462ms]
    Jan  5 09:52:57.020: INFO: Got endpoints: latency-svc-rkghc [747.295527ms]
    Jan  5 09:52:57.071: INFO: Got endpoints: latency-svc-mjc9h [750.024654ms]
    Jan  5 09:52:57.122: INFO: Got endpoints: latency-svc-hg4bz [749.973218ms]
    Jan  5 09:52:57.169: INFO: Got endpoints: latency-svc-9x6kb [749.902887ms]
    Jan  5 09:52:57.222: INFO: Got endpoints: latency-svc-nfgv6 [752.816988ms]
    Jan  5 09:52:57.268: INFO: Got endpoints: latency-svc-z2wv6 [750.16623ms]
    Jan  5 09:52:57.268: INFO: Latencies: [20.505876ms 29.84677ms 38.522461ms 47.156766ms 53.968568ms 59.992868ms 65.746634ms 84.425297ms 89.450301ms 91.694272ms 107.449084ms 118.930036ms 127.406106ms 127.678895ms 129.890706ms 130.554314ms 131.063856ms 131.889557ms 132.387929ms 133.958603ms 134.503139ms 134.565175ms 136.920403ms 137.686013ms 138.782231ms 139.011499ms 140.653916ms 141.177915ms 144.840204ms 146.937532ms 147.063317ms 147.68144ms 152.118547ms 153.167346ms 159.731383ms 161.575848ms 183.463084ms 226.182064ms 272.202988ms 311.728875ms 356.412233ms 401.690332ms 431.908766ms 481.470825ms 523.548958ms 565.885102ms 606.630987ms 627.532251ms 671.382313ms 709.213434ms 722.711635ms 725.309224ms 728.643179ms 735.150053ms 735.437019ms 738.544533ms 742.134497ms 743.93101ms 744.062596ms 744.740673ms 744.914368ms 745.170868ms 745.277737ms 745.331466ms 745.495323ms 746.270302ms 746.281284ms 746.521338ms 746.536237ms 746.723158ms 746.867136ms 746.87501ms 746.904034ms 746.953676ms 746.976167ms 747.081374ms 747.08336ms 747.256781ms 747.295527ms 747.345698ms 747.59278ms 747.633476ms 747.678562ms 747.766365ms 747.776194ms 747.782073ms 747.835161ms 747.962533ms 748.188481ms 748.383674ms 748.458405ms 748.505405ms 748.569493ms 748.589071ms 748.59467ms 748.595281ms 748.646108ms 748.709405ms 748.787611ms 748.907507ms 748.924705ms 748.995439ms 749.034393ms 749.147182ms 749.179032ms 749.238494ms 749.276513ms 749.384796ms 749.394336ms 749.426843ms 749.428047ms 749.479501ms 749.492958ms 749.535317ms 749.614173ms 749.66964ms 749.731244ms 749.736963ms 749.788609ms 749.823477ms 749.849044ms 749.902887ms 749.961665ms 749.973218ms 749.988405ms 749.991952ms 750.017266ms 750.024654ms 750.047895ms 750.078361ms 750.16623ms 750.196923ms 750.246047ms 750.302592ms 750.33352ms 750.337286ms 750.342577ms 750.376487ms 750.418207ms 750.457099ms 750.590718ms 750.608252ms 750.609584ms 750.623498ms 750.643828ms 750.658186ms 750.672361ms 750.703211ms 750.788327ms 750.834164ms 750.845886ms 750.847728ms 750.905047ms 750.91742ms 750.937287ms 751.069031ms 751.104688ms 751.163259ms 751.320842ms 751.344015ms 751.446758ms 751.450084ms 751.451958ms 751.543058ms 751.573956ms 751.608462ms 751.734574ms 751.919961ms 752.024415ms 752.174797ms 752.345456ms 752.409476ms 752.511767ms 752.670371ms 752.79256ms 752.816988ms 752.913688ms 753.147662ms 753.350893ms 753.368466ms 753.440299ms 753.615657ms 753.616209ms 753.660635ms 753.714714ms 753.773743ms 753.781818ms 753.898153ms 754.062722ms 754.770475ms 754.825196ms 755.116338ms 755.645307ms 756.603135ms 758.39805ms 760.622282ms 761.574931ms 770.465054ms 772.464619ms 780.672569ms]
    Jan  5 09:52:57.269: INFO: 50 %ile: 748.924705ms
    Jan  5 09:52:57.269: INFO: 90 %ile: 753.440299ms
    Jan  5 09:52:57.269: INFO: 99 %ile: 772.464619ms
    Jan  5 09:52:57.269: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jan  5 09:52:57.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-1" for this suite. 01/05/23 09:52:57.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:52:57.291
Jan  5 09:52:57.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename subpath 01/05/23 09:52:57.292
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:52:57.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:52:57.316
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 09:52:57.322
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-crwx 01/05/23 09:52:57.334
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 09:52:57.334
Jan  5 09:52:57.342: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-crwx" in namespace "subpath-7770" to be "Succeeded or Failed"
Jan  5 09:52:57.349: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.808896ms
Jan  5 09:52:59.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 2.013814267s
Jan  5 09:53:01.355: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 4.012627592s
Jan  5 09:53:03.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 6.013763185s
Jan  5 09:53:05.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 8.014272182s
Jan  5 09:53:07.388: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 10.045890618s
Jan  5 09:53:09.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 12.014328363s
Jan  5 09:53:11.357: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 14.014878885s
Jan  5 09:53:13.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 16.01362139s
Jan  5 09:53:15.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 18.01419507s
Jan  5 09:53:17.354: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 20.012202339s
Jan  5 09:53:19.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=false. Elapsed: 22.013804414s
Jan  5 09:53:21.354: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012098194s
STEP: Saw pod success 01/05/23 09:53:21.354
Jan  5 09:53:21.354: INFO: Pod "pod-subpath-test-configmap-crwx" satisfied condition "Succeeded or Failed"
Jan  5 09:53:21.358: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-subpath-test-configmap-crwx container test-container-subpath-configmap-crwx: <nil>
STEP: delete the pod 01/05/23 09:53:21.369
Jan  5 09:53:21.380: INFO: Waiting for pod pod-subpath-test-configmap-crwx to disappear
Jan  5 09:53:21.384: INFO: Pod pod-subpath-test-configmap-crwx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-crwx 01/05/23 09:53:21.384
Jan  5 09:53:21.384: INFO: Deleting pod "pod-subpath-test-configmap-crwx" in namespace "subpath-7770"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  5 09:53:21.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7770" for this suite. 01/05/23 09:53:21.394
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":199,"skipped":3582,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.108 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:52:57.291
    Jan  5 09:52:57.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename subpath 01/05/23 09:52:57.292
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:52:57.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:52:57.316
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 09:52:57.322
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-crwx 01/05/23 09:52:57.334
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 09:52:57.334
    Jan  5 09:52:57.342: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-crwx" in namespace "subpath-7770" to be "Succeeded or Failed"
    Jan  5 09:52:57.349: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.808896ms
    Jan  5 09:52:59.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 2.013814267s
    Jan  5 09:53:01.355: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 4.012627592s
    Jan  5 09:53:03.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 6.013763185s
    Jan  5 09:53:05.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 8.014272182s
    Jan  5 09:53:07.388: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 10.045890618s
    Jan  5 09:53:09.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 12.014328363s
    Jan  5 09:53:11.357: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 14.014878885s
    Jan  5 09:53:13.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 16.01362139s
    Jan  5 09:53:15.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 18.01419507s
    Jan  5 09:53:17.354: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=true. Elapsed: 20.012202339s
    Jan  5 09:53:19.356: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Running", Reason="", readiness=false. Elapsed: 22.013804414s
    Jan  5 09:53:21.354: INFO: Pod "pod-subpath-test-configmap-crwx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012098194s
    STEP: Saw pod success 01/05/23 09:53:21.354
    Jan  5 09:53:21.354: INFO: Pod "pod-subpath-test-configmap-crwx" satisfied condition "Succeeded or Failed"
    Jan  5 09:53:21.358: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-subpath-test-configmap-crwx container test-container-subpath-configmap-crwx: <nil>
    STEP: delete the pod 01/05/23 09:53:21.369
    Jan  5 09:53:21.380: INFO: Waiting for pod pod-subpath-test-configmap-crwx to disappear
    Jan  5 09:53:21.384: INFO: Pod pod-subpath-test-configmap-crwx no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-crwx 01/05/23 09:53:21.384
    Jan  5 09:53:21.384: INFO: Deleting pod "pod-subpath-test-configmap-crwx" in namespace "subpath-7770"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  5 09:53:21.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7770" for this suite. 01/05/23 09:53:21.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:53:21.403
Jan  5 09:53:21.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename sched-preemption 01/05/23 09:53:21.404
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:53:21.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:53:21.423
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan  5 09:53:21.443: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 09:54:21.526: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:54:21.53
Jan  5 09:54:21.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename sched-preemption-path 01/05/23 09:54:21.531
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:21.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:21.565
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jan  5 09:54:21.603: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan  5 09:54:21.608: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jan  5 09:54:21.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2663" for this suite. 01/05/23 09:54:21.655
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:54:21.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7346" for this suite. 01/05/23 09:54:21.68
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":200,"skipped":3597,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.363 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:53:21.403
    Jan  5 09:53:21.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename sched-preemption 01/05/23 09:53:21.404
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:53:21.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:53:21.423
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan  5 09:53:21.443: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 09:54:21.526: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:54:21.53
    Jan  5 09:54:21.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename sched-preemption-path 01/05/23 09:54:21.531
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:21.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:21.565
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jan  5 09:54:21.603: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan  5 09:54:21.608: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jan  5 09:54:21.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-2663" for this suite. 01/05/23 09:54:21.655
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:54:21.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7346" for this suite. 01/05/23 09:54:21.68
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:54:21.767
Jan  5 09:54:21.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 09:54:21.768
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:21.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:21.83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 01/05/23 09:54:21.835
Jan  5 09:54:21.845: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a" in namespace "downward-api-9796" to be "Succeeded or Failed"
Jan  5 09:54:21.849: INFO: Pod "downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.996834ms
Jan  5 09:54:23.859: INFO: Pod "downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013910062s
Jan  5 09:54:25.854: INFO: Pod "downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008754754s
STEP: Saw pod success 01/05/23 09:54:25.854
Jan  5 09:54:25.854: INFO: Pod "downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a" satisfied condition "Succeeded or Failed"
Jan  5 09:54:25.858: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a container client-container: <nil>
STEP: delete the pod 01/05/23 09:54:25.877
Jan  5 09:54:25.893: INFO: Waiting for pod downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a to disappear
Jan  5 09:54:25.897: INFO: Pod downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 09:54:25.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9796" for this suite. 01/05/23 09:54:25.905
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":201,"skipped":3647,"failed":0}
------------------------------
â€¢ [4.144 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:54:21.767
    Jan  5 09:54:21.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 09:54:21.768
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:21.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:21.83
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 01/05/23 09:54:21.835
    Jan  5 09:54:21.845: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a" in namespace "downward-api-9796" to be "Succeeded or Failed"
    Jan  5 09:54:21.849: INFO: Pod "downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.996834ms
    Jan  5 09:54:23.859: INFO: Pod "downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013910062s
    Jan  5 09:54:25.854: INFO: Pod "downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008754754s
    STEP: Saw pod success 01/05/23 09:54:25.854
    Jan  5 09:54:25.854: INFO: Pod "downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a" satisfied condition "Succeeded or Failed"
    Jan  5 09:54:25.858: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a container client-container: <nil>
    STEP: delete the pod 01/05/23 09:54:25.877
    Jan  5 09:54:25.893: INFO: Waiting for pod downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a to disappear
    Jan  5 09:54:25.897: INFO: Pod downwardapi-volume-84cc2864-35eb-466d-a766-528aefc84c1a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 09:54:25.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9796" for this suite. 01/05/23 09:54:25.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:54:25.914
Jan  5 09:54:25.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename daemonsets 01/05/23 09:54:25.915
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:25.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:25.938
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jan  5 09:54:25.974: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/05/23 09:54:25.98
Jan  5 09:54:25.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:54:25.983: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/05/23 09:54:25.983
Jan  5 09:54:26.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:54:26.016: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 is running 0 daemon pod, expected 1
Jan  5 09:54:27.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:54:27.022: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 is running 0 daemon pod, expected 1
Jan  5 09:54:28.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 09:54:28.023: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/05/23 09:54:28.029
Jan  5 09:54:28.061: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 09:54:28.061: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan  5 09:54:29.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:54:29.069: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/05/23 09:54:29.069
Jan  5 09:54:29.083: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:54:29.083: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 is running 0 daemon pod, expected 1
Jan  5 09:54:30.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:54:30.090: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 is running 0 daemon pod, expected 1
Jan  5 09:54:31.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:54:31.090: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 is running 0 daemon pod, expected 1
Jan  5 09:54:32.091: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 09:54:32.091: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/05/23 09:54:32.099
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4291, will wait for the garbage collector to delete the pods 01/05/23 09:54:32.099
Jan  5 09:54:32.160: INFO: Deleting DaemonSet.extensions daemon-set took: 6.691847ms
Jan  5 09:54:32.261: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.956978ms
Jan  5 09:54:34.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 09:54:34.266: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 09:54:34.270: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28323"},"items":null}

Jan  5 09:54:34.274: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28323"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:54:34.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4291" for this suite. 01/05/23 09:54:34.326
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":202,"skipped":3681,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.417 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:54:25.914
    Jan  5 09:54:25.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename daemonsets 01/05/23 09:54:25.915
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:25.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:25.938
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jan  5 09:54:25.974: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/05/23 09:54:25.98
    Jan  5 09:54:25.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:54:25.983: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/05/23 09:54:25.983
    Jan  5 09:54:26.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:54:26.016: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 is running 0 daemon pod, expected 1
    Jan  5 09:54:27.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:54:27.022: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 is running 0 daemon pod, expected 1
    Jan  5 09:54:28.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 09:54:28.023: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/05/23 09:54:28.029
    Jan  5 09:54:28.061: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 09:54:28.061: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan  5 09:54:29.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:54:29.069: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/05/23 09:54:29.069
    Jan  5 09:54:29.083: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:54:29.083: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 is running 0 daemon pod, expected 1
    Jan  5 09:54:30.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:54:30.090: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 is running 0 daemon pod, expected 1
    Jan  5 09:54:31.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:54:31.090: INFO: Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 is running 0 daemon pod, expected 1
    Jan  5 09:54:32.091: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 09:54:32.091: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 09:54:32.099
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4291, will wait for the garbage collector to delete the pods 01/05/23 09:54:32.099
    Jan  5 09:54:32.160: INFO: Deleting DaemonSet.extensions daemon-set took: 6.691847ms
    Jan  5 09:54:32.261: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.956978ms
    Jan  5 09:54:34.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 09:54:34.266: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 09:54:34.270: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28323"},"items":null}

    Jan  5 09:54:34.274: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28323"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:54:34.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4291" for this suite. 01/05/23 09:54:34.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:54:34.336
Jan  5 09:54:34.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename init-container 01/05/23 09:54:34.337
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:34.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:34.355
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 01/05/23 09:54:34.361
Jan  5 09:54:34.361: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 09:54:37.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5409" for this suite. 01/05/23 09:54:37.381
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":203,"skipped":3705,"failed":0}
------------------------------
â€¢ [3.051 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:54:34.336
    Jan  5 09:54:34.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename init-container 01/05/23 09:54:34.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:34.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:34.355
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 01/05/23 09:54:34.361
    Jan  5 09:54:34.361: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 09:54:37.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5409" for this suite. 01/05/23 09:54:37.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:54:37.387
Jan  5 09:54:37.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 09:54:37.388
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:37.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:37.413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 01/05/23 09:54:37.42
Jan  5 09:54:37.420: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan  5 09:54:37.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 create -f -'
Jan  5 09:54:37.953: INFO: stderr: ""
Jan  5 09:54:37.953: INFO: stdout: "service/agnhost-replica created\n"
Jan  5 09:54:37.953: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan  5 09:54:37.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 create -f -'
Jan  5 09:54:38.157: INFO: stderr: ""
Jan  5 09:54:38.157: INFO: stdout: "service/agnhost-primary created\n"
Jan  5 09:54:38.157: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan  5 09:54:38.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 create -f -'
Jan  5 09:54:38.346: INFO: stderr: ""
Jan  5 09:54:38.346: INFO: stdout: "service/frontend created\n"
Jan  5 09:54:38.347: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan  5 09:54:38.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 create -f -'
Jan  5 09:54:38.510: INFO: stderr: ""
Jan  5 09:54:38.510: INFO: stdout: "deployment.apps/frontend created\n"
Jan  5 09:54:38.510: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  5 09:54:38.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 create -f -'
Jan  5 09:54:38.694: INFO: stderr: ""
Jan  5 09:54:38.694: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan  5 09:54:38.694: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  5 09:54:38.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 create -f -'
Jan  5 09:54:38.867: INFO: stderr: ""
Jan  5 09:54:38.867: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/05/23 09:54:38.867
Jan  5 09:54:38.867: INFO: Waiting for all frontend pods to be Running.
Jan  5 09:54:43.918: INFO: Waiting for frontend to serve content.
Jan  5 09:54:44.029: INFO: Trying to add a new entry to the guestbook.
Jan  5 09:54:44.084: INFO: Verifying that added entry can be retrieved.
Jan  5 09:54:44.101: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources 01/05/23 09:54:49.211
Jan  5 09:54:49.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 delete --grace-period=0 --force -f -'
Jan  5 09:54:49.293: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 09:54:49.293: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 09:54:49.293
Jan  5 09:54:49.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 delete --grace-period=0 --force -f -'
Jan  5 09:54:49.507: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 09:54:49.507: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 09:54:49.507
Jan  5 09:54:49.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 delete --grace-period=0 --force -f -'
Jan  5 09:54:49.580: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 09:54:49.580: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 09:54:49.58
Jan  5 09:54:49.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 delete --grace-period=0 --force -f -'
Jan  5 09:54:49.644: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 09:54:49.644: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 09:54:49.644
Jan  5 09:54:49.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 delete --grace-period=0 --force -f -'
Jan  5 09:54:49.719: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 09:54:49.719: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 09:54:49.719
Jan  5 09:54:49.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 delete --grace-period=0 --force -f -'
Jan  5 09:54:49.794: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 09:54:49.794: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 09:54:49.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3961" for this suite. 01/05/23 09:54:49.803
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":204,"skipped":3725,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.424 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:54:37.387
    Jan  5 09:54:37.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 09:54:37.388
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:37.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:37.413
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 01/05/23 09:54:37.42
    Jan  5 09:54:37.420: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan  5 09:54:37.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 create -f -'
    Jan  5 09:54:37.953: INFO: stderr: ""
    Jan  5 09:54:37.953: INFO: stdout: "service/agnhost-replica created\n"
    Jan  5 09:54:37.953: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan  5 09:54:37.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 create -f -'
    Jan  5 09:54:38.157: INFO: stderr: ""
    Jan  5 09:54:38.157: INFO: stdout: "service/agnhost-primary created\n"
    Jan  5 09:54:38.157: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan  5 09:54:38.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 create -f -'
    Jan  5 09:54:38.346: INFO: stderr: ""
    Jan  5 09:54:38.346: INFO: stdout: "service/frontend created\n"
    Jan  5 09:54:38.347: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan  5 09:54:38.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 create -f -'
    Jan  5 09:54:38.510: INFO: stderr: ""
    Jan  5 09:54:38.510: INFO: stdout: "deployment.apps/frontend created\n"
    Jan  5 09:54:38.510: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan  5 09:54:38.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 create -f -'
    Jan  5 09:54:38.694: INFO: stderr: ""
    Jan  5 09:54:38.694: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan  5 09:54:38.694: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan  5 09:54:38.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 create -f -'
    Jan  5 09:54:38.867: INFO: stderr: ""
    Jan  5 09:54:38.867: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/05/23 09:54:38.867
    Jan  5 09:54:38.867: INFO: Waiting for all frontend pods to be Running.
    Jan  5 09:54:43.918: INFO: Waiting for frontend to serve content.
    Jan  5 09:54:44.029: INFO: Trying to add a new entry to the guestbook.
    Jan  5 09:54:44.084: INFO: Verifying that added entry can be retrieved.
    Jan  5 09:54:44.101: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
    STEP: using delete to clean up resources 01/05/23 09:54:49.211
    Jan  5 09:54:49.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 delete --grace-period=0 --force -f -'
    Jan  5 09:54:49.293: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 09:54:49.293: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 09:54:49.293
    Jan  5 09:54:49.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 delete --grace-period=0 --force -f -'
    Jan  5 09:54:49.507: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 09:54:49.507: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 09:54:49.507
    Jan  5 09:54:49.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 delete --grace-period=0 --force -f -'
    Jan  5 09:54:49.580: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 09:54:49.580: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 09:54:49.58
    Jan  5 09:54:49.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 delete --grace-period=0 --force -f -'
    Jan  5 09:54:49.644: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 09:54:49.644: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 09:54:49.644
    Jan  5 09:54:49.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 delete --grace-period=0 --force -f -'
    Jan  5 09:54:49.719: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 09:54:49.719: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 09:54:49.719
    Jan  5 09:54:49.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-3961 delete --grace-period=0 --force -f -'
    Jan  5 09:54:49.794: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 09:54:49.794: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 09:54:49.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3961" for this suite. 01/05/23 09:54:49.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:54:49.812
Jan  5 09:54:49.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename proxy 01/05/23 09:54:49.813
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:49.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:49.835
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan  5 09:54:49.841: INFO: Creating pod...
Jan  5 09:54:49.854: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7910" to be "running"
Jan  5 09:54:49.860: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051642ms
Jan  5 09:54:51.867: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.012519766s
Jan  5 09:54:51.867: INFO: Pod "agnhost" satisfied condition "running"
Jan  5 09:54:51.867: INFO: Creating service...
Jan  5 09:54:51.881: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=DELETE
Jan  5 09:54:51.994: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  5 09:54:51.994: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=OPTIONS
Jan  5 09:54:52.043: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  5 09:54:52.043: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=PATCH
Jan  5 09:54:52.051: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  5 09:54:52.051: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=POST
Jan  5 09:54:52.066: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  5 09:54:52.066: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=PUT
Jan  5 09:54:52.074: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  5 09:54:52.075: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=DELETE
Jan  5 09:54:52.084: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  5 09:54:52.084: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan  5 09:54:52.093: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  5 09:54:52.093: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=PATCH
Jan  5 09:54:52.109: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  5 09:54:52.109: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=POST
Jan  5 09:54:52.120: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  5 09:54:52.120: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=PUT
Jan  5 09:54:52.131: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  5 09:54:52.131: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=GET
Jan  5 09:54:52.134: INFO: http.Client request:GET StatusCode:301
Jan  5 09:54:52.134: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=GET
Jan  5 09:54:52.138: INFO: http.Client request:GET StatusCode:301
Jan  5 09:54:52.138: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=HEAD
Jan  5 09:54:52.142: INFO: http.Client request:HEAD StatusCode:301
Jan  5 09:54:52.142: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=HEAD
Jan  5 09:54:52.146: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan  5 09:54:52.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7910" for this suite. 01/05/23 09:54:52.155
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":205,"skipped":3735,"failed":0}
------------------------------
â€¢ [2.349 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:54:49.812
    Jan  5 09:54:49.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename proxy 01/05/23 09:54:49.813
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:49.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:49.835
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan  5 09:54:49.841: INFO: Creating pod...
    Jan  5 09:54:49.854: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7910" to be "running"
    Jan  5 09:54:49.860: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051642ms
    Jan  5 09:54:51.867: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.012519766s
    Jan  5 09:54:51.867: INFO: Pod "agnhost" satisfied condition "running"
    Jan  5 09:54:51.867: INFO: Creating service...
    Jan  5 09:54:51.881: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=DELETE
    Jan  5 09:54:51.994: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  5 09:54:51.994: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=OPTIONS
    Jan  5 09:54:52.043: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  5 09:54:52.043: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=PATCH
    Jan  5 09:54:52.051: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  5 09:54:52.051: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=POST
    Jan  5 09:54:52.066: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  5 09:54:52.066: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=PUT
    Jan  5 09:54:52.074: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  5 09:54:52.075: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan  5 09:54:52.084: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  5 09:54:52.084: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan  5 09:54:52.093: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  5 09:54:52.093: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan  5 09:54:52.109: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  5 09:54:52.109: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=POST
    Jan  5 09:54:52.120: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  5 09:54:52.120: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=PUT
    Jan  5 09:54:52.131: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  5 09:54:52.131: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=GET
    Jan  5 09:54:52.134: INFO: http.Client request:GET StatusCode:301
    Jan  5 09:54:52.134: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=GET
    Jan  5 09:54:52.138: INFO: http.Client request:GET StatusCode:301
    Jan  5 09:54:52.138: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/pods/agnhost/proxy?method=HEAD
    Jan  5 09:54:52.142: INFO: http.Client request:HEAD StatusCode:301
    Jan  5 09:54:52.142: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-7910/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan  5 09:54:52.146: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan  5 09:54:52.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-7910" for this suite. 01/05/23 09:54:52.155
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:54:52.162
Jan  5 09:54:52.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir-wrapper 01/05/23 09:54:52.163
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:52.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:52.184
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan  5 09:54:52.217: INFO: Waiting up to 5m0s for pod "pod-secrets-fd3237c9-fba1-464d-9dce-1f8fe2cd8282" in namespace "emptydir-wrapper-3391" to be "running and ready"
Jan  5 09:54:52.222: INFO: Pod "pod-secrets-fd3237c9-fba1-464d-9dce-1f8fe2cd8282": Phase="Pending", Reason="", readiness=false. Elapsed: 5.170857ms
Jan  5 09:54:52.222: INFO: The phase of Pod pod-secrets-fd3237c9-fba1-464d-9dce-1f8fe2cd8282 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:54:54.228: INFO: Pod "pod-secrets-fd3237c9-fba1-464d-9dce-1f8fe2cd8282": Phase="Running", Reason="", readiness=true. Elapsed: 2.011144511s
Jan  5 09:54:54.228: INFO: The phase of Pod pod-secrets-fd3237c9-fba1-464d-9dce-1f8fe2cd8282 is Running (Ready = true)
Jan  5 09:54:54.228: INFO: Pod "pod-secrets-fd3237c9-fba1-464d-9dce-1f8fe2cd8282" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/05/23 09:54:54.233
STEP: Cleaning up the configmap 01/05/23 09:54:54.238
STEP: Cleaning up the pod 01/05/23 09:54:54.242
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan  5 09:54:54.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3391" for this suite. 01/05/23 09:54:54.274
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":206,"skipped":3738,"failed":0}
------------------------------
â€¢ [2.120 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:54:52.162
    Jan  5 09:54:52.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir-wrapper 01/05/23 09:54:52.163
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:52.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:52.184
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan  5 09:54:52.217: INFO: Waiting up to 5m0s for pod "pod-secrets-fd3237c9-fba1-464d-9dce-1f8fe2cd8282" in namespace "emptydir-wrapper-3391" to be "running and ready"
    Jan  5 09:54:52.222: INFO: Pod "pod-secrets-fd3237c9-fba1-464d-9dce-1f8fe2cd8282": Phase="Pending", Reason="", readiness=false. Elapsed: 5.170857ms
    Jan  5 09:54:52.222: INFO: The phase of Pod pod-secrets-fd3237c9-fba1-464d-9dce-1f8fe2cd8282 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:54:54.228: INFO: Pod "pod-secrets-fd3237c9-fba1-464d-9dce-1f8fe2cd8282": Phase="Running", Reason="", readiness=true. Elapsed: 2.011144511s
    Jan  5 09:54:54.228: INFO: The phase of Pod pod-secrets-fd3237c9-fba1-464d-9dce-1f8fe2cd8282 is Running (Ready = true)
    Jan  5 09:54:54.228: INFO: Pod "pod-secrets-fd3237c9-fba1-464d-9dce-1f8fe2cd8282" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/05/23 09:54:54.233
    STEP: Cleaning up the configmap 01/05/23 09:54:54.238
    STEP: Cleaning up the pod 01/05/23 09:54:54.242
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan  5 09:54:54.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-3391" for this suite. 01/05/23 09:54:54.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:54:54.283
Jan  5 09:54:54.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pods 01/05/23 09:54:54.284
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:54.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:54.321
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 01/05/23 09:54:54.327
STEP: submitting the pod to kubernetes 01/05/23 09:54:54.327
Jan  5 09:54:54.338: INFO: Waiting up to 5m0s for pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb" in namespace "pods-1910" to be "running and ready"
Jan  5 09:54:54.342: INFO: Pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.481038ms
Jan  5 09:54:54.342: INFO: The phase of Pod pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:54:56.348: INFO: Pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb": Phase="Running", Reason="", readiness=true. Elapsed: 2.01011029s
Jan  5 09:54:56.348: INFO: The phase of Pod pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb is Running (Ready = true)
Jan  5 09:54:56.348: INFO: Pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/05/23 09:54:56.352
STEP: updating the pod 01/05/23 09:54:56.356
Jan  5 09:54:56.870: INFO: Successfully updated pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb"
Jan  5 09:54:56.870: INFO: Waiting up to 5m0s for pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb" in namespace "pods-1910" to be "running"
Jan  5 09:54:56.874: INFO: Pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb": Phase="Running", Reason="", readiness=true. Elapsed: 3.908719ms
Jan  5 09:54:56.874: INFO: Pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/05/23 09:54:56.874
Jan  5 09:54:56.879: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 09:54:56.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1910" for this suite. 01/05/23 09:54:56.888
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":207,"skipped":3763,"failed":0}
------------------------------
â€¢ [2.610 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:54:54.283
    Jan  5 09:54:54.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pods 01/05/23 09:54:54.284
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:54.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:54.321
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 01/05/23 09:54:54.327
    STEP: submitting the pod to kubernetes 01/05/23 09:54:54.327
    Jan  5 09:54:54.338: INFO: Waiting up to 5m0s for pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb" in namespace "pods-1910" to be "running and ready"
    Jan  5 09:54:54.342: INFO: Pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.481038ms
    Jan  5 09:54:54.342: INFO: The phase of Pod pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:54:56.348: INFO: Pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb": Phase="Running", Reason="", readiness=true. Elapsed: 2.01011029s
    Jan  5 09:54:56.348: INFO: The phase of Pod pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb is Running (Ready = true)
    Jan  5 09:54:56.348: INFO: Pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/05/23 09:54:56.352
    STEP: updating the pod 01/05/23 09:54:56.356
    Jan  5 09:54:56.870: INFO: Successfully updated pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb"
    Jan  5 09:54:56.870: INFO: Waiting up to 5m0s for pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb" in namespace "pods-1910" to be "running"
    Jan  5 09:54:56.874: INFO: Pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb": Phase="Running", Reason="", readiness=true. Elapsed: 3.908719ms
    Jan  5 09:54:56.874: INFO: Pod "pod-update-247297ae-d10f-4e2b-a783-2831f9feeecb" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/05/23 09:54:56.874
    Jan  5 09:54:56.879: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 09:54:56.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1910" for this suite. 01/05/23 09:54:56.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:54:56.894
Jan  5 09:54:56.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 09:54:56.895
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:56.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:56.918
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 01/05/23 09:54:56.925
Jan  5 09:54:56.925: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-1913 proxy --unix-socket=/tmp/kubectl-proxy-unix2817575989/test'
STEP: retrieving proxy /api/ output 01/05/23 09:54:56.965
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 09:54:56.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1913" for this suite. 01/05/23 09:54:56.974
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":208,"skipped":3776,"failed":0}
------------------------------
â€¢ [0.085 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:54:56.894
    Jan  5 09:54:56.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 09:54:56.895
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:56.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:56.918
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 01/05/23 09:54:56.925
    Jan  5 09:54:56.925: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-1913 proxy --unix-socket=/tmp/kubectl-proxy-unix2817575989/test'
    STEP: retrieving proxy /api/ output 01/05/23 09:54:56.965
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 09:54:56.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1913" for this suite. 01/05/23 09:54:56.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:54:56.981
Jan  5 09:54:56.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename certificates 01/05/23 09:54:56.982
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:57.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:57.033
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/05/23 09:54:57.616
STEP: getting /apis/certificates.k8s.io 01/05/23 09:54:57.625
STEP: getting /apis/certificates.k8s.io/v1 01/05/23 09:54:57.628
STEP: creating 01/05/23 09:54:57.631
STEP: getting 01/05/23 09:54:57.645
STEP: listing 01/05/23 09:54:57.649
STEP: watching 01/05/23 09:54:57.653
Jan  5 09:54:57.653: INFO: starting watch
STEP: patching 01/05/23 09:54:57.655
STEP: updating 01/05/23 09:54:57.661
Jan  5 09:54:57.666: INFO: waiting for watch events with expected annotations
Jan  5 09:54:57.666: INFO: saw patched and updated annotations
STEP: getting /approval 01/05/23 09:54:57.666
STEP: patching /approval 01/05/23 09:54:57.669
STEP: updating /approval 01/05/23 09:54:57.675
STEP: getting /status 01/05/23 09:54:57.68
STEP: patching /status 01/05/23 09:54:57.683
STEP: updating /status 01/05/23 09:54:57.69
STEP: deleting 01/05/23 09:54:57.697
STEP: deleting a collection 01/05/23 09:54:57.709
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:54:57.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7031" for this suite. 01/05/23 09:54:57.744
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":209,"skipped":3809,"failed":0}
------------------------------
â€¢ [0.768 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:54:56.981
    Jan  5 09:54:56.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename certificates 01/05/23 09:54:56.982
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:57.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:57.033
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/05/23 09:54:57.616
    STEP: getting /apis/certificates.k8s.io 01/05/23 09:54:57.625
    STEP: getting /apis/certificates.k8s.io/v1 01/05/23 09:54:57.628
    STEP: creating 01/05/23 09:54:57.631
    STEP: getting 01/05/23 09:54:57.645
    STEP: listing 01/05/23 09:54:57.649
    STEP: watching 01/05/23 09:54:57.653
    Jan  5 09:54:57.653: INFO: starting watch
    STEP: patching 01/05/23 09:54:57.655
    STEP: updating 01/05/23 09:54:57.661
    Jan  5 09:54:57.666: INFO: waiting for watch events with expected annotations
    Jan  5 09:54:57.666: INFO: saw patched and updated annotations
    STEP: getting /approval 01/05/23 09:54:57.666
    STEP: patching /approval 01/05/23 09:54:57.669
    STEP: updating /approval 01/05/23 09:54:57.675
    STEP: getting /status 01/05/23 09:54:57.68
    STEP: patching /status 01/05/23 09:54:57.683
    STEP: updating /status 01/05/23 09:54:57.69
    STEP: deleting 01/05/23 09:54:57.697
    STEP: deleting a collection 01/05/23 09:54:57.709
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:54:57.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-7031" for this suite. 01/05/23 09:54:57.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:54:57.75
Jan  5 09:54:57.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 09:54:57.75
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:57.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:57.771
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-9558 01/05/23 09:54:57.776
STEP: creating service affinity-nodeport-transition in namespace services-9558 01/05/23 09:54:57.776
STEP: creating replication controller affinity-nodeport-transition in namespace services-9558 01/05/23 09:54:57.806
I0105 09:54:57.813021      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9558, replica count: 3
I0105 09:55:00.863905      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 09:55:00.882: INFO: Creating new exec pod
Jan  5 09:55:00.898: INFO: Waiting up to 5m0s for pod "execpod-affinityj6gdb" in namespace "services-9558" to be "running"
Jan  5 09:55:00.914: INFO: Pod "execpod-affinityj6gdb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.190719ms
Jan  5 09:55:02.922: INFO: Pod "execpod-affinityj6gdb": Phase="Running", Reason="", readiness=true. Elapsed: 2.024239484s
Jan  5 09:55:02.922: INFO: Pod "execpod-affinityj6gdb" satisfied condition "running"
Jan  5 09:55:03.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9558 exec execpod-affinityj6gdb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan  5 09:55:04.458: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan  5 09:55:04.458: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:55:04.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9558 exec execpod-affinityj6gdb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.114.64.61 80'
Jan  5 09:55:04.896: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.114.64.61 80\nConnection to 10.114.64.61 80 port [tcp/http] succeeded!\n"
Jan  5 09:55:04.896: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:55:04.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9558 exec execpod-affinityj6gdb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.138 31257'
Jan  5 09:55:05.431: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.138 31257\nConnection to 10.250.2.138 31257 port [tcp/*] succeeded!\n"
Jan  5 09:55:05.431: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:55:05.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9558 exec execpod-affinityj6gdb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.174 31257'
Jan  5 09:55:05.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.174 31257\nConnection to 10.250.0.174 31257 port [tcp/*] succeeded!\n"
Jan  5 09:55:05.942: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 09:55:05.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9558 exec execpod-affinityj6gdb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.128:31257/ ; done'
Jan  5 09:55:06.559: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n"
Jan  5 09:55:06.559: INFO: stdout: "\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-2jpgk\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-grctg\naffinity-nodeport-transition-2jpgk\naffinity-nodeport-transition-2jpgk\naffinity-nodeport-transition-grctg\naffinity-nodeport-transition-2jpgk\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-2jpgk\naffinity-nodeport-transition-grctg\naffinity-nodeport-transition-grctg"
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-2jpgk
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-grctg
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-2jpgk
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-2jpgk
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-grctg
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-2jpgk
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-2jpgk
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-grctg
Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-grctg
Jan  5 09:55:06.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9558 exec execpod-affinityj6gdb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.128:31257/ ; done'
Jan  5 09:55:07.202: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n"
Jan  5 09:55:07.202: INFO: stdout: "\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2"
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
Jan  5 09:55:07.202: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9558, will wait for the garbage collector to delete the pods 01/05/23 09:55:07.215
Jan  5 09:55:07.275: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.250634ms
Jan  5 09:55:07.376: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.971736ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:55:09.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9558" for this suite. 01/05/23 09:55:09.746
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":210,"skipped":3822,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.003 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:54:57.75
    Jan  5 09:54:57.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 09:54:57.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:54:57.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:54:57.771
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-9558 01/05/23 09:54:57.776
    STEP: creating service affinity-nodeport-transition in namespace services-9558 01/05/23 09:54:57.776
    STEP: creating replication controller affinity-nodeport-transition in namespace services-9558 01/05/23 09:54:57.806
    I0105 09:54:57.813021      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9558, replica count: 3
    I0105 09:55:00.863905      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 09:55:00.882: INFO: Creating new exec pod
    Jan  5 09:55:00.898: INFO: Waiting up to 5m0s for pod "execpod-affinityj6gdb" in namespace "services-9558" to be "running"
    Jan  5 09:55:00.914: INFO: Pod "execpod-affinityj6gdb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.190719ms
    Jan  5 09:55:02.922: INFO: Pod "execpod-affinityj6gdb": Phase="Running", Reason="", readiness=true. Elapsed: 2.024239484s
    Jan  5 09:55:02.922: INFO: Pod "execpod-affinityj6gdb" satisfied condition "running"
    Jan  5 09:55:03.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9558 exec execpod-affinityj6gdb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jan  5 09:55:04.458: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan  5 09:55:04.458: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:55:04.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9558 exec execpod-affinityj6gdb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.114.64.61 80'
    Jan  5 09:55:04.896: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.114.64.61 80\nConnection to 10.114.64.61 80 port [tcp/http] succeeded!\n"
    Jan  5 09:55:04.896: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:55:04.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9558 exec execpod-affinityj6gdb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.138 31257'
    Jan  5 09:55:05.431: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.138 31257\nConnection to 10.250.2.138 31257 port [tcp/*] succeeded!\n"
    Jan  5 09:55:05.431: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:55:05.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9558 exec execpod-affinityj6gdb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.174 31257'
    Jan  5 09:55:05.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.174 31257\nConnection to 10.250.0.174 31257 port [tcp/*] succeeded!\n"
    Jan  5 09:55:05.942: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 09:55:05.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9558 exec execpod-affinityj6gdb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.128:31257/ ; done'
    Jan  5 09:55:06.559: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n"
    Jan  5 09:55:06.559: INFO: stdout: "\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-2jpgk\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-grctg\naffinity-nodeport-transition-2jpgk\naffinity-nodeport-transition-2jpgk\naffinity-nodeport-transition-grctg\naffinity-nodeport-transition-2jpgk\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-2jpgk\naffinity-nodeport-transition-grctg\naffinity-nodeport-transition-grctg"
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-2jpgk
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-grctg
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-2jpgk
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-2jpgk
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-grctg
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-2jpgk
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-2jpgk
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-grctg
    Jan  5 09:55:06.559: INFO: Received response from host: affinity-nodeport-transition-grctg
    Jan  5 09:55:06.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-9558 exec execpod-affinityj6gdb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.128:31257/ ; done'
    Jan  5 09:55:07.202: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.128:31257/\n"
    Jan  5 09:55:07.202: INFO: stdout: "\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2\naffinity-nodeport-transition-cbfx2"
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Received response from host: affinity-nodeport-transition-cbfx2
    Jan  5 09:55:07.202: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9558, will wait for the garbage collector to delete the pods 01/05/23 09:55:07.215
    Jan  5 09:55:07.275: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.250634ms
    Jan  5 09:55:07.376: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.971736ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:55:09.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9558" for this suite. 01/05/23 09:55:09.746
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:55:09.755
Jan  5 09:55:09.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename replication-controller 01/05/23 09:55:09.756
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:09.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:09.778
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 01/05/23 09:55:09.783
STEP: When the matched label of one of its pods change 01/05/23 09:55:09.788
Jan  5 09:55:09.792: INFO: Pod name pod-release: Found 0 pods out of 1
Jan  5 09:55:14.797: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/05/23 09:55:14.81
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  5 09:55:14.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-365" for this suite. 01/05/23 09:55:14.839
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":211,"skipped":3896,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.090 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:55:09.755
    Jan  5 09:55:09.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename replication-controller 01/05/23 09:55:09.756
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:09.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:09.778
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 01/05/23 09:55:09.783
    STEP: When the matched label of one of its pods change 01/05/23 09:55:09.788
    Jan  5 09:55:09.792: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan  5 09:55:14.797: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/05/23 09:55:14.81
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  5 09:55:14.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-365" for this suite. 01/05/23 09:55:14.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:55:14.846
Jan  5 09:55:14.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:55:14.847
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:14.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:14.872
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-e2a4c81a-9cab-4651-94d2-1fdf567c84e3 01/05/23 09:55:14.88
STEP: Creating a pod to test consume secrets 01/05/23 09:55:14.887
Jan  5 09:55:14.904: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319" in namespace "projected-6272" to be "Succeeded or Failed"
Jan  5 09:55:14.910: INFO: Pod "pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319": Phase="Pending", Reason="", readiness=false. Elapsed: 5.75622ms
Jan  5 09:55:16.918: INFO: Pod "pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014029892s
Jan  5 09:55:18.916: INFO: Pod "pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011531323s
STEP: Saw pod success 01/05/23 09:55:18.916
Jan  5 09:55:18.916: INFO: Pod "pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319" satisfied condition "Succeeded or Failed"
Jan  5 09:55:18.920: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 09:55:18.971
Jan  5 09:55:18.983: INFO: Waiting for pod pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319 to disappear
Jan  5 09:55:18.995: INFO: Pod pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 09:55:18.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6272" for this suite. 01/05/23 09:55:19.005
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":212,"skipped":3903,"failed":0}
------------------------------
â€¢ [4.165 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:55:14.846
    Jan  5 09:55:14.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:55:14.847
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:14.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:14.872
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-e2a4c81a-9cab-4651-94d2-1fdf567c84e3 01/05/23 09:55:14.88
    STEP: Creating a pod to test consume secrets 01/05/23 09:55:14.887
    Jan  5 09:55:14.904: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319" in namespace "projected-6272" to be "Succeeded or Failed"
    Jan  5 09:55:14.910: INFO: Pod "pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319": Phase="Pending", Reason="", readiness=false. Elapsed: 5.75622ms
    Jan  5 09:55:16.918: INFO: Pod "pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014029892s
    Jan  5 09:55:18.916: INFO: Pod "pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011531323s
    STEP: Saw pod success 01/05/23 09:55:18.916
    Jan  5 09:55:18.916: INFO: Pod "pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319" satisfied condition "Succeeded or Failed"
    Jan  5 09:55:18.920: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 09:55:18.971
    Jan  5 09:55:18.983: INFO: Waiting for pod pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319 to disappear
    Jan  5 09:55:18.995: INFO: Pod pod-projected-secrets-b2687701-7169-45d4-a061-04e767d5f319 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 09:55:18.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6272" for this suite. 01/05/23 09:55:19.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:55:19.012
Jan  5 09:55:19.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-runtime 01/05/23 09:55:19.013
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:19.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:19.033
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 01/05/23 09:55:19.041
STEP: wait for the container to reach Succeeded 01/05/23 09:55:19.053
STEP: get the container status 01/05/23 09:55:23.081
STEP: the container should be terminated 01/05/23 09:55:23.087
STEP: the termination message should be set 01/05/23 09:55:23.087
Jan  5 09:55:23.087: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/05/23 09:55:23.087
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  5 09:55:23.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9768" for this suite. 01/05/23 09:55:23.115
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":213,"skipped":3909,"failed":0}
------------------------------
â€¢ [4.110 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:55:19.012
    Jan  5 09:55:19.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-runtime 01/05/23 09:55:19.013
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:19.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:19.033
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 01/05/23 09:55:19.041
    STEP: wait for the container to reach Succeeded 01/05/23 09:55:19.053
    STEP: get the container status 01/05/23 09:55:23.081
    STEP: the container should be terminated 01/05/23 09:55:23.087
    STEP: the termination message should be set 01/05/23 09:55:23.087
    Jan  5 09:55:23.087: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/05/23 09:55:23.087
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  5 09:55:23.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9768" for this suite. 01/05/23 09:55:23.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:55:23.122
Jan  5 09:55:23.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename namespaces 01/05/23 09:55:23.123
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:23.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:23.145
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 01/05/23 09:55:23.15
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:23.163
STEP: Creating a pod in the namespace 01/05/23 09:55:23.169
STEP: Waiting for the pod to have running status 01/05/23 09:55:23.179
Jan  5 09:55:23.179: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6645" to be "running"
Jan  5 09:55:23.191: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.171592ms
Jan  5 09:55:25.197: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017841257s
Jan  5 09:55:25.197: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/05/23 09:55:25.197
STEP: Waiting for the namespace to be removed. 01/05/23 09:55:25.202
STEP: Recreating the namespace 01/05/23 09:55:36.208
STEP: Verifying there are no pods in the namespace 01/05/23 09:55:36.25
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:55:36.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2795" for this suite. 01/05/23 09:55:36.262
STEP: Destroying namespace "nsdeletetest-6645" for this suite. 01/05/23 09:55:36.267
Jan  5 09:55:36.272: INFO: Namespace nsdeletetest-6645 was already deleted
STEP: Destroying namespace "nsdeletetest-1937" for this suite. 01/05/23 09:55:36.272
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":214,"skipped":3920,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.155 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:55:23.122
    Jan  5 09:55:23.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename namespaces 01/05/23 09:55:23.123
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:23.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:23.145
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 01/05/23 09:55:23.15
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:23.163
    STEP: Creating a pod in the namespace 01/05/23 09:55:23.169
    STEP: Waiting for the pod to have running status 01/05/23 09:55:23.179
    Jan  5 09:55:23.179: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6645" to be "running"
    Jan  5 09:55:23.191: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.171592ms
    Jan  5 09:55:25.197: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017841257s
    Jan  5 09:55:25.197: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/05/23 09:55:25.197
    STEP: Waiting for the namespace to be removed. 01/05/23 09:55:25.202
    STEP: Recreating the namespace 01/05/23 09:55:36.208
    STEP: Verifying there are no pods in the namespace 01/05/23 09:55:36.25
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:55:36.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2795" for this suite. 01/05/23 09:55:36.262
    STEP: Destroying namespace "nsdeletetest-6645" for this suite. 01/05/23 09:55:36.267
    Jan  5 09:55:36.272: INFO: Namespace nsdeletetest-6645 was already deleted
    STEP: Destroying namespace "nsdeletetest-1937" for this suite. 01/05/23 09:55:36.272
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:55:36.277
Jan  5 09:55:36.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 09:55:36.278
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:36.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:36.298
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-98b35da7-1513-4ef6-b8ed-c0c7f0e48245 01/05/23 09:55:36.305
STEP: Creating a pod to test consume configMaps 01/05/23 09:55:36.311
Jan  5 09:55:36.324: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1" in namespace "configmap-7469" to be "Succeeded or Failed"
Jan  5 09:55:36.331: INFO: Pod "pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.474913ms
Jan  5 09:55:38.337: INFO: Pod "pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012639554s
Jan  5 09:55:40.337: INFO: Pod "pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013110934s
STEP: Saw pod success 01/05/23 09:55:40.337
Jan  5 09:55:40.338: INFO: Pod "pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1" satisfied condition "Succeeded or Failed"
Jan  5 09:55:40.345: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 09:55:40.407
Jan  5 09:55:40.417: INFO: Waiting for pod pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1 to disappear
Jan  5 09:55:40.421: INFO: Pod pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 09:55:40.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7469" for this suite. 01/05/23 09:55:40.428
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":215,"skipped":3923,"failed":0}
------------------------------
â€¢ [4.157 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:55:36.277
    Jan  5 09:55:36.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 09:55:36.278
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:36.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:36.298
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-98b35da7-1513-4ef6-b8ed-c0c7f0e48245 01/05/23 09:55:36.305
    STEP: Creating a pod to test consume configMaps 01/05/23 09:55:36.311
    Jan  5 09:55:36.324: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1" in namespace "configmap-7469" to be "Succeeded or Failed"
    Jan  5 09:55:36.331: INFO: Pod "pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.474913ms
    Jan  5 09:55:38.337: INFO: Pod "pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012639554s
    Jan  5 09:55:40.337: INFO: Pod "pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013110934s
    STEP: Saw pod success 01/05/23 09:55:40.337
    Jan  5 09:55:40.338: INFO: Pod "pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1" satisfied condition "Succeeded or Failed"
    Jan  5 09:55:40.345: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 09:55:40.407
    Jan  5 09:55:40.417: INFO: Waiting for pod pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1 to disappear
    Jan  5 09:55:40.421: INFO: Pod pod-configmaps-aa9c6ccf-060f-4f42-95ed-6adcd15d0cc1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 09:55:40.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7469" for this suite. 01/05/23 09:55:40.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:55:40.436
Jan  5 09:55:40.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 09:55:40.437
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:40.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:40.458
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 09:55:40.482
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:55:40.991
STEP: Deploying the webhook pod 01/05/23 09:55:40.998
STEP: Wait for the deployment to be ready 01/05/23 09:55:41.01
Jan  5 09:55:41.031: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 09:55:43.044
STEP: Verifying the service has paired with the endpoint 01/05/23 09:55:43.058
Jan  5 09:55:44.059: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 01/05/23 09:55:44.066
STEP: create a pod that should be denied by the webhook 01/05/23 09:55:44.21
STEP: create a pod that causes the webhook to hang 01/05/23 09:55:44.34
STEP: create a configmap that should be denied by the webhook 01/05/23 09:55:54.353
STEP: create a configmap that should be admitted by the webhook 01/05/23 09:55:54.429
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/05/23 09:55:54.538
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/05/23 09:55:54.557
STEP: create a namespace that bypass the webhook 01/05/23 09:55:54.611
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/05/23 09:55:54.618
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:55:54.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6303" for this suite. 01/05/23 09:55:54.719
STEP: Destroying namespace "webhook-6303-markers" for this suite. 01/05/23 09:55:54.725
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":216,"skipped":3955,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.328 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:55:40.436
    Jan  5 09:55:40.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 09:55:40.437
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:40.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:40.458
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 09:55:40.482
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 09:55:40.991
    STEP: Deploying the webhook pod 01/05/23 09:55:40.998
    STEP: Wait for the deployment to be ready 01/05/23 09:55:41.01
    Jan  5 09:55:41.031: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 09:55:43.044
    STEP: Verifying the service has paired with the endpoint 01/05/23 09:55:43.058
    Jan  5 09:55:44.059: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 01/05/23 09:55:44.066
    STEP: create a pod that should be denied by the webhook 01/05/23 09:55:44.21
    STEP: create a pod that causes the webhook to hang 01/05/23 09:55:44.34
    STEP: create a configmap that should be denied by the webhook 01/05/23 09:55:54.353
    STEP: create a configmap that should be admitted by the webhook 01/05/23 09:55:54.429
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/05/23 09:55:54.538
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/05/23 09:55:54.557
    STEP: create a namespace that bypass the webhook 01/05/23 09:55:54.611
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/05/23 09:55:54.618
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:55:54.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6303" for this suite. 01/05/23 09:55:54.719
    STEP: Destroying namespace "webhook-6303-markers" for this suite. 01/05/23 09:55:54.725
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:55:54.766
Jan  5 09:55:54.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename var-expansion 01/05/23 09:55:54.767
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:54.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:54.792
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 01/05/23 09:55:54.801
Jan  5 09:55:54.811: INFO: Waiting up to 5m0s for pod "var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a" in namespace "var-expansion-9382" to be "Succeeded or Failed"
Jan  5 09:55:54.822: INFO: Pod "var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.413148ms
Jan  5 09:55:56.827: INFO: Pod "var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a": Phase="Running", Reason="", readiness=false. Elapsed: 2.015687833s
Jan  5 09:55:58.828: INFO: Pod "var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016515427s
STEP: Saw pod success 01/05/23 09:55:58.828
Jan  5 09:55:58.828: INFO: Pod "var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a" satisfied condition "Succeeded or Failed"
Jan  5 09:55:58.832: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a container dapi-container: <nil>
STEP: delete the pod 01/05/23 09:55:58.845
Jan  5 09:55:58.854: INFO: Waiting for pod var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a to disappear
Jan  5 09:55:58.858: INFO: Pod var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 09:55:58.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9382" for this suite. 01/05/23 09:55:58.866
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":217,"skipped":3982,"failed":0}
------------------------------
â€¢ [4.105 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:55:54.766
    Jan  5 09:55:54.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename var-expansion 01/05/23 09:55:54.767
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:54.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:54.792
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 01/05/23 09:55:54.801
    Jan  5 09:55:54.811: INFO: Waiting up to 5m0s for pod "var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a" in namespace "var-expansion-9382" to be "Succeeded or Failed"
    Jan  5 09:55:54.822: INFO: Pod "var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.413148ms
    Jan  5 09:55:56.827: INFO: Pod "var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a": Phase="Running", Reason="", readiness=false. Elapsed: 2.015687833s
    Jan  5 09:55:58.828: INFO: Pod "var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016515427s
    STEP: Saw pod success 01/05/23 09:55:58.828
    Jan  5 09:55:58.828: INFO: Pod "var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a" satisfied condition "Succeeded or Failed"
    Jan  5 09:55:58.832: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a container dapi-container: <nil>
    STEP: delete the pod 01/05/23 09:55:58.845
    Jan  5 09:55:58.854: INFO: Waiting for pod var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a to disappear
    Jan  5 09:55:58.858: INFO: Pod var-expansion-78972f8a-154c-4e3b-8cbd-b4b9f62c685a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 09:55:58.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9382" for this suite. 01/05/23 09:55:58.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:55:58.872
Jan  5 09:55:58.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 09:55:58.873
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:58.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:58.893
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 01/05/23 09:55:58.9
Jan  5 09:55:58.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan  5 09:55:58.970: INFO: stderr: ""
Jan  5 09:55:58.970: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 01/05/23 09:55:58.97
Jan  5 09:55:58.970: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan  5 09:55:58.970: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4278" to be "running and ready, or succeeded"
Jan  5 09:55:58.975: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.171319ms
Jan  5 09:55:58.975: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv' to be 'Running' but was 'Pending'
Jan  5 09:56:00.982: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.012320609s
Jan  5 09:56:00.982: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan  5 09:56:00.982: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/05/23 09:56:00.982
Jan  5 09:56:00.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 logs logs-generator logs-generator'
Jan  5 09:56:01.073: INFO: stderr: ""
Jan  5 09:56:01.073: INFO: stdout: "I0105 09:55:59.795630       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/46v 359\nI0105 09:55:59.995801       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/s9vw 588\nI0105 09:56:00.196345       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/vctj 212\nI0105 09:56:00.395652       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/n7w 395\nI0105 09:56:00.596018       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/4n4 224\nI0105 09:56:00.796409       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/xp4 494\nI0105 09:56:00.995704       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/f5r 317\n"
STEP: limiting log lines 01/05/23 09:56:01.073
Jan  5 09:56:01.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 logs logs-generator logs-generator --tail=1'
Jan  5 09:56:01.154: INFO: stderr: ""
Jan  5 09:56:01.154: INFO: stdout: "I0105 09:56:00.995704       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/f5r 317\n"
Jan  5 09:56:01.154: INFO: got output "I0105 09:56:00.995704       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/f5r 317\n"
STEP: limiting log bytes 01/05/23 09:56:01.154
Jan  5 09:56:01.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 logs logs-generator logs-generator --limit-bytes=1'
Jan  5 09:56:01.233: INFO: stderr: ""
Jan  5 09:56:01.233: INFO: stdout: "I"
Jan  5 09:56:01.233: INFO: got output "I"
STEP: exposing timestamps 01/05/23 09:56:01.233
Jan  5 09:56:01.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 logs logs-generator logs-generator --tail=1 --timestamps'
Jan  5 09:56:01.309: INFO: stderr: ""
Jan  5 09:56:01.309: INFO: stdout: "2023-01-05T09:56:01.196263838Z I0105 09:56:01.196131       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bv68 574\n"
Jan  5 09:56:01.309: INFO: got output "2023-01-05T09:56:01.196263838Z I0105 09:56:01.196131       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bv68 574\n"
STEP: restricting to a time range 01/05/23 09:56:01.309
Jan  5 09:56:03.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 logs logs-generator logs-generator --since=1s'
Jan  5 09:56:03.899: INFO: stderr: ""
Jan  5 09:56:03.899: INFO: stdout: "I0105 09:56:02.996093       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/4q4w 232\nI0105 09:56:03.196408       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/lp9 350\nI0105 09:56:03.395690       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/v7gd 557\nI0105 09:56:03.596050       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/jxq 453\nI0105 09:56:03.796432       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/grb 551\n"
Jan  5 09:56:03.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 logs logs-generator logs-generator --since=24h'
Jan  5 09:56:03.977: INFO: stderr: ""
Jan  5 09:56:03.977: INFO: stdout: "I0105 09:55:59.795630       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/46v 359\nI0105 09:55:59.995801       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/s9vw 588\nI0105 09:56:00.196345       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/vctj 212\nI0105 09:56:00.395652       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/n7w 395\nI0105 09:56:00.596018       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/4n4 224\nI0105 09:56:00.796409       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/xp4 494\nI0105 09:56:00.995704       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/f5r 317\nI0105 09:56:01.196131       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bv68 574\nI0105 09:56:01.396479       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/7vqp 306\nI0105 09:56:01.595774       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/w879 314\nI0105 09:56:01.796133       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/j5xc 231\nI0105 09:56:01.996515       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/rqx 406\nI0105 09:56:02.195782       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/n6d 468\nI0105 09:56:02.396138       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/jgv 378\nI0105 09:56:02.596475       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/hzgd 545\nI0105 09:56:02.795755       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/zww 514\nI0105 09:56:02.996093       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/4q4w 232\nI0105 09:56:03.196408       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/lp9 350\nI0105 09:56:03.395690       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/v7gd 557\nI0105 09:56:03.596050       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/jxq 453\nI0105 09:56:03.796432       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/grb 551\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jan  5 09:56:03.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 delete pod logs-generator'
Jan  5 09:56:04.596: INFO: stderr: ""
Jan  5 09:56:04.596: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 09:56:04.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4278" for this suite. 01/05/23 09:56:04.606
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":218,"skipped":3989,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.741 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:55:58.872
    Jan  5 09:55:58.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 09:55:58.873
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:55:58.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:55:58.893
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 01/05/23 09:55:58.9
    Jan  5 09:55:58.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan  5 09:55:58.970: INFO: stderr: ""
    Jan  5 09:55:58.970: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 01/05/23 09:55:58.97
    Jan  5 09:55:58.970: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan  5 09:55:58.970: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4278" to be "running and ready, or succeeded"
    Jan  5 09:55:58.975: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.171319ms
    Jan  5 09:55:58.975: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv' to be 'Running' but was 'Pending'
    Jan  5 09:56:00.982: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.012320609s
    Jan  5 09:56:00.982: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan  5 09:56:00.982: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/05/23 09:56:00.982
    Jan  5 09:56:00.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 logs logs-generator logs-generator'
    Jan  5 09:56:01.073: INFO: stderr: ""
    Jan  5 09:56:01.073: INFO: stdout: "I0105 09:55:59.795630       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/46v 359\nI0105 09:55:59.995801       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/s9vw 588\nI0105 09:56:00.196345       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/vctj 212\nI0105 09:56:00.395652       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/n7w 395\nI0105 09:56:00.596018       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/4n4 224\nI0105 09:56:00.796409       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/xp4 494\nI0105 09:56:00.995704       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/f5r 317\n"
    STEP: limiting log lines 01/05/23 09:56:01.073
    Jan  5 09:56:01.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 logs logs-generator logs-generator --tail=1'
    Jan  5 09:56:01.154: INFO: stderr: ""
    Jan  5 09:56:01.154: INFO: stdout: "I0105 09:56:00.995704       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/f5r 317\n"
    Jan  5 09:56:01.154: INFO: got output "I0105 09:56:00.995704       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/f5r 317\n"
    STEP: limiting log bytes 01/05/23 09:56:01.154
    Jan  5 09:56:01.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 logs logs-generator logs-generator --limit-bytes=1'
    Jan  5 09:56:01.233: INFO: stderr: ""
    Jan  5 09:56:01.233: INFO: stdout: "I"
    Jan  5 09:56:01.233: INFO: got output "I"
    STEP: exposing timestamps 01/05/23 09:56:01.233
    Jan  5 09:56:01.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan  5 09:56:01.309: INFO: stderr: ""
    Jan  5 09:56:01.309: INFO: stdout: "2023-01-05T09:56:01.196263838Z I0105 09:56:01.196131       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bv68 574\n"
    Jan  5 09:56:01.309: INFO: got output "2023-01-05T09:56:01.196263838Z I0105 09:56:01.196131       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bv68 574\n"
    STEP: restricting to a time range 01/05/23 09:56:01.309
    Jan  5 09:56:03.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 logs logs-generator logs-generator --since=1s'
    Jan  5 09:56:03.899: INFO: stderr: ""
    Jan  5 09:56:03.899: INFO: stdout: "I0105 09:56:02.996093       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/4q4w 232\nI0105 09:56:03.196408       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/lp9 350\nI0105 09:56:03.395690       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/v7gd 557\nI0105 09:56:03.596050       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/jxq 453\nI0105 09:56:03.796432       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/grb 551\n"
    Jan  5 09:56:03.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 logs logs-generator logs-generator --since=24h'
    Jan  5 09:56:03.977: INFO: stderr: ""
    Jan  5 09:56:03.977: INFO: stdout: "I0105 09:55:59.795630       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/46v 359\nI0105 09:55:59.995801       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/s9vw 588\nI0105 09:56:00.196345       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/vctj 212\nI0105 09:56:00.395652       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/n7w 395\nI0105 09:56:00.596018       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/4n4 224\nI0105 09:56:00.796409       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/xp4 494\nI0105 09:56:00.995704       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/f5r 317\nI0105 09:56:01.196131       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bv68 574\nI0105 09:56:01.396479       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/7vqp 306\nI0105 09:56:01.595774       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/w879 314\nI0105 09:56:01.796133       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/j5xc 231\nI0105 09:56:01.996515       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/rqx 406\nI0105 09:56:02.195782       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/n6d 468\nI0105 09:56:02.396138       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/jgv 378\nI0105 09:56:02.596475       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/hzgd 545\nI0105 09:56:02.795755       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/zww 514\nI0105 09:56:02.996093       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/4q4w 232\nI0105 09:56:03.196408       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/lp9 350\nI0105 09:56:03.395690       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/v7gd 557\nI0105 09:56:03.596050       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/jxq 453\nI0105 09:56:03.796432       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/grb 551\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jan  5 09:56:03.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-4278 delete pod logs-generator'
    Jan  5 09:56:04.596: INFO: stderr: ""
    Jan  5 09:56:04.596: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 09:56:04.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4278" for this suite. 01/05/23 09:56:04.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:56:04.614
Jan  5 09:56:04.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename gc 01/05/23 09:56:04.615
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:04.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:04.637
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/05/23 09:56:04.644
STEP: Wait for the Deployment to create new ReplicaSet 01/05/23 09:56:04.65
STEP: delete the deployment 01/05/23 09:56:05.16
STEP: wait for all rs to be garbage collected 01/05/23 09:56:05.167
STEP: expected 0 rs, got 1 rs 01/05/23 09:56:05.176
STEP: expected 0 pods, got 2 pods 01/05/23 09:56:05.182
STEP: Gathering metrics 01/05/23 09:56:05.694
W0105 09:56:05.707354      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 09:56:05.707: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 09:56:05.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6599" for this suite. 01/05/23 09:56:05.715
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":219,"skipped":4007,"failed":0}
------------------------------
â€¢ [1.109 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:56:04.614
    Jan  5 09:56:04.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename gc 01/05/23 09:56:04.615
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:04.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:04.637
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/05/23 09:56:04.644
    STEP: Wait for the Deployment to create new ReplicaSet 01/05/23 09:56:04.65
    STEP: delete the deployment 01/05/23 09:56:05.16
    STEP: wait for all rs to be garbage collected 01/05/23 09:56:05.167
    STEP: expected 0 rs, got 1 rs 01/05/23 09:56:05.176
    STEP: expected 0 pods, got 2 pods 01/05/23 09:56:05.182
    STEP: Gathering metrics 01/05/23 09:56:05.694
    W0105 09:56:05.707354      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 09:56:05.707: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 09:56:05.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6599" for this suite. 01/05/23 09:56:05.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:56:05.726
Jan  5 09:56:05.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename podtemplate 01/05/23 09:56:05.726
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:05.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:05.749
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan  5 09:56:05.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3619" for this suite. 01/05/23 09:56:05.793
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":220,"skipped":4053,"failed":0}
------------------------------
â€¢ [0.073 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:56:05.726
    Jan  5 09:56:05.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename podtemplate 01/05/23 09:56:05.726
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:05.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:05.749
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan  5 09:56:05.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3619" for this suite. 01/05/23 09:56:05.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:56:05.8
Jan  5 09:56:05.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:56:05.801
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:05.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:05.824
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-998eab68-a072-4c76-b878-7984b9412211 01/05/23 09:56:05.83
STEP: Creating a pod to test consume secrets 01/05/23 09:56:05.836
Jan  5 09:56:05.848: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b" in namespace "projected-3986" to be "Succeeded or Failed"
Jan  5 09:56:05.856: INFO: Pod "pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.658985ms
Jan  5 09:56:07.862: INFO: Pod "pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013583774s
Jan  5 09:56:09.862: INFO: Pod "pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013822549s
STEP: Saw pod success 01/05/23 09:56:09.863
Jan  5 09:56:09.863: INFO: Pod "pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b" satisfied condition "Succeeded or Failed"
Jan  5 09:56:09.867: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 09:56:09.881
Jan  5 09:56:09.893: INFO: Waiting for pod pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b to disappear
Jan  5 09:56:09.896: INFO: Pod pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 09:56:09.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3986" for this suite. 01/05/23 09:56:09.905
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":221,"skipped":4070,"failed":0}
------------------------------
â€¢ [4.111 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:56:05.8
    Jan  5 09:56:05.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:56:05.801
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:05.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:05.824
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-998eab68-a072-4c76-b878-7984b9412211 01/05/23 09:56:05.83
    STEP: Creating a pod to test consume secrets 01/05/23 09:56:05.836
    Jan  5 09:56:05.848: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b" in namespace "projected-3986" to be "Succeeded or Failed"
    Jan  5 09:56:05.856: INFO: Pod "pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.658985ms
    Jan  5 09:56:07.862: INFO: Pod "pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013583774s
    Jan  5 09:56:09.862: INFO: Pod "pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013822549s
    STEP: Saw pod success 01/05/23 09:56:09.863
    Jan  5 09:56:09.863: INFO: Pod "pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b" satisfied condition "Succeeded or Failed"
    Jan  5 09:56:09.867: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 09:56:09.881
    Jan  5 09:56:09.893: INFO: Waiting for pod pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b to disappear
    Jan  5 09:56:09.896: INFO: Pod pod-projected-secrets-8439ce00-79dd-410e-bf07-f0f0877f7d1b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 09:56:09.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3986" for this suite. 01/05/23 09:56:09.905
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:56:09.913
Jan  5 09:56:09.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename sysctl 01/05/23 09:56:09.914
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:09.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:09.935
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/05/23 09:56:09.94
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 09:56:09.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9822" for this suite. 01/05/23 09:56:09.957
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":222,"skipped":4070,"failed":0}
------------------------------
â€¢ [0.050 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:56:09.913
    Jan  5 09:56:09.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename sysctl 01/05/23 09:56:09.914
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:09.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:09.935
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/05/23 09:56:09.94
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 09:56:09.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-9822" for this suite. 01/05/23 09:56:09.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:56:09.965
Jan  5 09:56:09.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 09:56:09.966
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:09.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:09.988
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-c2afb938-efd9-4eff-a645-f83935b27bba 01/05/23 09:56:09.992
STEP: Creating a pod to test consume configMaps 01/05/23 09:56:09.999
Jan  5 09:56:10.009: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2" in namespace "projected-2612" to be "Succeeded or Failed"
Jan  5 09:56:10.013: INFO: Pod "pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.467384ms
Jan  5 09:56:12.035: INFO: Pod "pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0261625s
Jan  5 09:56:14.019: INFO: Pod "pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010622346s
STEP: Saw pod success 01/05/23 09:56:14.019
Jan  5 09:56:14.019: INFO: Pod "pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2" satisfied condition "Succeeded or Failed"
Jan  5 09:56:14.023: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 09:56:14.077
Jan  5 09:56:14.085: INFO: Waiting for pod pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2 to disappear
Jan  5 09:56:14.089: INFO: Pod pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 09:56:14.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2612" for this suite. 01/05/23 09:56:14.098
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":223,"skipped":4098,"failed":0}
------------------------------
â€¢ [4.138 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:56:09.965
    Jan  5 09:56:09.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 09:56:09.966
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:09.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:09.988
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-c2afb938-efd9-4eff-a645-f83935b27bba 01/05/23 09:56:09.992
    STEP: Creating a pod to test consume configMaps 01/05/23 09:56:09.999
    Jan  5 09:56:10.009: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2" in namespace "projected-2612" to be "Succeeded or Failed"
    Jan  5 09:56:10.013: INFO: Pod "pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.467384ms
    Jan  5 09:56:12.035: INFO: Pod "pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0261625s
    Jan  5 09:56:14.019: INFO: Pod "pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010622346s
    STEP: Saw pod success 01/05/23 09:56:14.019
    Jan  5 09:56:14.019: INFO: Pod "pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2" satisfied condition "Succeeded or Failed"
    Jan  5 09:56:14.023: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 09:56:14.077
    Jan  5 09:56:14.085: INFO: Waiting for pod pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2 to disappear
    Jan  5 09:56:14.089: INFO: Pod pod-projected-configmaps-dfccf16d-480d-4284-b2d2-175cddf301f2 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 09:56:14.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2612" for this suite. 01/05/23 09:56:14.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:56:14.106
Jan  5 09:56:14.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename dns 01/05/23 09:56:14.107
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:14.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:14.126
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1828.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1828.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/05/23 09:56:14.132
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1828.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1828.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/05/23 09:56:14.132
STEP: creating a pod to probe /etc/hosts 01/05/23 09:56:14.132
STEP: submitting the pod to kubernetes 01/05/23 09:56:14.132
Jan  5 09:56:14.145: INFO: Waiting up to 15m0s for pod "dns-test-0abd4526-e8bc-40fb-96a2-c25aaaeed801" in namespace "dns-1828" to be "running"
Jan  5 09:56:14.150: INFO: Pod "dns-test-0abd4526-e8bc-40fb-96a2-c25aaaeed801": Phase="Pending", Reason="", readiness=false. Elapsed: 5.470637ms
Jan  5 09:56:16.156: INFO: Pod "dns-test-0abd4526-e8bc-40fb-96a2-c25aaaeed801": Phase="Running", Reason="", readiness=true. Elapsed: 2.011558091s
Jan  5 09:56:16.156: INFO: Pod "dns-test-0abd4526-e8bc-40fb-96a2-c25aaaeed801" satisfied condition "running"
STEP: retrieving the pod 01/05/23 09:56:16.156
STEP: looking for the results for each expected name from probers 01/05/23 09:56:16.161
Jan  5 09:56:16.345: INFO: DNS probes using dns-1828/dns-test-0abd4526-e8bc-40fb-96a2-c25aaaeed801 succeeded

STEP: deleting the pod 01/05/23 09:56:16.345
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 09:56:16.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1828" for this suite. 01/05/23 09:56:16.367
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":224,"skipped":4174,"failed":0}
------------------------------
â€¢ [2.267 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:56:14.106
    Jan  5 09:56:14.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename dns 01/05/23 09:56:14.107
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:14.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:14.126
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1828.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1828.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/05/23 09:56:14.132
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1828.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1828.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/05/23 09:56:14.132
    STEP: creating a pod to probe /etc/hosts 01/05/23 09:56:14.132
    STEP: submitting the pod to kubernetes 01/05/23 09:56:14.132
    Jan  5 09:56:14.145: INFO: Waiting up to 15m0s for pod "dns-test-0abd4526-e8bc-40fb-96a2-c25aaaeed801" in namespace "dns-1828" to be "running"
    Jan  5 09:56:14.150: INFO: Pod "dns-test-0abd4526-e8bc-40fb-96a2-c25aaaeed801": Phase="Pending", Reason="", readiness=false. Elapsed: 5.470637ms
    Jan  5 09:56:16.156: INFO: Pod "dns-test-0abd4526-e8bc-40fb-96a2-c25aaaeed801": Phase="Running", Reason="", readiness=true. Elapsed: 2.011558091s
    Jan  5 09:56:16.156: INFO: Pod "dns-test-0abd4526-e8bc-40fb-96a2-c25aaaeed801" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 09:56:16.156
    STEP: looking for the results for each expected name from probers 01/05/23 09:56:16.161
    Jan  5 09:56:16.345: INFO: DNS probes using dns-1828/dns-test-0abd4526-e8bc-40fb-96a2-c25aaaeed801 succeeded

    STEP: deleting the pod 01/05/23 09:56:16.345
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 09:56:16.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1828" for this suite. 01/05/23 09:56:16.367
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:56:16.374
Jan  5 09:56:16.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename dns 01/05/23 09:56:16.375
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:16.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:16.397
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/05/23 09:56:16.404
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-713.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local; sleep 1; done
 01/05/23 09:56:16.41
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-713.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-713.svc.cluster.local; sleep 1; done
 01/05/23 09:56:16.41
STEP: creating a pod to probe DNS 01/05/23 09:56:16.41
STEP: submitting the pod to kubernetes 01/05/23 09:56:16.41
Jan  5 09:56:16.424: INFO: Waiting up to 15m0s for pod "dns-test-0d18b2b6-38d3-4b7f-94df-9e92a0584a34" in namespace "dns-713" to be "running"
Jan  5 09:56:16.434: INFO: Pod "dns-test-0d18b2b6-38d3-4b7f-94df-9e92a0584a34": Phase="Pending", Reason="", readiness=false. Elapsed: 9.478562ms
Jan  5 09:56:18.441: INFO: Pod "dns-test-0d18b2b6-38d3-4b7f-94df-9e92a0584a34": Phase="Running", Reason="", readiness=true. Elapsed: 2.016955972s
Jan  5 09:56:18.441: INFO: Pod "dns-test-0d18b2b6-38d3-4b7f-94df-9e92a0584a34" satisfied condition "running"
STEP: retrieving the pod 01/05/23 09:56:18.441
STEP: looking for the results for each expected name from probers 01/05/23 09:56:18.447
Jan  5 09:56:18.659: INFO: DNS probes using dns-test-0d18b2b6-38d3-4b7f-94df-9e92a0584a34 succeeded

STEP: deleting the pod 01/05/23 09:56:18.659
STEP: changing the externalName to bar.example.com 01/05/23 09:56:18.673
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-713.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local; sleep 1; done
 01/05/23 09:56:18.684
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-713.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-713.svc.cluster.local; sleep 1; done
 01/05/23 09:56:18.684
STEP: creating a second pod to probe DNS 01/05/23 09:56:18.684
STEP: submitting the pod to kubernetes 01/05/23 09:56:18.684
Jan  5 09:56:18.702: INFO: Waiting up to 15m0s for pod "dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54" in namespace "dns-713" to be "running"
Jan  5 09:56:18.718: INFO: Pod "dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54": Phase="Pending", Reason="", readiness=false. Elapsed: 15.967693ms
Jan  5 09:56:20.723: INFO: Pod "dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54": Phase="Running", Reason="", readiness=true. Elapsed: 2.02103835s
Jan  5 09:56:20.723: INFO: Pod "dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54" satisfied condition "running"
STEP: retrieving the pod 01/05/23 09:56:20.723
STEP: looking for the results for each expected name from probers 01/05/23 09:56:20.727
Jan  5 09:56:20.837: INFO: File wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 09:56:20.885: INFO: File jessie_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 09:56:20.885: INFO: Lookups using dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 failed for: [wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local jessie_udp@dns-test-service-3.dns-713.svc.cluster.local]

Jan  5 09:56:25.895: INFO: File wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 09:56:25.942: INFO: File jessie_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 09:56:25.942: INFO: Lookups using dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 failed for: [wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local jessie_udp@dns-test-service-3.dns-713.svc.cluster.local]

Jan  5 09:56:30.896: INFO: File wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 09:56:30.942: INFO: File jessie_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 09:56:30.942: INFO: Lookups using dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 failed for: [wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local jessie_udp@dns-test-service-3.dns-713.svc.cluster.local]

Jan  5 09:56:35.901: INFO: File wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 09:56:35.952: INFO: File jessie_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 09:56:35.952: INFO: Lookups using dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 failed for: [wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local jessie_udp@dns-test-service-3.dns-713.svc.cluster.local]

Jan  5 09:56:40.900: INFO: File wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 09:56:40.949: INFO: File jessie_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 09:56:40.949: INFO: Lookups using dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 failed for: [wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local jessie_udp@dns-test-service-3.dns-713.svc.cluster.local]

Jan  5 09:56:45.899: INFO: File wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 09:56:45.949: INFO: File jessie_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains '' instead of 'bar.example.com.'
Jan  5 09:56:45.949: INFO: Lookups using dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 failed for: [wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local jessie_udp@dns-test-service-3.dns-713.svc.cluster.local]

Jan  5 09:56:50.944: INFO: DNS probes using dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 succeeded

STEP: deleting the pod 01/05/23 09:56:50.945
STEP: changing the service to type=ClusterIP 01/05/23 09:56:50.96
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-713.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local; sleep 1; done
 01/05/23 09:56:50.987
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-713.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-713.svc.cluster.local; sleep 1; done
 01/05/23 09:56:50.987
STEP: creating a third pod to probe DNS 01/05/23 09:56:50.987
STEP: submitting the pod to kubernetes 01/05/23 09:56:50.991
Jan  5 09:56:51.004: INFO: Waiting up to 15m0s for pod "dns-test-256a437a-ac3b-4854-8faf-ea6f39f6429a" in namespace "dns-713" to be "running"
Jan  5 09:56:51.010: INFO: Pod "dns-test-256a437a-ac3b-4854-8faf-ea6f39f6429a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.895869ms
Jan  5 09:56:53.017: INFO: Pod "dns-test-256a437a-ac3b-4854-8faf-ea6f39f6429a": Phase="Running", Reason="", readiness=true. Elapsed: 2.013121602s
Jan  5 09:56:53.017: INFO: Pod "dns-test-256a437a-ac3b-4854-8faf-ea6f39f6429a" satisfied condition "running"
STEP: retrieving the pod 01/05/23 09:56:53.017
STEP: looking for the results for each expected name from probers 01/05/23 09:56:53.021
Jan  5 09:56:53.129: INFO: DNS probes using dns-test-256a437a-ac3b-4854-8faf-ea6f39f6429a succeeded

STEP: deleting the pod 01/05/23 09:56:53.129
STEP: deleting the test externalName service 01/05/23 09:56:53.144
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 09:56:53.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-713" for this suite. 01/05/23 09:56:53.163
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":225,"skipped":4178,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.795 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:56:16.374
    Jan  5 09:56:16.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename dns 01/05/23 09:56:16.375
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:16.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:16.397
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/05/23 09:56:16.404
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-713.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local; sleep 1; done
     01/05/23 09:56:16.41
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-713.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-713.svc.cluster.local; sleep 1; done
     01/05/23 09:56:16.41
    STEP: creating a pod to probe DNS 01/05/23 09:56:16.41
    STEP: submitting the pod to kubernetes 01/05/23 09:56:16.41
    Jan  5 09:56:16.424: INFO: Waiting up to 15m0s for pod "dns-test-0d18b2b6-38d3-4b7f-94df-9e92a0584a34" in namespace "dns-713" to be "running"
    Jan  5 09:56:16.434: INFO: Pod "dns-test-0d18b2b6-38d3-4b7f-94df-9e92a0584a34": Phase="Pending", Reason="", readiness=false. Elapsed: 9.478562ms
    Jan  5 09:56:18.441: INFO: Pod "dns-test-0d18b2b6-38d3-4b7f-94df-9e92a0584a34": Phase="Running", Reason="", readiness=true. Elapsed: 2.016955972s
    Jan  5 09:56:18.441: INFO: Pod "dns-test-0d18b2b6-38d3-4b7f-94df-9e92a0584a34" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 09:56:18.441
    STEP: looking for the results for each expected name from probers 01/05/23 09:56:18.447
    Jan  5 09:56:18.659: INFO: DNS probes using dns-test-0d18b2b6-38d3-4b7f-94df-9e92a0584a34 succeeded

    STEP: deleting the pod 01/05/23 09:56:18.659
    STEP: changing the externalName to bar.example.com 01/05/23 09:56:18.673
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-713.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local; sleep 1; done
     01/05/23 09:56:18.684
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-713.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-713.svc.cluster.local; sleep 1; done
     01/05/23 09:56:18.684
    STEP: creating a second pod to probe DNS 01/05/23 09:56:18.684
    STEP: submitting the pod to kubernetes 01/05/23 09:56:18.684
    Jan  5 09:56:18.702: INFO: Waiting up to 15m0s for pod "dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54" in namespace "dns-713" to be "running"
    Jan  5 09:56:18.718: INFO: Pod "dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54": Phase="Pending", Reason="", readiness=false. Elapsed: 15.967693ms
    Jan  5 09:56:20.723: INFO: Pod "dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54": Phase="Running", Reason="", readiness=true. Elapsed: 2.02103835s
    Jan  5 09:56:20.723: INFO: Pod "dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 09:56:20.723
    STEP: looking for the results for each expected name from probers 01/05/23 09:56:20.727
    Jan  5 09:56:20.837: INFO: File wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 09:56:20.885: INFO: File jessie_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 09:56:20.885: INFO: Lookups using dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 failed for: [wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local jessie_udp@dns-test-service-3.dns-713.svc.cluster.local]

    Jan  5 09:56:25.895: INFO: File wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 09:56:25.942: INFO: File jessie_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 09:56:25.942: INFO: Lookups using dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 failed for: [wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local jessie_udp@dns-test-service-3.dns-713.svc.cluster.local]

    Jan  5 09:56:30.896: INFO: File wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 09:56:30.942: INFO: File jessie_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 09:56:30.942: INFO: Lookups using dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 failed for: [wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local jessie_udp@dns-test-service-3.dns-713.svc.cluster.local]

    Jan  5 09:56:35.901: INFO: File wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 09:56:35.952: INFO: File jessie_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 09:56:35.952: INFO: Lookups using dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 failed for: [wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local jessie_udp@dns-test-service-3.dns-713.svc.cluster.local]

    Jan  5 09:56:40.900: INFO: File wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 09:56:40.949: INFO: File jessie_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 09:56:40.949: INFO: Lookups using dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 failed for: [wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local jessie_udp@dns-test-service-3.dns-713.svc.cluster.local]

    Jan  5 09:56:45.899: INFO: File wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 09:56:45.949: INFO: File jessie_udp@dns-test-service-3.dns-713.svc.cluster.local from pod  dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 contains '' instead of 'bar.example.com.'
    Jan  5 09:56:45.949: INFO: Lookups using dns-713/dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 failed for: [wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local jessie_udp@dns-test-service-3.dns-713.svc.cluster.local]

    Jan  5 09:56:50.944: INFO: DNS probes using dns-test-82fbaf77-0faa-43cc-8d9f-10c6c5bccb54 succeeded

    STEP: deleting the pod 01/05/23 09:56:50.945
    STEP: changing the service to type=ClusterIP 01/05/23 09:56:50.96
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-713.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-713.svc.cluster.local; sleep 1; done
     01/05/23 09:56:50.987
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-713.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-713.svc.cluster.local; sleep 1; done
     01/05/23 09:56:50.987
    STEP: creating a third pod to probe DNS 01/05/23 09:56:50.987
    STEP: submitting the pod to kubernetes 01/05/23 09:56:50.991
    Jan  5 09:56:51.004: INFO: Waiting up to 15m0s for pod "dns-test-256a437a-ac3b-4854-8faf-ea6f39f6429a" in namespace "dns-713" to be "running"
    Jan  5 09:56:51.010: INFO: Pod "dns-test-256a437a-ac3b-4854-8faf-ea6f39f6429a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.895869ms
    Jan  5 09:56:53.017: INFO: Pod "dns-test-256a437a-ac3b-4854-8faf-ea6f39f6429a": Phase="Running", Reason="", readiness=true. Elapsed: 2.013121602s
    Jan  5 09:56:53.017: INFO: Pod "dns-test-256a437a-ac3b-4854-8faf-ea6f39f6429a" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 09:56:53.017
    STEP: looking for the results for each expected name from probers 01/05/23 09:56:53.021
    Jan  5 09:56:53.129: INFO: DNS probes using dns-test-256a437a-ac3b-4854-8faf-ea6f39f6429a succeeded

    STEP: deleting the pod 01/05/23 09:56:53.129
    STEP: deleting the test externalName service 01/05/23 09:56:53.144
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 09:56:53.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-713" for this suite. 01/05/23 09:56:53.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:56:53.17
Jan  5 09:56:53.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename statefulset 01/05/23 09:56:53.171
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:53.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:53.189
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3915 01/05/23 09:56:53.195
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 01/05/23 09:56:53.203
STEP: Creating pod with conflicting port in namespace statefulset-3915 01/05/23 09:56:53.219
STEP: Waiting until pod test-pod will start running in namespace statefulset-3915 01/05/23 09:56:53.228
Jan  5 09:56:53.228: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3915" to be "running"
Jan  5 09:56:53.232: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.519354ms
Jan  5 09:56:55.238: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009986106s
Jan  5 09:56:55.238: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3915 01/05/23 09:56:55.238
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3915 01/05/23 09:56:55.244
Jan  5 09:56:55.260: INFO: Observed stateful pod in namespace: statefulset-3915, name: ss-0, uid: afb6f388-1fcb-4617-9883-8a3c08bb1a80, status phase: Pending. Waiting for statefulset controller to delete.
Jan  5 09:56:55.276: INFO: Observed stateful pod in namespace: statefulset-3915, name: ss-0, uid: afb6f388-1fcb-4617-9883-8a3c08bb1a80, status phase: Failed. Waiting for statefulset controller to delete.
Jan  5 09:56:55.282: INFO: Observed stateful pod in namespace: statefulset-3915, name: ss-0, uid: afb6f388-1fcb-4617-9883-8a3c08bb1a80, status phase: Failed. Waiting for statefulset controller to delete.
Jan  5 09:56:55.285: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3915
STEP: Removing pod with conflicting port in namespace statefulset-3915 01/05/23 09:56:55.285
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3915 and will be in running state 01/05/23 09:56:55.301
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 09:56:57.315: INFO: Deleting all statefulset in ns statefulset-3915
Jan  5 09:56:57.320: INFO: Scaling statefulset ss to 0
Jan  5 09:57:07.340: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 09:57:07.344: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 09:57:07.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3915" for this suite. 01/05/23 09:57:07.37
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":226,"skipped":4183,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.206 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:56:53.17
    Jan  5 09:56:53.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename statefulset 01/05/23 09:56:53.171
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:56:53.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:56:53.189
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3915 01/05/23 09:56:53.195
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 01/05/23 09:56:53.203
    STEP: Creating pod with conflicting port in namespace statefulset-3915 01/05/23 09:56:53.219
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3915 01/05/23 09:56:53.228
    Jan  5 09:56:53.228: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3915" to be "running"
    Jan  5 09:56:53.232: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.519354ms
    Jan  5 09:56:55.238: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009986106s
    Jan  5 09:56:55.238: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3915 01/05/23 09:56:55.238
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3915 01/05/23 09:56:55.244
    Jan  5 09:56:55.260: INFO: Observed stateful pod in namespace: statefulset-3915, name: ss-0, uid: afb6f388-1fcb-4617-9883-8a3c08bb1a80, status phase: Pending. Waiting for statefulset controller to delete.
    Jan  5 09:56:55.276: INFO: Observed stateful pod in namespace: statefulset-3915, name: ss-0, uid: afb6f388-1fcb-4617-9883-8a3c08bb1a80, status phase: Failed. Waiting for statefulset controller to delete.
    Jan  5 09:56:55.282: INFO: Observed stateful pod in namespace: statefulset-3915, name: ss-0, uid: afb6f388-1fcb-4617-9883-8a3c08bb1a80, status phase: Failed. Waiting for statefulset controller to delete.
    Jan  5 09:56:55.285: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3915
    STEP: Removing pod with conflicting port in namespace statefulset-3915 01/05/23 09:56:55.285
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3915 and will be in running state 01/05/23 09:56:55.301
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 09:56:57.315: INFO: Deleting all statefulset in ns statefulset-3915
    Jan  5 09:56:57.320: INFO: Scaling statefulset ss to 0
    Jan  5 09:57:07.340: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 09:57:07.344: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 09:57:07.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3915" for this suite. 01/05/23 09:57:07.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:57:07.377
Jan  5 09:57:07.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 09:57:07.377
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:57:07.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:57:07.398
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan  5 09:57:07.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:57:10.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3332" for this suite. 01/05/23 09:57:10.546
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":227,"skipped":4191,"failed":0}
------------------------------
â€¢ [3.175 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:57:07.377
    Jan  5 09:57:07.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 09:57:07.377
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:57:07.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:57:07.398
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan  5 09:57:07.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:57:10.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3332" for this suite. 01/05/23 09:57:10.546
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:57:10.551
Jan  5 09:57:10.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename crd-webhook 01/05/23 09:57:10.552
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:57:10.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:57:10.573
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/05/23 09:57:10.58
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/05/23 09:57:10.967
STEP: Deploying the custom resource conversion webhook pod 01/05/23 09:57:10.98
STEP: Wait for the deployment to be ready 01/05/23 09:57:10.991
Jan  5 09:57:11.011: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 09:57:13.024
STEP: Verifying the service has paired with the endpoint 01/05/23 09:57:13.037
Jan  5 09:57:14.037: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan  5 09:57:14.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Creating a v1 custom resource 01/05/23 09:57:16.816
STEP: v2 custom resource should be converted 01/05/23 09:57:16.823
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 09:57:17.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3510" for this suite. 01/05/23 09:57:17.366
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":228,"skipped":4191,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.866 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:57:10.551
    Jan  5 09:57:10.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename crd-webhook 01/05/23 09:57:10.552
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:57:10.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:57:10.573
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/05/23 09:57:10.58
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/05/23 09:57:10.967
    STEP: Deploying the custom resource conversion webhook pod 01/05/23 09:57:10.98
    STEP: Wait for the deployment to be ready 01/05/23 09:57:10.991
    Jan  5 09:57:11.011: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 09:57:13.024
    STEP: Verifying the service has paired with the endpoint 01/05/23 09:57:13.037
    Jan  5 09:57:14.037: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan  5 09:57:14.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Creating a v1 custom resource 01/05/23 09:57:16.816
    STEP: v2 custom resource should be converted 01/05/23 09:57:16.823
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 09:57:17.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3510" for this suite. 01/05/23 09:57:17.366
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:57:17.419
Jan  5 09:57:17.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename gc 01/05/23 09:57:17.419
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:57:17.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:57:17.46
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/05/23 09:57:17.477
STEP: create the rc2 01/05/23 09:57:17.484
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/05/23 09:57:22.498
STEP: delete the rc simpletest-rc-to-be-deleted 01/05/23 09:57:23.212
STEP: wait for the rc to be deleted 01/05/23 09:57:23.219
Jan  5 09:57:28.237: INFO: 96 pods remaining
Jan  5 09:57:28.237: INFO: 70 pods has nil DeletionTimestamp
Jan  5 09:57:28.237: INFO: 
Jan  5 09:57:33.236: INFO: 85 pods remaining
Jan  5 09:57:33.236: INFO: 50 pods has nil DeletionTimestamp
Jan  5 09:57:33.236: INFO: 
STEP: Gathering metrics 01/05/23 09:57:38.233
W0105 09:57:38.249344      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 09:57:38.249: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan  5 09:57:38.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-26b6m" in namespace "gc-2277"
Jan  5 09:57:38.261: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bfkw" in namespace "gc-2277"
Jan  5 09:57:38.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bmzp" in namespace "gc-2277"
Jan  5 09:57:38.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xbqd" in namespace "gc-2277"
Jan  5 09:57:38.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-42dhh" in namespace "gc-2277"
Jan  5 09:57:38.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-48jdg" in namespace "gc-2277"
Jan  5 09:57:38.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qdz6" in namespace "gc-2277"
Jan  5 09:57:38.353: INFO: Deleting pod "simpletest-rc-to-be-deleted-4w6w7" in namespace "gc-2277"
Jan  5 09:57:38.376: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zsrr" in namespace "gc-2277"
Jan  5 09:57:38.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-56l8q" in namespace "gc-2277"
Jan  5 09:57:38.403: INFO: Deleting pod "simpletest-rc-to-be-deleted-56zkg" in namespace "gc-2277"
Jan  5 09:57:38.420: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jvlb" in namespace "gc-2277"
Jan  5 09:57:38.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rgpg" in namespace "gc-2277"
Jan  5 09:57:38.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-6w2gm" in namespace "gc-2277"
Jan  5 09:57:38.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-6x5sm" in namespace "gc-2277"
Jan  5 09:57:38.491: INFO: Deleting pod "simpletest-rc-to-be-deleted-74xpl" in namespace "gc-2277"
Jan  5 09:57:38.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-78cxl" in namespace "gc-2277"
Jan  5 09:57:38.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-7k8qx" in namespace "gc-2277"
Jan  5 09:57:38.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qk2w" in namespace "gc-2277"
Jan  5 09:57:38.546: INFO: Deleting pod "simpletest-rc-to-be-deleted-7z4k4" in namespace "gc-2277"
Jan  5 09:57:38.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-8drnm" in namespace "gc-2277"
Jan  5 09:57:38.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f7sb" in namespace "gc-2277"
Jan  5 09:57:38.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fc9x" in namespace "gc-2277"
Jan  5 09:57:38.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tsmg" in namespace "gc-2277"
Jan  5 09:57:38.620: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xs5b" in namespace "gc-2277"
Jan  5 09:57:38.632: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zq7m" in namespace "gc-2277"
Jan  5 09:57:38.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-9br4w" in namespace "gc-2277"
Jan  5 09:57:38.670: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jwsl" in namespace "gc-2277"
Jan  5 09:57:38.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9xvl" in namespace "gc-2277"
Jan  5 09:57:38.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgqrd" in namespace "gc-2277"
Jan  5 09:57:38.708: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq7fz" in namespace "gc-2277"
Jan  5 09:57:38.719: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6lk9" in namespace "gc-2277"
Jan  5 09:57:38.731: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6z8t" in namespace "gc-2277"
Jan  5 09:57:38.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmcgn" in namespace "gc-2277"
Jan  5 09:57:38.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctwhw" in namespace "gc-2277"
Jan  5 09:57:38.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcqks" in namespace "gc-2277"
Jan  5 09:57:38.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh6qq" in namespace "gc-2277"
Jan  5 09:57:38.808: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl6qh" in namespace "gc-2277"
Jan  5 09:57:38.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-dn4x9" in namespace "gc-2277"
Jan  5 09:57:38.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7zdr" in namespace "gc-2277"
Jan  5 09:57:38.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8zz2" in namespace "gc-2277"
Jan  5 09:57:38.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkgwx" in namespace "gc-2277"
Jan  5 09:57:38.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8tsd" in namespace "gc-2277"
Jan  5 09:57:38.886: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtqxd" in namespace "gc-2277"
Jan  5 09:57:38.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzlf4" in namespace "gc-2277"
Jan  5 09:57:38.936: INFO: Deleting pod "simpletest-rc-to-be-deleted-hsg4d" in namespace "gc-2277"
Jan  5 09:57:38.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-jc78m" in namespace "gc-2277"
Jan  5 09:57:38.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-jglnz" in namespace "gc-2277"
Jan  5 09:57:38.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnrgb" in namespace "gc-2277"
Jan  5 09:57:38.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-jtzvp" in namespace "gc-2277"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 09:57:39.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2277" for this suite. 01/05/23 09:57:39.012
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":229,"skipped":4207,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.599 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:57:17.419
    Jan  5 09:57:17.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename gc 01/05/23 09:57:17.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:57:17.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:57:17.46
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/05/23 09:57:17.477
    STEP: create the rc2 01/05/23 09:57:17.484
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/05/23 09:57:22.498
    STEP: delete the rc simpletest-rc-to-be-deleted 01/05/23 09:57:23.212
    STEP: wait for the rc to be deleted 01/05/23 09:57:23.219
    Jan  5 09:57:28.237: INFO: 96 pods remaining
    Jan  5 09:57:28.237: INFO: 70 pods has nil DeletionTimestamp
    Jan  5 09:57:28.237: INFO: 
    Jan  5 09:57:33.236: INFO: 85 pods remaining
    Jan  5 09:57:33.236: INFO: 50 pods has nil DeletionTimestamp
    Jan  5 09:57:33.236: INFO: 
    STEP: Gathering metrics 01/05/23 09:57:38.233
    W0105 09:57:38.249344      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 09:57:38.249: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan  5 09:57:38.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-26b6m" in namespace "gc-2277"
    Jan  5 09:57:38.261: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bfkw" in namespace "gc-2277"
    Jan  5 09:57:38.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bmzp" in namespace "gc-2277"
    Jan  5 09:57:38.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xbqd" in namespace "gc-2277"
    Jan  5 09:57:38.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-42dhh" in namespace "gc-2277"
    Jan  5 09:57:38.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-48jdg" in namespace "gc-2277"
    Jan  5 09:57:38.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qdz6" in namespace "gc-2277"
    Jan  5 09:57:38.353: INFO: Deleting pod "simpletest-rc-to-be-deleted-4w6w7" in namespace "gc-2277"
    Jan  5 09:57:38.376: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zsrr" in namespace "gc-2277"
    Jan  5 09:57:38.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-56l8q" in namespace "gc-2277"
    Jan  5 09:57:38.403: INFO: Deleting pod "simpletest-rc-to-be-deleted-56zkg" in namespace "gc-2277"
    Jan  5 09:57:38.420: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jvlb" in namespace "gc-2277"
    Jan  5 09:57:38.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rgpg" in namespace "gc-2277"
    Jan  5 09:57:38.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-6w2gm" in namespace "gc-2277"
    Jan  5 09:57:38.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-6x5sm" in namespace "gc-2277"
    Jan  5 09:57:38.491: INFO: Deleting pod "simpletest-rc-to-be-deleted-74xpl" in namespace "gc-2277"
    Jan  5 09:57:38.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-78cxl" in namespace "gc-2277"
    Jan  5 09:57:38.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-7k8qx" in namespace "gc-2277"
    Jan  5 09:57:38.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qk2w" in namespace "gc-2277"
    Jan  5 09:57:38.546: INFO: Deleting pod "simpletest-rc-to-be-deleted-7z4k4" in namespace "gc-2277"
    Jan  5 09:57:38.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-8drnm" in namespace "gc-2277"
    Jan  5 09:57:38.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f7sb" in namespace "gc-2277"
    Jan  5 09:57:38.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fc9x" in namespace "gc-2277"
    Jan  5 09:57:38.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tsmg" in namespace "gc-2277"
    Jan  5 09:57:38.620: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xs5b" in namespace "gc-2277"
    Jan  5 09:57:38.632: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zq7m" in namespace "gc-2277"
    Jan  5 09:57:38.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-9br4w" in namespace "gc-2277"
    Jan  5 09:57:38.670: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jwsl" in namespace "gc-2277"
    Jan  5 09:57:38.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9xvl" in namespace "gc-2277"
    Jan  5 09:57:38.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgqrd" in namespace "gc-2277"
    Jan  5 09:57:38.708: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq7fz" in namespace "gc-2277"
    Jan  5 09:57:38.719: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6lk9" in namespace "gc-2277"
    Jan  5 09:57:38.731: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6z8t" in namespace "gc-2277"
    Jan  5 09:57:38.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmcgn" in namespace "gc-2277"
    Jan  5 09:57:38.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctwhw" in namespace "gc-2277"
    Jan  5 09:57:38.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcqks" in namespace "gc-2277"
    Jan  5 09:57:38.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh6qq" in namespace "gc-2277"
    Jan  5 09:57:38.808: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl6qh" in namespace "gc-2277"
    Jan  5 09:57:38.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-dn4x9" in namespace "gc-2277"
    Jan  5 09:57:38.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7zdr" in namespace "gc-2277"
    Jan  5 09:57:38.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8zz2" in namespace "gc-2277"
    Jan  5 09:57:38.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkgwx" in namespace "gc-2277"
    Jan  5 09:57:38.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8tsd" in namespace "gc-2277"
    Jan  5 09:57:38.886: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtqxd" in namespace "gc-2277"
    Jan  5 09:57:38.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzlf4" in namespace "gc-2277"
    Jan  5 09:57:38.936: INFO: Deleting pod "simpletest-rc-to-be-deleted-hsg4d" in namespace "gc-2277"
    Jan  5 09:57:38.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-jc78m" in namespace "gc-2277"
    Jan  5 09:57:38.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-jglnz" in namespace "gc-2277"
    Jan  5 09:57:38.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnrgb" in namespace "gc-2277"
    Jan  5 09:57:38.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-jtzvp" in namespace "gc-2277"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 09:57:39.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2277" for this suite. 01/05/23 09:57:39.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:57:39.019
Jan  5 09:57:39.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename resourcequota 01/05/23 09:57:39.02
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:57:39.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:57:39.044
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 01/05/23 09:57:39.05
STEP: Creating a ResourceQuota 01/05/23 09:57:44.057
STEP: Ensuring resource quota status is calculated 01/05/23 09:57:44.062
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 09:57:46.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9805" for this suite. 01/05/23 09:57:46.078
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":230,"skipped":4234,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.067 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:57:39.019
    Jan  5 09:57:39.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename resourcequota 01/05/23 09:57:39.02
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:57:39.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:57:39.044
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 01/05/23 09:57:39.05
    STEP: Creating a ResourceQuota 01/05/23 09:57:44.057
    STEP: Ensuring resource quota status is calculated 01/05/23 09:57:44.062
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 09:57:46.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9805" for this suite. 01/05/23 09:57:46.078
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:57:46.086
Jan  5 09:57:46.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-probe 01/05/23 09:57:46.088
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:57:46.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:57:46.12
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 09:58:46.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9631" for this suite. 01/05/23 09:58:46.151
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":231,"skipped":4235,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.072 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:57:46.086
    Jan  5 09:57:46.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-probe 01/05/23 09:57:46.088
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:57:46.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:57:46.12
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 09:58:46.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9631" for this suite. 01/05/23 09:58:46.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:58:46.16
Jan  5 09:58:46.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-runtime 01/05/23 09:58:46.161
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:58:46.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:58:46.196
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 01/05/23 09:58:46.202
STEP: wait for the container to reach Failed 01/05/23 09:58:46.211
STEP: get the container status 01/05/23 09:58:50.242
STEP: the container should be terminated 01/05/23 09:58:50.248
STEP: the termination message should be set 01/05/23 09:58:50.248
Jan  5 09:58:50.248: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/05/23 09:58:50.248
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  5 09:58:50.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6333" for this suite. 01/05/23 09:58:50.272
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":232,"skipped":4247,"failed":0}
------------------------------
â€¢ [4.119 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:58:46.16
    Jan  5 09:58:46.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-runtime 01/05/23 09:58:46.161
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:58:46.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:58:46.196
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 01/05/23 09:58:46.202
    STEP: wait for the container to reach Failed 01/05/23 09:58:46.211
    STEP: get the container status 01/05/23 09:58:50.242
    STEP: the container should be terminated 01/05/23 09:58:50.248
    STEP: the termination message should be set 01/05/23 09:58:50.248
    Jan  5 09:58:50.248: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/05/23 09:58:50.248
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  5 09:58:50.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6333" for this suite. 01/05/23 09:58:50.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:58:50.279
Jan  5 09:58:50.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-probe 01/05/23 09:58:50.28
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:58:50.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:58:50.299
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jan  5 09:58:50.313: INFO: Waiting up to 5m0s for pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f" in namespace "container-probe-5682" to be "running and ready"
Jan  5 09:58:50.318: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.566327ms
Jan  5 09:58:50.318: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:58:52.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 2.011905403s
Jan  5 09:58:52.325: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
Jan  5 09:58:54.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 4.012241651s
Jan  5 09:58:54.325: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
Jan  5 09:58:56.324: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 6.011698496s
Jan  5 09:58:56.324: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
Jan  5 09:58:58.326: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 8.013648601s
Jan  5 09:58:58.326: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
Jan  5 09:59:00.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 10.012659296s
Jan  5 09:59:00.326: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
Jan  5 09:59:02.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 12.012165939s
Jan  5 09:59:02.325: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
Jan  5 09:59:04.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 14.01223843s
Jan  5 09:59:04.325: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
Jan  5 09:59:06.324: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 16.011693848s
Jan  5 09:59:06.325: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
Jan  5 09:59:08.326: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 18.012823695s
Jan  5 09:59:08.326: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
Jan  5 09:59:10.324: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 20.010862009s
Jan  5 09:59:10.324: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
Jan  5 09:59:12.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=true. Elapsed: 22.012026934s
Jan  5 09:59:12.325: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = true)
Jan  5 09:59:12.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f" satisfied condition "running and ready"
Jan  5 09:59:12.330: INFO: Container started at 2023-01-05 09:58:51 +0000 UTC, pod became ready at 2023-01-05 09:59:10 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 09:59:12.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5682" for this suite. 01/05/23 09:59:12.339
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":233,"skipped":4252,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.066 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:58:50.279
    Jan  5 09:58:50.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-probe 01/05/23 09:58:50.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:58:50.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:58:50.299
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jan  5 09:58:50.313: INFO: Waiting up to 5m0s for pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f" in namespace "container-probe-5682" to be "running and ready"
    Jan  5 09:58:50.318: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.566327ms
    Jan  5 09:58:50.318: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:58:52.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 2.011905403s
    Jan  5 09:58:52.325: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
    Jan  5 09:58:54.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 4.012241651s
    Jan  5 09:58:54.325: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
    Jan  5 09:58:56.324: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 6.011698496s
    Jan  5 09:58:56.324: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
    Jan  5 09:58:58.326: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 8.013648601s
    Jan  5 09:58:58.326: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
    Jan  5 09:59:00.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 10.012659296s
    Jan  5 09:59:00.326: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
    Jan  5 09:59:02.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 12.012165939s
    Jan  5 09:59:02.325: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
    Jan  5 09:59:04.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 14.01223843s
    Jan  5 09:59:04.325: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
    Jan  5 09:59:06.324: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 16.011693848s
    Jan  5 09:59:06.325: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
    Jan  5 09:59:08.326: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 18.012823695s
    Jan  5 09:59:08.326: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
    Jan  5 09:59:10.324: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=false. Elapsed: 20.010862009s
    Jan  5 09:59:10.324: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = false)
    Jan  5 09:59:12.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f": Phase="Running", Reason="", readiness=true. Elapsed: 22.012026934s
    Jan  5 09:59:12.325: INFO: The phase of Pod test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f is Running (Ready = true)
    Jan  5 09:59:12.325: INFO: Pod "test-webserver-09f8065e-4bd9-4e45-a44b-ceb16ad0b00f" satisfied condition "running and ready"
    Jan  5 09:59:12.330: INFO: Container started at 2023-01-05 09:58:51 +0000 UTC, pod became ready at 2023-01-05 09:59:10 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 09:59:12.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5682" for this suite. 01/05/23 09:59:12.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:59:12.346
Jan  5 09:59:12.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-probe 01/05/23 09:59:12.347
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:59:12.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:59:12.37
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad in namespace container-probe-1243 01/05/23 09:59:12.376
Jan  5 09:59:12.387: INFO: Waiting up to 5m0s for pod "test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad" in namespace "container-probe-1243" to be "not pending"
Jan  5 09:59:12.393: INFO: Pod "test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.578259ms
Jan  5 09:59:14.400: INFO: Pod "test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad": Phase="Running", Reason="", readiness=true. Elapsed: 2.01265234s
Jan  5 09:59:14.400: INFO: Pod "test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad" satisfied condition "not pending"
Jan  5 09:59:14.400: INFO: Started pod test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad in namespace container-probe-1243
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 09:59:14.4
Jan  5 09:59:14.406: INFO: Initial restart count of pod test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad is 0
STEP: deleting the pod 01/05/23 10:03:15.204
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 10:03:15.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1243" for this suite. 01/05/23 10:03:15.229
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":234,"skipped":4263,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.890 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:59:12.346
    Jan  5 09:59:12.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-probe 01/05/23 09:59:12.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:59:12.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:59:12.37
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad in namespace container-probe-1243 01/05/23 09:59:12.376
    Jan  5 09:59:12.387: INFO: Waiting up to 5m0s for pod "test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad" in namespace "container-probe-1243" to be "not pending"
    Jan  5 09:59:12.393: INFO: Pod "test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.578259ms
    Jan  5 09:59:14.400: INFO: Pod "test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad": Phase="Running", Reason="", readiness=true. Elapsed: 2.01265234s
    Jan  5 09:59:14.400: INFO: Pod "test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad" satisfied condition "not pending"
    Jan  5 09:59:14.400: INFO: Started pod test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad in namespace container-probe-1243
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 09:59:14.4
    Jan  5 09:59:14.406: INFO: Initial restart count of pod test-webserver-40362ee8-5a45-4111-8b0b-12f45f9e7aad is 0
    STEP: deleting the pod 01/05/23 10:03:15.204
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 10:03:15.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1243" for this suite. 01/05/23 10:03:15.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:03:15.24
Jan  5 10:03:15.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename limitrange 01/05/23 10:03:15.241
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:15.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:15.284
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 01/05/23 10:03:15.291
STEP: Setting up watch 01/05/23 10:03:15.291
STEP: Submitting a LimitRange 01/05/23 10:03:15.395
STEP: Verifying LimitRange creation was observed 01/05/23 10:03:15.411
STEP: Fetching the LimitRange to ensure it has proper values 01/05/23 10:03:15.411
Jan  5 10:03:15.418: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan  5 10:03:15.418: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/05/23 10:03:15.418
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/05/23 10:03:15.43
Jan  5 10:03:15.435: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan  5 10:03:15.435: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/05/23 10:03:15.435
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/05/23 10:03:15.444
Jan  5 10:03:15.449: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan  5 10:03:15.450: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/05/23 10:03:15.45
STEP: Failing to create a Pod with more than max resources 01/05/23 10:03:15.458
STEP: Updating a LimitRange 01/05/23 10:03:15.464
STEP: Verifying LimitRange updating is effective 01/05/23 10:03:15.469
STEP: Creating a Pod with less than former min resources 01/05/23 10:03:17.475
STEP: Failing to create a Pod with more than max resources 01/05/23 10:03:17.484
STEP: Deleting a LimitRange 01/05/23 10:03:17.491
STEP: Verifying the LimitRange was deleted 01/05/23 10:03:17.496
Jan  5 10:03:22.503: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/05/23 10:03:22.503
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jan  5 10:03:22.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4043" for this suite. 01/05/23 10:03:22.524
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":235,"skipped":4327,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.295 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:03:15.24
    Jan  5 10:03:15.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename limitrange 01/05/23 10:03:15.241
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:15.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:15.284
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 01/05/23 10:03:15.291
    STEP: Setting up watch 01/05/23 10:03:15.291
    STEP: Submitting a LimitRange 01/05/23 10:03:15.395
    STEP: Verifying LimitRange creation was observed 01/05/23 10:03:15.411
    STEP: Fetching the LimitRange to ensure it has proper values 01/05/23 10:03:15.411
    Jan  5 10:03:15.418: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan  5 10:03:15.418: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/05/23 10:03:15.418
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/05/23 10:03:15.43
    Jan  5 10:03:15.435: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan  5 10:03:15.435: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/05/23 10:03:15.435
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/05/23 10:03:15.444
    Jan  5 10:03:15.449: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan  5 10:03:15.450: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/05/23 10:03:15.45
    STEP: Failing to create a Pod with more than max resources 01/05/23 10:03:15.458
    STEP: Updating a LimitRange 01/05/23 10:03:15.464
    STEP: Verifying LimitRange updating is effective 01/05/23 10:03:15.469
    STEP: Creating a Pod with less than former min resources 01/05/23 10:03:17.475
    STEP: Failing to create a Pod with more than max resources 01/05/23 10:03:17.484
    STEP: Deleting a LimitRange 01/05/23 10:03:17.491
    STEP: Verifying the LimitRange was deleted 01/05/23 10:03:17.496
    Jan  5 10:03:22.503: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/05/23 10:03:22.503
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jan  5 10:03:22.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-4043" for this suite. 01/05/23 10:03:22.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:03:22.537
Jan  5 10:03:22.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 10:03:22.537
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:22.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:22.56
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 01/05/23 10:03:22.567
Jan  5 10:03:22.567: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-1550 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/05/23 10:03:22.608
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 10:03:22.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1550" for this suite. 01/05/23 10:03:22.632
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":236,"skipped":4402,"failed":0}
------------------------------
â€¢ [0.103 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:03:22.537
    Jan  5 10:03:22.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 10:03:22.537
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:22.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:22.56
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 01/05/23 10:03:22.567
    Jan  5 10:03:22.567: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-1550 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/05/23 10:03:22.608
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 10:03:22.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1550" for this suite. 01/05/23 10:03:22.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:03:22.641
Jan  5 10:03:22.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename replicaset 01/05/23 10:03:22.642
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:22.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:22.662
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/05/23 10:03:22.668
STEP: Verify that the required pods have come up 01/05/23 10:03:22.674
Jan  5 10:03:22.677: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan  5 10:03:27.684: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/05/23 10:03:27.684
Jan  5 10:03:27.689: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/05/23 10:03:27.689
STEP: DeleteCollection of the ReplicaSets 01/05/23 10:03:27.711
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/05/23 10:03:27.72
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  5 10:03:27.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7172" for this suite. 01/05/23 10:03:27.758
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":237,"skipped":4439,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.140 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:03:22.641
    Jan  5 10:03:22.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename replicaset 01/05/23 10:03:22.642
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:22.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:22.662
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/05/23 10:03:22.668
    STEP: Verify that the required pods have come up 01/05/23 10:03:22.674
    Jan  5 10:03:22.677: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan  5 10:03:27.684: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/05/23 10:03:27.684
    Jan  5 10:03:27.689: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/05/23 10:03:27.689
    STEP: DeleteCollection of the ReplicaSets 01/05/23 10:03:27.711
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/05/23 10:03:27.72
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  5 10:03:27.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7172" for this suite. 01/05/23 10:03:27.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:03:27.782
Jan  5 10:03:27.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename disruption 01/05/23 10:03:27.783
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:27.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:27.815
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 01/05/23 10:03:27.828
STEP: Waiting for all pods to be running 01/05/23 10:03:29.892
Jan  5 10:03:29.898: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  5 10:03:31.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1928" for this suite. 01/05/23 10:03:31.918
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":238,"skipped":4447,"failed":0}
------------------------------
â€¢ [4.142 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:03:27.782
    Jan  5 10:03:27.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename disruption 01/05/23 10:03:27.783
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:27.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:27.815
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 01/05/23 10:03:27.828
    STEP: Waiting for all pods to be running 01/05/23 10:03:29.892
    Jan  5 10:03:29.898: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  5 10:03:31.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1928" for this suite. 01/05/23 10:03:31.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:03:31.924
Jan  5 10:03:31.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pods 01/05/23 10:03:31.925
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:31.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:31.946
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jan  5 10:03:31.967: INFO: Waiting up to 5m0s for pod "server-envvars-98b026f6-cbd9-41cd-8112-53bb241c2b87" in namespace "pods-6022" to be "running and ready"
Jan  5 10:03:31.975: INFO: Pod "server-envvars-98b026f6-cbd9-41cd-8112-53bb241c2b87": Phase="Pending", Reason="", readiness=false. Elapsed: 7.963066ms
Jan  5 10:03:31.975: INFO: The phase of Pod server-envvars-98b026f6-cbd9-41cd-8112-53bb241c2b87 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:03:33.983: INFO: Pod "server-envvars-98b026f6-cbd9-41cd-8112-53bb241c2b87": Phase="Running", Reason="", readiness=true. Elapsed: 2.015536881s
Jan  5 10:03:33.983: INFO: The phase of Pod server-envvars-98b026f6-cbd9-41cd-8112-53bb241c2b87 is Running (Ready = true)
Jan  5 10:03:33.983: INFO: Pod "server-envvars-98b026f6-cbd9-41cd-8112-53bb241c2b87" satisfied condition "running and ready"
Jan  5 10:03:34.012: INFO: Waiting up to 5m0s for pod "client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95" in namespace "pods-6022" to be "Succeeded or Failed"
Jan  5 10:03:34.029: INFO: Pod "client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95": Phase="Pending", Reason="", readiness=false. Elapsed: 17.765007ms
Jan  5 10:03:36.036: INFO: Pod "client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024487916s
Jan  5 10:03:38.035: INFO: Pod "client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023493047s
STEP: Saw pod success 01/05/23 10:03:38.035
Jan  5 10:03:38.035: INFO: Pod "client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95" satisfied condition "Succeeded or Failed"
Jan  5 10:03:38.040: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t pod client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95 container env3cont: <nil>
STEP: delete the pod 01/05/23 10:03:38.056
Jan  5 10:03:38.068: INFO: Waiting for pod client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95 to disappear
Jan  5 10:03:38.072: INFO: Pod client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 10:03:38.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6022" for this suite. 01/05/23 10:03:38.08
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":239,"skipped":4454,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.160 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:03:31.924
    Jan  5 10:03:31.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pods 01/05/23 10:03:31.925
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:31.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:31.946
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jan  5 10:03:31.967: INFO: Waiting up to 5m0s for pod "server-envvars-98b026f6-cbd9-41cd-8112-53bb241c2b87" in namespace "pods-6022" to be "running and ready"
    Jan  5 10:03:31.975: INFO: Pod "server-envvars-98b026f6-cbd9-41cd-8112-53bb241c2b87": Phase="Pending", Reason="", readiness=false. Elapsed: 7.963066ms
    Jan  5 10:03:31.975: INFO: The phase of Pod server-envvars-98b026f6-cbd9-41cd-8112-53bb241c2b87 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:03:33.983: INFO: Pod "server-envvars-98b026f6-cbd9-41cd-8112-53bb241c2b87": Phase="Running", Reason="", readiness=true. Elapsed: 2.015536881s
    Jan  5 10:03:33.983: INFO: The phase of Pod server-envvars-98b026f6-cbd9-41cd-8112-53bb241c2b87 is Running (Ready = true)
    Jan  5 10:03:33.983: INFO: Pod "server-envvars-98b026f6-cbd9-41cd-8112-53bb241c2b87" satisfied condition "running and ready"
    Jan  5 10:03:34.012: INFO: Waiting up to 5m0s for pod "client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95" in namespace "pods-6022" to be "Succeeded or Failed"
    Jan  5 10:03:34.029: INFO: Pod "client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95": Phase="Pending", Reason="", readiness=false. Elapsed: 17.765007ms
    Jan  5 10:03:36.036: INFO: Pod "client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024487916s
    Jan  5 10:03:38.035: INFO: Pod "client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023493047s
    STEP: Saw pod success 01/05/23 10:03:38.035
    Jan  5 10:03:38.035: INFO: Pod "client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95" satisfied condition "Succeeded or Failed"
    Jan  5 10:03:38.040: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t pod client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95 container env3cont: <nil>
    STEP: delete the pod 01/05/23 10:03:38.056
    Jan  5 10:03:38.068: INFO: Waiting for pod client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95 to disappear
    Jan  5 10:03:38.072: INFO: Pod client-envvars-ae7e3f21-8824-4f02-8800-a6fc6f10ba95 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 10:03:38.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6022" for this suite. 01/05/23 10:03:38.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:03:38.085
Jan  5 10:03:38.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 10:03:38.085
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:38.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:38.107
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-68c7bd91-76ad-4c6e-921d-681f8e360c96 01/05/23 10:03:38.113
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 10:03:38.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5535" for this suite. 01/05/23 10:03:38.125
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":240,"skipped":4460,"failed":0}
------------------------------
â€¢ [0.046 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:03:38.085
    Jan  5 10:03:38.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 10:03:38.085
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:38.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:38.107
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-68c7bd91-76ad-4c6e-921d-681f8e360c96 01/05/23 10:03:38.113
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 10:03:38.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5535" for this suite. 01/05/23 10:03:38.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:03:38.131
Jan  5 10:03:38.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 10:03:38.132
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:38.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:38.152
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jan  5 10:03:38.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 10:03:40.695
Jan  5 10:03:40.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-4058 --namespace=crd-publish-openapi-4058 create -f -'
Jan  5 10:03:41.313: INFO: stderr: ""
Jan  5 10:03:41.313: INFO: stdout: "e2e-test-crd-publish-openapi-8064-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  5 10:03:41.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-4058 --namespace=crd-publish-openapi-4058 delete e2e-test-crd-publish-openapi-8064-crds test-cr'
Jan  5 10:03:41.387: INFO: stderr: ""
Jan  5 10:03:41.387: INFO: stdout: "e2e-test-crd-publish-openapi-8064-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan  5 10:03:41.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-4058 --namespace=crd-publish-openapi-4058 apply -f -'
Jan  5 10:03:41.937: INFO: stderr: ""
Jan  5 10:03:41.937: INFO: stdout: "e2e-test-crd-publish-openapi-8064-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  5 10:03:41.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-4058 --namespace=crd-publish-openapi-4058 delete e2e-test-crd-publish-openapi-8064-crds test-cr'
Jan  5 10:03:42.008: INFO: stderr: ""
Jan  5 10:03:42.008: INFO: stdout: "e2e-test-crd-publish-openapi-8064-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/05/23 10:03:42.008
Jan  5 10:03:42.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-4058 explain e2e-test-crd-publish-openapi-8064-crds'
Jan  5 10:03:42.167: INFO: stderr: ""
Jan  5 10:03:42.167: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8064-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:03:44.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4058" for this suite. 01/05/23 10:03:44.721
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":241,"skipped":4482,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.597 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:03:38.131
    Jan  5 10:03:38.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 10:03:38.132
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:38.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:38.152
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jan  5 10:03:38.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 10:03:40.695
    Jan  5 10:03:40.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-4058 --namespace=crd-publish-openapi-4058 create -f -'
    Jan  5 10:03:41.313: INFO: stderr: ""
    Jan  5 10:03:41.313: INFO: stdout: "e2e-test-crd-publish-openapi-8064-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan  5 10:03:41.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-4058 --namespace=crd-publish-openapi-4058 delete e2e-test-crd-publish-openapi-8064-crds test-cr'
    Jan  5 10:03:41.387: INFO: stderr: ""
    Jan  5 10:03:41.387: INFO: stdout: "e2e-test-crd-publish-openapi-8064-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan  5 10:03:41.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-4058 --namespace=crd-publish-openapi-4058 apply -f -'
    Jan  5 10:03:41.937: INFO: stderr: ""
    Jan  5 10:03:41.937: INFO: stdout: "e2e-test-crd-publish-openapi-8064-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan  5 10:03:41.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-4058 --namespace=crd-publish-openapi-4058 delete e2e-test-crd-publish-openapi-8064-crds test-cr'
    Jan  5 10:03:42.008: INFO: stderr: ""
    Jan  5 10:03:42.008: INFO: stdout: "e2e-test-crd-publish-openapi-8064-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/05/23 10:03:42.008
    Jan  5 10:03:42.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-4058 explain e2e-test-crd-publish-openapi-8064-crds'
    Jan  5 10:03:42.167: INFO: stderr: ""
    Jan  5 10:03:42.167: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8064-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:03:44.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4058" for this suite. 01/05/23 10:03:44.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:03:44.732
Jan  5 10:03:44.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 10:03:44.732
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:44.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:44.755
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jan  5 10:03:44.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/05/23 10:03:48.266
Jan  5 10:03:48.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 create -f -'
Jan  5 10:03:48.868: INFO: stderr: ""
Jan  5 10:03:48.868: INFO: stdout: "e2e-test-crd-publish-openapi-5155-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  5 10:03:48.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 delete e2e-test-crd-publish-openapi-5155-crds test-foo'
Jan  5 10:03:48.937: INFO: stderr: ""
Jan  5 10:03:48.937: INFO: stdout: "e2e-test-crd-publish-openapi-5155-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan  5 10:03:48.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 apply -f -'
Jan  5 10:03:49.118: INFO: stderr: ""
Jan  5 10:03:49.118: INFO: stdout: "e2e-test-crd-publish-openapi-5155-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  5 10:03:49.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 delete e2e-test-crd-publish-openapi-5155-crds test-foo'
Jan  5 10:03:49.182: INFO: stderr: ""
Jan  5 10:03:49.182: INFO: stdout: "e2e-test-crd-publish-openapi-5155-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/05/23 10:03:49.182
Jan  5 10:03:49.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 create -f -'
Jan  5 10:03:49.339: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/05/23 10:03:49.339
Jan  5 10:03:49.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 create -f -'
Jan  5 10:03:49.808: INFO: rc: 1
Jan  5 10:03:49.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 apply -f -'
Jan  5 10:03:49.977: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/05/23 10:03:49.978
Jan  5 10:03:49.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 create -f -'
Jan  5 10:03:50.146: INFO: rc: 1
Jan  5 10:03:50.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 apply -f -'
Jan  5 10:03:50.312: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/05/23 10:03:50.312
Jan  5 10:03:50.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 explain e2e-test-crd-publish-openapi-5155-crds'
Jan  5 10:03:50.468: INFO: stderr: ""
Jan  5 10:03:50.468: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/05/23 10:03:50.469
Jan  5 10:03:50.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 explain e2e-test-crd-publish-openapi-5155-crds.metadata'
Jan  5 10:03:50.628: INFO: stderr: ""
Jan  5 10:03:50.628: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan  5 10:03:50.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 explain e2e-test-crd-publish-openapi-5155-crds.spec'
Jan  5 10:03:50.789: INFO: stderr: ""
Jan  5 10:03:50.789: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan  5 10:03:50.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 explain e2e-test-crd-publish-openapi-5155-crds.spec.bars'
Jan  5 10:03:50.947: INFO: stderr: ""
Jan  5 10:03:50.948: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/05/23 10:03:50.948
Jan  5 10:03:50.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 explain e2e-test-crd-publish-openapi-5155-crds.spec.bars2'
Jan  5 10:03:51.099: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:03:53.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9938" for this suite. 01/05/23 10:03:53.592
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":242,"skipped":4509,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.867 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:03:44.732
    Jan  5 10:03:44.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 10:03:44.732
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:44.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:44.755
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jan  5 10:03:44.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/05/23 10:03:48.266
    Jan  5 10:03:48.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 create -f -'
    Jan  5 10:03:48.868: INFO: stderr: ""
    Jan  5 10:03:48.868: INFO: stdout: "e2e-test-crd-publish-openapi-5155-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan  5 10:03:48.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 delete e2e-test-crd-publish-openapi-5155-crds test-foo'
    Jan  5 10:03:48.937: INFO: stderr: ""
    Jan  5 10:03:48.937: INFO: stdout: "e2e-test-crd-publish-openapi-5155-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan  5 10:03:48.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 apply -f -'
    Jan  5 10:03:49.118: INFO: stderr: ""
    Jan  5 10:03:49.118: INFO: stdout: "e2e-test-crd-publish-openapi-5155-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan  5 10:03:49.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 delete e2e-test-crd-publish-openapi-5155-crds test-foo'
    Jan  5 10:03:49.182: INFO: stderr: ""
    Jan  5 10:03:49.182: INFO: stdout: "e2e-test-crd-publish-openapi-5155-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/05/23 10:03:49.182
    Jan  5 10:03:49.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 create -f -'
    Jan  5 10:03:49.339: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/05/23 10:03:49.339
    Jan  5 10:03:49.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 create -f -'
    Jan  5 10:03:49.808: INFO: rc: 1
    Jan  5 10:03:49.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 apply -f -'
    Jan  5 10:03:49.977: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/05/23 10:03:49.978
    Jan  5 10:03:49.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 create -f -'
    Jan  5 10:03:50.146: INFO: rc: 1
    Jan  5 10:03:50.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 --namespace=crd-publish-openapi-9938 apply -f -'
    Jan  5 10:03:50.312: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/05/23 10:03:50.312
    Jan  5 10:03:50.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 explain e2e-test-crd-publish-openapi-5155-crds'
    Jan  5 10:03:50.468: INFO: stderr: ""
    Jan  5 10:03:50.468: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/05/23 10:03:50.469
    Jan  5 10:03:50.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 explain e2e-test-crd-publish-openapi-5155-crds.metadata'
    Jan  5 10:03:50.628: INFO: stderr: ""
    Jan  5 10:03:50.628: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan  5 10:03:50.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 explain e2e-test-crd-publish-openapi-5155-crds.spec'
    Jan  5 10:03:50.789: INFO: stderr: ""
    Jan  5 10:03:50.789: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan  5 10:03:50.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 explain e2e-test-crd-publish-openapi-5155-crds.spec.bars'
    Jan  5 10:03:50.947: INFO: stderr: ""
    Jan  5 10:03:50.948: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/05/23 10:03:50.948
    Jan  5 10:03:50.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-9938 explain e2e-test-crd-publish-openapi-5155-crds.spec.bars2'
    Jan  5 10:03:51.099: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:03:53.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9938" for this suite. 01/05/23 10:03:53.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:03:53.6
Jan  5 10:03:53.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename replicaset 01/05/23 10:03:53.601
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:53.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:53.623
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/05/23 10:03:53.63
Jan  5 10:03:53.641: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  5 10:03:58.649: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 10:03:58.649
STEP: getting scale subresource 01/05/23 10:03:58.65
STEP: updating a scale subresource 01/05/23 10:03:58.655
STEP: verifying the replicaset Spec.Replicas was modified 01/05/23 10:03:58.662
STEP: Patch a scale subresource 01/05/23 10:03:58.667
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  5 10:03:58.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8972" for this suite. 01/05/23 10:03:58.693
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":243,"skipped":4521,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.103 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:03:53.6
    Jan  5 10:03:53.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename replicaset 01/05/23 10:03:53.601
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:53.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:53.623
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/05/23 10:03:53.63
    Jan  5 10:03:53.641: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  5 10:03:58.649: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 10:03:58.649
    STEP: getting scale subresource 01/05/23 10:03:58.65
    STEP: updating a scale subresource 01/05/23 10:03:58.655
    STEP: verifying the replicaset Spec.Replicas was modified 01/05/23 10:03:58.662
    STEP: Patch a scale subresource 01/05/23 10:03:58.667
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  5 10:03:58.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8972" for this suite. 01/05/23 10:03:58.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:03:58.707
Jan  5 10:03:58.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 10:03:58.708
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:58.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:58.732
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 01/05/23 10:03:58.738
Jan  5 10:03:58.747: INFO: Waiting up to 5m0s for pod "pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2" in namespace "emptydir-8506" to be "Succeeded or Failed"
Jan  5 10:03:58.772: INFO: Pod "pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.377765ms
Jan  5 10:04:00.779: INFO: Pod "pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031761749s
Jan  5 10:04:02.781: INFO: Pod "pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033236104s
STEP: Saw pod success 01/05/23 10:04:02.781
Jan  5 10:04:02.781: INFO: Pod "pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2" satisfied condition "Succeeded or Failed"
Jan  5 10:04:02.786: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2 container test-container: <nil>
STEP: delete the pod 01/05/23 10:04:02.842
Jan  5 10:04:02.856: INFO: Waiting for pod pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2 to disappear
Jan  5 10:04:02.861: INFO: Pod pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 10:04:02.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8506" for this suite. 01/05/23 10:04:02.871
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":244,"skipped":4569,"failed":0}
------------------------------
â€¢ [4.173 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:03:58.707
    Jan  5 10:03:58.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 10:03:58.708
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:03:58.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:03:58.732
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/05/23 10:03:58.738
    Jan  5 10:03:58.747: INFO: Waiting up to 5m0s for pod "pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2" in namespace "emptydir-8506" to be "Succeeded or Failed"
    Jan  5 10:03:58.772: INFO: Pod "pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.377765ms
    Jan  5 10:04:00.779: INFO: Pod "pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031761749s
    Jan  5 10:04:02.781: INFO: Pod "pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033236104s
    STEP: Saw pod success 01/05/23 10:04:02.781
    Jan  5 10:04:02.781: INFO: Pod "pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2" satisfied condition "Succeeded or Failed"
    Jan  5 10:04:02.786: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2 container test-container: <nil>
    STEP: delete the pod 01/05/23 10:04:02.842
    Jan  5 10:04:02.856: INFO: Waiting for pod pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2 to disappear
    Jan  5 10:04:02.861: INFO: Pod pod-25e56d48-5ddf-4edd-925b-d42b9699d2a2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 10:04:02.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8506" for this suite. 01/05/23 10:04:02.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:04:02.881
Jan  5 10:04:02.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename deployment 01/05/23 10:04:02.881
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:02.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:02.902
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan  5 10:04:02.908: INFO: Creating deployment "webserver-deployment"
Jan  5 10:04:02.913: INFO: Waiting for observed generation 1
Jan  5 10:04:04.932: INFO: Waiting for all required pods to come up
Jan  5 10:04:04.942: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/05/23 10:04:04.942
Jan  5 10:04:04.942: INFO: Waiting for deployment "webserver-deployment" to complete
Jan  5 10:04:04.953: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan  5 10:04:04.969: INFO: Updating deployment webserver-deployment
Jan  5 10:04:04.969: INFO: Waiting for observed generation 2
Jan  5 10:04:06.988: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan  5 10:04:06.996: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan  5 10:04:07.000: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  5 10:04:07.016: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan  5 10:04:07.016: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan  5 10:04:07.021: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  5 10:04:07.031: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan  5 10:04:07.031: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan  5 10:04:07.043: INFO: Updating deployment webserver-deployment
Jan  5 10:04:07.043: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan  5 10:04:07.057: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan  5 10:04:09.074: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 10:04:09.085: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5108  a8a42e48-1f6c-4167-abf1-7b26673494cc 33577 3 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005394b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:20,UnavailableReplicas:13,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-05 10:04:07 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-05 10:04:08 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,},},ReadyReplicas:20,CollisionCount:nil,},}

Jan  5 10:04:09.090: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-5108  0dd6a040-ec50-4818-82f0-0f8269d0b0a8 33501 3 2023-01-05 10:04:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a8a42e48-1f6c-4167-abf1-7b26673494cc 0xc0053c4097 0xc0053c4098}] [] [{kube-controller-manager Update apps/v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8a42e48-1f6c-4167-abf1-7b26673494cc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053c4208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 10:04:09.090: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan  5 10:04:09.091: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-5108  ceef2461-f73e-4d96-852b-f072f0d42c48 33576 3 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a8a42e48-1f6c-4167-abf1-7b26673494cc 0xc0053c4287 0xc0053c4288}] [] [{kube-controller-manager Update apps/v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8a42e48-1f6c-4167-abf1-7b26673494cc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053c4338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:20,AvailableReplicas:20,Conditions:[]ReplicaSetCondition{},},}
Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-4r87k" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-4r87k webserver-deployment-69b7448995- deployment-5108  8e22de13-5c6b-4203-869f-4051a15ef8f7 33553 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c4917 0xc0053c4918}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6l5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6l5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.150,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-6hklg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6hklg webserver-deployment-69b7448995- deployment-5108  80a4557d-f64d-4421-8ec9-2b7029819489 33506 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c4b30 0xc0053c4b31}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pf67b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pf67b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-6q4gm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6q4gm webserver-deployment-69b7448995- deployment-5108  eeeb7b13-2fa5-4e80-8eb5-7fcb6a8a2939 33494 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c4d40 0xc0053c4d41}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xl2zb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xl2zb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-7dc2s" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-7dc2s webserver-deployment-69b7448995- deployment-5108  6d3fdbd4-06ed-42a0-9e06-8b418e2d14ad 33432 0 2023-01-05 10:04:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c4fb0 0xc0053c4fb1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2dl4p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2dl4p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.171,StartTime:2023-01-05 10:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-d6zmj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-d6zmj webserver-deployment-69b7448995- deployment-5108  cd8c48a9-6b48-4208-b7b5-f96f98e9c3cf 33495 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c5220 0xc0053c5221}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sm85h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sm85h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-j56rc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-j56rc webserver-deployment-69b7448995- deployment-5108  7c9414ad-41fd-4508-a685-617b9833844f 33435 0 2023-01-05 10:04:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c5467 0xc0053c5468}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gw657,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gw657,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.49,StartTime:2023-01-05 10:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-jlz9z" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-jlz9z webserver-deployment-69b7448995- deployment-5108  4a6b8e9c-a59c-4b89-a77b-84139e27a573 33434 0 2023-01-05 10:04:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c56d0 0xc0053c56d1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72lmv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72lmv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.181,StartTime:2023-01-05 10:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-69b7448995-pz9n7" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-pz9n7 webserver-deployment-69b7448995- deployment-5108  0ca21308-4bc1-4b66-8ead-635d7152ab3b 33489 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c58e0 0xc0053c58e1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-prk4m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-prk4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-69b7448995-qq2xj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qq2xj webserver-deployment-69b7448995- deployment-5108  900a73c3-1ba2-4249-bd1b-ad1271c49f5d 33539 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c5ab7 0xc0053c5ab8}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fr8r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fr8r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.91,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-69b7448995-sbr5c" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-sbr5c webserver-deployment-69b7448995- deployment-5108  3406e45d-7e29-49dc-8063-f3910fa654b3 33493 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c5d30 0xc0053c5d31}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6jmm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6jmm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-69b7448995-sfmzc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-sfmzc webserver-deployment-69b7448995- deployment-5108  ed9013d9-2c21-4430-b274-e9825ca45088 33436 0 2023-01-05 10:04:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c5f07 0xc0053c5f08}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6q7lk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6q7lk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.29,StartTime:2023-01-05 10:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-69b7448995-svk44" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-svk44 webserver-deployment-69b7448995- deployment-5108  76658138-9e65-4af2-a112-37d34572be54 33578 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053e6110 0xc0053e6111}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xvlsm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xvlsm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.88,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-69b7448995-z8n6w" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-z8n6w webserver-deployment-69b7448995- deployment-5108  c9c3e9d6-0394-495e-8074-8c4b0a24d086 33528 0 2023-01-05 10:04:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053e6350 0xc0053e6351}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pdl9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pdl9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.158,StartTime:2023-01-05 10:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-845c8977d9-42vx6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-42vx6 webserver-deployment-845c8977d9- deployment-5108  40a86113-5ef9-41e2-9448-97fe66fa7970 33570 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e6580 0xc0053e6581}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q27nf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q27nf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.201,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2af8e03dd2db94b60a3d627d2a6761e4fed49b66addedc076ef9c505a9bbc117,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-845c8977d9-58dj6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-58dj6 webserver-deployment-845c8977d9- deployment-5108  676fed6c-da98-4536-9992-3ba449ee970a 33357 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e67b0 0xc0053e67b1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6sqg6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6sqg6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.175,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3a74ae897054e9dce21a6ab61a0ca839eb57ffa2371687d14a1bbadb98533d7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-6g9rb" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6g9rb webserver-deployment-845c8977d9- deployment-5108  2a5af792-612f-4212-9f8d-dec79db246e6 33552 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e69e0 0xc0053e69e1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zsbpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zsbpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.191,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4c289d1dd389728b3eb4b65023ad0174fb293441f9f4e86697b8b07cf14218fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-82jwf" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-82jwf webserver-deployment-845c8977d9- deployment-5108  a8ccbbcd-b7f6-477a-88af-f9a366cb974c 33574 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e6be0 0xc0053e6be1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78bsw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78bsw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.204,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5bd58d4df681650103cd6d770862007553db1e625000b836cffde9e659a7d8cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-9xwpg" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9xwpg webserver-deployment-845c8977d9- deployment-5108  f4d6f493-6a43-4781-9033-d1b0e09ccddc 33378 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e6e00 0xc0053e6e01}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8gb5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8gb5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.254,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://80f20daa10c3227380f8dd1a5b574e7ff0172c7e63751ce07ab6ff2a3ae92454,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.254,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-cc8wl" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-cc8wl webserver-deployment-845c8977d9- deployment-5108  da5ab532-37b1-42ab-b22e-906c49b8eed9 33353 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e6fe0 0xc0053e6fe1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwx9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwx9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.40,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://de79e2902801f69aa081a479d6438938bae074fe07cd652a047c15c6eabb0114,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-dlnkk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dlnkk webserver-deployment-845c8977d9- deployment-5108  649d7ee0-2a40-48c7-be87-f91bf32691ec 33549 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e71b0 0xc0053e71b1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6t88,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6t88,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.51,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://88205ff0a08c68ac414d4616ac650b4620ef6f873fc32737a15c954aa37d4d70,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-fhkgw" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-fhkgw webserver-deployment-845c8977d9- deployment-5108  2a8bb38c-df3b-4c2c-9f75-658f4f08b066 33565 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e73e0 0xc0053e73e1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lqx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lqx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.3,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c83122b8c3f01969f52fcc4e6a9e19f22051c51e18fc29dfee1cf8036fcd97d1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-hfkjj" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hfkjj webserver-deployment-845c8977d9- deployment-5108  7d3c1566-5fe0-4cc6-abdb-818d7fa7fcd6 33568 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e7620 0xc0053e7621}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xnwdr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xnwdr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.150,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8c0876abd44b8abc35b1f765702cb8cc8fe0fb5e521e7e786568b0c92f8a9d5e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-j7c8d" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-j7c8d webserver-deployment-845c8977d9- deployment-5108  49671d9b-b27d-4a69-b9d4-ea513e994185 33556 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e78e0 0xc0053e78e1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kjspw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kjspw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.31,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8ad6600ebc0900d760c741c1108e9127494a925b787969ed92226ae2397a372f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-jkvgn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jkvgn webserver-deployment-845c8977d9- deployment-5108  6cf48084-f457-4684-8d1a-6135c123b66e 33350 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e7b40 0xc0053e7b41}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c6kzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c6kzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.158,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c3e9711e003a5da19b0c8153bbdf4922de78c94d4b54dd931653cf65fcfe9342,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-k2lsn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-k2lsn webserver-deployment-845c8977d9- deployment-5108  736466ac-5cce-4b87-abd0-993f758ca621 33360 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e7d90 0xc0053e7d91}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-msz8m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-msz8m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.192,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://31e78806f404f296fce10d1813d3e3d73c297cb46631db05e3874ec9fa807d0a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-k6v98" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-k6v98 webserver-deployment-845c8977d9- deployment-5108  81d73be5-e630-400a-8bfb-f8198f9cdbf2 33366 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e7f80 0xc0053e7f81}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xm9vn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xm9vn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.207,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://dcd8de179878149649c48358bfb1a80cd3c452fce633c761671296d3e6b97cf2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-nht2l" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-nht2l webserver-deployment-845c8977d9- deployment-5108  9a934c48-e051-471f-8ce7-d8232d81fb28 33540 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540a1c0 0xc00540a1c1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qvcfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvcfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.93,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b09b60973828faeaa0b066fdc97ca244360aa59f6ffaa7813df7babde60982c9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-rs92x" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rs92x webserver-deployment-845c8977d9- deployment-5108  d874e281-e310-4d8b-b4eb-987c9de590ba 33542 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540a3d0 0xc00540a3d1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vh2rc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vh2rc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.182,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9c523b94e9aaf425ef4401bb4d2ea072f14e2b97e4dd91deb82df660658b4cd8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-rzkx8" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rzkx8 webserver-deployment-845c8977d9- deployment-5108  b8d7bb7a-64ab-4731-9014-9b3787a0507d 33370 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540a610 0xc00540a611}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8nhnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8nhnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.17,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e9ad3f9dee0a7b24dd675a2722d4bcc09acb332f211ab7fa203642be0ad8cb6a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-sd5q9" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-sd5q9 webserver-deployment-845c8977d9- deployment-5108  6a0319fe-65c1-45bb-bdfa-777d385b29e5 33562 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540a800 0xc00540a801}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vk4x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vk4x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.146,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a6906f0edb9bbdcbf3a0e6543c6c5a35c70dfda52836668c9bb33de572f8e965,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-tqr95" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-tqr95 webserver-deployment-845c8977d9- deployment-5108  b9378646-d7fe-4099-b6a8-85c6697c9394 33546 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540a9e0 0xc00540a9e1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rpwtc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rpwtc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.88,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b438bde5e1d3391f26a2b786ae943352ec03860928f98bb9097104203bda7702,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.114: INFO: Pod "webserver-deployment-845c8977d9-x85tp" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-x85tp webserver-deployment-845c8977d9- deployment-5108  3b8c74c0-d206-406f-a56f-1d2a61180e76 33557 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540ac30 0xc00540ac31}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vnjk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vnjk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.58,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cc9b9658bfd0433a2aa6c16318c3a9862faeb7d64a59f535baec1d46706ea992,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:04:09.114: INFO: Pod "webserver-deployment-845c8977d9-zq2w4" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-zq2w4 webserver-deployment-845c8977d9- deployment-5108  d4cf7742-21d6-4056-b833-d11217adcb33 33375 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540ae80 0xc00540ae81}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5hqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5hqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.68,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8e53755d6ae10fd843e80c12c3caf7e6c4db680cd4767d20a84b708d141901cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 10:04:09.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5108" for this suite. 01/05/23 10:04:09.125
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":245,"skipped":4594,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.261 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:04:02.881
    Jan  5 10:04:02.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename deployment 01/05/23 10:04:02.881
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:02.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:02.902
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan  5 10:04:02.908: INFO: Creating deployment "webserver-deployment"
    Jan  5 10:04:02.913: INFO: Waiting for observed generation 1
    Jan  5 10:04:04.932: INFO: Waiting for all required pods to come up
    Jan  5 10:04:04.942: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/05/23 10:04:04.942
    Jan  5 10:04:04.942: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan  5 10:04:04.953: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan  5 10:04:04.969: INFO: Updating deployment webserver-deployment
    Jan  5 10:04:04.969: INFO: Waiting for observed generation 2
    Jan  5 10:04:06.988: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan  5 10:04:06.996: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan  5 10:04:07.000: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan  5 10:04:07.016: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan  5 10:04:07.016: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan  5 10:04:07.021: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan  5 10:04:07.031: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan  5 10:04:07.031: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan  5 10:04:07.043: INFO: Updating deployment webserver-deployment
    Jan  5 10:04:07.043: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan  5 10:04:07.057: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan  5 10:04:09.074: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 10:04:09.085: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-5108  a8a42e48-1f6c-4167-abf1-7b26673494cc 33577 3 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005394b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:20,UnavailableReplicas:13,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-05 10:04:07 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-05 10:04:08 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,},},ReadyReplicas:20,CollisionCount:nil,},}

    Jan  5 10:04:09.090: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-5108  0dd6a040-ec50-4818-82f0-0f8269d0b0a8 33501 3 2023-01-05 10:04:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a8a42e48-1f6c-4167-abf1-7b26673494cc 0xc0053c4097 0xc0053c4098}] [] [{kube-controller-manager Update apps/v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8a42e48-1f6c-4167-abf1-7b26673494cc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053c4208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 10:04:09.090: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan  5 10:04:09.091: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-5108  ceef2461-f73e-4d96-852b-f072f0d42c48 33576 3 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a8a42e48-1f6c-4167-abf1-7b26673494cc 0xc0053c4287 0xc0053c4288}] [] [{kube-controller-manager Update apps/v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8a42e48-1f6c-4167-abf1-7b26673494cc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053c4338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:20,AvailableReplicas:20,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-4r87k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-4r87k webserver-deployment-69b7448995- deployment-5108  8e22de13-5c6b-4203-869f-4051a15ef8f7 33553 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c4917 0xc0053c4918}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6l5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6l5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.150,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-6hklg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6hklg webserver-deployment-69b7448995- deployment-5108  80a4557d-f64d-4421-8ec9-2b7029819489 33506 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c4b30 0xc0053c4b31}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pf67b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pf67b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-6q4gm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6q4gm webserver-deployment-69b7448995- deployment-5108  eeeb7b13-2fa5-4e80-8eb5-7fcb6a8a2939 33494 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c4d40 0xc0053c4d41}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xl2zb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xl2zb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-7dc2s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-7dc2s webserver-deployment-69b7448995- deployment-5108  6d3fdbd4-06ed-42a0-9e06-8b418e2d14ad 33432 0 2023-01-05 10:04:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c4fb0 0xc0053c4fb1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2dl4p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2dl4p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.171,StartTime:2023-01-05 10:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-d6zmj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-d6zmj webserver-deployment-69b7448995- deployment-5108  cd8c48a9-6b48-4208-b7b5-f96f98e9c3cf 33495 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c5220 0xc0053c5221}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sm85h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sm85h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-j56rc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-j56rc webserver-deployment-69b7448995- deployment-5108  7c9414ad-41fd-4508-a685-617b9833844f 33435 0 2023-01-05 10:04:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c5467 0xc0053c5468}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gw657,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gw657,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.49,StartTime:2023-01-05 10:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.110: INFO: Pod "webserver-deployment-69b7448995-jlz9z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-jlz9z webserver-deployment-69b7448995- deployment-5108  4a6b8e9c-a59c-4b89-a77b-84139e27a573 33434 0 2023-01-05 10:04:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c56d0 0xc0053c56d1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72lmv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72lmv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.181,StartTime:2023-01-05 10:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-69b7448995-pz9n7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-pz9n7 webserver-deployment-69b7448995- deployment-5108  0ca21308-4bc1-4b66-8ead-635d7152ab3b 33489 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c58e0 0xc0053c58e1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-prk4m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-prk4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-69b7448995-qq2xj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qq2xj webserver-deployment-69b7448995- deployment-5108  900a73c3-1ba2-4249-bd1b-ad1271c49f5d 33539 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c5ab7 0xc0053c5ab8}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fr8r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fr8r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.91,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-69b7448995-sbr5c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-sbr5c webserver-deployment-69b7448995- deployment-5108  3406e45d-7e29-49dc-8063-f3910fa654b3 33493 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c5d30 0xc0053c5d31}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6jmm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6jmm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-69b7448995-sfmzc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-sfmzc webserver-deployment-69b7448995- deployment-5108  ed9013d9-2c21-4430-b274-e9825ca45088 33436 0 2023-01-05 10:04:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053c5f07 0xc0053c5f08}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6q7lk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6q7lk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.29,StartTime:2023-01-05 10:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-69b7448995-svk44" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-svk44 webserver-deployment-69b7448995- deployment-5108  76658138-9e65-4af2-a112-37d34572be54 33578 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053e6110 0xc0053e6111}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xvlsm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xvlsm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.88,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-69b7448995-z8n6w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-z8n6w webserver-deployment-69b7448995- deployment-5108  c9c3e9d6-0394-495e-8074-8c4b0a24d086 33528 0 2023-01-05 10:04:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 0dd6a040-ec50-4818-82f0-0f8269d0b0a8 0xc0053e6350 0xc0053e6351}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd6a040-ec50-4818-82f0-0f8269d0b0a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pdl9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pdl9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.158,StartTime:2023-01-05 10:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-845c8977d9-42vx6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-42vx6 webserver-deployment-845c8977d9- deployment-5108  40a86113-5ef9-41e2-9448-97fe66fa7970 33570 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e6580 0xc0053e6581}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q27nf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q27nf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.201,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2af8e03dd2db94b60a3d627d2a6761e4fed49b66addedc076ef9c505a9bbc117,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.111: INFO: Pod "webserver-deployment-845c8977d9-58dj6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-58dj6 webserver-deployment-845c8977d9- deployment-5108  676fed6c-da98-4536-9992-3ba449ee970a 33357 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e67b0 0xc0053e67b1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6sqg6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6sqg6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.175,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3a74ae897054e9dce21a6ab61a0ca839eb57ffa2371687d14a1bbadb98533d7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-6g9rb" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6g9rb webserver-deployment-845c8977d9- deployment-5108  2a5af792-612f-4212-9f8d-dec79db246e6 33552 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e69e0 0xc0053e69e1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zsbpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zsbpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.191,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4c289d1dd389728b3eb4b65023ad0174fb293441f9f4e86697b8b07cf14218fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-82jwf" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-82jwf webserver-deployment-845c8977d9- deployment-5108  a8ccbbcd-b7f6-477a-88af-f9a366cb974c 33574 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e6be0 0xc0053e6be1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78bsw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78bsw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.204,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5bd58d4df681650103cd6d770862007553db1e625000b836cffde9e659a7d8cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-9xwpg" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9xwpg webserver-deployment-845c8977d9- deployment-5108  f4d6f493-6a43-4781-9033-d1b0e09ccddc 33378 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e6e00 0xc0053e6e01}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8gb5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8gb5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.254,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://80f20daa10c3227380f8dd1a5b574e7ff0172c7e63751ce07ab6ff2a3ae92454,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.254,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-cc8wl" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-cc8wl webserver-deployment-845c8977d9- deployment-5108  da5ab532-37b1-42ab-b22e-906c49b8eed9 33353 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e6fe0 0xc0053e6fe1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwx9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwx9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.40,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://de79e2902801f69aa081a479d6438938bae074fe07cd652a047c15c6eabb0114,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-dlnkk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dlnkk webserver-deployment-845c8977d9- deployment-5108  649d7ee0-2a40-48c7-be87-f91bf32691ec 33549 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e71b0 0xc0053e71b1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6t88,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6t88,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.51,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://88205ff0a08c68ac414d4616ac650b4620ef6f873fc32737a15c954aa37d4d70,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-fhkgw" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-fhkgw webserver-deployment-845c8977d9- deployment-5108  2a8bb38c-df3b-4c2c-9f75-658f4f08b066 33565 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e73e0 0xc0053e73e1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lqx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lqx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.3,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c83122b8c3f01969f52fcc4e6a9e19f22051c51e18fc29dfee1cf8036fcd97d1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.112: INFO: Pod "webserver-deployment-845c8977d9-hfkjj" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hfkjj webserver-deployment-845c8977d9- deployment-5108  7d3c1566-5fe0-4cc6-abdb-818d7fa7fcd6 33568 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e7620 0xc0053e7621}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xnwdr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xnwdr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.150,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8c0876abd44b8abc35b1f765702cb8cc8fe0fb5e521e7e786568b0c92f8a9d5e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-j7c8d" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-j7c8d webserver-deployment-845c8977d9- deployment-5108  49671d9b-b27d-4a69-b9d4-ea513e994185 33556 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e78e0 0xc0053e78e1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kjspw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kjspw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.31,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8ad6600ebc0900d760c741c1108e9127494a925b787969ed92226ae2397a372f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-jkvgn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jkvgn webserver-deployment-845c8977d9- deployment-5108  6cf48084-f457-4684-8d1a-6135c123b66e 33350 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e7b40 0xc0053e7b41}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c6kzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c6kzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.158,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c3e9711e003a5da19b0c8153bbdf4922de78c94d4b54dd931653cf65fcfe9342,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-k2lsn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-k2lsn webserver-deployment-845c8977d9- deployment-5108  736466ac-5cce-4b87-abd0-993f758ca621 33360 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e7d90 0xc0053e7d91}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-msz8m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-msz8m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.192,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://31e78806f404f296fce10d1813d3e3d73c297cb46631db05e3874ec9fa807d0a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-k6v98" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-k6v98 webserver-deployment-845c8977d9- deployment-5108  81d73be5-e630-400a-8bfb-f8198f9cdbf2 33366 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc0053e7f80 0xc0053e7f81}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xm9vn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xm9vn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.207,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://dcd8de179878149649c48358bfb1a80cd3c452fce633c761671296d3e6b97cf2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-nht2l" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-nht2l webserver-deployment-845c8977d9- deployment-5108  9a934c48-e051-471f-8ce7-d8232d81fb28 33540 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540a1c0 0xc00540a1c1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qvcfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvcfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.93,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b09b60973828faeaa0b066fdc97ca244360aa59f6ffaa7813df7babde60982c9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-rs92x" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rs92x webserver-deployment-845c8977d9- deployment-5108  d874e281-e310-4d8b-b4eb-987c9de590ba 33542 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540a3d0 0xc00540a3d1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vh2rc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vh2rc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.182,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9c523b94e9aaf425ef4401bb4d2ea072f14e2b97e4dd91deb82df660658b4cd8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-rzkx8" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rzkx8 webserver-deployment-845c8977d9- deployment-5108  b8d7bb7a-64ab-4731-9014-9b3787a0507d 33370 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540a610 0xc00540a611}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8nhnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8nhnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.17,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e9ad3f9dee0a7b24dd675a2722d4bcc09acb332f211ab7fa203642be0ad8cb6a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-sd5q9" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-sd5q9 webserver-deployment-845c8977d9- deployment-5108  6a0319fe-65c1-45bb-bdfa-777d385b29e5 33562 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540a800 0xc00540a801}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vk4x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vk4x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.146,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a6906f0edb9bbdcbf3a0e6543c6c5a35c70dfda52836668c9bb33de572f8e965,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.113: INFO: Pod "webserver-deployment-845c8977d9-tqr95" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-tqr95 webserver-deployment-845c8977d9- deployment-5108  b9378646-d7fe-4099-b6a8-85c6697c9394 33546 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540a9e0 0xc00540a9e1}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rpwtc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rpwtc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.174,PodIP:10.96.0.88,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b438bde5e1d3391f26a2b786ae943352ec03860928f98bb9097104203bda7702,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.114: INFO: Pod "webserver-deployment-845c8977d9-x85tp" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-x85tp webserver-deployment-845c8977d9- deployment-5108  3b8c74c0-d206-406f-a56f-1d2a61180e76 33557 0 2023-01-05 10:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540ac30 0xc00540ac31}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vnjk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vnjk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.128,PodIP:10.96.3.58,StartTime:2023-01-05 10:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cc9b9658bfd0433a2aa6c16318c3a9862faeb7d64a59f535baec1d46706ea992,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:04:09.114: INFO: Pod "webserver-deployment-845c8977d9-zq2w4" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-zq2w4 webserver-deployment-845c8977d9- deployment-5108  d4cf7742-21d6-4056-b833-d11217adcb33 33375 0 2023-01-05 10:04:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 ceef2461-f73e-4d96-852b-f072f0d42c48 0xc00540ae80 0xc00540ae81}] [] [{kube-controller-manager Update v1 2023-01-05 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceef2461-f73e-4d96-852b-f072f0d42c48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:04:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5hqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5hqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:10.96.1.68,StartTime:2023-01-05 10:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8e53755d6ae10fd843e80c12c3caf7e6c4db680cd4767d20a84b708d141901cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 10:04:09.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5108" for this suite. 01/05/23 10:04:09.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:04:09.143
Jan  5 10:04:09.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename server-version 01/05/23 10:04:09.145
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:09.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:09.168
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/05/23 10:04:09.174
STEP: Confirm major version 01/05/23 10:04:09.178
Jan  5 10:04:09.178: INFO: Major version: 1
STEP: Confirm minor version 01/05/23 10:04:09.178
Jan  5 10:04:09.178: INFO: cleanMinorVersion: 25
Jan  5 10:04:09.178: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jan  5 10:04:09.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7335" for this suite. 01/05/23 10:04:09.189
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":246,"skipped":4604,"failed":0}
------------------------------
â€¢ [0.058 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:04:09.143
    Jan  5 10:04:09.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename server-version 01/05/23 10:04:09.145
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:09.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:09.168
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/05/23 10:04:09.174
    STEP: Confirm major version 01/05/23 10:04:09.178
    Jan  5 10:04:09.178: INFO: Major version: 1
    STEP: Confirm minor version 01/05/23 10:04:09.178
    Jan  5 10:04:09.178: INFO: cleanMinorVersion: 25
    Jan  5 10:04:09.178: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jan  5 10:04:09.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-7335" for this suite. 01/05/23 10:04:09.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:04:09.203
Jan  5 10:04:09.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 10:04:09.203
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:09.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:09.229
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 10:04:09.251
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:04:09.849
STEP: Deploying the webhook pod 01/05/23 10:04:09.859
STEP: Wait for the deployment to be ready 01/05/23 10:04:09.871
Jan  5 10:04:09.885: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 10:04:11.909
STEP: Verifying the service has paired with the endpoint 01/05/23 10:04:11.934
Jan  5 10:04:12.934: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jan  5 10:04:12.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-287-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 10:04:13.46
STEP: Creating a custom resource while v1 is storage version 01/05/23 10:04:13.55
STEP: Patching Custom Resource Definition to set v2 as storage 01/05/23 10:04:15.813
STEP: Patching the custom resource while v2 is storage version 01/05/23 10:04:15.861
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:04:16.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9043" for this suite. 01/05/23 10:04:16.499
STEP: Destroying namespace "webhook-9043-markers" for this suite. 01/05/23 10:04:16.506
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":247,"skipped":4637,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.360 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:04:09.203
    Jan  5 10:04:09.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 10:04:09.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:09.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:09.229
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 10:04:09.251
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:04:09.849
    STEP: Deploying the webhook pod 01/05/23 10:04:09.859
    STEP: Wait for the deployment to be ready 01/05/23 10:04:09.871
    Jan  5 10:04:09.885: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 10:04:11.909
    STEP: Verifying the service has paired with the endpoint 01/05/23 10:04:11.934
    Jan  5 10:04:12.934: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jan  5 10:04:12.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-287-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 10:04:13.46
    STEP: Creating a custom resource while v1 is storage version 01/05/23 10:04:13.55
    STEP: Patching Custom Resource Definition to set v2 as storage 01/05/23 10:04:15.813
    STEP: Patching the custom resource while v2 is storage version 01/05/23 10:04:15.861
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:04:16.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9043" for this suite. 01/05/23 10:04:16.499
    STEP: Destroying namespace "webhook-9043-markers" for this suite. 01/05/23 10:04:16.506
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:04:16.563
Jan  5 10:04:16.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 10:04:16.564
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:16.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:16.592
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 01/05/23 10:04:16.599
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/05/23 10:04:16.606
STEP: patching the secret 01/05/23 10:04:16.623
STEP: deleting the secret using a LabelSelector 01/05/23 10:04:16.635
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/05/23 10:04:16.642
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan  5 10:04:16.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3187" for this suite. 01/05/23 10:04:16.667
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":248,"skipped":4639,"failed":0}
------------------------------
â€¢ [0.112 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:04:16.563
    Jan  5 10:04:16.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 10:04:16.564
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:16.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:16.592
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 01/05/23 10:04:16.599
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/05/23 10:04:16.606
    STEP: patching the secret 01/05/23 10:04:16.623
    STEP: deleting the secret using a LabelSelector 01/05/23 10:04:16.635
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/05/23 10:04:16.642
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 10:04:16.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3187" for this suite. 01/05/23 10:04:16.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:04:16.682
Jan  5 10:04:16.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:04:16.683
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:16.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:16.706
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 01/05/23 10:04:16.712
Jan  5 10:04:16.725: INFO: Waiting up to 5m0s for pod "annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb" in namespace "projected-3225" to be "running and ready"
Jan  5 10:04:16.731: INFO: Pod "annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.524891ms
Jan  5 10:04:16.731: INFO: The phase of Pod annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:04:18.742: INFO: Pod "annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb": Phase="Running", Reason="", readiness=true. Elapsed: 2.016773875s
Jan  5 10:04:18.742: INFO: The phase of Pod annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb is Running (Ready = true)
Jan  5 10:04:18.742: INFO: Pod "annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb" satisfied condition "running and ready"
Jan  5 10:04:19.342: INFO: Successfully updated pod "annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 10:04:23.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3225" for this suite. 01/05/23 10:04:23.577
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":249,"skipped":4736,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.906 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:04:16.682
    Jan  5 10:04:16.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:04:16.683
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:16.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:16.706
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 01/05/23 10:04:16.712
    Jan  5 10:04:16.725: INFO: Waiting up to 5m0s for pod "annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb" in namespace "projected-3225" to be "running and ready"
    Jan  5 10:04:16.731: INFO: Pod "annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.524891ms
    Jan  5 10:04:16.731: INFO: The phase of Pod annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:04:18.742: INFO: Pod "annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb": Phase="Running", Reason="", readiness=true. Elapsed: 2.016773875s
    Jan  5 10:04:18.742: INFO: The phase of Pod annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb is Running (Ready = true)
    Jan  5 10:04:18.742: INFO: Pod "annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb" satisfied condition "running and ready"
    Jan  5 10:04:19.342: INFO: Successfully updated pod "annotationupdate78007eba-e29e-45e0-abaf-dcc30a11ebdb"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 10:04:23.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3225" for this suite. 01/05/23 10:04:23.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:04:23.591
Jan  5 10:04:23.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 10:04:23.591
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:23.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:23.615
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 01/05/23 10:04:23.622
Jan  5 10:04:23.638: INFO: Waiting up to 5m0s for pod "annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616" in namespace "downward-api-2141" to be "running and ready"
Jan  5 10:04:23.646: INFO: Pod "annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616": Phase="Pending", Reason="", readiness=false. Elapsed: 7.993278ms
Jan  5 10:04:23.646: INFO: The phase of Pod annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:04:25.653: INFO: Pod "annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616": Phase="Running", Reason="", readiness=true. Elapsed: 2.015490105s
Jan  5 10:04:25.653: INFO: The phase of Pod annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616 is Running (Ready = true)
Jan  5 10:04:25.653: INFO: Pod "annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616" satisfied condition "running and ready"
Jan  5 10:04:26.239: INFO: Successfully updated pod "annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 10:04:30.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2141" for this suite. 01/05/23 10:04:30.321
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":250,"skipped":4756,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.739 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:04:23.591
    Jan  5 10:04:23.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 10:04:23.591
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:23.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:23.615
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 01/05/23 10:04:23.622
    Jan  5 10:04:23.638: INFO: Waiting up to 5m0s for pod "annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616" in namespace "downward-api-2141" to be "running and ready"
    Jan  5 10:04:23.646: INFO: Pod "annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616": Phase="Pending", Reason="", readiness=false. Elapsed: 7.993278ms
    Jan  5 10:04:23.646: INFO: The phase of Pod annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:04:25.653: INFO: Pod "annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616": Phase="Running", Reason="", readiness=true. Elapsed: 2.015490105s
    Jan  5 10:04:25.653: INFO: The phase of Pod annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616 is Running (Ready = true)
    Jan  5 10:04:25.653: INFO: Pod "annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616" satisfied condition "running and ready"
    Jan  5 10:04:26.239: INFO: Successfully updated pod "annotationupdatefbcb9ffd-9590-4614-a348-93ade4407616"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 10:04:30.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2141" for this suite. 01/05/23 10:04:30.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:04:30.33
Jan  5 10:04:30.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename cronjob 01/05/23 10:04:30.331
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:30.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:30.362
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/05/23 10:04:30.37
STEP: Ensuring a job is scheduled 01/05/23 10:04:30.383
STEP: Ensuring exactly one is scheduled 01/05/23 10:05:00.389
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/05/23 10:05:00.394
STEP: Ensuring no more jobs are scheduled 01/05/23 10:05:00.399
STEP: Removing cronjob 01/05/23 10:10:00.413
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  5 10:10:00.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6128" for this suite. 01/05/23 10:10:00.434
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":251,"skipped":4765,"failed":0}
------------------------------
â€¢ [SLOW TEST] [330.111 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:04:30.33
    Jan  5 10:04:30.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename cronjob 01/05/23 10:04:30.331
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:04:30.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:04:30.362
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/05/23 10:04:30.37
    STEP: Ensuring a job is scheduled 01/05/23 10:04:30.383
    STEP: Ensuring exactly one is scheduled 01/05/23 10:05:00.389
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/05/23 10:05:00.394
    STEP: Ensuring no more jobs are scheduled 01/05/23 10:05:00.399
    STEP: Removing cronjob 01/05/23 10:10:00.413
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  5 10:10:00.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6128" for this suite. 01/05/23 10:10:00.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:00.446
Jan  5 10:10:00.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pods 01/05/23 10:10:00.447
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:00.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:00.472
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jan  5 10:10:00.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: creating the pod 01/05/23 10:10:00.479
STEP: submitting the pod to kubernetes 01/05/23 10:10:00.479
Jan  5 10:10:00.498: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-3607a667-f9d3-428c-93ba-6efa4168e32b" in namespace "pods-5491" to be "running and ready"
Jan  5 10:10:00.504: INFO: Pod "pod-exec-websocket-3607a667-f9d3-428c-93ba-6efa4168e32b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.770404ms
Jan  5 10:10:00.504: INFO: The phase of Pod pod-exec-websocket-3607a667-f9d3-428c-93ba-6efa4168e32b is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:10:02.512: INFO: Pod "pod-exec-websocket-3607a667-f9d3-428c-93ba-6efa4168e32b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013545904s
Jan  5 10:10:02.512: INFO: The phase of Pod pod-exec-websocket-3607a667-f9d3-428c-93ba-6efa4168e32b is Running (Ready = true)
Jan  5 10:10:02.512: INFO: Pod "pod-exec-websocket-3607a667-f9d3-428c-93ba-6efa4168e32b" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 10:10:02.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5491" for this suite. 01/05/23 10:10:02.739
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":252,"skipped":4791,"failed":0}
------------------------------
â€¢ [2.302 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:00.446
    Jan  5 10:10:00.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pods 01/05/23 10:10:00.447
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:00.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:00.472
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jan  5 10:10:00.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: creating the pod 01/05/23 10:10:00.479
    STEP: submitting the pod to kubernetes 01/05/23 10:10:00.479
    Jan  5 10:10:00.498: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-3607a667-f9d3-428c-93ba-6efa4168e32b" in namespace "pods-5491" to be "running and ready"
    Jan  5 10:10:00.504: INFO: Pod "pod-exec-websocket-3607a667-f9d3-428c-93ba-6efa4168e32b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.770404ms
    Jan  5 10:10:00.504: INFO: The phase of Pod pod-exec-websocket-3607a667-f9d3-428c-93ba-6efa4168e32b is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:10:02.512: INFO: Pod "pod-exec-websocket-3607a667-f9d3-428c-93ba-6efa4168e32b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013545904s
    Jan  5 10:10:02.512: INFO: The phase of Pod pod-exec-websocket-3607a667-f9d3-428c-93ba-6efa4168e32b is Running (Ready = true)
    Jan  5 10:10:02.512: INFO: Pod "pod-exec-websocket-3607a667-f9d3-428c-93ba-6efa4168e32b" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 10:10:02.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5491" for this suite. 01/05/23 10:10:02.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:02.749
Jan  5 10:10:02.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 10:10:02.75
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:02.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:02.778
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jan  5 10:10:02.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 10:10:05.379
Jan  5 10:10:05.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-5577 --namespace=crd-publish-openapi-5577 create -f -'
Jan  5 10:10:05.962: INFO: stderr: ""
Jan  5 10:10:05.962: INFO: stdout: "e2e-test-crd-publish-openapi-7667-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  5 10:10:05.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-5577 --namespace=crd-publish-openapi-5577 delete e2e-test-crd-publish-openapi-7667-crds test-cr'
Jan  5 10:10:06.030: INFO: stderr: ""
Jan  5 10:10:06.030: INFO: stdout: "e2e-test-crd-publish-openapi-7667-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan  5 10:10:06.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-5577 --namespace=crd-publish-openapi-5577 apply -f -'
Jan  5 10:10:06.202: INFO: stderr: ""
Jan  5 10:10:06.202: INFO: stdout: "e2e-test-crd-publish-openapi-7667-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  5 10:10:06.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-5577 --namespace=crd-publish-openapi-5577 delete e2e-test-crd-publish-openapi-7667-crds test-cr'
Jan  5 10:10:06.270: INFO: stderr: ""
Jan  5 10:10:06.270: INFO: stdout: "e2e-test-crd-publish-openapi-7667-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/05/23 10:10:06.27
Jan  5 10:10:06.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-5577 explain e2e-test-crd-publish-openapi-7667-crds'
Jan  5 10:10:06.741: INFO: stderr: ""
Jan  5 10:10:06.741: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7667-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:10:10.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5577" for this suite. 01/05/23 10:10:10.338
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":253,"skipped":4803,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.595 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:02.749
    Jan  5 10:10:02.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 10:10:02.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:02.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:02.778
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jan  5 10:10:02.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 10:10:05.379
    Jan  5 10:10:05.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-5577 --namespace=crd-publish-openapi-5577 create -f -'
    Jan  5 10:10:05.962: INFO: stderr: ""
    Jan  5 10:10:05.962: INFO: stdout: "e2e-test-crd-publish-openapi-7667-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan  5 10:10:05.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-5577 --namespace=crd-publish-openapi-5577 delete e2e-test-crd-publish-openapi-7667-crds test-cr'
    Jan  5 10:10:06.030: INFO: stderr: ""
    Jan  5 10:10:06.030: INFO: stdout: "e2e-test-crd-publish-openapi-7667-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan  5 10:10:06.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-5577 --namespace=crd-publish-openapi-5577 apply -f -'
    Jan  5 10:10:06.202: INFO: stderr: ""
    Jan  5 10:10:06.202: INFO: stdout: "e2e-test-crd-publish-openapi-7667-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan  5 10:10:06.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-5577 --namespace=crd-publish-openapi-5577 delete e2e-test-crd-publish-openapi-7667-crds test-cr'
    Jan  5 10:10:06.270: INFO: stderr: ""
    Jan  5 10:10:06.270: INFO: stdout: "e2e-test-crd-publish-openapi-7667-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/05/23 10:10:06.27
    Jan  5 10:10:06.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-5577 explain e2e-test-crd-publish-openapi-7667-crds'
    Jan  5 10:10:06.741: INFO: stderr: ""
    Jan  5 10:10:06.741: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7667-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:10:10.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5577" for this suite. 01/05/23 10:10:10.338
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:10.345
Jan  5 10:10:10.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 10:10:10.347
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:10.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:10.368
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 01/05/23 10:10:10.373
Jan  5 10:10:10.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-2934 create -f -'
Jan  5 10:10:10.946: INFO: stderr: ""
Jan  5 10:10:10.946: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/05/23 10:10:10.946
Jan  5 10:10:10.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-2934 diff -f -'
Jan  5 10:10:11.130: INFO: rc: 1
Jan  5 10:10:11.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-2934 delete -f -'
Jan  5 10:10:11.204: INFO: stderr: ""
Jan  5 10:10:11.204: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 10:10:11.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2934" for this suite. 01/05/23 10:10:11.212
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":254,"skipped":4804,"failed":0}
------------------------------
â€¢ [0.879 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:10.345
    Jan  5 10:10:10.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 10:10:10.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:10.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:10.368
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 01/05/23 10:10:10.373
    Jan  5 10:10:10.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-2934 create -f -'
    Jan  5 10:10:10.946: INFO: stderr: ""
    Jan  5 10:10:10.946: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/05/23 10:10:10.946
    Jan  5 10:10:10.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-2934 diff -f -'
    Jan  5 10:10:11.130: INFO: rc: 1
    Jan  5 10:10:11.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-2934 delete -f -'
    Jan  5 10:10:11.204: INFO: stderr: ""
    Jan  5 10:10:11.204: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 10:10:11.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2934" for this suite. 01/05/23 10:10:11.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:11.224
Jan  5 10:10:11.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 10:10:11.225
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:11.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:11.243
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/05/23 10:10:11.248
Jan  5 10:10:11.261: INFO: Waiting up to 5m0s for pod "pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860" in namespace "emptydir-1797" to be "Succeeded or Failed"
Jan  5 10:10:11.264: INFO: Pod "pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719114ms
Jan  5 10:10:13.286: INFO: Pod "pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024911424s
Jan  5 10:10:15.271: INFO: Pod "pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009981873s
STEP: Saw pod success 01/05/23 10:10:15.271
Jan  5 10:10:15.271: INFO: Pod "pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860" satisfied condition "Succeeded or Failed"
Jan  5 10:10:15.280: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860 container test-container: <nil>
STEP: delete the pod 01/05/23 10:10:15.335
Jan  5 10:10:15.347: INFO: Waiting for pod pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860 to disappear
Jan  5 10:10:15.351: INFO: Pod pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 10:10:15.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1797" for this suite. 01/05/23 10:10:15.362
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":255,"skipped":4821,"failed":0}
------------------------------
â€¢ [4.142 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:11.224
    Jan  5 10:10:11.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 10:10:11.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:11.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:11.243
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/05/23 10:10:11.248
    Jan  5 10:10:11.261: INFO: Waiting up to 5m0s for pod "pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860" in namespace "emptydir-1797" to be "Succeeded or Failed"
    Jan  5 10:10:11.264: INFO: Pod "pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719114ms
    Jan  5 10:10:13.286: INFO: Pod "pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024911424s
    Jan  5 10:10:15.271: INFO: Pod "pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009981873s
    STEP: Saw pod success 01/05/23 10:10:15.271
    Jan  5 10:10:15.271: INFO: Pod "pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860" satisfied condition "Succeeded or Failed"
    Jan  5 10:10:15.280: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860 container test-container: <nil>
    STEP: delete the pod 01/05/23 10:10:15.335
    Jan  5 10:10:15.347: INFO: Waiting for pod pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860 to disappear
    Jan  5 10:10:15.351: INFO: Pod pod-a9a2ac05-49d3-4d2c-bc6d-a4ee6273a860 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 10:10:15.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1797" for this suite. 01/05/23 10:10:15.362
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:15.367
Jan  5 10:10:15.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename deployment 01/05/23 10:10:15.368
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:15.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:15.432
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/05/23 10:10:15.443
Jan  5 10:10:15.443: INFO: Creating simple deployment test-deployment-q8dxr
Jan  5 10:10:15.464: INFO: deployment "test-deployment-q8dxr" doesn't have the required revision set
STEP: Getting /status 01/05/23 10:10:17.485
Jan  5 10:10:17.490: INFO: Deployment test-deployment-q8dxr has Conditions: [{Available True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q8dxr-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 01/05/23 10:10:17.49
Jan  5 10:10:17.500: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 10, 10, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 10, 10, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 10, 10, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 10, 10, 15, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-q8dxr-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/05/23 10:10:17.5
Jan  5 10:10:17.504: INFO: Observed &Deployment event: ADDED
Jan  5 10:10:17.504: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q8dxr-777898ffcc"}
Jan  5 10:10:17.504: INFO: Observed &Deployment event: MODIFIED
Jan  5 10:10:17.504: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q8dxr-777898ffcc"}
Jan  5 10:10:17.505: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  5 10:10:17.505: INFO: Observed &Deployment event: MODIFIED
Jan  5 10:10:17.505: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  5 10:10:17.505: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-q8dxr-777898ffcc" is progressing.}
Jan  5 10:10:17.505: INFO: Observed &Deployment event: MODIFIED
Jan  5 10:10:17.505: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  5 10:10:17.506: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q8dxr-777898ffcc" has successfully progressed.}
Jan  5 10:10:17.506: INFO: Observed &Deployment event: MODIFIED
Jan  5 10:10:17.506: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  5 10:10:17.506: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q8dxr-777898ffcc" has successfully progressed.}
Jan  5 10:10:17.506: INFO: Found Deployment test-deployment-q8dxr in namespace deployment-5222 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 10:10:17.506: INFO: Deployment test-deployment-q8dxr has an updated status
STEP: patching the Statefulset Status 01/05/23 10:10:17.506
Jan  5 10:10:17.506: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  5 10:10:17.513: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/05/23 10:10:17.513
Jan  5 10:10:17.522: INFO: Observed &Deployment event: ADDED
Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q8dxr-777898ffcc"}
Jan  5 10:10:17.522: INFO: Observed &Deployment event: MODIFIED
Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q8dxr-777898ffcc"}
Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  5 10:10:17.522: INFO: Observed &Deployment event: MODIFIED
Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-q8dxr-777898ffcc" is progressing.}
Jan  5 10:10:17.522: INFO: Observed &Deployment event: MODIFIED
Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q8dxr-777898ffcc" has successfully progressed.}
Jan  5 10:10:17.523: INFO: Observed &Deployment event: MODIFIED
Jan  5 10:10:17.523: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  5 10:10:17.523: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q8dxr-777898ffcc" has successfully progressed.}
Jan  5 10:10:17.523: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 10:10:17.523: INFO: Observed &Deployment event: MODIFIED
Jan  5 10:10:17.523: INFO: Found deployment test-deployment-q8dxr in namespace deployment-5222 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan  5 10:10:17.523: INFO: Deployment test-deployment-q8dxr has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 10:10:17.528: INFO: Deployment "test-deployment-q8dxr":
&Deployment{ObjectMeta:{test-deployment-q8dxr  deployment-5222  59ef8054-b5b2-4cb4-8375-6937af7923e2 35829 1 2023-01-05 10:10:15 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-05 10:10:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-05 10:10:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-05 10:10:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b7cd48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-q8dxr-777898ffcc",LastUpdateTime:2023-01-05 10:10:17 +0000 UTC,LastTransitionTime:2023-01-05 10:10:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  5 10:10:17.537: INFO: New ReplicaSet "test-deployment-q8dxr-777898ffcc" of Deployment "test-deployment-q8dxr":
&ReplicaSet{ObjectMeta:{test-deployment-q8dxr-777898ffcc  deployment-5222  bc19d6bf-5b41-4235-85ef-60f078b9a7be 35822 1 2023-01-05 10:10:15 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-q8dxr 59ef8054-b5b2-4cb4-8375-6937af7923e2 0xc006b7d1e0 0xc006b7d1e1}] [] [{kube-controller-manager Update apps/v1 2023-01-05 10:10:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ef8054-b5b2-4cb4-8375-6937af7923e2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:10:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b7d2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 10:10:17.542: INFO: Pod "test-deployment-q8dxr-777898ffcc-xlfgd" is available:
&Pod{ObjectMeta:{test-deployment-q8dxr-777898ffcc-xlfgd test-deployment-q8dxr-777898ffcc- deployment-5222  dbd006f7-44a3-4c72-bd2f-af39dec7c332 35821 0 2023-01-05 10:10:15 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-q8dxr-777898ffcc bc19d6bf-5b41-4235-85ef-60f078b9a7be 0xc006b7d6a0 0xc006b7d6a1}] [] [{kube-controller-manager Update v1 2023-01-05 10:10:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc19d6bf-5b41-4235-85ef-60f078b9a7be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:10:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b2lm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b2lm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:10:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:10:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:10:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:10:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.81,StartTime:2023-01-05 10:10:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:10:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://63976a329ab027a18e69dddb61a12c043a91f118ea462d0665b3f7f1f61bdcc6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 10:10:17.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5222" for this suite. 01/05/23 10:10:17.549
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":256,"skipped":4823,"failed":0}
------------------------------
â€¢ [2.188 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:15.367
    Jan  5 10:10:15.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename deployment 01/05/23 10:10:15.368
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:15.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:15.432
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/05/23 10:10:15.443
    Jan  5 10:10:15.443: INFO: Creating simple deployment test-deployment-q8dxr
    Jan  5 10:10:15.464: INFO: deployment "test-deployment-q8dxr" doesn't have the required revision set
    STEP: Getting /status 01/05/23 10:10:17.485
    Jan  5 10:10:17.490: INFO: Deployment test-deployment-q8dxr has Conditions: [{Available True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q8dxr-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 01/05/23 10:10:17.49
    Jan  5 10:10:17.500: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 10, 10, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 10, 10, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 10, 10, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 10, 10, 15, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-q8dxr-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/05/23 10:10:17.5
    Jan  5 10:10:17.504: INFO: Observed &Deployment event: ADDED
    Jan  5 10:10:17.504: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q8dxr-777898ffcc"}
    Jan  5 10:10:17.504: INFO: Observed &Deployment event: MODIFIED
    Jan  5 10:10:17.504: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q8dxr-777898ffcc"}
    Jan  5 10:10:17.505: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  5 10:10:17.505: INFO: Observed &Deployment event: MODIFIED
    Jan  5 10:10:17.505: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  5 10:10:17.505: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-q8dxr-777898ffcc" is progressing.}
    Jan  5 10:10:17.505: INFO: Observed &Deployment event: MODIFIED
    Jan  5 10:10:17.505: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  5 10:10:17.506: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q8dxr-777898ffcc" has successfully progressed.}
    Jan  5 10:10:17.506: INFO: Observed &Deployment event: MODIFIED
    Jan  5 10:10:17.506: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  5 10:10:17.506: INFO: Observed Deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q8dxr-777898ffcc" has successfully progressed.}
    Jan  5 10:10:17.506: INFO: Found Deployment test-deployment-q8dxr in namespace deployment-5222 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 10:10:17.506: INFO: Deployment test-deployment-q8dxr has an updated status
    STEP: patching the Statefulset Status 01/05/23 10:10:17.506
    Jan  5 10:10:17.506: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  5 10:10:17.513: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/05/23 10:10:17.513
    Jan  5 10:10:17.522: INFO: Observed &Deployment event: ADDED
    Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q8dxr-777898ffcc"}
    Jan  5 10:10:17.522: INFO: Observed &Deployment event: MODIFIED
    Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q8dxr-777898ffcc"}
    Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  5 10:10:17.522: INFO: Observed &Deployment event: MODIFIED
    Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:15 +0000 UTC 2023-01-05 10:10:15 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-q8dxr-777898ffcc" is progressing.}
    Jan  5 10:10:17.522: INFO: Observed &Deployment event: MODIFIED
    Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  5 10:10:17.522: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q8dxr-777898ffcc" has successfully progressed.}
    Jan  5 10:10:17.523: INFO: Observed &Deployment event: MODIFIED
    Jan  5 10:10:17.523: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  5 10:10:17.523: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 10:10:16 +0000 UTC 2023-01-05 10:10:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q8dxr-777898ffcc" has successfully progressed.}
    Jan  5 10:10:17.523: INFO: Observed deployment test-deployment-q8dxr in namespace deployment-5222 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 10:10:17.523: INFO: Observed &Deployment event: MODIFIED
    Jan  5 10:10:17.523: INFO: Found deployment test-deployment-q8dxr in namespace deployment-5222 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan  5 10:10:17.523: INFO: Deployment test-deployment-q8dxr has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 10:10:17.528: INFO: Deployment "test-deployment-q8dxr":
    &Deployment{ObjectMeta:{test-deployment-q8dxr  deployment-5222  59ef8054-b5b2-4cb4-8375-6937af7923e2 35829 1 2023-01-05 10:10:15 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-05 10:10:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-05 10:10:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-05 10:10:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b7cd48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-q8dxr-777898ffcc",LastUpdateTime:2023-01-05 10:10:17 +0000 UTC,LastTransitionTime:2023-01-05 10:10:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  5 10:10:17.537: INFO: New ReplicaSet "test-deployment-q8dxr-777898ffcc" of Deployment "test-deployment-q8dxr":
    &ReplicaSet{ObjectMeta:{test-deployment-q8dxr-777898ffcc  deployment-5222  bc19d6bf-5b41-4235-85ef-60f078b9a7be 35822 1 2023-01-05 10:10:15 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-q8dxr 59ef8054-b5b2-4cb4-8375-6937af7923e2 0xc006b7d1e0 0xc006b7d1e1}] [] [{kube-controller-manager Update apps/v1 2023-01-05 10:10:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ef8054-b5b2-4cb4-8375-6937af7923e2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:10:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b7d2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 10:10:17.542: INFO: Pod "test-deployment-q8dxr-777898ffcc-xlfgd" is available:
    &Pod{ObjectMeta:{test-deployment-q8dxr-777898ffcc-xlfgd test-deployment-q8dxr-777898ffcc- deployment-5222  dbd006f7-44a3-4c72-bd2f-af39dec7c332 35821 0 2023-01-05 10:10:15 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-q8dxr-777898ffcc bc19d6bf-5b41-4235-85ef-60f078b9a7be 0xc006b7d6a0 0xc006b7d6a1}] [] [{kube-controller-manager Update v1 2023-01-05 10:10:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc19d6bf-5b41-4235-85ef-60f078b9a7be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:10:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b2lm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b2lm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:10:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:10:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:10:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:10:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.81,StartTime:2023-01-05 10:10:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:10:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://63976a329ab027a18e69dddb61a12c043a91f118ea462d0665b3f7f1f61bdcc6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 10:10:17.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5222" for this suite. 01/05/23 10:10:17.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:17.557
Jan  5 10:10:17.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 10:10:17.558
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:17.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:17.579
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 01/05/23 10:10:17.585
Jan  5 10:10:17.595: INFO: Waiting up to 5m0s for pod "pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2" in namespace "emptydir-7564" to be "Succeeded or Failed"
Jan  5 10:10:17.605: INFO: Pod "pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.453762ms
Jan  5 10:10:19.610: INFO: Pod "pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015169573s
Jan  5 10:10:21.610: INFO: Pod "pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015272286s
STEP: Saw pod success 01/05/23 10:10:21.61
Jan  5 10:10:21.611: INFO: Pod "pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2" satisfied condition "Succeeded or Failed"
Jan  5 10:10:21.617: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2 container test-container: <nil>
STEP: delete the pod 01/05/23 10:10:21.629
Jan  5 10:10:21.639: INFO: Waiting for pod pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2 to disappear
Jan  5 10:10:21.642: INFO: Pod pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 10:10:21.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7564" for this suite. 01/05/23 10:10:21.649
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":257,"skipped":4828,"failed":0}
------------------------------
â€¢ [4.098 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:17.557
    Jan  5 10:10:17.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 10:10:17.558
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:17.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:17.579
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/05/23 10:10:17.585
    Jan  5 10:10:17.595: INFO: Waiting up to 5m0s for pod "pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2" in namespace "emptydir-7564" to be "Succeeded or Failed"
    Jan  5 10:10:17.605: INFO: Pod "pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.453762ms
    Jan  5 10:10:19.610: INFO: Pod "pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015169573s
    Jan  5 10:10:21.610: INFO: Pod "pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015272286s
    STEP: Saw pod success 01/05/23 10:10:21.61
    Jan  5 10:10:21.611: INFO: Pod "pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2" satisfied condition "Succeeded or Failed"
    Jan  5 10:10:21.617: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2 container test-container: <nil>
    STEP: delete the pod 01/05/23 10:10:21.629
    Jan  5 10:10:21.639: INFO: Waiting for pod pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2 to disappear
    Jan  5 10:10:21.642: INFO: Pod pod-cda9a4a3-7ba6-43a4-bebf-5a9d07bc43d2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 10:10:21.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7564" for this suite. 01/05/23 10:10:21.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:21.655
Jan  5 10:10:21.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename job 01/05/23 10:10:21.656
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:21.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:21.676
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 01/05/23 10:10:21.682
STEP: Ensuring active pods == parallelism 01/05/23 10:10:21.688
STEP: Orphaning one of the Job's Pods 01/05/23 10:10:23.695
Jan  5 10:10:24.214: INFO: Successfully updated pod "adopt-release-dzlpr"
STEP: Checking that the Job readopts the Pod 01/05/23 10:10:24.214
Jan  5 10:10:24.214: INFO: Waiting up to 15m0s for pod "adopt-release-dzlpr" in namespace "job-3056" to be "adopted"
Jan  5 10:10:24.218: INFO: Pod "adopt-release-dzlpr": Phase="Running", Reason="", readiness=true. Elapsed: 3.8077ms
Jan  5 10:10:26.225: INFO: Pod "adopt-release-dzlpr": Phase="Running", Reason="", readiness=true. Elapsed: 2.010995238s
Jan  5 10:10:26.225: INFO: Pod "adopt-release-dzlpr" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/05/23 10:10:26.225
Jan  5 10:10:26.741: INFO: Successfully updated pod "adopt-release-dzlpr"
STEP: Checking that the Job releases the Pod 01/05/23 10:10:26.741
Jan  5 10:10:26.741: INFO: Waiting up to 15m0s for pod "adopt-release-dzlpr" in namespace "job-3056" to be "released"
Jan  5 10:10:26.745: INFO: Pod "adopt-release-dzlpr": Phase="Running", Reason="", readiness=true. Elapsed: 4.37697ms
Jan  5 10:10:28.751: INFO: Pod "adopt-release-dzlpr": Phase="Running", Reason="", readiness=true. Elapsed: 2.010293313s
Jan  5 10:10:28.751: INFO: Pod "adopt-release-dzlpr" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  5 10:10:28.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3056" for this suite. 01/05/23 10:10:28.759
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":258,"skipped":4834,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.108 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:21.655
    Jan  5 10:10:21.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename job 01/05/23 10:10:21.656
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:21.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:21.676
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 01/05/23 10:10:21.682
    STEP: Ensuring active pods == parallelism 01/05/23 10:10:21.688
    STEP: Orphaning one of the Job's Pods 01/05/23 10:10:23.695
    Jan  5 10:10:24.214: INFO: Successfully updated pod "adopt-release-dzlpr"
    STEP: Checking that the Job readopts the Pod 01/05/23 10:10:24.214
    Jan  5 10:10:24.214: INFO: Waiting up to 15m0s for pod "adopt-release-dzlpr" in namespace "job-3056" to be "adopted"
    Jan  5 10:10:24.218: INFO: Pod "adopt-release-dzlpr": Phase="Running", Reason="", readiness=true. Elapsed: 3.8077ms
    Jan  5 10:10:26.225: INFO: Pod "adopt-release-dzlpr": Phase="Running", Reason="", readiness=true. Elapsed: 2.010995238s
    Jan  5 10:10:26.225: INFO: Pod "adopt-release-dzlpr" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/05/23 10:10:26.225
    Jan  5 10:10:26.741: INFO: Successfully updated pod "adopt-release-dzlpr"
    STEP: Checking that the Job releases the Pod 01/05/23 10:10:26.741
    Jan  5 10:10:26.741: INFO: Waiting up to 15m0s for pod "adopt-release-dzlpr" in namespace "job-3056" to be "released"
    Jan  5 10:10:26.745: INFO: Pod "adopt-release-dzlpr": Phase="Running", Reason="", readiness=true. Elapsed: 4.37697ms
    Jan  5 10:10:28.751: INFO: Pod "adopt-release-dzlpr": Phase="Running", Reason="", readiness=true. Elapsed: 2.010293313s
    Jan  5 10:10:28.751: INFO: Pod "adopt-release-dzlpr" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  5 10:10:28.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3056" for this suite. 01/05/23 10:10:28.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:28.764
Jan  5 10:10:28.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:10:28.765
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:28.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:28.783
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 01/05/23 10:10:28.788
Jan  5 10:10:28.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d" in namespace "projected-2822" to be "Succeeded or Failed"
Jan  5 10:10:28.804: INFO: Pod "downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.184315ms
Jan  5 10:10:30.811: INFO: Pod "downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012736655s
Jan  5 10:10:32.818: INFO: Pod "downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019414263s
STEP: Saw pod success 01/05/23 10:10:32.818
Jan  5 10:10:32.818: INFO: Pod "downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d" satisfied condition "Succeeded or Failed"
Jan  5 10:10:32.822: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d container client-container: <nil>
STEP: delete the pod 01/05/23 10:10:32.837
Jan  5 10:10:32.849: INFO: Waiting for pod downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d to disappear
Jan  5 10:10:32.852: INFO: Pod downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 10:10:32.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2822" for this suite. 01/05/23 10:10:32.861
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":259,"skipped":4843,"failed":0}
------------------------------
â€¢ [4.103 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:28.764
    Jan  5 10:10:28.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:10:28.765
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:28.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:28.783
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 01/05/23 10:10:28.788
    Jan  5 10:10:28.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d" in namespace "projected-2822" to be "Succeeded or Failed"
    Jan  5 10:10:28.804: INFO: Pod "downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.184315ms
    Jan  5 10:10:30.811: INFO: Pod "downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012736655s
    Jan  5 10:10:32.818: INFO: Pod "downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019414263s
    STEP: Saw pod success 01/05/23 10:10:32.818
    Jan  5 10:10:32.818: INFO: Pod "downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d" satisfied condition "Succeeded or Failed"
    Jan  5 10:10:32.822: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d container client-container: <nil>
    STEP: delete the pod 01/05/23 10:10:32.837
    Jan  5 10:10:32.849: INFO: Waiting for pod downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d to disappear
    Jan  5 10:10:32.852: INFO: Pod downwardapi-volume-59757a8e-bff1-4967-873b-975d4a7c244d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 10:10:32.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2822" for this suite. 01/05/23 10:10:32.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:32.869
Jan  5 10:10:32.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 10:10:32.87
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:32.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:32.889
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-431 01/05/23 10:10:32.895
STEP: changing the ExternalName service to type=NodePort 01/05/23 10:10:32.901
STEP: creating replication controller externalname-service in namespace services-431 01/05/23 10:10:32.923
I0105 10:10:32.930467      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-431, replica count: 2
I0105 10:10:35.981636      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 10:10:35.981: INFO: Creating new exec pod
Jan  5 10:10:35.992: INFO: Waiting up to 5m0s for pod "execpod4mv24" in namespace "services-431" to be "running"
Jan  5 10:10:35.997: INFO: Pod "execpod4mv24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.53263ms
Jan  5 10:10:38.003: INFO: Pod "execpod4mv24": Phase="Running", Reason="", readiness=true. Elapsed: 2.010455415s
Jan  5 10:10:38.003: INFO: Pod "execpod4mv24" satisfied condition "running"
Jan  5 10:10:39.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-431 exec execpod4mv24 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan  5 10:10:39.486: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  5 10:10:39.486: INFO: stdout: "externalname-service-79mfl"
Jan  5 10:10:39.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-431 exec execpod4mv24 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.114.162.28 80'
Jan  5 10:10:40.041: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.114.162.28 80\nConnection to 10.114.162.28 80 port [tcp/http] succeeded!\n"
Jan  5 10:10:40.041: INFO: stdout: "externalname-service-8xzlk"
Jan  5 10:10:40.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-431 exec execpod4mv24 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.128 30242'
Jan  5 10:10:40.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.128 30242\nConnection to 10.250.0.128 30242 port [tcp/*] succeeded!\n"
Jan  5 10:10:40.566: INFO: stdout: "externalname-service-8xzlk"
Jan  5 10:10:40.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-431 exec execpod4mv24 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.174 30242'
Jan  5 10:10:41.127: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.174 30242\nConnection to 10.250.0.174 30242 port [tcp/*] succeeded!\n"
Jan  5 10:10:41.127: INFO: stdout: "externalname-service-8xzlk"
Jan  5 10:10:41.127: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 10:10:41.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-431" for this suite. 01/05/23 10:10:41.166
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":260,"skipped":4852,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.305 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:32.869
    Jan  5 10:10:32.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 10:10:32.87
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:32.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:32.889
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-431 01/05/23 10:10:32.895
    STEP: changing the ExternalName service to type=NodePort 01/05/23 10:10:32.901
    STEP: creating replication controller externalname-service in namespace services-431 01/05/23 10:10:32.923
    I0105 10:10:32.930467      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-431, replica count: 2
    I0105 10:10:35.981636      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 10:10:35.981: INFO: Creating new exec pod
    Jan  5 10:10:35.992: INFO: Waiting up to 5m0s for pod "execpod4mv24" in namespace "services-431" to be "running"
    Jan  5 10:10:35.997: INFO: Pod "execpod4mv24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.53263ms
    Jan  5 10:10:38.003: INFO: Pod "execpod4mv24": Phase="Running", Reason="", readiness=true. Elapsed: 2.010455415s
    Jan  5 10:10:38.003: INFO: Pod "execpod4mv24" satisfied condition "running"
    Jan  5 10:10:39.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-431 exec execpod4mv24 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan  5 10:10:39.486: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  5 10:10:39.486: INFO: stdout: "externalname-service-79mfl"
    Jan  5 10:10:39.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-431 exec execpod4mv24 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.114.162.28 80'
    Jan  5 10:10:40.041: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.114.162.28 80\nConnection to 10.114.162.28 80 port [tcp/http] succeeded!\n"
    Jan  5 10:10:40.041: INFO: stdout: "externalname-service-8xzlk"
    Jan  5 10:10:40.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-431 exec execpod4mv24 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.128 30242'
    Jan  5 10:10:40.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.128 30242\nConnection to 10.250.0.128 30242 port [tcp/*] succeeded!\n"
    Jan  5 10:10:40.566: INFO: stdout: "externalname-service-8xzlk"
    Jan  5 10:10:40.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-431 exec execpod4mv24 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.174 30242'
    Jan  5 10:10:41.127: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.174 30242\nConnection to 10.250.0.174 30242 port [tcp/*] succeeded!\n"
    Jan  5 10:10:41.127: INFO: stdout: "externalname-service-8xzlk"
    Jan  5 10:10:41.127: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 10:10:41.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-431" for this suite. 01/05/23 10:10:41.166
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:41.174
Jan  5 10:10:41.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 10:10:41.175
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:41.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:41.198
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-6973 01/05/23 10:10:41.204
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6973 to expose endpoints map[] 01/05/23 10:10:41.214
Jan  5 10:10:41.236: INFO: successfully validated that service multi-endpoint-test in namespace services-6973 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6973 01/05/23 10:10:41.236
Jan  5 10:10:41.246: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6973" to be "running and ready"
Jan  5 10:10:41.255: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.3283ms
Jan  5 10:10:41.255: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:10:43.262: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015819264s
Jan  5 10:10:43.262: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  5 10:10:43.262: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6973 to expose endpoints map[pod1:[100]] 01/05/23 10:10:43.266
Jan  5 10:10:43.281: INFO: successfully validated that service multi-endpoint-test in namespace services-6973 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6973 01/05/23 10:10:43.281
Jan  5 10:10:43.289: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6973" to be "running and ready"
Jan  5 10:10:43.295: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.91306ms
Jan  5 10:10:43.295: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:10:45.301: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011970321s
Jan  5 10:10:45.301: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  5 10:10:45.301: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6973 to expose endpoints map[pod1:[100] pod2:[101]] 01/05/23 10:10:45.306
Jan  5 10:10:45.323: INFO: successfully validated that service multi-endpoint-test in namespace services-6973 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/05/23 10:10:45.323
Jan  5 10:10:45.323: INFO: Creating new exec pod
Jan  5 10:10:45.331: INFO: Waiting up to 5m0s for pod "execpodp9z5k" in namespace "services-6973" to be "running"
Jan  5 10:10:45.339: INFO: Pod "execpodp9z5k": Phase="Pending", Reason="", readiness=false. Elapsed: 7.924276ms
Jan  5 10:10:47.345: INFO: Pod "execpodp9z5k": Phase="Running", Reason="", readiness=true. Elapsed: 2.013224007s
Jan  5 10:10:47.345: INFO: Pod "execpodp9z5k" satisfied condition "running"
Jan  5 10:10:48.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6973 exec execpodp9z5k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan  5 10:10:48.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan  5 10:10:48.871: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 10:10:48.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6973 exec execpodp9z5k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.112.179.26 80'
Jan  5 10:10:49.384: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.112.179.26 80\nConnection to 10.112.179.26 80 port [tcp/http] succeeded!\n"
Jan  5 10:10:49.384: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 10:10:49.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6973 exec execpodp9z5k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan  5 10:10:49.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan  5 10:10:49.891: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 10:10:49.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6973 exec execpodp9z5k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.112.179.26 81'
Jan  5 10:10:50.303: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.112.179.26 81\nConnection to 10.112.179.26 81 port [tcp/*] succeeded!\n"
Jan  5 10:10:50.303: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6973 01/05/23 10:10:50.303
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6973 to expose endpoints map[pod2:[101]] 01/05/23 10:10:50.313
Jan  5 10:10:50.331: INFO: successfully validated that service multi-endpoint-test in namespace services-6973 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6973 01/05/23 10:10:50.331
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6973 to expose endpoints map[] 01/05/23 10:10:50.341
Jan  5 10:10:50.355: INFO: successfully validated that service multi-endpoint-test in namespace services-6973 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 10:10:50.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6973" for this suite. 01/05/23 10:10:50.375
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":261,"skipped":4881,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.206 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:41.174
    Jan  5 10:10:41.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 10:10:41.175
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:41.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:41.198
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-6973 01/05/23 10:10:41.204
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6973 to expose endpoints map[] 01/05/23 10:10:41.214
    Jan  5 10:10:41.236: INFO: successfully validated that service multi-endpoint-test in namespace services-6973 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-6973 01/05/23 10:10:41.236
    Jan  5 10:10:41.246: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6973" to be "running and ready"
    Jan  5 10:10:41.255: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.3283ms
    Jan  5 10:10:41.255: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:10:43.262: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015819264s
    Jan  5 10:10:43.262: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  5 10:10:43.262: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6973 to expose endpoints map[pod1:[100]] 01/05/23 10:10:43.266
    Jan  5 10:10:43.281: INFO: successfully validated that service multi-endpoint-test in namespace services-6973 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-6973 01/05/23 10:10:43.281
    Jan  5 10:10:43.289: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6973" to be "running and ready"
    Jan  5 10:10:43.295: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.91306ms
    Jan  5 10:10:43.295: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:10:45.301: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011970321s
    Jan  5 10:10:45.301: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  5 10:10:45.301: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6973 to expose endpoints map[pod1:[100] pod2:[101]] 01/05/23 10:10:45.306
    Jan  5 10:10:45.323: INFO: successfully validated that service multi-endpoint-test in namespace services-6973 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/05/23 10:10:45.323
    Jan  5 10:10:45.323: INFO: Creating new exec pod
    Jan  5 10:10:45.331: INFO: Waiting up to 5m0s for pod "execpodp9z5k" in namespace "services-6973" to be "running"
    Jan  5 10:10:45.339: INFO: Pod "execpodp9z5k": Phase="Pending", Reason="", readiness=false. Elapsed: 7.924276ms
    Jan  5 10:10:47.345: INFO: Pod "execpodp9z5k": Phase="Running", Reason="", readiness=true. Elapsed: 2.013224007s
    Jan  5 10:10:47.345: INFO: Pod "execpodp9z5k" satisfied condition "running"
    Jan  5 10:10:48.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6973 exec execpodp9z5k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jan  5 10:10:48.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan  5 10:10:48.871: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 10:10:48.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6973 exec execpodp9z5k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.112.179.26 80'
    Jan  5 10:10:49.384: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.112.179.26 80\nConnection to 10.112.179.26 80 port [tcp/http] succeeded!\n"
    Jan  5 10:10:49.384: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 10:10:49.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6973 exec execpodp9z5k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jan  5 10:10:49.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan  5 10:10:49.891: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 10:10:49.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-6973 exec execpodp9z5k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.112.179.26 81'
    Jan  5 10:10:50.303: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.112.179.26 81\nConnection to 10.112.179.26 81 port [tcp/*] succeeded!\n"
    Jan  5 10:10:50.303: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-6973 01/05/23 10:10:50.303
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6973 to expose endpoints map[pod2:[101]] 01/05/23 10:10:50.313
    Jan  5 10:10:50.331: INFO: successfully validated that service multi-endpoint-test in namespace services-6973 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-6973 01/05/23 10:10:50.331
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6973 to expose endpoints map[] 01/05/23 10:10:50.341
    Jan  5 10:10:50.355: INFO: successfully validated that service multi-endpoint-test in namespace services-6973 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 10:10:50.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6973" for this suite. 01/05/23 10:10:50.375
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:50.381
Jan  5 10:10:50.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 10:10:50.382
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:50.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:50.408
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-7861/configmap-test-4916a140-b39d-436a-ab28-e86a2207796b 01/05/23 10:10:50.417
STEP: Creating a pod to test consume configMaps 01/05/23 10:10:50.423
Jan  5 10:10:50.433: INFO: Waiting up to 5m0s for pod "pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7" in namespace "configmap-7861" to be "Succeeded or Failed"
Jan  5 10:10:50.440: INFO: Pod "pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.735021ms
Jan  5 10:10:52.446: INFO: Pod "pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012395531s
Jan  5 10:10:54.445: INFO: Pod "pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012173658s
STEP: Saw pod success 01/05/23 10:10:54.446
Jan  5 10:10:54.446: INFO: Pod "pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7" satisfied condition "Succeeded or Failed"
Jan  5 10:10:54.450: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7 container env-test: <nil>
STEP: delete the pod 01/05/23 10:10:54.463
Jan  5 10:10:54.484: INFO: Waiting for pod pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7 to disappear
Jan  5 10:10:54.495: INFO: Pod pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 10:10:54.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7861" for this suite. 01/05/23 10:10:54.534
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":262,"skipped":4889,"failed":0}
------------------------------
â€¢ [4.171 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:50.381
    Jan  5 10:10:50.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 10:10:50.382
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:50.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:50.408
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-7861/configmap-test-4916a140-b39d-436a-ab28-e86a2207796b 01/05/23 10:10:50.417
    STEP: Creating a pod to test consume configMaps 01/05/23 10:10:50.423
    Jan  5 10:10:50.433: INFO: Waiting up to 5m0s for pod "pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7" in namespace "configmap-7861" to be "Succeeded or Failed"
    Jan  5 10:10:50.440: INFO: Pod "pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.735021ms
    Jan  5 10:10:52.446: INFO: Pod "pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012395531s
    Jan  5 10:10:54.445: INFO: Pod "pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012173658s
    STEP: Saw pod success 01/05/23 10:10:54.446
    Jan  5 10:10:54.446: INFO: Pod "pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7" satisfied condition "Succeeded or Failed"
    Jan  5 10:10:54.450: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7 container env-test: <nil>
    STEP: delete the pod 01/05/23 10:10:54.463
    Jan  5 10:10:54.484: INFO: Waiting for pod pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7 to disappear
    Jan  5 10:10:54.495: INFO: Pod pod-configmaps-264bc8cc-2f47-4a72-b197-60c30180eff7 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 10:10:54.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7861" for this suite. 01/05/23 10:10:54.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:54.554
Jan  5 10:10:54.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 10:10:54.555
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:54.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:54.593
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jan  5 10:10:54.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-2813 version'
Jan  5 10:10:54.655: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan  5 10:10:54.655: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:02Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:08:09Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 10:10:54.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2813" for this suite. 01/05/23 10:10:54.664
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":263,"skipped":4905,"failed":0}
------------------------------
â€¢ [0.115 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:54.554
    Jan  5 10:10:54.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 10:10:54.555
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:54.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:54.593
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jan  5 10:10:54.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-2813 version'
    Jan  5 10:10:54.655: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan  5 10:10:54.655: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:02Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:08:09Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 10:10:54.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2813" for this suite. 01/05/23 10:10:54.664
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:54.67
Jan  5 10:10:54.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename ingressclass 01/05/23 10:10:54.671
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:54.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:54.694
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/05/23 10:10:54.699
STEP: getting /apis/networking.k8s.io 01/05/23 10:10:54.704
STEP: getting /apis/networking.k8s.iov1 01/05/23 10:10:54.706
STEP: creating 01/05/23 10:10:54.709
STEP: getting 01/05/23 10:10:54.729
STEP: listing 01/05/23 10:10:54.732
STEP: watching 01/05/23 10:10:54.736
Jan  5 10:10:54.736: INFO: starting watch
STEP: patching 01/05/23 10:10:54.738
STEP: updating 01/05/23 10:10:54.743
Jan  5 10:10:54.748: INFO: waiting for watch events with expected annotations
Jan  5 10:10:54.748: INFO: saw patched and updated annotations
STEP: deleting 01/05/23 10:10:54.748
STEP: deleting a collection 01/05/23 10:10:54.762
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jan  5 10:10:54.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7029" for this suite. 01/05/23 10:10:54.783
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":264,"skipped":4907,"failed":0}
------------------------------
â€¢ [0.118 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:54.67
    Jan  5 10:10:54.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename ingressclass 01/05/23 10:10:54.671
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:54.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:54.694
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/05/23 10:10:54.699
    STEP: getting /apis/networking.k8s.io 01/05/23 10:10:54.704
    STEP: getting /apis/networking.k8s.iov1 01/05/23 10:10:54.706
    STEP: creating 01/05/23 10:10:54.709
    STEP: getting 01/05/23 10:10:54.729
    STEP: listing 01/05/23 10:10:54.732
    STEP: watching 01/05/23 10:10:54.736
    Jan  5 10:10:54.736: INFO: starting watch
    STEP: patching 01/05/23 10:10:54.738
    STEP: updating 01/05/23 10:10:54.743
    Jan  5 10:10:54.748: INFO: waiting for watch events with expected annotations
    Jan  5 10:10:54.748: INFO: saw patched and updated annotations
    STEP: deleting 01/05/23 10:10:54.748
    STEP: deleting a collection 01/05/23 10:10:54.762
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jan  5 10:10:54.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-7029" for this suite. 01/05/23 10:10:54.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:10:54.788
Jan  5 10:10:54.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 10:10:54.789
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:54.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:54.811
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan  5 10:10:54.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:11:01.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6908" for this suite. 01/05/23 10:11:01.169
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":265,"skipped":4915,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.386 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:10:54.788
    Jan  5 10:10:54.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 10:10:54.789
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:10:54.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:10:54.811
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan  5 10:10:54.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:11:01.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6908" for this suite. 01/05/23 10:11:01.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:11:01.175
Jan  5 10:11:01.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename resourcequota 01/05/23 10:11:01.175
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:11:01.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:11:01.196
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 01/05/23 10:11:01.203
STEP: Creating a ResourceQuota 01/05/23 10:11:06.209
STEP: Ensuring resource quota status is calculated 01/05/23 10:11:06.213
STEP: Creating a ReplicaSet 01/05/23 10:11:08.221
STEP: Ensuring resource quota status captures replicaset creation 01/05/23 10:11:08.23
STEP: Deleting a ReplicaSet 01/05/23 10:11:10.237
STEP: Ensuring resource quota status released usage 01/05/23 10:11:10.245
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 10:11:12.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2464" for this suite. 01/05/23 10:11:12.268
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":266,"skipped":4926,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.099 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:11:01.175
    Jan  5 10:11:01.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename resourcequota 01/05/23 10:11:01.175
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:11:01.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:11:01.196
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 01/05/23 10:11:01.203
    STEP: Creating a ResourceQuota 01/05/23 10:11:06.209
    STEP: Ensuring resource quota status is calculated 01/05/23 10:11:06.213
    STEP: Creating a ReplicaSet 01/05/23 10:11:08.221
    STEP: Ensuring resource quota status captures replicaset creation 01/05/23 10:11:08.23
    STEP: Deleting a ReplicaSet 01/05/23 10:11:10.237
    STEP: Ensuring resource quota status released usage 01/05/23 10:11:10.245
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 10:11:12.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2464" for this suite. 01/05/23 10:11:12.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:11:12.275
Jan  5 10:11:12.275: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename cronjob 01/05/23 10:11:12.276
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:11:12.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:11:12.297
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/05/23 10:11:12.302
STEP: Ensuring more than one job is running at a time 01/05/23 10:11:12.308
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/05/23 10:13:00.316
STEP: Removing cronjob 01/05/23 10:13:00.321
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  5 10:13:00.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6021" for this suite. 01/05/23 10:13:00.337
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":267,"skipped":4949,"failed":0}
------------------------------
â€¢ [SLOW TEST] [108.067 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:11:12.275
    Jan  5 10:11:12.275: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename cronjob 01/05/23 10:11:12.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:11:12.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:11:12.297
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/05/23 10:11:12.302
    STEP: Ensuring more than one job is running at a time 01/05/23 10:11:12.308
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/05/23 10:13:00.316
    STEP: Removing cronjob 01/05/23 10:13:00.321
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  5 10:13:00.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6021" for this suite. 01/05/23 10:13:00.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:13:00.343
Jan  5 10:13:00.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename disruption 01/05/23 10:13:00.343
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:00.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:00.372
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:13:00.379
Jan  5 10:13:00.379: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename disruption-2 01/05/23 10:13:00.379
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:00.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:00.41
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 01/05/23 10:13:00.42
STEP: Waiting for the pdb to be processed 01/05/23 10:13:02.441
STEP: Waiting for the pdb to be processed 01/05/23 10:13:02.454
STEP: listing a collection of PDBs across all namespaces 01/05/23 10:13:02.459
STEP: listing a collection of PDBs in namespace disruption-9488 01/05/23 10:13:02.463
STEP: deleting a collection of PDBs 01/05/23 10:13:02.468
STEP: Waiting for the PDB collection to be deleted 01/05/23 10:13:02.478
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jan  5 10:13:02.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-2505" for this suite. 01/05/23 10:13:02.491
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  5 10:13:02.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9488" for this suite. 01/05/23 10:13:02.502
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":268,"skipped":4954,"failed":0}
------------------------------
â€¢ [2.165 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:13:00.343
    Jan  5 10:13:00.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename disruption 01/05/23 10:13:00.343
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:00.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:00.372
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:13:00.379
    Jan  5 10:13:00.379: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename disruption-2 01/05/23 10:13:00.379
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:00.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:00.41
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 01/05/23 10:13:00.42
    STEP: Waiting for the pdb to be processed 01/05/23 10:13:02.441
    STEP: Waiting for the pdb to be processed 01/05/23 10:13:02.454
    STEP: listing a collection of PDBs across all namespaces 01/05/23 10:13:02.459
    STEP: listing a collection of PDBs in namespace disruption-9488 01/05/23 10:13:02.463
    STEP: deleting a collection of PDBs 01/05/23 10:13:02.468
    STEP: Waiting for the PDB collection to be deleted 01/05/23 10:13:02.478
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jan  5 10:13:02.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-2505" for this suite. 01/05/23 10:13:02.491
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  5 10:13:02.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9488" for this suite. 01/05/23 10:13:02.502
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:13:02.509
Jan  5 10:13:02.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 10:13:02.51
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:02.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:02.532
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 10:13:02.548
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:13:02.992
STEP: Deploying the webhook pod 01/05/23 10:13:03.001
STEP: Wait for the deployment to be ready 01/05/23 10:13:03.012
Jan  5 10:13:03.023: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 10:13:05.038
STEP: Verifying the service has paired with the endpoint 01/05/23 10:13:05.06
Jan  5 10:13:06.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/05/23 10:13:06.066
STEP: create a pod that should be updated by the webhook 01/05/23 10:13:06.195
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:13:06.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1824" for this suite. 01/05/23 10:13:06.331
STEP: Destroying namespace "webhook-1824-markers" for this suite. 01/05/23 10:13:06.339
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":269,"skipped":4957,"failed":0}
------------------------------
â€¢ [3.862 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:13:02.509
    Jan  5 10:13:02.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 10:13:02.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:02.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:02.532
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 10:13:02.548
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:13:02.992
    STEP: Deploying the webhook pod 01/05/23 10:13:03.001
    STEP: Wait for the deployment to be ready 01/05/23 10:13:03.012
    Jan  5 10:13:03.023: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 10:13:05.038
    STEP: Verifying the service has paired with the endpoint 01/05/23 10:13:05.06
    Jan  5 10:13:06.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/05/23 10:13:06.066
    STEP: create a pod that should be updated by the webhook 01/05/23 10:13:06.195
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:13:06.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1824" for this suite. 01/05/23 10:13:06.331
    STEP: Destroying namespace "webhook-1824-markers" for this suite. 01/05/23 10:13:06.339
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:13:06.375
Jan  5 10:13:06.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pod-network-test 01/05/23 10:13:06.376
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:06.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:06.399
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-5516 01/05/23 10:13:06.408
STEP: creating a selector 01/05/23 10:13:06.408
STEP: Creating the service pods in kubernetes 01/05/23 10:13:06.408
Jan  5 10:13:06.408: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  5 10:13:06.482: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5516" to be "running and ready"
Jan  5 10:13:06.487: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430244ms
Jan  5 10:13:06.487: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:13:08.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011473291s
Jan  5 10:13:08.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:13:10.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01176375s
Jan  5 10:13:10.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:13:12.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010401625s
Jan  5 10:13:12.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:13:14.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.0120587s
Jan  5 10:13:14.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:13:16.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010386796s
Jan  5 10:13:16.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:13:18.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.032129198s
Jan  5 10:13:18.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:13:20.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011614854s
Jan  5 10:13:20.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:13:22.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009962956s
Jan  5 10:13:22.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:13:24.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010308862s
Jan  5 10:13:24.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:13:26.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011852643s
Jan  5 10:13:26.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:13:28.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011616369s
Jan  5 10:13:28.494: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  5 10:13:28.494: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  5 10:13:28.498: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5516" to be "running and ready"
Jan  5 10:13:28.503: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.801686ms
Jan  5 10:13:28.503: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  5 10:13:28.503: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  5 10:13:28.508: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5516" to be "running and ready"
Jan  5 10:13:28.512: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.81395ms
Jan  5 10:13:28.513: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  5 10:13:28.513: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan  5 10:13:28.518: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5516" to be "running and ready"
Jan  5 10:13:28.523: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.103579ms
Jan  5 10:13:28.523: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan  5 10:13:28.523: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 01/05/23 10:13:28.527
Jan  5 10:13:28.538: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5516" to be "running"
Jan  5 10:13:28.545: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124654ms
Jan  5 10:13:30.552: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014283163s
Jan  5 10:13:30.552: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  5 10:13:30.557: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jan  5 10:13:30.557: INFO: Breadth first check of 10.96.3.173 on host 10.250.0.128...
Jan  5 10:13:30.561: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.245:9080/dial?request=hostname&protocol=udp&host=10.96.3.173&port=8081&tries=1'] Namespace:pod-network-test-5516 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:13:30.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:13:30.562: INFO: ExecWithOptions: Clientset creation
Jan  5 10:13:30.562: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-5516/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.3.173%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 10:13:31.035: INFO: Waiting for responses: map[]
Jan  5 10:13:31.035: INFO: reached 10.96.3.173 after 0/1 tries
Jan  5 10:13:31.035: INFO: Breadth first check of 10.96.1.146 on host 10.250.1.19...
Jan  5 10:13:31.041: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.245:9080/dial?request=hostname&protocol=udp&host=10.96.1.146&port=8081&tries=1'] Namespace:pod-network-test-5516 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:13:31.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:13:31.042: INFO: ExecWithOptions: Clientset creation
Jan  5 10:13:31.042: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-5516/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.1.146%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 10:13:31.478: INFO: Waiting for responses: map[]
Jan  5 10:13:31.478: INFO: reached 10.96.1.146 after 0/1 tries
Jan  5 10:13:31.478: INFO: Breadth first check of 10.96.0.184 on host 10.250.0.174...
Jan  5 10:13:31.483: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.245:9080/dial?request=hostname&protocol=udp&host=10.96.0.184&port=8081&tries=1'] Namespace:pod-network-test-5516 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:13:31.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:13:31.483: INFO: ExecWithOptions: Clientset creation
Jan  5 10:13:31.483: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-5516/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.0.184%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 10:13:31.930: INFO: Waiting for responses: map[]
Jan  5 10:13:31.930: INFO: reached 10.96.0.184 after 0/1 tries
Jan  5 10:13:31.930: INFO: Breadth first check of 10.96.2.209 on host 10.250.2.138...
Jan  5 10:13:31.935: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.245:9080/dial?request=hostname&protocol=udp&host=10.96.2.209&port=8081&tries=1'] Namespace:pod-network-test-5516 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:13:31.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:13:31.935: INFO: ExecWithOptions: Clientset creation
Jan  5 10:13:31.935: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-5516/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.2.209%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 10:13:32.329: INFO: Waiting for responses: map[]
Jan  5 10:13:32.329: INFO: reached 10.96.2.209 after 0/1 tries
Jan  5 10:13:32.329: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan  5 10:13:32.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5516" for this suite. 01/05/23 10:13:32.337
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":270,"skipped":5025,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.967 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:13:06.375
    Jan  5 10:13:06.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pod-network-test 01/05/23 10:13:06.376
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:06.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:06.399
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-5516 01/05/23 10:13:06.408
    STEP: creating a selector 01/05/23 10:13:06.408
    STEP: Creating the service pods in kubernetes 01/05/23 10:13:06.408
    Jan  5 10:13:06.408: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  5 10:13:06.482: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5516" to be "running and ready"
    Jan  5 10:13:06.487: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430244ms
    Jan  5 10:13:06.487: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:13:08.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011473291s
    Jan  5 10:13:08.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:13:10.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01176375s
    Jan  5 10:13:10.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:13:12.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010401625s
    Jan  5 10:13:12.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:13:14.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.0120587s
    Jan  5 10:13:14.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:13:16.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010386796s
    Jan  5 10:13:16.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:13:18.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.032129198s
    Jan  5 10:13:18.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:13:20.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011614854s
    Jan  5 10:13:20.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:13:22.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009962956s
    Jan  5 10:13:22.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:13:24.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010308862s
    Jan  5 10:13:24.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:13:26.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011852643s
    Jan  5 10:13:26.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:13:28.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011616369s
    Jan  5 10:13:28.494: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  5 10:13:28.494: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  5 10:13:28.498: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5516" to be "running and ready"
    Jan  5 10:13:28.503: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.801686ms
    Jan  5 10:13:28.503: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  5 10:13:28.503: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  5 10:13:28.508: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5516" to be "running and ready"
    Jan  5 10:13:28.512: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.81395ms
    Jan  5 10:13:28.513: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  5 10:13:28.513: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan  5 10:13:28.518: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5516" to be "running and ready"
    Jan  5 10:13:28.523: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.103579ms
    Jan  5 10:13:28.523: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan  5 10:13:28.523: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 01/05/23 10:13:28.527
    Jan  5 10:13:28.538: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5516" to be "running"
    Jan  5 10:13:28.545: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124654ms
    Jan  5 10:13:30.552: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014283163s
    Jan  5 10:13:30.552: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  5 10:13:30.557: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Jan  5 10:13:30.557: INFO: Breadth first check of 10.96.3.173 on host 10.250.0.128...
    Jan  5 10:13:30.561: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.245:9080/dial?request=hostname&protocol=udp&host=10.96.3.173&port=8081&tries=1'] Namespace:pod-network-test-5516 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:13:30.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:13:30.562: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:13:30.562: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-5516/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.3.173%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 10:13:31.035: INFO: Waiting for responses: map[]
    Jan  5 10:13:31.035: INFO: reached 10.96.3.173 after 0/1 tries
    Jan  5 10:13:31.035: INFO: Breadth first check of 10.96.1.146 on host 10.250.1.19...
    Jan  5 10:13:31.041: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.245:9080/dial?request=hostname&protocol=udp&host=10.96.1.146&port=8081&tries=1'] Namespace:pod-network-test-5516 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:13:31.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:13:31.042: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:13:31.042: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-5516/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.1.146%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 10:13:31.478: INFO: Waiting for responses: map[]
    Jan  5 10:13:31.478: INFO: reached 10.96.1.146 after 0/1 tries
    Jan  5 10:13:31.478: INFO: Breadth first check of 10.96.0.184 on host 10.250.0.174...
    Jan  5 10:13:31.483: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.245:9080/dial?request=hostname&protocol=udp&host=10.96.0.184&port=8081&tries=1'] Namespace:pod-network-test-5516 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:13:31.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:13:31.483: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:13:31.483: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-5516/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.0.184%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 10:13:31.930: INFO: Waiting for responses: map[]
    Jan  5 10:13:31.930: INFO: reached 10.96.0.184 after 0/1 tries
    Jan  5 10:13:31.930: INFO: Breadth first check of 10.96.2.209 on host 10.250.2.138...
    Jan  5 10:13:31.935: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.245:9080/dial?request=hostname&protocol=udp&host=10.96.2.209&port=8081&tries=1'] Namespace:pod-network-test-5516 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:13:31.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:13:31.935: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:13:31.935: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-5516/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.2.209%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 10:13:32.329: INFO: Waiting for responses: map[]
    Jan  5 10:13:32.329: INFO: reached 10.96.2.209 after 0/1 tries
    Jan  5 10:13:32.329: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan  5 10:13:32.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5516" for this suite. 01/05/23 10:13:32.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:13:32.344
Jan  5 10:13:32.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename sched-pred 01/05/23 10:13:32.345
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:32.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:32.363
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan  5 10:13:32.370: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  5 10:13:32.385: INFO: Waiting for terminating namespaces to be deleted...
Jan  5 10:13:32.389: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t before test
Jan  5 10:13:32.405: INFO: apiserver-proxy-m4wgr from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.405: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:13:32.405: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:13:32.405: INFO: cilium-5qvr2 from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.405: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:13:32.405: INFO: csi-driver-node-nt6bm from kube-system started at 2023-01-05 08:52:50 +0000 UTC (3 container statuses recorded)
Jan  5 10:13:32.405: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:13:32.405: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:13:32.405: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:13:32.405: INFO: kube-proxy-worker-omyby-v1.25.5-xvlqq from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.405: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:13:32.405: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:13:32.405: INFO: node-exporter-hckzs from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.405: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:13:32.405: INFO: node-problem-detector-5ln6w from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.405: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:13:32.405: INFO: netserver-0 from pod-network-test-5516 started at 2023-01-05 10:13:06 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.405: INFO: 	Container webserver ready: true, restart count 0
Jan  5 10:13:32.405: INFO: sonobuoy-e2e-job-2d2d23fb3e814b3d from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.405: INFO: 	Container e2e ready: true, restart count 0
Jan  5 10:13:32.405: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:13:32.405: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-m6xzl from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.405: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:13:32.405: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:13:32.405: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r before test
Jan  5 10:13:32.421: INFO: apiserver-proxy-4w6xb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.421: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:13:32.421: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:13:32.421: INFO: cilium-cjv9l from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.421: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:13:32.421: INFO: cilium-operator-6bf67c77c6-r9hbq from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.421: INFO: 	Container cilium-operator ready: true, restart count 0
Jan  5 10:13:32.421: INFO: csi-driver-node-r2l8x from kube-system started at 2023-01-05 08:52:30 +0000 UTC (3 container statuses recorded)
Jan  5 10:13:32.421: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:13:32.421: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:13:32.421: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:13:32.421: INFO: kube-proxy-worker-omyby-v1.25.5-fz64v from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.421: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:13:32.421: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:13:32.421: INFO: node-exporter-7hcrb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.421: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:13:32.421: INFO: node-problem-detector-l85nm from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.421: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:13:32.421: INFO: netserver-1 from pod-network-test-5516 started at 2023-01-05 10:13:06 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.421: INFO: 	Container webserver ready: true, restart count 0
Jan  5 10:13:32.421: INFO: test-container-pod from pod-network-test-5516 started at 2023-01-05 10:13:28 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.421: INFO: 	Container webserver ready: true, restart count 0
Jan  5 10:13:32.421: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-xghsj from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.421: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:13:32.421: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:13:32.421: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 before test
Jan  5 10:13:32.437: INFO: apiserver-proxy-fnbwn from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:13:32.437: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:13:32.437: INFO: blackbox-exporter-c866d5696-wkcmf from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan  5 10:13:32.437: INFO: blackbox-exporter-c866d5696-z5dx5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan  5 10:13:32.437: INFO: cilium-operator-6bf67c77c6-mmgs4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container cilium-operator ready: true, restart count 0
Jan  5 10:13:32.437: INFO: cilium-x98hw from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:13:32.437: INFO: coredns-7594945774-g79vz from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container coredns ready: true, restart count 0
Jan  5 10:13:32.437: INFO: coredns-7594945774-wghlc from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container coredns ready: true, restart count 0
Jan  5 10:13:32.437: INFO: csi-driver-node-wcklx from kube-system started at 2023-01-05 08:52:24 +0000 UTC (3 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:13:32.437: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:13:32.437: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:13:32.437: INFO: kube-proxy-worker-vot5k-v1.25.5-86pmg from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:13:32.437: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:13:32.437: INFO: metrics-server-b58b76d9c-jzq89 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container metrics-server ready: true, restart count 0
Jan  5 10:13:32.437: INFO: metrics-server-b58b76d9c-kwfd5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container metrics-server ready: true, restart count 0
Jan  5 10:13:32.437: INFO: node-exporter-xlnb4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:13:32.437: INFO: node-problem-detector-bzpk7 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:13:32.437: INFO: vpn-shoot-69957f7fdd-rmrkr from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container vpn-shoot ready: true, restart count 0
Jan  5 10:13:32.437: INFO: netserver-2 from pod-network-test-5516 started at 2023-01-05 10:13:06 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container webserver ready: true, restart count 0
Jan  5 10:13:32.437: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-mw7c6 from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.437: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:13:32.437: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:13:32.437: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv before test
Jan  5 10:13:32.449: INFO: concurrent-27881893-wlfs2 from cronjob-6021 started at 2023-01-05 10:13:00 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.449: INFO: 	Container c ready: false, restart count 0
Jan  5 10:13:32.449: INFO: apiserver-proxy-4j65b from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.449: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:13:32.449: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:13:32.449: INFO: cilium-57wqw from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.449: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:13:32.449: INFO: csi-driver-node-rj5cq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (3 container statuses recorded)
Jan  5 10:13:32.449: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:13:32.449: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:13:32.449: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:13:32.449: INFO: kube-proxy-worker-vot5k-v1.25.5-pg2pl from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.449: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:13:32.449: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:13:32.449: INFO: node-exporter-25qsq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.449: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:13:32.449: INFO: node-problem-detector-7xm42 from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.449: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:13:32.449: INFO: netserver-3 from pod-network-test-5516 started at 2023-01-05 10:13:06 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.449: INFO: 	Container webserver ready: true, restart count 0
Jan  5 10:13:32.449: INFO: sonobuoy from sonobuoy started at 2023-01-05 09:06:38 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.449: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  5 10:13:32.449: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-qvcts from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:13:32.449: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:13:32.449: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:13:32.449: INFO: webhook-to-be-mutated from webhook-1824 started at 2023-01-05 10:13:06 +0000 UTC (1 container statuses recorded)
Jan  5 10:13:32.449: INFO: 	Container example ready: false, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t 01/05/23 10:13:32.479
STEP: verifying the node has the label node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r 01/05/23 10:13:32.511
STEP: verifying the node has the label node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 01/05/23 10:13:32.54
STEP: verifying the node has the label node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv 01/05/23 10:13:32.617
Jan  5 10:13:32.640: INFO: Pod concurrent-27881893-wlfs2 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
Jan  5 10:13:32.640: INFO: Pod apiserver-proxy-4j65b requesting resource cpu=40m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
Jan  5 10:13:32.640: INFO: Pod apiserver-proxy-4w6xb requesting resource cpu=40m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
Jan  5 10:13:32.640: INFO: Pod apiserver-proxy-fnbwn requesting resource cpu=40m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.640: INFO: Pod apiserver-proxy-m4wgr requesting resource cpu=40m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
Jan  5 10:13:32.640: INFO: Pod blackbox-exporter-c866d5696-wkcmf requesting resource cpu=10m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.640: INFO: Pod blackbox-exporter-c866d5696-z5dx5 requesting resource cpu=10m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.640: INFO: Pod cilium-57wqw requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
Jan  5 10:13:32.640: INFO: Pod cilium-5qvr2 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
Jan  5 10:13:32.640: INFO: Pod cilium-cjv9l requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
Jan  5 10:13:32.640: INFO: Pod cilium-operator-6bf67c77c6-mmgs4 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.640: INFO: Pod cilium-operator-6bf67c77c6-r9hbq requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
Jan  5 10:13:32.640: INFO: Pod cilium-x98hw requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.640: INFO: Pod coredns-7594945774-g79vz requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.640: INFO: Pod coredns-7594945774-wghlc requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.640: INFO: Pod csi-driver-node-nt6bm requesting resource cpu=37m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
Jan  5 10:13:32.640: INFO: Pod csi-driver-node-r2l8x requesting resource cpu=37m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
Jan  5 10:13:32.640: INFO: Pod csi-driver-node-rj5cq requesting resource cpu=37m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
Jan  5 10:13:32.640: INFO: Pod csi-driver-node-wcklx requesting resource cpu=37m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.640: INFO: Pod kube-proxy-worker-omyby-v1.25.5-fz64v requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
Jan  5 10:13:32.640: INFO: Pod kube-proxy-worker-omyby-v1.25.5-xvlqq requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
Jan  5 10:13:32.640: INFO: Pod kube-proxy-worker-vot5k-v1.25.5-86pmg requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.640: INFO: Pod kube-proxy-worker-vot5k-v1.25.5-pg2pl requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
Jan  5 10:13:32.640: INFO: Pod metrics-server-b58b76d9c-jzq89 requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.640: INFO: Pod metrics-server-b58b76d9c-kwfd5 requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.640: INFO: Pod node-exporter-25qsq requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
Jan  5 10:13:32.640: INFO: Pod node-exporter-7hcrb requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
Jan  5 10:13:32.640: INFO: Pod node-exporter-hckzs requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
Jan  5 10:13:32.640: INFO: Pod node-exporter-xlnb4 requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.640: INFO: Pod node-problem-detector-5ln6w requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
Jan  5 10:13:32.641: INFO: Pod node-problem-detector-7xm42 requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
Jan  5 10:13:32.641: INFO: Pod node-problem-detector-bzpk7 requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.641: INFO: Pod node-problem-detector-l85nm requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
Jan  5 10:13:32.641: INFO: Pod vpn-shoot-69957f7fdd-rmrkr requesting resource cpu=100m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.641: INFO: Pod netserver-0 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
Jan  5 10:13:32.641: INFO: Pod netserver-1 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
Jan  5 10:13:32.641: INFO: Pod netserver-2 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.641: INFO: Pod netserver-3 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
Jan  5 10:13:32.641: INFO: Pod test-container-pod requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
Jan  5 10:13:32.641: INFO: Pod sonobuoy requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
Jan  5 10:13:32.641: INFO: Pod sonobuoy-e2e-job-2d2d23fb3e814b3d requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
Jan  5 10:13:32.641: INFO: Pod sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-m6xzl requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
Jan  5 10:13:32.641: INFO: Pod sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-mw7c6 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.641: INFO: Pod sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-qvcts requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
Jan  5 10:13:32.641: INFO: Pod sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-xghsj requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
Jan  5 10:13:32.641: INFO: Pod webhook-to-be-mutated requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
STEP: Starting Pods to consume most of the cluster CPU. 01/05/23 10:13:32.641
Jan  5 10:13:32.641: INFO: Creating a pod which consumes cpu=2627m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
Jan  5 10:13:32.651: INFO: Creating a pod which consumes cpu=2403m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
Jan  5 10:13:32.661: INFO: Creating a pod which consumes cpu=2627m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
Jan  5 10:13:32.668: INFO: Creating a pod which consumes cpu=2627m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
Jan  5 10:13:32.677: INFO: Waiting up to 5m0s for pod "filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b" in namespace "sched-pred-9091" to be "running"
Jan  5 10:13:32.682: INFO: Pod "filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.637771ms
Jan  5 10:13:34.689: INFO: Pod "filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011452947s
Jan  5 10:13:34.689: INFO: Pod "filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b" satisfied condition "running"
Jan  5 10:13:34.689: INFO: Waiting up to 5m0s for pod "filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b" in namespace "sched-pred-9091" to be "running"
Jan  5 10:13:34.695: INFO: Pod "filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b": Phase="Running", Reason="", readiness=true. Elapsed: 6.32714ms
Jan  5 10:13:34.695: INFO: Pod "filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b" satisfied condition "running"
Jan  5 10:13:34.695: INFO: Waiting up to 5m0s for pod "filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae" in namespace "sched-pred-9091" to be "running"
Jan  5 10:13:34.700: INFO: Pod "filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae": Phase="Running", Reason="", readiness=true. Elapsed: 4.736225ms
Jan  5 10:13:34.700: INFO: Pod "filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae" satisfied condition "running"
Jan  5 10:13:34.700: INFO: Waiting up to 5m0s for pod "filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c" in namespace "sched-pred-9091" to be "running"
Jan  5 10:13:34.705: INFO: Pod "filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c": Phase="Running", Reason="", readiness=true. Elapsed: 4.912163ms
Jan  5 10:13:34.705: INFO: Pod "filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/05/23 10:13:34.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b.1737621136893449], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9091/filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b to shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22] 01/05/23 10:13:34.712
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b.1737621160ab2b75], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/05/23 10:13:34.712
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b.17376211648b67a0], Reason = [Created], Message = [Created container filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b] 01/05/23 10:13:34.712
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b.1737621169bf6cb8], Reason = [Started], Message = [Started container filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b] 01/05/23 10:13:34.712
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae.17376211374fa4c0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9091/filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae to shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv] 01/05/23 10:13:34.713
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae.1737621162c94b61], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/05/23 10:13:34.713
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae.1737621168b754db], Reason = [Created], Message = [Created container filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae] 01/05/23 10:13:34.713
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae.173762116c6c4e49], Reason = [Started], Message = [Started container filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae] 01/05/23 10:13:34.713
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c.17376211378bff24], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9091/filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c to shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t] 01/05/23 10:13:34.713
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c.1737621160091b94], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/05/23 10:13:34.713
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c.173762116882da34], Reason = [Created], Message = [Created container filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c] 01/05/23 10:13:34.713
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c.173762116cb8039b], Reason = [Started], Message = [Started container filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c] 01/05/23 10:13:34.713
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b.17376211360cd410], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9091/filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b to shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r] 01/05/23 10:13:34.714
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b.173762116286d09d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/05/23 10:13:34.714
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b.173762116625ec94], Reason = [Created], Message = [Created container filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b] 01/05/23 10:13:34.714
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b.1737621169ff17a2], Reason = [Started], Message = [Started container filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b] 01/05/23 10:13:34.714
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17376211b1336e1b], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu. preemption: 0/4 nodes are available: 4 No preemption victims found for incoming pod.] 01/05/23 10:13:34.728
STEP: removing the label node off the node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t 01/05/23 10:13:35.73
STEP: verifying the node doesn't have the label node 01/05/23 10:13:35.752
STEP: removing the label node off the node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r 01/05/23 10:13:35.761
STEP: verifying the node doesn't have the label node 01/05/23 10:13:35.804
STEP: removing the label node off the node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 01/05/23 10:13:35.812
STEP: verifying the node doesn't have the label node 01/05/23 10:13:35.832
STEP: removing the label node off the node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv 01/05/23 10:13:35.837
STEP: verifying the node doesn't have the label node 01/05/23 10:13:35.86
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan  5 10:13:35.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9091" for this suite. 01/05/23 10:13:35.875
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":271,"skipped":5030,"failed":0}
------------------------------
â€¢ [3.541 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:13:32.344
    Jan  5 10:13:32.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename sched-pred 01/05/23 10:13:32.345
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:32.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:32.363
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan  5 10:13:32.370: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  5 10:13:32.385: INFO: Waiting for terminating namespaces to be deleted...
    Jan  5 10:13:32.389: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t before test
    Jan  5 10:13:32.405: INFO: apiserver-proxy-m4wgr from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.405: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: cilium-5qvr2 from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.405: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: csi-driver-node-nt6bm from kube-system started at 2023-01-05 08:52:50 +0000 UTC (3 container statuses recorded)
    Jan  5 10:13:32.405: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: kube-proxy-worker-omyby-v1.25.5-xvlqq from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.405: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: node-exporter-hckzs from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.405: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: node-problem-detector-5ln6w from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.405: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: netserver-0 from pod-network-test-5516 started at 2023-01-05 10:13:06 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.405: INFO: 	Container webserver ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: sonobuoy-e2e-job-2d2d23fb3e814b3d from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.405: INFO: 	Container e2e ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-m6xzl from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.405: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:13:32.405: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r before test
    Jan  5 10:13:32.421: INFO: apiserver-proxy-4w6xb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.421: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: cilium-cjv9l from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.421: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: cilium-operator-6bf67c77c6-r9hbq from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.421: INFO: 	Container cilium-operator ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: csi-driver-node-r2l8x from kube-system started at 2023-01-05 08:52:30 +0000 UTC (3 container statuses recorded)
    Jan  5 10:13:32.421: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: kube-proxy-worker-omyby-v1.25.5-fz64v from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.421: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: node-exporter-7hcrb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.421: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: node-problem-detector-l85nm from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.421: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: netserver-1 from pod-network-test-5516 started at 2023-01-05 10:13:06 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.421: INFO: 	Container webserver ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: test-container-pod from pod-network-test-5516 started at 2023-01-05 10:13:28 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.421: INFO: 	Container webserver ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-xghsj from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.421: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:13:32.421: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 before test
    Jan  5 10:13:32.437: INFO: apiserver-proxy-fnbwn from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: blackbox-exporter-c866d5696-wkcmf from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: blackbox-exporter-c866d5696-z5dx5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: cilium-operator-6bf67c77c6-mmgs4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container cilium-operator ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: cilium-x98hw from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: coredns-7594945774-g79vz from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container coredns ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: coredns-7594945774-wghlc from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container coredns ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: csi-driver-node-wcklx from kube-system started at 2023-01-05 08:52:24 +0000 UTC (3 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: kube-proxy-worker-vot5k-v1.25.5-86pmg from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: metrics-server-b58b76d9c-jzq89 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: metrics-server-b58b76d9c-kwfd5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: node-exporter-xlnb4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: node-problem-detector-bzpk7 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: vpn-shoot-69957f7fdd-rmrkr from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container vpn-shoot ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: netserver-2 from pod-network-test-5516 started at 2023-01-05 10:13:06 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container webserver ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-mw7c6 from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.437: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:13:32.437: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv before test
    Jan  5 10:13:32.449: INFO: concurrent-27881893-wlfs2 from cronjob-6021 started at 2023-01-05 10:13:00 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.449: INFO: 	Container c ready: false, restart count 0
    Jan  5 10:13:32.449: INFO: apiserver-proxy-4j65b from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.449: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: cilium-57wqw from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.449: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: csi-driver-node-rj5cq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (3 container statuses recorded)
    Jan  5 10:13:32.449: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: kube-proxy-worker-vot5k-v1.25.5-pg2pl from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.449: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: node-exporter-25qsq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.449: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: node-problem-detector-7xm42 from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.449: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: netserver-3 from pod-network-test-5516 started at 2023-01-05 10:13:06 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.449: INFO: 	Container webserver ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: sonobuoy from sonobuoy started at 2023-01-05 09:06:38 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.449: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-qvcts from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:13:32.449: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:13:32.449: INFO: webhook-to-be-mutated from webhook-1824 started at 2023-01-05 10:13:06 +0000 UTC (1 container statuses recorded)
    Jan  5 10:13:32.449: INFO: 	Container example ready: false, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t 01/05/23 10:13:32.479
    STEP: verifying the node has the label node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r 01/05/23 10:13:32.511
    STEP: verifying the node has the label node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 01/05/23 10:13:32.54
    STEP: verifying the node has the label node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv 01/05/23 10:13:32.617
    Jan  5 10:13:32.640: INFO: Pod concurrent-27881893-wlfs2 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
    Jan  5 10:13:32.640: INFO: Pod apiserver-proxy-4j65b requesting resource cpu=40m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
    Jan  5 10:13:32.640: INFO: Pod apiserver-proxy-4w6xb requesting resource cpu=40m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
    Jan  5 10:13:32.640: INFO: Pod apiserver-proxy-fnbwn requesting resource cpu=40m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.640: INFO: Pod apiserver-proxy-m4wgr requesting resource cpu=40m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
    Jan  5 10:13:32.640: INFO: Pod blackbox-exporter-c866d5696-wkcmf requesting resource cpu=10m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.640: INFO: Pod blackbox-exporter-c866d5696-z5dx5 requesting resource cpu=10m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.640: INFO: Pod cilium-57wqw requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
    Jan  5 10:13:32.640: INFO: Pod cilium-5qvr2 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
    Jan  5 10:13:32.640: INFO: Pod cilium-cjv9l requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
    Jan  5 10:13:32.640: INFO: Pod cilium-operator-6bf67c77c6-mmgs4 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.640: INFO: Pod cilium-operator-6bf67c77c6-r9hbq requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
    Jan  5 10:13:32.640: INFO: Pod cilium-x98hw requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.640: INFO: Pod coredns-7594945774-g79vz requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.640: INFO: Pod coredns-7594945774-wghlc requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.640: INFO: Pod csi-driver-node-nt6bm requesting resource cpu=37m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
    Jan  5 10:13:32.640: INFO: Pod csi-driver-node-r2l8x requesting resource cpu=37m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
    Jan  5 10:13:32.640: INFO: Pod csi-driver-node-rj5cq requesting resource cpu=37m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
    Jan  5 10:13:32.640: INFO: Pod csi-driver-node-wcklx requesting resource cpu=37m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.640: INFO: Pod kube-proxy-worker-omyby-v1.25.5-fz64v requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
    Jan  5 10:13:32.640: INFO: Pod kube-proxy-worker-omyby-v1.25.5-xvlqq requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
    Jan  5 10:13:32.640: INFO: Pod kube-proxy-worker-vot5k-v1.25.5-86pmg requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.640: INFO: Pod kube-proxy-worker-vot5k-v1.25.5-pg2pl requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
    Jan  5 10:13:32.640: INFO: Pod metrics-server-b58b76d9c-jzq89 requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.640: INFO: Pod metrics-server-b58b76d9c-kwfd5 requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.640: INFO: Pod node-exporter-25qsq requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
    Jan  5 10:13:32.640: INFO: Pod node-exporter-7hcrb requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
    Jan  5 10:13:32.640: INFO: Pod node-exporter-hckzs requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
    Jan  5 10:13:32.640: INFO: Pod node-exporter-xlnb4 requesting resource cpu=50m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.640: INFO: Pod node-problem-detector-5ln6w requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
    Jan  5 10:13:32.641: INFO: Pod node-problem-detector-7xm42 requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
    Jan  5 10:13:32.641: INFO: Pod node-problem-detector-bzpk7 requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.641: INFO: Pod node-problem-detector-l85nm requesting resource cpu=20m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
    Jan  5 10:13:32.641: INFO: Pod vpn-shoot-69957f7fdd-rmrkr requesting resource cpu=100m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.641: INFO: Pod netserver-0 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
    Jan  5 10:13:32.641: INFO: Pod netserver-1 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
    Jan  5 10:13:32.641: INFO: Pod netserver-2 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.641: INFO: Pod netserver-3 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
    Jan  5 10:13:32.641: INFO: Pod test-container-pod requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
    Jan  5 10:13:32.641: INFO: Pod sonobuoy requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
    Jan  5 10:13:32.641: INFO: Pod sonobuoy-e2e-job-2d2d23fb3e814b3d requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
    Jan  5 10:13:32.641: INFO: Pod sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-m6xzl requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
    Jan  5 10:13:32.641: INFO: Pod sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-mw7c6 requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.641: INFO: Pod sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-qvcts requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
    Jan  5 10:13:32.641: INFO: Pod sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-xghsj requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
    Jan  5 10:13:32.641: INFO: Pod webhook-to-be-mutated requesting resource cpu=0m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
    STEP: Starting Pods to consume most of the cluster CPU. 01/05/23 10:13:32.641
    Jan  5 10:13:32.641: INFO: Creating a pod which consumes cpu=2627m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r
    Jan  5 10:13:32.651: INFO: Creating a pod which consumes cpu=2403m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22
    Jan  5 10:13:32.661: INFO: Creating a pod which consumes cpu=2627m on Node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv
    Jan  5 10:13:32.668: INFO: Creating a pod which consumes cpu=2627m on Node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t
    Jan  5 10:13:32.677: INFO: Waiting up to 5m0s for pod "filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b" in namespace "sched-pred-9091" to be "running"
    Jan  5 10:13:32.682: INFO: Pod "filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.637771ms
    Jan  5 10:13:34.689: INFO: Pod "filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011452947s
    Jan  5 10:13:34.689: INFO: Pod "filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b" satisfied condition "running"
    Jan  5 10:13:34.689: INFO: Waiting up to 5m0s for pod "filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b" in namespace "sched-pred-9091" to be "running"
    Jan  5 10:13:34.695: INFO: Pod "filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b": Phase="Running", Reason="", readiness=true. Elapsed: 6.32714ms
    Jan  5 10:13:34.695: INFO: Pod "filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b" satisfied condition "running"
    Jan  5 10:13:34.695: INFO: Waiting up to 5m0s for pod "filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae" in namespace "sched-pred-9091" to be "running"
    Jan  5 10:13:34.700: INFO: Pod "filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae": Phase="Running", Reason="", readiness=true. Elapsed: 4.736225ms
    Jan  5 10:13:34.700: INFO: Pod "filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae" satisfied condition "running"
    Jan  5 10:13:34.700: INFO: Waiting up to 5m0s for pod "filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c" in namespace "sched-pred-9091" to be "running"
    Jan  5 10:13:34.705: INFO: Pod "filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c": Phase="Running", Reason="", readiness=true. Elapsed: 4.912163ms
    Jan  5 10:13:34.705: INFO: Pod "filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/05/23 10:13:34.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b.1737621136893449], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9091/filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b to shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22] 01/05/23 10:13:34.712
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b.1737621160ab2b75], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/05/23 10:13:34.712
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b.17376211648b67a0], Reason = [Created], Message = [Created container filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b] 01/05/23 10:13:34.712
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b.1737621169bf6cb8], Reason = [Started], Message = [Started container filler-pod-2f7e6297-6299-41c6-9a34-76dba131e63b] 01/05/23 10:13:34.712
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae.17376211374fa4c0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9091/filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae to shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv] 01/05/23 10:13:34.713
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae.1737621162c94b61], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/05/23 10:13:34.713
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae.1737621168b754db], Reason = [Created], Message = [Created container filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae] 01/05/23 10:13:34.713
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae.173762116c6c4e49], Reason = [Started], Message = [Started container filler-pod-45064c60-2ec2-4d14-b0a9-aa0f0685f7ae] 01/05/23 10:13:34.713
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c.17376211378bff24], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9091/filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c to shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t] 01/05/23 10:13:34.713
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c.1737621160091b94], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/05/23 10:13:34.713
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c.173762116882da34], Reason = [Created], Message = [Created container filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c] 01/05/23 10:13:34.713
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c.173762116cb8039b], Reason = [Started], Message = [Started container filler-pod-7cfcca94-c5c1-438d-b80d-cfbb63b4111c] 01/05/23 10:13:34.713
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b.17376211360cd410], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9091/filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b to shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r] 01/05/23 10:13:34.714
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b.173762116286d09d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/05/23 10:13:34.714
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b.173762116625ec94], Reason = [Created], Message = [Created container filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b] 01/05/23 10:13:34.714
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b.1737621169ff17a2], Reason = [Started], Message = [Started container filler-pod-dd04614f-c4ec-4bf0-92a6-0981fd407f9b] 01/05/23 10:13:34.714
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17376211b1336e1b], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu. preemption: 0/4 nodes are available: 4 No preemption victims found for incoming pod.] 01/05/23 10:13:34.728
    STEP: removing the label node off the node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t 01/05/23 10:13:35.73
    STEP: verifying the node doesn't have the label node 01/05/23 10:13:35.752
    STEP: removing the label node off the node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r 01/05/23 10:13:35.761
    STEP: verifying the node doesn't have the label node 01/05/23 10:13:35.804
    STEP: removing the label node off the node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 01/05/23 10:13:35.812
    STEP: verifying the node doesn't have the label node 01/05/23 10:13:35.832
    STEP: removing the label node off the node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv 01/05/23 10:13:35.837
    STEP: verifying the node doesn't have the label node 01/05/23 10:13:35.86
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 10:13:35.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9091" for this suite. 01/05/23 10:13:35.875
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:13:35.888
Jan  5 10:13:35.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 10:13:35.889
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:35.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:35.927
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-e46815e2-b9b6-4160-8cfa-1efb5e9f89a2 01/05/23 10:13:35.932
STEP: Creating a pod to test consume configMaps 01/05/23 10:13:35.937
Jan  5 10:13:35.945: INFO: Waiting up to 5m0s for pod "pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be" in namespace "configmap-1089" to be "Succeeded or Failed"
Jan  5 10:13:35.949: INFO: Pod "pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be": Phase="Pending", Reason="", readiness=false. Elapsed: 3.856555ms
Jan  5 10:13:37.955: INFO: Pod "pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010450997s
Jan  5 10:13:39.956: INFO: Pod "pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010882548s
STEP: Saw pod success 01/05/23 10:13:39.956
Jan  5 10:13:39.956: INFO: Pod "pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be" satisfied condition "Succeeded or Failed"
Jan  5 10:13:39.961: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be container agnhost-container: <nil>
STEP: delete the pod 01/05/23 10:13:39.979
Jan  5 10:13:39.989: INFO: Waiting for pod pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be to disappear
Jan  5 10:13:39.994: INFO: Pod pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 10:13:39.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1089" for this suite. 01/05/23 10:13:40.002
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":272,"skipped":5066,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:13:35.888
    Jan  5 10:13:35.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 10:13:35.889
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:35.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:35.927
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-e46815e2-b9b6-4160-8cfa-1efb5e9f89a2 01/05/23 10:13:35.932
    STEP: Creating a pod to test consume configMaps 01/05/23 10:13:35.937
    Jan  5 10:13:35.945: INFO: Waiting up to 5m0s for pod "pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be" in namespace "configmap-1089" to be "Succeeded or Failed"
    Jan  5 10:13:35.949: INFO: Pod "pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be": Phase="Pending", Reason="", readiness=false. Elapsed: 3.856555ms
    Jan  5 10:13:37.955: INFO: Pod "pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010450997s
    Jan  5 10:13:39.956: INFO: Pod "pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010882548s
    STEP: Saw pod success 01/05/23 10:13:39.956
    Jan  5 10:13:39.956: INFO: Pod "pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be" satisfied condition "Succeeded or Failed"
    Jan  5 10:13:39.961: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 10:13:39.979
    Jan  5 10:13:39.989: INFO: Waiting for pod pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be to disappear
    Jan  5 10:13:39.994: INFO: Pod pod-configmaps-d52a4c73-450c-4da3-b98d-1b01379d78be no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 10:13:39.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1089" for this suite. 01/05/23 10:13:40.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:13:40.009
Jan  5 10:13:40.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 10:13:40.01
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:40.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:40.061
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 10:13:40.081
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:13:40.613
STEP: Deploying the webhook pod 01/05/23 10:13:40.62
STEP: Wait for the deployment to be ready 01/05/23 10:13:40.631
Jan  5 10:13:40.641: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 10:13:42.657
STEP: Verifying the service has paired with the endpoint 01/05/23 10:13:42.671
Jan  5 10:13:43.672: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 01/05/23 10:13:43.728
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 10:13:43.809
STEP: Deleting the collection of validation webhooks 01/05/23 10:13:43.893
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 10:13:43.924
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:13:43.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2904" for this suite. 01/05/23 10:13:43.942
STEP: Destroying namespace "webhook-2904-markers" for this suite. 01/05/23 10:13:43.948
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":273,"skipped":5088,"failed":0}
------------------------------
â€¢ [3.982 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:13:40.009
    Jan  5 10:13:40.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 10:13:40.01
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:40.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:40.061
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 10:13:40.081
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:13:40.613
    STEP: Deploying the webhook pod 01/05/23 10:13:40.62
    STEP: Wait for the deployment to be ready 01/05/23 10:13:40.631
    Jan  5 10:13:40.641: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 10:13:42.657
    STEP: Verifying the service has paired with the endpoint 01/05/23 10:13:42.671
    Jan  5 10:13:43.672: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 01/05/23 10:13:43.728
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 10:13:43.809
    STEP: Deleting the collection of validation webhooks 01/05/23 10:13:43.893
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 10:13:43.924
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:13:43.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2904" for this suite. 01/05/23 10:13:43.942
    STEP: Destroying namespace "webhook-2904-markers" for this suite. 01/05/23 10:13:43.948
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:13:43.993
Jan  5 10:13:43.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename containers 01/05/23 10:13:43.994
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:44.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:44.027
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 01/05/23 10:13:44.036
Jan  5 10:13:44.046: INFO: Waiting up to 5m0s for pod "client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655" in namespace "containers-3627" to be "Succeeded or Failed"
Jan  5 10:13:44.052: INFO: Pod "client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655": Phase="Pending", Reason="", readiness=false. Elapsed: 5.837707ms
Jan  5 10:13:46.058: INFO: Pod "client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012035198s
Jan  5 10:13:48.058: INFO: Pod "client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012169003s
STEP: Saw pod success 01/05/23 10:13:48.058
Jan  5 10:13:48.058: INFO: Pod "client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655" satisfied condition "Succeeded or Failed"
Jan  5 10:13:48.063: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 10:13:48.076
Jan  5 10:13:48.088: INFO: Waiting for pod client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655 to disappear
Jan  5 10:13:48.092: INFO: Pod client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan  5 10:13:48.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3627" for this suite. 01/05/23 10:13:48.101
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":274,"skipped":5130,"failed":0}
------------------------------
â€¢ [4.114 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:13:43.993
    Jan  5 10:13:43.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename containers 01/05/23 10:13:43.994
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:44.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:44.027
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 01/05/23 10:13:44.036
    Jan  5 10:13:44.046: INFO: Waiting up to 5m0s for pod "client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655" in namespace "containers-3627" to be "Succeeded or Failed"
    Jan  5 10:13:44.052: INFO: Pod "client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655": Phase="Pending", Reason="", readiness=false. Elapsed: 5.837707ms
    Jan  5 10:13:46.058: INFO: Pod "client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012035198s
    Jan  5 10:13:48.058: INFO: Pod "client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012169003s
    STEP: Saw pod success 01/05/23 10:13:48.058
    Jan  5 10:13:48.058: INFO: Pod "client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655" satisfied condition "Succeeded or Failed"
    Jan  5 10:13:48.063: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 10:13:48.076
    Jan  5 10:13:48.088: INFO: Waiting for pod client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655 to disappear
    Jan  5 10:13:48.092: INFO: Pod client-containers-96a7a115-9d06-4a09-bd0b-88e97a76c655 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan  5 10:13:48.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3627" for this suite. 01/05/23 10:13:48.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:13:48.107
Jan  5 10:13:48.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 10:13:48.109
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:48.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:48.13
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 01/05/23 10:13:48.138
Jan  5 10:13:48.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-9821 create -f -'
Jan  5 10:13:48.872: INFO: stderr: ""
Jan  5 10:13:48.872: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/05/23 10:13:48.872
Jan  5 10:13:49.879: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 10:13:49.879: INFO: Found 0 / 1
Jan  5 10:13:50.879: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 10:13:50.879: INFO: Found 1 / 1
Jan  5 10:13:50.879: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/05/23 10:13:50.879
Jan  5 10:13:50.882: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 10:13:50.882: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  5 10:13:50.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-9821 patch pod agnhost-primary-ttf5z -p {"metadata":{"annotations":{"x":"y"}}}'
Jan  5 10:13:50.952: INFO: stderr: ""
Jan  5 10:13:50.952: INFO: stdout: "pod/agnhost-primary-ttf5z patched\n"
STEP: checking annotations 01/05/23 10:13:50.952
Jan  5 10:13:50.958: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 10:13:50.958: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 10:13:50.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9821" for this suite. 01/05/23 10:13:50.965
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":275,"skipped":5138,"failed":0}
------------------------------
â€¢ [2.863 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:13:48.107
    Jan  5 10:13:48.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 10:13:48.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:48.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:48.13
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 01/05/23 10:13:48.138
    Jan  5 10:13:48.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-9821 create -f -'
    Jan  5 10:13:48.872: INFO: stderr: ""
    Jan  5 10:13:48.872: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/05/23 10:13:48.872
    Jan  5 10:13:49.879: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 10:13:49.879: INFO: Found 0 / 1
    Jan  5 10:13:50.879: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 10:13:50.879: INFO: Found 1 / 1
    Jan  5 10:13:50.879: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/05/23 10:13:50.879
    Jan  5 10:13:50.882: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 10:13:50.882: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  5 10:13:50.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-9821 patch pod agnhost-primary-ttf5z -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan  5 10:13:50.952: INFO: stderr: ""
    Jan  5 10:13:50.952: INFO: stdout: "pod/agnhost-primary-ttf5z patched\n"
    STEP: checking annotations 01/05/23 10:13:50.952
    Jan  5 10:13:50.958: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 10:13:50.958: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 10:13:50.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9821" for this suite. 01/05/23 10:13:50.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:13:50.976
Jan  5 10:13:50.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename security-context 01/05/23 10:13:50.977
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:50.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:50.997
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/05/23 10:13:51.004
Jan  5 10:13:51.016: INFO: Waiting up to 5m0s for pod "security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8" in namespace "security-context-5312" to be "Succeeded or Failed"
Jan  5 10:13:51.023: INFO: Pod "security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.954889ms
Jan  5 10:13:53.029: INFO: Pod "security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013012376s
Jan  5 10:13:55.028: INFO: Pod "security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012436002s
STEP: Saw pod success 01/05/23 10:13:55.028
Jan  5 10:13:55.029: INFO: Pod "security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8" satisfied condition "Succeeded or Failed"
Jan  5 10:13:55.032: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8 container test-container: <nil>
STEP: delete the pod 01/05/23 10:13:55.044
Jan  5 10:13:55.055: INFO: Waiting for pod security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8 to disappear
Jan  5 10:13:55.059: INFO: Pod security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  5 10:13:55.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5312" for this suite. 01/05/23 10:13:55.068
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":276,"skipped":5232,"failed":0}
------------------------------
â€¢ [4.097 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:13:50.976
    Jan  5 10:13:50.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename security-context 01/05/23 10:13:50.977
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:50.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:50.997
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/05/23 10:13:51.004
    Jan  5 10:13:51.016: INFO: Waiting up to 5m0s for pod "security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8" in namespace "security-context-5312" to be "Succeeded or Failed"
    Jan  5 10:13:51.023: INFO: Pod "security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.954889ms
    Jan  5 10:13:53.029: INFO: Pod "security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013012376s
    Jan  5 10:13:55.028: INFO: Pod "security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012436002s
    STEP: Saw pod success 01/05/23 10:13:55.028
    Jan  5 10:13:55.029: INFO: Pod "security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8" satisfied condition "Succeeded or Failed"
    Jan  5 10:13:55.032: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8 container test-container: <nil>
    STEP: delete the pod 01/05/23 10:13:55.044
    Jan  5 10:13:55.055: INFO: Waiting for pod security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8 to disappear
    Jan  5 10:13:55.059: INFO: Pod security-context-72ac91af-4af6-4c70-9a5e-9dd7e398a5b8 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  5 10:13:55.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-5312" for this suite. 01/05/23 10:13:55.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:13:55.075
Jan  5 10:13:55.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:13:55.076
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:55.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:55.098
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 01/05/23 10:13:55.103
Jan  5 10:13:55.113: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb" in namespace "projected-6154" to be "Succeeded or Failed"
Jan  5 10:13:55.123: INFO: Pod "downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.18216ms
Jan  5 10:13:57.128: INFO: Pod "downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014510742s
Jan  5 10:13:59.130: INFO: Pod "downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016379177s
STEP: Saw pod success 01/05/23 10:13:59.13
Jan  5 10:13:59.130: INFO: Pod "downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb" satisfied condition "Succeeded or Failed"
Jan  5 10:13:59.139: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb container client-container: <nil>
STEP: delete the pod 01/05/23 10:13:59.152
Jan  5 10:13:59.233: INFO: Waiting for pod downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb to disappear
Jan  5 10:13:59.238: INFO: Pod downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 10:13:59.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6154" for this suite. 01/05/23 10:13:59.249
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":277,"skipped":5264,"failed":0}
------------------------------
â€¢ [4.180 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:13:55.075
    Jan  5 10:13:55.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:13:55.076
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:55.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:55.098
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 01/05/23 10:13:55.103
    Jan  5 10:13:55.113: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb" in namespace "projected-6154" to be "Succeeded or Failed"
    Jan  5 10:13:55.123: INFO: Pod "downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.18216ms
    Jan  5 10:13:57.128: INFO: Pod "downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014510742s
    Jan  5 10:13:59.130: INFO: Pod "downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016379177s
    STEP: Saw pod success 01/05/23 10:13:59.13
    Jan  5 10:13:59.130: INFO: Pod "downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb" satisfied condition "Succeeded or Failed"
    Jan  5 10:13:59.139: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb container client-container: <nil>
    STEP: delete the pod 01/05/23 10:13:59.152
    Jan  5 10:13:59.233: INFO: Waiting for pod downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb to disappear
    Jan  5 10:13:59.238: INFO: Pod downwardapi-volume-ca9ab0b6-7c9e-42a5-83d8-b909b9da30eb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 10:13:59.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6154" for this suite. 01/05/23 10:13:59.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:13:59.255
Jan  5 10:13:59.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:13:59.256
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:59.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:59.279
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 01/05/23 10:13:59.286
Jan  5 10:13:59.296: INFO: Waiting up to 5m0s for pod "labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be" in namespace "projected-8737" to be "running and ready"
Jan  5 10:13:59.319: INFO: Pod "labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be": Phase="Pending", Reason="", readiness=false. Elapsed: 22.793392ms
Jan  5 10:13:59.319: INFO: The phase of Pod labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:14:01.325: INFO: Pod "labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be": Phase="Running", Reason="", readiness=true. Elapsed: 2.029298653s
Jan  5 10:14:01.326: INFO: The phase of Pod labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be is Running (Ready = true)
Jan  5 10:14:01.326: INFO: Pod "labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be" satisfied condition "running and ready"
Jan  5 10:14:01.858: INFO: Successfully updated pod "labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 10:14:05.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8737" for this suite. 01/05/23 10:14:05.956
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":278,"skipped":5270,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.708 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:13:59.255
    Jan  5 10:13:59.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:13:59.256
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:13:59.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:13:59.279
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 01/05/23 10:13:59.286
    Jan  5 10:13:59.296: INFO: Waiting up to 5m0s for pod "labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be" in namespace "projected-8737" to be "running and ready"
    Jan  5 10:13:59.319: INFO: Pod "labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be": Phase="Pending", Reason="", readiness=false. Elapsed: 22.793392ms
    Jan  5 10:13:59.319: INFO: The phase of Pod labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:14:01.325: INFO: Pod "labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be": Phase="Running", Reason="", readiness=true. Elapsed: 2.029298653s
    Jan  5 10:14:01.326: INFO: The phase of Pod labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be is Running (Ready = true)
    Jan  5 10:14:01.326: INFO: Pod "labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be" satisfied condition "running and ready"
    Jan  5 10:14:01.858: INFO: Successfully updated pod "labelsupdate9b39058e-e87e-4d49-b740-83a5f13c51be"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 10:14:05.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8737" for this suite. 01/05/23 10:14:05.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:14:05.966
Jan  5 10:14:05.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename hostport 01/05/23 10:14:05.967
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:14:05.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:14:05.985
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/05/23 10:14:05.997
Jan  5 10:14:06.005: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8958" to be "running and ready"
Jan  5 10:14:06.009: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063202ms
Jan  5 10:14:06.009: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:14:08.016: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010977564s
Jan  5 10:14:08.016: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  5 10:14:08.016: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.0.128 on the node which pod1 resides and expect scheduled 01/05/23 10:14:08.017
Jan  5 10:14:08.025: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8958" to be "running and ready"
Jan  5 10:14:08.029: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.708751ms
Jan  5 10:14:08.029: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:14:10.034: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009133889s
Jan  5 10:14:10.034: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  5 10:14:10.034: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.0.128 but use UDP protocol on the node which pod2 resides 01/05/23 10:14:10.034
Jan  5 10:14:10.042: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8958" to be "running and ready"
Jan  5 10:14:10.046: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829967ms
Jan  5 10:14:10.046: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:14:12.052: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010224404s
Jan  5 10:14:12.052: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan  5 10:14:12.052: INFO: Pod "pod3" satisfied condition "running and ready"
Jan  5 10:14:12.059: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8958" to be "running and ready"
Jan  5 10:14:12.064: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.38959ms
Jan  5 10:14:12.064: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:14:14.071: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.011228555s
Jan  5 10:14:14.071: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan  5 10:14:14.071: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/05/23 10:14:14.075
Jan  5 10:14:14.075: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.0.128 http://127.0.0.1:54323/hostname] Namespace:hostport-8958 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:14:14.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:14:14.076: INFO: ExecWithOptions: Clientset creation
Jan  5 10:14:14.076: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/hostport-8958/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.250.0.128+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.128, port: 54323 01/05/23 10:14:14.506
Jan  5 10:14:14.506: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.0.128:54323/hostname] Namespace:hostport-8958 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:14:14.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:14:14.507: INFO: ExecWithOptions: Clientset creation
Jan  5 10:14:14.507: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/hostport-8958/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.250.0.128%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.128, port: 54323 UDP 01/05/23 10:14:15.019
Jan  5 10:14:15.019: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.250.0.128 54323] Namespace:hostport-8958 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:14:15.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:14:15.020: INFO: ExecWithOptions: Clientset creation
Jan  5 10:14:15.020: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/hostport-8958/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.250.0.128+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jan  5 10:14:20.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-8958" for this suite. 01/05/23 10:14:20.346
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":279,"skipped":5327,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.389 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:14:05.966
    Jan  5 10:14:05.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename hostport 01/05/23 10:14:05.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:14:05.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:14:05.985
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/05/23 10:14:05.997
    Jan  5 10:14:06.005: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8958" to be "running and ready"
    Jan  5 10:14:06.009: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063202ms
    Jan  5 10:14:06.009: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:14:08.016: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010977564s
    Jan  5 10:14:08.016: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  5 10:14:08.016: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.0.128 on the node which pod1 resides and expect scheduled 01/05/23 10:14:08.017
    Jan  5 10:14:08.025: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8958" to be "running and ready"
    Jan  5 10:14:08.029: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.708751ms
    Jan  5 10:14:08.029: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:14:10.034: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009133889s
    Jan  5 10:14:10.034: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  5 10:14:10.034: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.0.128 but use UDP protocol on the node which pod2 resides 01/05/23 10:14:10.034
    Jan  5 10:14:10.042: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8958" to be "running and ready"
    Jan  5 10:14:10.046: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829967ms
    Jan  5 10:14:10.046: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:14:12.052: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010224404s
    Jan  5 10:14:12.052: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan  5 10:14:12.052: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan  5 10:14:12.059: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8958" to be "running and ready"
    Jan  5 10:14:12.064: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.38959ms
    Jan  5 10:14:12.064: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:14:14.071: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.011228555s
    Jan  5 10:14:14.071: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan  5 10:14:14.071: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/05/23 10:14:14.075
    Jan  5 10:14:14.075: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.0.128 http://127.0.0.1:54323/hostname] Namespace:hostport-8958 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:14:14.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:14:14.076: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:14:14.076: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/hostport-8958/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.250.0.128+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.128, port: 54323 01/05/23 10:14:14.506
    Jan  5 10:14:14.506: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.0.128:54323/hostname] Namespace:hostport-8958 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:14:14.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:14:14.507: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:14:14.507: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/hostport-8958/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.250.0.128%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.128, port: 54323 UDP 01/05/23 10:14:15.019
    Jan  5 10:14:15.019: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.250.0.128 54323] Namespace:hostport-8958 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:14:15.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:14:15.020: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:14:15.020: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/hostport-8958/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.250.0.128+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jan  5 10:14:20.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-8958" for this suite. 01/05/23 10:14:20.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:14:20.356
Jan  5 10:14:20.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename taint-multiple-pods 01/05/23 10:14:20.357
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:14:20.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:14:20.387
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan  5 10:14:20.402: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 10:15:02.444: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false instead of true. Reason: KubeletNotReady, message: [container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes "shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c" not found]
Jan  5 10:15:02.456: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false instead of true. Reason: KubeletNotReady, message: [container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes "shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c" not found]
Jan  5 10:15:04.435: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:04.448: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:06.435: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:06.448: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:08.436: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:08.451: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:10.434: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:10.449: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:12.441: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:12.454: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:14.435: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:14.448: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:16.436: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:16.450: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:18.439: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:18.460: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:20.437: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:20.448: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:20.454: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:20.465: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:20.465: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jan  5 10:15:20.469: INFO: Starting informer...
STEP: Starting pods... 01/05/23 10:15:20.469
Jan  5 10:15:20.692: INFO: Pod1 is running on shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv. Tainting Node
Jan  5 10:15:20.908: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7614" to be "running"
Jan  5 10:15:20.913: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.811859ms
Jan  5 10:15:22.922: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013791745s
Jan  5 10:15:22.922: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan  5 10:15:22.922: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7614" to be "running"
Jan  5 10:15:22.932: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 9.301147ms
Jan  5 10:15:22.932: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan  5 10:15:22.932: INFO: Pod2 is running on shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv. Tainting Node
STEP: Trying to apply a taint on the Node 01/05/23 10:15:22.932
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 10:15:22.952
STEP: Waiting for Pod1 and Pod2 to be deleted 01/05/23 10:15:22.96
Jan  5 10:15:29.085: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan  5 10:15:49.130: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 10:15:49.149
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jan  5 10:15:49.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
Jan  5 10:15:49.163: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:51.174: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:53.172: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:55.173: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:57.173: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:15:59.173: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:16:01.175: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:16:03.174: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:16:05.172: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:16:07.174: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:16:09.173: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:16:11.174: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:16:13.175: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:16:15.177: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:16:17.173: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:16:19.176: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:16:21.173: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
Jan  5 10:16:23.172: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
STEP: Destroying namespace "taint-multiple-pods-7614" for this suite. 01/05/23 10:16:25.18
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":280,"skipped":5349,"failed":0}
------------------------------
â€¢ [SLOW TEST] [124.830 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:14:20.356
    Jan  5 10:14:20.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename taint-multiple-pods 01/05/23 10:14:20.357
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:14:20.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:14:20.387
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jan  5 10:14:20.402: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 10:15:02.444: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false instead of true. Reason: KubeletNotReady, message: [container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes "shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c" not found]
    Jan  5 10:15:02.456: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false instead of true. Reason: KubeletNotReady, message: [container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes "shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c" not found]
    Jan  5 10:15:04.435: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:04.448: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:06.435: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:06.448: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:08.436: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:08.451: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:10.434: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:10.449: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:12.441: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:12.454: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:14.435: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:14.448: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:16.436: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:16.450: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:18.439: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:18.460: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:20.437: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:20.448: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:20.454: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:20.465: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:20.465: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jan  5 10:15:20.469: INFO: Starting informer...
    STEP: Starting pods... 01/05/23 10:15:20.469
    Jan  5 10:15:20.692: INFO: Pod1 is running on shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv. Tainting Node
    Jan  5 10:15:20.908: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7614" to be "running"
    Jan  5 10:15:20.913: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.811859ms
    Jan  5 10:15:22.922: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013791745s
    Jan  5 10:15:22.922: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan  5 10:15:22.922: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7614" to be "running"
    Jan  5 10:15:22.932: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 9.301147ms
    Jan  5 10:15:22.932: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan  5 10:15:22.932: INFO: Pod2 is running on shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv. Tainting Node
    STEP: Trying to apply a taint on the Node 01/05/23 10:15:22.932
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 10:15:22.952
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/05/23 10:15:22.96
    Jan  5 10:15:29.085: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan  5 10:15:49.130: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 10:15:49.149
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 10:15:49.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    Jan  5 10:15:49.163: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:51.174: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:53.172: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:55.173: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:57.173: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:15:59.173: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:16:01.175: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:16:03.174: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:16:05.172: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:16:07.174: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:16:09.173: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:16:11.174: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:16:13.175: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:16:15.177: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:16:17.173: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:16:19.176: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:16:21.173: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    Jan  5 10:16:23.172: INFO: Condition Ready of node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2023-01-05 10:15:04 +0000 UTC}]. Failure
    STEP: Destroying namespace "taint-multiple-pods-7614" for this suite. 01/05/23 10:16:25.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:16:25.188
Jan  5 10:16:25.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename dns 01/05/23 10:16:25.189
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:16:25.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:16:25.21
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/05/23 10:16:25.217
Jan  5 10:16:25.225: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1995  2e95fbe8-4f5b-48f4-83e8-8db7e0d8b5e6 38546 0 2023-01-05 10:16:25 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-05 10:16:25 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7j9g7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7j9g7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:16:25.226: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1995" to be "running and ready"
Jan  5 10:16:25.232: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 6.455483ms
Jan  5 10:16:25.232: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:16:27.239: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.013706788s
Jan  5 10:16:27.239: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan  5 10:16:27.239: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/05/23 10:16:27.239
Jan  5 10:16:27.239: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1995 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:16:27.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:16:27.240: INFO: ExecWithOptions: Clientset creation
Jan  5 10:16:27.240: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/dns-1995/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/05/23 10:16:27.559
Jan  5 10:16:27.559: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1995 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:16:27.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:16:27.560: INFO: ExecWithOptions: Clientset creation
Jan  5 10:16:27.560: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/dns-1995/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 10:16:28.054: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 10:16:28.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1995" for this suite. 01/05/23 10:16:28.074
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":281,"skipped":5377,"failed":0}
------------------------------
â€¢ [2.892 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:16:25.188
    Jan  5 10:16:25.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename dns 01/05/23 10:16:25.189
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:16:25.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:16:25.21
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/05/23 10:16:25.217
    Jan  5 10:16:25.225: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1995  2e95fbe8-4f5b-48f4-83e8-8db7e0d8b5e6 38546 0 2023-01-05 10:16:25 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-05 10:16:25 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7j9g7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7j9g7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:16:25.226: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1995" to be "running and ready"
    Jan  5 10:16:25.232: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 6.455483ms
    Jan  5 10:16:25.232: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:16:27.239: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.013706788s
    Jan  5 10:16:27.239: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan  5 10:16:27.239: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/05/23 10:16:27.239
    Jan  5 10:16:27.239: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1995 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:16:27.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:16:27.240: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:16:27.240: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/dns-1995/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/05/23 10:16:27.559
    Jan  5 10:16:27.559: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1995 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:16:27.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:16:27.560: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:16:27.560: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/dns-1995/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 10:16:28.054: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 10:16:28.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1995" for this suite. 01/05/23 10:16:28.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:16:28.083
Jan  5 10:16:28.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename events 01/05/23 10:16:28.084
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:16:28.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:16:28.104
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/05/23 10:16:28.11
STEP: get a list of Events with a label in the current namespace 01/05/23 10:16:28.122
STEP: delete a list of events 01/05/23 10:16:28.125
Jan  5 10:16:28.125: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/05/23 10:16:28.137
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan  5 10:16:28.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1747" for this suite. 01/05/23 10:16:28.15
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":282,"skipped":5404,"failed":0}
------------------------------
â€¢ [0.071 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:16:28.083
    Jan  5 10:16:28.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename events 01/05/23 10:16:28.084
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:16:28.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:16:28.104
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/05/23 10:16:28.11
    STEP: get a list of Events with a label in the current namespace 01/05/23 10:16:28.122
    STEP: delete a list of events 01/05/23 10:16:28.125
    Jan  5 10:16:28.125: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/05/23 10:16:28.137
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan  5 10:16:28.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1747" for this suite. 01/05/23 10:16:28.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:16:28.158
Jan  5 10:16:28.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename deployment 01/05/23 10:16:28.159
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:16:28.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:16:28.182
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan  5 10:16:28.197: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan  5 10:16:33.204: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 10:16:33.204
Jan  5 10:16:33.204: INFO: Waiting up to 5m0s for pod "test-cleanup-controller-tsw9m" in namespace "deployment-9527" to be "running"
Jan  5 10:16:33.208: INFO: Pod "test-cleanup-controller-tsw9m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.250715ms
Jan  5 10:16:35.213: INFO: Pod "test-cleanup-controller-tsw9m": Phase="Running", Reason="", readiness=true. Elapsed: 2.009224436s
Jan  5 10:16:35.213: INFO: Pod "test-cleanup-controller-tsw9m" satisfied condition "running"
Jan  5 10:16:35.213: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/05/23 10:16:35.226
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 10:16:35.240: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9527  cdbbeec9-140d-48e0-85a3-c63aaa189b92 38637 1 2023-01-05 10:16:35 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-05 10:16:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eae1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan  5 10:16:35.247: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-9527  c83431f0-5912-49c0-a8d1-9a83c19b9b56 38639 1 2023-01-05 10:16:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment cdbbeec9-140d-48e0-85a3-c63aaa189b92 0xc004eae647 0xc004eae648}] [] [{kube-controller-manager Update apps/v1 2023-01-05 10:16:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cdbbeec9-140d-48e0-85a3-c63aaa189b92\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eae6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 10:16:35.247: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan  5 10:16:35.247: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9527  27a86131-2339-44df-a16f-9ff1bd624ecd 38638 1 2023-01-05 10:16:28 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment cdbbeec9-140d-48e0-85a3-c63aaa189b92 0xc004eae517 0xc004eae518}] [] [{e2e.test Update apps/v1 2023-01-05 10:16:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:16:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-05 10:16:35 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"cdbbeec9-140d-48e0-85a3-c63aaa189b92\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004eae5d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 10:16:35.253: INFO: Pod "test-cleanup-controller-tsw9m" is available:
&Pod{ObjectMeta:{test-cleanup-controller-tsw9m test-cleanup-controller- deployment-9527  6a3fa4e3-a190-4021-92eb-e11e285044b4 38624 0 2023-01-05 10:16:28 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 27a86131-2339-44df-a16f-9ff1bd624ecd 0xc003edefd7 0xc003edefd8}] [] [{kube-controller-manager Update v1 2023-01-05 10:16:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a86131-2339-44df-a16f-9ff1bd624ecd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:16:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.4.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rz22w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rz22w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:16:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:16:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:16:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:16:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.234,PodIP:10.96.4.175,StartTime:2023-01-05 10:16:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:16:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3eabd614b1ff2f28af958ed389f529ec83f6da74d1d9e8696ee605ca2fcf75e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.4.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 10:16:35.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9527" for this suite. 01/05/23 10:16:35.271
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":283,"skipped":5432,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.123 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:16:28.158
    Jan  5 10:16:28.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename deployment 01/05/23 10:16:28.159
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:16:28.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:16:28.182
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan  5 10:16:28.197: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan  5 10:16:33.204: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 10:16:33.204
    Jan  5 10:16:33.204: INFO: Waiting up to 5m0s for pod "test-cleanup-controller-tsw9m" in namespace "deployment-9527" to be "running"
    Jan  5 10:16:33.208: INFO: Pod "test-cleanup-controller-tsw9m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.250715ms
    Jan  5 10:16:35.213: INFO: Pod "test-cleanup-controller-tsw9m": Phase="Running", Reason="", readiness=true. Elapsed: 2.009224436s
    Jan  5 10:16:35.213: INFO: Pod "test-cleanup-controller-tsw9m" satisfied condition "running"
    Jan  5 10:16:35.213: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/05/23 10:16:35.226
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 10:16:35.240: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9527  cdbbeec9-140d-48e0-85a3-c63aaa189b92 38637 1 2023-01-05 10:16:35 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-05 10:16:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eae1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan  5 10:16:35.247: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-9527  c83431f0-5912-49c0-a8d1-9a83c19b9b56 38639 1 2023-01-05 10:16:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment cdbbeec9-140d-48e0-85a3-c63aaa189b92 0xc004eae647 0xc004eae648}] [] [{kube-controller-manager Update apps/v1 2023-01-05 10:16:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cdbbeec9-140d-48e0-85a3-c63aaa189b92\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eae6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 10:16:35.247: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan  5 10:16:35.247: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9527  27a86131-2339-44df-a16f-9ff1bd624ecd 38638 1 2023-01-05 10:16:28 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment cdbbeec9-140d-48e0-85a3-c63aaa189b92 0xc004eae517 0xc004eae518}] [] [{e2e.test Update apps/v1 2023-01-05 10:16:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:16:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-05 10:16:35 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"cdbbeec9-140d-48e0-85a3-c63aaa189b92\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004eae5d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 10:16:35.253: INFO: Pod "test-cleanup-controller-tsw9m" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-tsw9m test-cleanup-controller- deployment-9527  6a3fa4e3-a190-4021-92eb-e11e285044b4 38624 0 2023-01-05 10:16:28 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 27a86131-2339-44df-a16f-9ff1bd624ecd 0xc003edefd7 0xc003edefd8}] [] [{kube-controller-manager Update v1 2023-01-05 10:16:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a86131-2339-44df-a16f-9ff1bd624ecd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:16:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.4.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rz22w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rz22w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:16:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:16:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:16:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:16:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.234,PodIP:10.96.4.175,StartTime:2023-01-05 10:16:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:16:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3eabd614b1ff2f28af958ed389f529ec83f6da74d1d9e8696ee605ca2fcf75e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.4.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 10:16:35.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9527" for this suite. 01/05/23 10:16:35.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:16:35.282
Jan  5 10:16:35.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename ingress 01/05/23 10:16:35.283
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:16:35.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:16:35.31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/05/23 10:16:35.316
STEP: getting /apis/networking.k8s.io 01/05/23 10:16:35.32
STEP: getting /apis/networking.k8s.iov1 01/05/23 10:16:35.322
STEP: creating 01/05/23 10:16:35.324
STEP: getting 01/05/23 10:16:35.338
STEP: listing 01/05/23 10:16:35.341
STEP: watching 01/05/23 10:16:35.345
Jan  5 10:16:35.345: INFO: starting watch
STEP: cluster-wide listing 01/05/23 10:16:35.347
STEP: cluster-wide watching 01/05/23 10:16:35.356
Jan  5 10:16:35.356: INFO: starting watch
STEP: patching 01/05/23 10:16:35.358
STEP: updating 01/05/23 10:16:35.364
Jan  5 10:16:35.372: INFO: waiting for watch events with expected annotations
Jan  5 10:16:35.372: INFO: saw patched and updated annotations
STEP: patching /status 01/05/23 10:16:35.372
STEP: updating /status 01/05/23 10:16:35.377
STEP: get /status 01/05/23 10:16:35.393
STEP: deleting 01/05/23 10:16:35.398
STEP: deleting a collection 01/05/23 10:16:35.423
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jan  5 10:16:35.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2943" for this suite. 01/05/23 10:16:35.445
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":284,"skipped":5469,"failed":0}
------------------------------
â€¢ [0.168 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:16:35.282
    Jan  5 10:16:35.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename ingress 01/05/23 10:16:35.283
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:16:35.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:16:35.31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/05/23 10:16:35.316
    STEP: getting /apis/networking.k8s.io 01/05/23 10:16:35.32
    STEP: getting /apis/networking.k8s.iov1 01/05/23 10:16:35.322
    STEP: creating 01/05/23 10:16:35.324
    STEP: getting 01/05/23 10:16:35.338
    STEP: listing 01/05/23 10:16:35.341
    STEP: watching 01/05/23 10:16:35.345
    Jan  5 10:16:35.345: INFO: starting watch
    STEP: cluster-wide listing 01/05/23 10:16:35.347
    STEP: cluster-wide watching 01/05/23 10:16:35.356
    Jan  5 10:16:35.356: INFO: starting watch
    STEP: patching 01/05/23 10:16:35.358
    STEP: updating 01/05/23 10:16:35.364
    Jan  5 10:16:35.372: INFO: waiting for watch events with expected annotations
    Jan  5 10:16:35.372: INFO: saw patched and updated annotations
    STEP: patching /status 01/05/23 10:16:35.372
    STEP: updating /status 01/05/23 10:16:35.377
    STEP: get /status 01/05/23 10:16:35.393
    STEP: deleting 01/05/23 10:16:35.398
    STEP: deleting a collection 01/05/23 10:16:35.423
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jan  5 10:16:35.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-2943" for this suite. 01/05/23 10:16:35.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:16:35.452
Jan  5 10:16:35.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename init-container 01/05/23 10:16:35.453
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:16:35.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:16:35.476
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 01/05/23 10:16:35.483
Jan  5 10:16:35.483: INFO: PodSpec: initContainers in spec.initContainers
Jan  5 10:17:20.583: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-77c8eccb-9ec1-43f8-9944-bdd35bd917d0", GenerateName:"", Namespace:"init-container-5936", SelfLink:"", UID:"e3949268-9b7b-46c4-bbd6-082c8c0c74e9", ResourceVersion:"38913", Generation:0, CreationTimestamp:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"483528637"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e96060), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 10, 17, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e96090), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-5l9st", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc002860020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5l9st", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5l9st", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5l9st", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004f34108), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000870000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f34180)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f341a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004f341a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004f341ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00100e040), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.1.19", PodIP:"10.96.1.107", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.96.1.107"}}, StartTime:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008700e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000870150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://76b454e7679caabfaf9b4340ee0ce76e1e8037762856b78799efc6789b88336c", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002860120), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0028600e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc004f3423f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 10:17:20.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5936" for this suite. 01/05/23 10:17:20.593
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":285,"skipped":5483,"failed":0}
------------------------------
â€¢ [SLOW TEST] [45.147 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:16:35.452
    Jan  5 10:16:35.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename init-container 01/05/23 10:16:35.453
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:16:35.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:16:35.476
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 01/05/23 10:16:35.483
    Jan  5 10:16:35.483: INFO: PodSpec: initContainers in spec.initContainers
    Jan  5 10:17:20.583: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-77c8eccb-9ec1-43f8-9944-bdd35bd917d0", GenerateName:"", Namespace:"init-container-5936", SelfLink:"", UID:"e3949268-9b7b-46c4-bbd6-082c8c0c74e9", ResourceVersion:"38913", Generation:0, CreationTimestamp:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"483528637"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e96060), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 10, 17, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e96090), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-5l9st", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc002860020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5l9st", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5l9st", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5l9st", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004f34108), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000870000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f34180)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f341a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004f341a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004f341ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00100e040), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.1.19", PodIP:"10.96.1.107", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.96.1.107"}}, StartTime:time.Date(2023, time.January, 5, 10, 16, 35, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008700e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000870150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://76b454e7679caabfaf9b4340ee0ce76e1e8037762856b78799efc6789b88336c", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002860120), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0028600e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc004f3423f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 10:17:20.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5936" for this suite. 01/05/23 10:17:20.593
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:17:20.601
Jan  5 10:17:20.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 10:17:20.602
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:17:20.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:17:20.623
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan  5 10:17:20.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:17:21.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2759" for this suite. 01/05/23 10:17:21.188
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":286,"skipped":5484,"failed":0}
------------------------------
â€¢ [0.592 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:17:20.601
    Jan  5 10:17:20.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 10:17:20.602
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:17:20.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:17:20.623
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan  5 10:17:20.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:17:21.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2759" for this suite. 01/05/23 10:17:21.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:17:21.196
Jan  5 10:17:21.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:17:21.196
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:17:21.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:17:21.228
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-3537e88c-9b15-4736-9342-5631322ef809 01/05/23 10:17:21.233
STEP: Creating a pod to test consume configMaps 01/05/23 10:17:21.237
Jan  5 10:17:21.251: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09" in namespace "projected-71" to be "Succeeded or Failed"
Jan  5 10:17:21.256: INFO: Pod "pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.74681ms
Jan  5 10:17:23.261: INFO: Pod "pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010096112s
Jan  5 10:17:25.265: INFO: Pod "pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013684168s
STEP: Saw pod success 01/05/23 10:17:25.265
Jan  5 10:17:25.265: INFO: Pod "pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09" satisfied condition "Succeeded or Failed"
Jan  5 10:17:25.270: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 10:17:25.29
Jan  5 10:17:25.302: INFO: Waiting for pod pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09 to disappear
Jan  5 10:17:25.305: INFO: Pod pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 10:17:25.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-71" for this suite. 01/05/23 10:17:25.316
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":287,"skipped":5505,"failed":0}
------------------------------
â€¢ [4.125 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:17:21.196
    Jan  5 10:17:21.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:17:21.196
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:17:21.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:17:21.228
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-3537e88c-9b15-4736-9342-5631322ef809 01/05/23 10:17:21.233
    STEP: Creating a pod to test consume configMaps 01/05/23 10:17:21.237
    Jan  5 10:17:21.251: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09" in namespace "projected-71" to be "Succeeded or Failed"
    Jan  5 10:17:21.256: INFO: Pod "pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.74681ms
    Jan  5 10:17:23.261: INFO: Pod "pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010096112s
    Jan  5 10:17:25.265: INFO: Pod "pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013684168s
    STEP: Saw pod success 01/05/23 10:17:25.265
    Jan  5 10:17:25.265: INFO: Pod "pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09" satisfied condition "Succeeded or Failed"
    Jan  5 10:17:25.270: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 10:17:25.29
    Jan  5 10:17:25.302: INFO: Waiting for pod pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09 to disappear
    Jan  5 10:17:25.305: INFO: Pod pod-projected-configmaps-2676411d-5b2d-4d48-9b76-00430418fd09 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 10:17:25.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-71" for this suite. 01/05/23 10:17:25.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:17:25.322
Jan  5 10:17:25.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 10:17:25.323
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:17:25.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:17:25.341
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-6129c232-edfa-4d2b-8bac-3240a6f38f55 01/05/23 10:17:25.362
STEP: Creating a pod to test consume secrets 01/05/23 10:17:25.367
Jan  5 10:17:25.377: INFO: Waiting up to 5m0s for pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83" in namespace "secrets-5224" to be "Succeeded or Failed"
Jan  5 10:17:25.408: INFO: Pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83": Phase="Pending", Reason="", readiness=false. Elapsed: 30.923209ms
Jan  5 10:17:27.414: INFO: Pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036544699s
Jan  5 10:17:29.413: INFO: Pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035874472s
Jan  5 10:17:31.414: INFO: Pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037109998s
Jan  5 10:17:33.416: INFO: Pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.038839464s
STEP: Saw pod success 01/05/23 10:17:33.416
Jan  5 10:17:33.416: INFO: Pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83" satisfied condition "Succeeded or Failed"
Jan  5 10:17:33.427: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 10:17:33.445
Jan  5 10:17:33.454: INFO: Waiting for pod pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83 to disappear
Jan  5 10:17:33.458: INFO: Pod pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 10:17:33.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5224" for this suite. 01/05/23 10:17:33.466
STEP: Destroying namespace "secret-namespace-8656" for this suite. 01/05/23 10:17:33.47
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":288,"skipped":5515,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.154 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:17:25.322
    Jan  5 10:17:25.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 10:17:25.323
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:17:25.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:17:25.341
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-6129c232-edfa-4d2b-8bac-3240a6f38f55 01/05/23 10:17:25.362
    STEP: Creating a pod to test consume secrets 01/05/23 10:17:25.367
    Jan  5 10:17:25.377: INFO: Waiting up to 5m0s for pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83" in namespace "secrets-5224" to be "Succeeded or Failed"
    Jan  5 10:17:25.408: INFO: Pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83": Phase="Pending", Reason="", readiness=false. Elapsed: 30.923209ms
    Jan  5 10:17:27.414: INFO: Pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036544699s
    Jan  5 10:17:29.413: INFO: Pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035874472s
    Jan  5 10:17:31.414: INFO: Pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037109998s
    Jan  5 10:17:33.416: INFO: Pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.038839464s
    STEP: Saw pod success 01/05/23 10:17:33.416
    Jan  5 10:17:33.416: INFO: Pod "pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83" satisfied condition "Succeeded or Failed"
    Jan  5 10:17:33.427: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 10:17:33.445
    Jan  5 10:17:33.454: INFO: Waiting for pod pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83 to disappear
    Jan  5 10:17:33.458: INFO: Pod pod-secrets-69a428ba-a67b-4925-a236-4c62149d5e83 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 10:17:33.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5224" for this suite. 01/05/23 10:17:33.466
    STEP: Destroying namespace "secret-namespace-8656" for this suite. 01/05/23 10:17:33.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:17:33.476
Jan  5 10:17:33.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:17:33.477
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:17:33.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:17:33.496
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-29121ac2-04cd-4530-96aa-5b2779ed84e2 01/05/23 10:17:33.51
STEP: Creating secret with name s-test-opt-upd-dcfa7e69-be5b-4db2-b82e-e760a1e48945 01/05/23 10:17:33.517
STEP: Creating the pod 01/05/23 10:17:33.522
Jan  5 10:17:33.568: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c" in namespace "projected-9614" to be "running and ready"
Jan  5 10:17:33.575: INFO: Pod "pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.873885ms
Jan  5 10:17:33.575: INFO: The phase of Pod pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:17:35.581: INFO: Pod "pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013487914s
Jan  5 10:17:35.581: INFO: The phase of Pod pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:17:37.581: INFO: Pod "pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c": Phase="Running", Reason="", readiness=true. Elapsed: 4.013611127s
Jan  5 10:17:37.581: INFO: The phase of Pod pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c is Running (Ready = true)
Jan  5 10:17:37.581: INFO: Pod "pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-29121ac2-04cd-4530-96aa-5b2779ed84e2 01/05/23 10:17:37.797
STEP: Updating secret s-test-opt-upd-dcfa7e69-be5b-4db2-b82e-e760a1e48945 01/05/23 10:17:37.802
STEP: Creating secret with name s-test-opt-create-9eeb462e-e970-4425-8973-69cdbc9de9a2 01/05/23 10:17:37.808
STEP: waiting to observe update in volume 01/05/23 10:17:37.813
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 10:19:06.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9614" for this suite. 01/05/23 10:19:06.758
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":289,"skipped":5528,"failed":0}
------------------------------
â€¢ [SLOW TEST] [93.288 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:17:33.476
    Jan  5 10:17:33.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:17:33.477
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:17:33.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:17:33.496
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-29121ac2-04cd-4530-96aa-5b2779ed84e2 01/05/23 10:17:33.51
    STEP: Creating secret with name s-test-opt-upd-dcfa7e69-be5b-4db2-b82e-e760a1e48945 01/05/23 10:17:33.517
    STEP: Creating the pod 01/05/23 10:17:33.522
    Jan  5 10:17:33.568: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c" in namespace "projected-9614" to be "running and ready"
    Jan  5 10:17:33.575: INFO: Pod "pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.873885ms
    Jan  5 10:17:33.575: INFO: The phase of Pod pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:17:35.581: INFO: Pod "pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013487914s
    Jan  5 10:17:35.581: INFO: The phase of Pod pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:17:37.581: INFO: Pod "pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c": Phase="Running", Reason="", readiness=true. Elapsed: 4.013611127s
    Jan  5 10:17:37.581: INFO: The phase of Pod pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c is Running (Ready = true)
    Jan  5 10:17:37.581: INFO: Pod "pod-projected-secrets-8975782f-8344-458f-b573-6f61599a785c" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-29121ac2-04cd-4530-96aa-5b2779ed84e2 01/05/23 10:17:37.797
    STEP: Updating secret s-test-opt-upd-dcfa7e69-be5b-4db2-b82e-e760a1e48945 01/05/23 10:17:37.802
    STEP: Creating secret with name s-test-opt-create-9eeb462e-e970-4425-8973-69cdbc9de9a2 01/05/23 10:17:37.808
    STEP: waiting to observe update in volume 01/05/23 10:17:37.813
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 10:19:06.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9614" for this suite. 01/05/23 10:19:06.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:19:06.767
Jan  5 10:19:06.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubelet-test 01/05/23 10:19:06.768
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:06.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:06.791
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  5 10:19:06.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9384" for this suite. 01/05/23 10:19:06.83
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":290,"skipped":5570,"failed":0}
------------------------------
â€¢ [0.067 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:19:06.767
    Jan  5 10:19:06.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 10:19:06.768
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:06.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:06.791
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  5 10:19:06.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9384" for this suite. 01/05/23 10:19:06.83
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:19:06.835
Jan  5 10:19:06.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename init-container 01/05/23 10:19:06.836
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:06.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:06.855
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 01/05/23 10:19:06.862
Jan  5 10:19:06.862: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 10:19:12.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2268" for this suite. 01/05/23 10:19:12.819
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":291,"skipped":5570,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.992 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:19:06.835
    Jan  5 10:19:06.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename init-container 01/05/23 10:19:06.836
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:06.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:06.855
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 01/05/23 10:19:06.862
    Jan  5 10:19:06.862: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 10:19:12.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-2268" for this suite. 01/05/23 10:19:12.819
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:19:12.827
Jan  5 10:19:12.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 10:19:12.828
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:12.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:12.856
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 10:19:12.872
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:19:13.613
STEP: Deploying the webhook pod 01/05/23 10:19:13.621
STEP: Wait for the deployment to be ready 01/05/23 10:19:13.632
Jan  5 10:19:13.641: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 10:19:15.655
STEP: Verifying the service has paired with the endpoint 01/05/23 10:19:15.681
Jan  5 10:19:16.682: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jan  5 10:19:16.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5362-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 10:19:17.21
STEP: Creating a custom resource that should be mutated by the webhook 01/05/23 10:19:17.34
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:19:20.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1497" for this suite. 01/05/23 10:19:20.288
STEP: Destroying namespace "webhook-1497-markers" for this suite. 01/05/23 10:19:20.295
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":292,"skipped":5573,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.541 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:19:12.827
    Jan  5 10:19:12.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 10:19:12.828
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:12.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:12.856
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 10:19:12.872
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:19:13.613
    STEP: Deploying the webhook pod 01/05/23 10:19:13.621
    STEP: Wait for the deployment to be ready 01/05/23 10:19:13.632
    Jan  5 10:19:13.641: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 10:19:15.655
    STEP: Verifying the service has paired with the endpoint 01/05/23 10:19:15.681
    Jan  5 10:19:16.682: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jan  5 10:19:16.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5362-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 10:19:17.21
    STEP: Creating a custom resource that should be mutated by the webhook 01/05/23 10:19:17.34
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:19:20.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1497" for this suite. 01/05/23 10:19:20.288
    STEP: Destroying namespace "webhook-1497-markers" for this suite. 01/05/23 10:19:20.295
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:19:20.368
Jan  5 10:19:20.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 10:19:20.369
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:20.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:20.418
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 01/05/23 10:19:20.425
Jan  5 10:19:20.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 create -f -'
Jan  5 10:19:21.505: INFO: stderr: ""
Jan  5 10:19:21.505: INFO: stdout: "pod/pause created\n"
Jan  5 10:19:21.505: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan  5 10:19:21.505: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6994" to be "running and ready"
Jan  5 10:19:21.511: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.111846ms
Jan  5 10:19:21.511: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c' to be 'Running' but was 'Pending'
Jan  5 10:19:23.528: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023067361s
Jan  5 10:19:23.528: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c' to be 'Running' but was 'Pending'
Jan  5 10:19:25.519: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.013550413s
Jan  5 10:19:25.519: INFO: Pod "pause" satisfied condition "running and ready"
Jan  5 10:19:25.519: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 01/05/23 10:19:25.519
Jan  5 10:19:25.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 label pods pause testing-label=testing-label-value'
Jan  5 10:19:25.600: INFO: stderr: ""
Jan  5 10:19:25.600: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/05/23 10:19:25.6
Jan  5 10:19:25.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 get pod pause -L testing-label'
Jan  5 10:19:25.667: INFO: stderr: ""
Jan  5 10:19:25.667: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/05/23 10:19:25.667
Jan  5 10:19:25.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 label pods pause testing-label-'
Jan  5 10:19:25.755: INFO: stderr: ""
Jan  5 10:19:25.755: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/05/23 10:19:25.755
Jan  5 10:19:25.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 get pod pause -L testing-label'
Jan  5 10:19:25.824: INFO: stderr: ""
Jan  5 10:19:25.824: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 01/05/23 10:19:25.824
Jan  5 10:19:25.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 delete --grace-period=0 --force -f -'
Jan  5 10:19:25.911: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 10:19:25.911: INFO: stdout: "pod \"pause\" force deleted\n"
Jan  5 10:19:25.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 get rc,svc -l name=pause --no-headers'
Jan  5 10:19:25.986: INFO: stderr: "No resources found in kubectl-6994 namespace.\n"
Jan  5 10:19:25.986: INFO: stdout: ""
Jan  5 10:19:25.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  5 10:19:26.057: INFO: stderr: ""
Jan  5 10:19:26.057: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 10:19:26.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6994" for this suite. 01/05/23 10:19:26.068
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":293,"skipped":5584,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.705 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:19:20.368
    Jan  5 10:19:20.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 10:19:20.369
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:20.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:20.418
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 01/05/23 10:19:20.425
    Jan  5 10:19:20.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 create -f -'
    Jan  5 10:19:21.505: INFO: stderr: ""
    Jan  5 10:19:21.505: INFO: stdout: "pod/pause created\n"
    Jan  5 10:19:21.505: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan  5 10:19:21.505: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6994" to be "running and ready"
    Jan  5 10:19:21.511: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.111846ms
    Jan  5 10:19:21.511: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c' to be 'Running' but was 'Pending'
    Jan  5 10:19:23.528: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023067361s
    Jan  5 10:19:23.528: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c' to be 'Running' but was 'Pending'
    Jan  5 10:19:25.519: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.013550413s
    Jan  5 10:19:25.519: INFO: Pod "pause" satisfied condition "running and ready"
    Jan  5 10:19:25.519: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 01/05/23 10:19:25.519
    Jan  5 10:19:25.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 label pods pause testing-label=testing-label-value'
    Jan  5 10:19:25.600: INFO: stderr: ""
    Jan  5 10:19:25.600: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/05/23 10:19:25.6
    Jan  5 10:19:25.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 get pod pause -L testing-label'
    Jan  5 10:19:25.667: INFO: stderr: ""
    Jan  5 10:19:25.667: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/05/23 10:19:25.667
    Jan  5 10:19:25.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 label pods pause testing-label-'
    Jan  5 10:19:25.755: INFO: stderr: ""
    Jan  5 10:19:25.755: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/05/23 10:19:25.755
    Jan  5 10:19:25.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 get pod pause -L testing-label'
    Jan  5 10:19:25.824: INFO: stderr: ""
    Jan  5 10:19:25.824: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 01/05/23 10:19:25.824
    Jan  5 10:19:25.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 delete --grace-period=0 --force -f -'
    Jan  5 10:19:25.911: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 10:19:25.911: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan  5 10:19:25.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 get rc,svc -l name=pause --no-headers'
    Jan  5 10:19:25.986: INFO: stderr: "No resources found in kubectl-6994 namespace.\n"
    Jan  5 10:19:25.986: INFO: stdout: ""
    Jan  5 10:19:25.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-6994 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  5 10:19:26.057: INFO: stderr: ""
    Jan  5 10:19:26.057: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 10:19:26.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6994" for this suite. 01/05/23 10:19:26.068
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:19:26.074
Jan  5 10:19:26.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename gc 01/05/23 10:19:26.075
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:26.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:26.098
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/05/23 10:19:26.112
STEP: delete the rc 01/05/23 10:19:31.124
STEP: wait for the rc to be deleted 01/05/23 10:19:31.135
Jan  5 10:19:32.163: INFO: 95 pods remaining
Jan  5 10:19:32.163: INFO: 90 pods has nil DeletionTimestamp
Jan  5 10:19:32.163: INFO: 
Jan  5 10:19:33.156: INFO: 90 pods remaining
Jan  5 10:19:33.156: INFO: 70 pods has nil DeletionTimestamp
Jan  5 10:19:33.156: INFO: 
Jan  5 10:19:34.152: INFO: 90 pods remaining
Jan  5 10:19:34.152: INFO: 70 pods has nil DeletionTimestamp
Jan  5 10:19:34.152: INFO: 
Jan  5 10:19:35.159: INFO: 90 pods remaining
Jan  5 10:19:35.159: INFO: 40 pods has nil DeletionTimestamp
Jan  5 10:19:35.159: INFO: 
Jan  5 10:19:36.152: INFO: 90 pods remaining
Jan  5 10:19:36.152: INFO: 40 pods has nil DeletionTimestamp
Jan  5 10:19:36.152: INFO: 
Jan  5 10:19:37.155: INFO: 90 pods remaining
Jan  5 10:19:37.155: INFO: 10 pods has nil DeletionTimestamp
Jan  5 10:19:37.155: INFO: 
Jan  5 10:19:38.153: INFO: 90 pods remaining
Jan  5 10:19:38.153: INFO: 10 pods has nil DeletionTimestamp
Jan  5 10:19:38.153: INFO: 
Jan  5 10:19:39.164: INFO: 86 pods remaining
Jan  5 10:19:39.164: INFO: 0 pods has nil DeletionTimestamp
Jan  5 10:19:39.164: INFO: 
Jan  5 10:19:40.164: INFO: 78 pods remaining
Jan  5 10:19:40.164: INFO: 0 pods has nil DeletionTimestamp
Jan  5 10:19:40.164: INFO: 
Jan  5 10:19:41.156: INFO: 71 pods remaining
Jan  5 10:19:41.156: INFO: 0 pods has nil DeletionTimestamp
Jan  5 10:19:41.156: INFO: 
Jan  5 10:19:42.154: INFO: 59 pods remaining
Jan  5 10:19:42.154: INFO: 0 pods has nil DeletionTimestamp
Jan  5 10:19:42.154: INFO: 
Jan  5 10:19:43.150: INFO: 51 pods remaining
Jan  5 10:19:43.150: INFO: 0 pods has nil DeletionTimestamp
Jan  5 10:19:43.150: INFO: 
Jan  5 10:19:44.154: INFO: 40 pods remaining
Jan  5 10:19:44.154: INFO: 0 pods has nil DeletionTimestamp
Jan  5 10:19:44.154: INFO: 
Jan  5 10:19:45.170: INFO: 30 pods remaining
Jan  5 10:19:45.170: INFO: 0 pods has nil DeletionTimestamp
Jan  5 10:19:45.170: INFO: 
Jan  5 10:19:46.160: INFO: 20 pods remaining
Jan  5 10:19:46.161: INFO: 0 pods has nil DeletionTimestamp
Jan  5 10:19:46.161: INFO: 
Jan  5 10:19:47.150: INFO: 3 pods remaining
Jan  5 10:19:47.150: INFO: 0 pods has nil DeletionTimestamp
Jan  5 10:19:47.150: INFO: 
STEP: Gathering metrics 01/05/23 10:19:48.145
W0105 10:19:48.162365      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 10:19:48.162: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 10:19:48.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1278" for this suite. 01/05/23 10:19:48.172
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":294,"skipped":5586,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.107 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:19:26.074
    Jan  5 10:19:26.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename gc 01/05/23 10:19:26.075
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:26.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:26.098
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/05/23 10:19:26.112
    STEP: delete the rc 01/05/23 10:19:31.124
    STEP: wait for the rc to be deleted 01/05/23 10:19:31.135
    Jan  5 10:19:32.163: INFO: 95 pods remaining
    Jan  5 10:19:32.163: INFO: 90 pods has nil DeletionTimestamp
    Jan  5 10:19:32.163: INFO: 
    Jan  5 10:19:33.156: INFO: 90 pods remaining
    Jan  5 10:19:33.156: INFO: 70 pods has nil DeletionTimestamp
    Jan  5 10:19:33.156: INFO: 
    Jan  5 10:19:34.152: INFO: 90 pods remaining
    Jan  5 10:19:34.152: INFO: 70 pods has nil DeletionTimestamp
    Jan  5 10:19:34.152: INFO: 
    Jan  5 10:19:35.159: INFO: 90 pods remaining
    Jan  5 10:19:35.159: INFO: 40 pods has nil DeletionTimestamp
    Jan  5 10:19:35.159: INFO: 
    Jan  5 10:19:36.152: INFO: 90 pods remaining
    Jan  5 10:19:36.152: INFO: 40 pods has nil DeletionTimestamp
    Jan  5 10:19:36.152: INFO: 
    Jan  5 10:19:37.155: INFO: 90 pods remaining
    Jan  5 10:19:37.155: INFO: 10 pods has nil DeletionTimestamp
    Jan  5 10:19:37.155: INFO: 
    Jan  5 10:19:38.153: INFO: 90 pods remaining
    Jan  5 10:19:38.153: INFO: 10 pods has nil DeletionTimestamp
    Jan  5 10:19:38.153: INFO: 
    Jan  5 10:19:39.164: INFO: 86 pods remaining
    Jan  5 10:19:39.164: INFO: 0 pods has nil DeletionTimestamp
    Jan  5 10:19:39.164: INFO: 
    Jan  5 10:19:40.164: INFO: 78 pods remaining
    Jan  5 10:19:40.164: INFO: 0 pods has nil DeletionTimestamp
    Jan  5 10:19:40.164: INFO: 
    Jan  5 10:19:41.156: INFO: 71 pods remaining
    Jan  5 10:19:41.156: INFO: 0 pods has nil DeletionTimestamp
    Jan  5 10:19:41.156: INFO: 
    Jan  5 10:19:42.154: INFO: 59 pods remaining
    Jan  5 10:19:42.154: INFO: 0 pods has nil DeletionTimestamp
    Jan  5 10:19:42.154: INFO: 
    Jan  5 10:19:43.150: INFO: 51 pods remaining
    Jan  5 10:19:43.150: INFO: 0 pods has nil DeletionTimestamp
    Jan  5 10:19:43.150: INFO: 
    Jan  5 10:19:44.154: INFO: 40 pods remaining
    Jan  5 10:19:44.154: INFO: 0 pods has nil DeletionTimestamp
    Jan  5 10:19:44.154: INFO: 
    Jan  5 10:19:45.170: INFO: 30 pods remaining
    Jan  5 10:19:45.170: INFO: 0 pods has nil DeletionTimestamp
    Jan  5 10:19:45.170: INFO: 
    Jan  5 10:19:46.160: INFO: 20 pods remaining
    Jan  5 10:19:46.161: INFO: 0 pods has nil DeletionTimestamp
    Jan  5 10:19:46.161: INFO: 
    Jan  5 10:19:47.150: INFO: 3 pods remaining
    Jan  5 10:19:47.150: INFO: 0 pods has nil DeletionTimestamp
    Jan  5 10:19:47.150: INFO: 
    STEP: Gathering metrics 01/05/23 10:19:48.145
    W0105 10:19:48.162365      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 10:19:48.162: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 10:19:48.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1278" for this suite. 01/05/23 10:19:48.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:19:48.181
Jan  5 10:19:48.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:19:48.182
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:48.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:48.204
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 01/05/23 10:19:48.21
Jan  5 10:19:48.225: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29" in namespace "projected-2272" to be "Succeeded or Failed"
Jan  5 10:19:48.232: INFO: Pod "downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29": Phase="Pending", Reason="", readiness=false. Elapsed: 6.89711ms
Jan  5 10:19:50.238: INFO: Pod "downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29": Phase="Running", Reason="", readiness=true. Elapsed: 2.01325407s
Jan  5 10:19:52.242: INFO: Pod "downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29": Phase="Running", Reason="", readiness=false. Elapsed: 4.016859518s
Jan  5 10:19:54.238: INFO: Pod "downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013186779s
STEP: Saw pod success 01/05/23 10:19:54.238
Jan  5 10:19:54.238: INFO: Pod "downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29" satisfied condition "Succeeded or Failed"
Jan  5 10:19:54.242: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29 container client-container: <nil>
STEP: delete the pod 01/05/23 10:19:54.255
Jan  5 10:19:54.268: INFO: Waiting for pod downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29 to disappear
Jan  5 10:19:54.280: INFO: Pod downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 10:19:54.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2272" for this suite. 01/05/23 10:19:54.289
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":295,"skipped":5594,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.112 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:19:48.181
    Jan  5 10:19:48.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:19:48.182
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:48.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:48.204
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 01/05/23 10:19:48.21
    Jan  5 10:19:48.225: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29" in namespace "projected-2272" to be "Succeeded or Failed"
    Jan  5 10:19:48.232: INFO: Pod "downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29": Phase="Pending", Reason="", readiness=false. Elapsed: 6.89711ms
    Jan  5 10:19:50.238: INFO: Pod "downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29": Phase="Running", Reason="", readiness=true. Elapsed: 2.01325407s
    Jan  5 10:19:52.242: INFO: Pod "downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29": Phase="Running", Reason="", readiness=false. Elapsed: 4.016859518s
    Jan  5 10:19:54.238: INFO: Pod "downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013186779s
    STEP: Saw pod success 01/05/23 10:19:54.238
    Jan  5 10:19:54.238: INFO: Pod "downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29" satisfied condition "Succeeded or Failed"
    Jan  5 10:19:54.242: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29 container client-container: <nil>
    STEP: delete the pod 01/05/23 10:19:54.255
    Jan  5 10:19:54.268: INFO: Waiting for pod downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29 to disappear
    Jan  5 10:19:54.280: INFO: Pod downwardapi-volume-7585165a-26c0-492c-96b8-dde02e3aca29 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 10:19:54.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2272" for this suite. 01/05/23 10:19:54.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:19:54.296
Jan  5 10:19:54.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 10:19:54.296
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:54.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:54.323
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-102be258-5667-4dbb-91aa-ced9ca1a300b 01/05/23 10:19:54.327
STEP: Creating a pod to test consume secrets 01/05/23 10:19:54.332
Jan  5 10:19:54.344: INFO: Waiting up to 5m0s for pod "pod-secrets-d22783d4-f16e-49c1-b936-634678614d92" in namespace "secrets-1295" to be "Succeeded or Failed"
Jan  5 10:19:54.349: INFO: Pod "pod-secrets-d22783d4-f16e-49c1-b936-634678614d92": Phase="Pending", Reason="", readiness=false. Elapsed: 5.117342ms
Jan  5 10:19:56.354: INFO: Pod "pod-secrets-d22783d4-f16e-49c1-b936-634678614d92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010059495s
Jan  5 10:19:58.355: INFO: Pod "pod-secrets-d22783d4-f16e-49c1-b936-634678614d92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01134976s
STEP: Saw pod success 01/05/23 10:19:58.355
Jan  5 10:19:58.356: INFO: Pod "pod-secrets-d22783d4-f16e-49c1-b936-634678614d92" satisfied condition "Succeeded or Failed"
Jan  5 10:19:58.361: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-secrets-d22783d4-f16e-49c1-b936-634678614d92 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 10:19:58.387
Jan  5 10:19:58.395: INFO: Waiting for pod pod-secrets-d22783d4-f16e-49c1-b936-634678614d92 to disappear
Jan  5 10:19:58.399: INFO: Pod pod-secrets-d22783d4-f16e-49c1-b936-634678614d92 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 10:19:58.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1295" for this suite. 01/05/23 10:19:58.408
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":296,"skipped":5617,"failed":0}
------------------------------
â€¢ [4.118 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:19:54.296
    Jan  5 10:19:54.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 10:19:54.296
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:54.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:54.323
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-102be258-5667-4dbb-91aa-ced9ca1a300b 01/05/23 10:19:54.327
    STEP: Creating a pod to test consume secrets 01/05/23 10:19:54.332
    Jan  5 10:19:54.344: INFO: Waiting up to 5m0s for pod "pod-secrets-d22783d4-f16e-49c1-b936-634678614d92" in namespace "secrets-1295" to be "Succeeded or Failed"
    Jan  5 10:19:54.349: INFO: Pod "pod-secrets-d22783d4-f16e-49c1-b936-634678614d92": Phase="Pending", Reason="", readiness=false. Elapsed: 5.117342ms
    Jan  5 10:19:56.354: INFO: Pod "pod-secrets-d22783d4-f16e-49c1-b936-634678614d92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010059495s
    Jan  5 10:19:58.355: INFO: Pod "pod-secrets-d22783d4-f16e-49c1-b936-634678614d92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01134976s
    STEP: Saw pod success 01/05/23 10:19:58.355
    Jan  5 10:19:58.356: INFO: Pod "pod-secrets-d22783d4-f16e-49c1-b936-634678614d92" satisfied condition "Succeeded or Failed"
    Jan  5 10:19:58.361: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-secrets-d22783d4-f16e-49c1-b936-634678614d92 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 10:19:58.387
    Jan  5 10:19:58.395: INFO: Waiting for pod pod-secrets-d22783d4-f16e-49c1-b936-634678614d92 to disappear
    Jan  5 10:19:58.399: INFO: Pod pod-secrets-d22783d4-f16e-49c1-b936-634678614d92 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 10:19:58.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1295" for this suite. 01/05/23 10:19:58.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:19:58.415
Jan  5 10:19:58.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename cronjob 01/05/23 10:19:58.416
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:58.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:58.436
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/05/23 10:19:58.442
STEP: Ensuring no jobs are scheduled 01/05/23 10:19:58.447
STEP: Ensuring no job exists by listing jobs explicitly 01/05/23 10:24:58.457
STEP: Removing cronjob 01/05/23 10:24:58.461
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  5 10:24:58.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8172" for this suite. 01/05/23 10:24:58.476
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":297,"skipped":5628,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.068 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:19:58.415
    Jan  5 10:19:58.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename cronjob 01/05/23 10:19:58.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:19:58.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:19:58.436
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/05/23 10:19:58.442
    STEP: Ensuring no jobs are scheduled 01/05/23 10:19:58.447
    STEP: Ensuring no job exists by listing jobs explicitly 01/05/23 10:24:58.457
    STEP: Removing cronjob 01/05/23 10:24:58.461
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  5 10:24:58.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8172" for this suite. 01/05/23 10:24:58.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:24:58.485
Jan  5 10:24:58.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 10:24:58.486
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:24:58.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:24:58.509
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-25273bc0-0718-455b-a7b5-7c11346d9362 01/05/23 10:24:58.525
STEP: Creating the pod 01/05/23 10:24:58.53
Jan  5 10:24:58.542: INFO: Waiting up to 5m0s for pod "pod-configmaps-7c1b9f2d-b81d-41f2-9f5e-452b18cf01b2" in namespace "configmap-2648" to be "running and ready"
Jan  5 10:24:58.547: INFO: Pod "pod-configmaps-7c1b9f2d-b81d-41f2-9f5e-452b18cf01b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.668968ms
Jan  5 10:24:58.547: INFO: The phase of Pod pod-configmaps-7c1b9f2d-b81d-41f2-9f5e-452b18cf01b2 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:25:00.554: INFO: Pod "pod-configmaps-7c1b9f2d-b81d-41f2-9f5e-452b18cf01b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011408065s
Jan  5 10:25:00.554: INFO: The phase of Pod pod-configmaps-7c1b9f2d-b81d-41f2-9f5e-452b18cf01b2 is Running (Ready = true)
Jan  5 10:25:00.554: INFO: Pod "pod-configmaps-7c1b9f2d-b81d-41f2-9f5e-452b18cf01b2" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-25273bc0-0718-455b-a7b5-7c11346d9362 01/05/23 10:25:00.58
STEP: waiting to observe update in volume 01/05/23 10:25:00.585
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 10:25:02.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2648" for this suite. 01/05/23 10:25:02.701
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":298,"skipped":5638,"failed":0}
------------------------------
â€¢ [4.235 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:24:58.485
    Jan  5 10:24:58.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 10:24:58.486
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:24:58.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:24:58.509
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-25273bc0-0718-455b-a7b5-7c11346d9362 01/05/23 10:24:58.525
    STEP: Creating the pod 01/05/23 10:24:58.53
    Jan  5 10:24:58.542: INFO: Waiting up to 5m0s for pod "pod-configmaps-7c1b9f2d-b81d-41f2-9f5e-452b18cf01b2" in namespace "configmap-2648" to be "running and ready"
    Jan  5 10:24:58.547: INFO: Pod "pod-configmaps-7c1b9f2d-b81d-41f2-9f5e-452b18cf01b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.668968ms
    Jan  5 10:24:58.547: INFO: The phase of Pod pod-configmaps-7c1b9f2d-b81d-41f2-9f5e-452b18cf01b2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:25:00.554: INFO: Pod "pod-configmaps-7c1b9f2d-b81d-41f2-9f5e-452b18cf01b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011408065s
    Jan  5 10:25:00.554: INFO: The phase of Pod pod-configmaps-7c1b9f2d-b81d-41f2-9f5e-452b18cf01b2 is Running (Ready = true)
    Jan  5 10:25:00.554: INFO: Pod "pod-configmaps-7c1b9f2d-b81d-41f2-9f5e-452b18cf01b2" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-25273bc0-0718-455b-a7b5-7c11346d9362 01/05/23 10:25:00.58
    STEP: waiting to observe update in volume 01/05/23 10:25:00.585
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 10:25:02.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2648" for this suite. 01/05/23 10:25:02.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:25:02.721
Jan  5 10:25:02.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename watch 01/05/23 10:25:02.722
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:02.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:02.747
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/05/23 10:25:02.753
STEP: modifying the configmap once 01/05/23 10:25:02.76
STEP: modifying the configmap a second time 01/05/23 10:25:02.768
STEP: deleting the configmap 01/05/23 10:25:02.777
STEP: creating a watch on configmaps from the resource version returned by the first update 01/05/23 10:25:02.782
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/05/23 10:25:02.785
Jan  5 10:25:02.785: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1055  3ec58069-3b7d-4cd6-b9df-347ba5788f72 42598 0 2023-01-05 10:25:02 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-05 10:25:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 10:25:02.785: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1055  3ec58069-3b7d-4cd6-b9df-347ba5788f72 42599 0 2023-01-05 10:25:02 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-05 10:25:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  5 10:25:02.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1055" for this suite. 01/05/23 10:25:02.793
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":299,"skipped":5656,"failed":0}
------------------------------
â€¢ [0.078 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:25:02.721
    Jan  5 10:25:02.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename watch 01/05/23 10:25:02.722
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:02.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:02.747
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/05/23 10:25:02.753
    STEP: modifying the configmap once 01/05/23 10:25:02.76
    STEP: modifying the configmap a second time 01/05/23 10:25:02.768
    STEP: deleting the configmap 01/05/23 10:25:02.777
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/05/23 10:25:02.782
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/05/23 10:25:02.785
    Jan  5 10:25:02.785: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1055  3ec58069-3b7d-4cd6-b9df-347ba5788f72 42598 0 2023-01-05 10:25:02 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-05 10:25:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 10:25:02.785: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1055  3ec58069-3b7d-4cd6-b9df-347ba5788f72 42599 0 2023-01-05 10:25:02 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-05 10:25:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  5 10:25:02.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1055" for this suite. 01/05/23 10:25:02.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:25:02.801
Jan  5 10:25:02.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubelet-test 01/05/23 10:25:02.802
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:02.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:02.822
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan  5 10:25:02.839: INFO: Waiting up to 5m0s for pod "busybox-scheduling-4a259971-c2fa-41a1-9efe-c937f0dbda60" in namespace "kubelet-test-5478" to be "running and ready"
Jan  5 10:25:02.847: INFO: Pod "busybox-scheduling-4a259971-c2fa-41a1-9efe-c937f0dbda60": Phase="Pending", Reason="", readiness=false. Elapsed: 8.17531ms
Jan  5 10:25:02.847: INFO: The phase of Pod busybox-scheduling-4a259971-c2fa-41a1-9efe-c937f0dbda60 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:25:04.853: INFO: Pod "busybox-scheduling-4a259971-c2fa-41a1-9efe-c937f0dbda60": Phase="Running", Reason="", readiness=true. Elapsed: 2.0138014s
Jan  5 10:25:04.853: INFO: The phase of Pod busybox-scheduling-4a259971-c2fa-41a1-9efe-c937f0dbda60 is Running (Ready = true)
Jan  5 10:25:04.853: INFO: Pod "busybox-scheduling-4a259971-c2fa-41a1-9efe-c937f0dbda60" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  5 10:25:04.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5478" for this suite. 01/05/23 10:25:04.887
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":300,"skipped":5668,"failed":0}
------------------------------
â€¢ [2.091 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:25:02.801
    Jan  5 10:25:02.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 10:25:02.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:02.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:02.822
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan  5 10:25:02.839: INFO: Waiting up to 5m0s for pod "busybox-scheduling-4a259971-c2fa-41a1-9efe-c937f0dbda60" in namespace "kubelet-test-5478" to be "running and ready"
    Jan  5 10:25:02.847: INFO: Pod "busybox-scheduling-4a259971-c2fa-41a1-9efe-c937f0dbda60": Phase="Pending", Reason="", readiness=false. Elapsed: 8.17531ms
    Jan  5 10:25:02.847: INFO: The phase of Pod busybox-scheduling-4a259971-c2fa-41a1-9efe-c937f0dbda60 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:25:04.853: INFO: Pod "busybox-scheduling-4a259971-c2fa-41a1-9efe-c937f0dbda60": Phase="Running", Reason="", readiness=true. Elapsed: 2.0138014s
    Jan  5 10:25:04.853: INFO: The phase of Pod busybox-scheduling-4a259971-c2fa-41a1-9efe-c937f0dbda60 is Running (Ready = true)
    Jan  5 10:25:04.853: INFO: Pod "busybox-scheduling-4a259971-c2fa-41a1-9efe-c937f0dbda60" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  5 10:25:04.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5478" for this suite. 01/05/23 10:25:04.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:25:04.895
Jan  5 10:25:04.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 10:25:04.896
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:04.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:04.926
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/05/23 10:25:04.949
Jan  5 10:25:04.968: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4111" to be "running and ready"
Jan  5 10:25:04.976: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.119756ms
Jan  5 10:25:04.976: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:25:06.981: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013252776s
Jan  5 10:25:06.981: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  5 10:25:06.981: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 01/05/23 10:25:06.986
Jan  5 10:25:06.994: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4111" to be "running and ready"
Jan  5 10:25:07.000: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.456163ms
Jan  5 10:25:07.000: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:25:09.005: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011718918s
Jan  5 10:25:09.005: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan  5 10:25:09.005: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/05/23 10:25:09.01
STEP: delete the pod with lifecycle hook 01/05/23 10:25:09.026
Jan  5 10:25:09.038: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  5 10:25:09.042: INFO: Pod pod-with-poststart-http-hook still exists
Jan  5 10:25:11.043: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  5 10:25:11.049: INFO: Pod pod-with-poststart-http-hook still exists
Jan  5 10:25:13.043: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  5 10:25:13.048: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan  5 10:25:13.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4111" for this suite. 01/05/23 10:25:13.057
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":301,"skipped":5710,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.168 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:25:04.895
    Jan  5 10:25:04.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 10:25:04.896
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:04.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:04.926
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/05/23 10:25:04.949
    Jan  5 10:25:04.968: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4111" to be "running and ready"
    Jan  5 10:25:04.976: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.119756ms
    Jan  5 10:25:04.976: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:25:06.981: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013252776s
    Jan  5 10:25:06.981: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  5 10:25:06.981: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 01/05/23 10:25:06.986
    Jan  5 10:25:06.994: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4111" to be "running and ready"
    Jan  5 10:25:07.000: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.456163ms
    Jan  5 10:25:07.000: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:25:09.005: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011718918s
    Jan  5 10:25:09.005: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan  5 10:25:09.005: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/05/23 10:25:09.01
    STEP: delete the pod with lifecycle hook 01/05/23 10:25:09.026
    Jan  5 10:25:09.038: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  5 10:25:09.042: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  5 10:25:11.043: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  5 10:25:11.049: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  5 10:25:13.043: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  5 10:25:13.048: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan  5 10:25:13.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4111" for this suite. 01/05/23 10:25:13.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:25:13.064
Jan  5 10:25:13.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:25:13.064
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:13.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:13.088
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-785ad8e4-7624-472f-a38f-f204d939fa4b 01/05/23 10:25:13.101
STEP: Creating the pod 01/05/23 10:25:13.106
Jan  5 10:25:13.115: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5cc7f65-5841-41de-addf-c2519c87239b" in namespace "projected-9026" to be "running and ready"
Jan  5 10:25:13.123: INFO: Pod "pod-projected-configmaps-a5cc7f65-5841-41de-addf-c2519c87239b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.26388ms
Jan  5 10:25:13.123: INFO: The phase of Pod pod-projected-configmaps-a5cc7f65-5841-41de-addf-c2519c87239b is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:25:15.128: INFO: Pod "pod-projected-configmaps-a5cc7f65-5841-41de-addf-c2519c87239b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013105944s
Jan  5 10:25:15.128: INFO: The phase of Pod pod-projected-configmaps-a5cc7f65-5841-41de-addf-c2519c87239b is Running (Ready = true)
Jan  5 10:25:15.128: INFO: Pod "pod-projected-configmaps-a5cc7f65-5841-41de-addf-c2519c87239b" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-785ad8e4-7624-472f-a38f-f204d939fa4b 01/05/23 10:25:15.147
STEP: waiting to observe update in volume 01/05/23 10:25:15.152
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 10:25:17.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9026" for this suite. 01/05/23 10:25:17.302
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":302,"skipped":5718,"failed":0}
------------------------------
â€¢ [4.243 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:25:13.064
    Jan  5 10:25:13.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:25:13.064
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:13.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:13.088
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-785ad8e4-7624-472f-a38f-f204d939fa4b 01/05/23 10:25:13.101
    STEP: Creating the pod 01/05/23 10:25:13.106
    Jan  5 10:25:13.115: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5cc7f65-5841-41de-addf-c2519c87239b" in namespace "projected-9026" to be "running and ready"
    Jan  5 10:25:13.123: INFO: Pod "pod-projected-configmaps-a5cc7f65-5841-41de-addf-c2519c87239b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.26388ms
    Jan  5 10:25:13.123: INFO: The phase of Pod pod-projected-configmaps-a5cc7f65-5841-41de-addf-c2519c87239b is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:25:15.128: INFO: Pod "pod-projected-configmaps-a5cc7f65-5841-41de-addf-c2519c87239b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013105944s
    Jan  5 10:25:15.128: INFO: The phase of Pod pod-projected-configmaps-a5cc7f65-5841-41de-addf-c2519c87239b is Running (Ready = true)
    Jan  5 10:25:15.128: INFO: Pod "pod-projected-configmaps-a5cc7f65-5841-41de-addf-c2519c87239b" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-785ad8e4-7624-472f-a38f-f204d939fa4b 01/05/23 10:25:15.147
    STEP: waiting to observe update in volume 01/05/23 10:25:15.152
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 10:25:17.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9026" for this suite. 01/05/23 10:25:17.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:25:17.308
Jan  5 10:25:17.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename replicaset 01/05/23 10:25:17.309
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:17.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:17.334
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/05/23 10:25:17.34
Jan  5 10:25:17.349: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-974" to be "running and ready"
Jan  5 10:25:17.355: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.117486ms
Jan  5 10:25:17.355: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:25:19.360: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.010875668s
Jan  5 10:25:19.360: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan  5 10:25:19.360: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/05/23 10:25:19.364
STEP: Then the orphan pod is adopted 01/05/23 10:25:19.37
STEP: When the matched label of one of its pods change 01/05/23 10:25:20.381
Jan  5 10:25:20.386: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/05/23 10:25:20.398
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  5 10:25:21.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-974" for this suite. 01/05/23 10:25:21.42
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":303,"skipped":5727,"failed":0}
------------------------------
â€¢ [4.119 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:25:17.308
    Jan  5 10:25:17.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename replicaset 01/05/23 10:25:17.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:17.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:17.334
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/05/23 10:25:17.34
    Jan  5 10:25:17.349: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-974" to be "running and ready"
    Jan  5 10:25:17.355: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.117486ms
    Jan  5 10:25:17.355: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:25:19.360: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.010875668s
    Jan  5 10:25:19.360: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan  5 10:25:19.360: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/05/23 10:25:19.364
    STEP: Then the orphan pod is adopted 01/05/23 10:25:19.37
    STEP: When the matched label of one of its pods change 01/05/23 10:25:20.381
    Jan  5 10:25:20.386: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/05/23 10:25:20.398
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  5 10:25:21.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-974" for this suite. 01/05/23 10:25:21.42
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:25:21.428
Jan  5 10:25:21.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename runtimeclass 01/05/23 10:25:21.429
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:21.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:21.468
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/05/23 10:25:21.474
STEP: getting /apis/node.k8s.io 01/05/23 10:25:21.48
STEP: getting /apis/node.k8s.io/v1 01/05/23 10:25:21.483
STEP: creating 01/05/23 10:25:21.485
STEP: watching 01/05/23 10:25:21.498
Jan  5 10:25:21.498: INFO: starting watch
STEP: getting 01/05/23 10:25:21.507
STEP: listing 01/05/23 10:25:21.512
STEP: patching 01/05/23 10:25:21.517
STEP: updating 01/05/23 10:25:21.523
Jan  5 10:25:21.529: INFO: waiting for watch events with expected annotations
STEP: deleting 01/05/23 10:25:21.529
STEP: deleting a collection 01/05/23 10:25:21.54
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  5 10:25:21.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6403" for this suite. 01/05/23 10:25:21.56
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":304,"skipped":5728,"failed":0}
------------------------------
â€¢ [0.138 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:25:21.428
    Jan  5 10:25:21.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 10:25:21.429
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:21.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:21.468
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/05/23 10:25:21.474
    STEP: getting /apis/node.k8s.io 01/05/23 10:25:21.48
    STEP: getting /apis/node.k8s.io/v1 01/05/23 10:25:21.483
    STEP: creating 01/05/23 10:25:21.485
    STEP: watching 01/05/23 10:25:21.498
    Jan  5 10:25:21.498: INFO: starting watch
    STEP: getting 01/05/23 10:25:21.507
    STEP: listing 01/05/23 10:25:21.512
    STEP: patching 01/05/23 10:25:21.517
    STEP: updating 01/05/23 10:25:21.523
    Jan  5 10:25:21.529: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/05/23 10:25:21.529
    STEP: deleting a collection 01/05/23 10:25:21.54
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  5 10:25:21.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6403" for this suite. 01/05/23 10:25:21.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:25:21.566
Jan  5 10:25:21.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 10:25:21.567
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:21.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:21.587
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-4a1ccb6f-0020-44b3-959c-3eaf1ef9514f 01/05/23 10:25:21.593
STEP: Creating a pod to test consume secrets 01/05/23 10:25:21.607
Jan  5 10:25:21.617: INFO: Waiting up to 5m0s for pod "pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64" in namespace "secrets-9586" to be "Succeeded or Failed"
Jan  5 10:25:21.622: INFO: Pod "pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.368919ms
Jan  5 10:25:23.627: INFO: Pod "pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00944076s
Jan  5 10:25:25.628: INFO: Pod "pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010676544s
STEP: Saw pod success 01/05/23 10:25:25.628
Jan  5 10:25:25.628: INFO: Pod "pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64" satisfied condition "Succeeded or Failed"
Jan  5 10:25:25.633: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64 container secret-env-test: <nil>
STEP: delete the pod 01/05/23 10:25:25.647
Jan  5 10:25:25.658: INFO: Waiting for pod pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64 to disappear
Jan  5 10:25:25.661: INFO: Pod pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan  5 10:25:25.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9586" for this suite. 01/05/23 10:25:25.671
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":305,"skipped":5745,"failed":0}
------------------------------
â€¢ [4.110 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:25:21.566
    Jan  5 10:25:21.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 10:25:21.567
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:21.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:21.587
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-4a1ccb6f-0020-44b3-959c-3eaf1ef9514f 01/05/23 10:25:21.593
    STEP: Creating a pod to test consume secrets 01/05/23 10:25:21.607
    Jan  5 10:25:21.617: INFO: Waiting up to 5m0s for pod "pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64" in namespace "secrets-9586" to be "Succeeded or Failed"
    Jan  5 10:25:21.622: INFO: Pod "pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.368919ms
    Jan  5 10:25:23.627: INFO: Pod "pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00944076s
    Jan  5 10:25:25.628: INFO: Pod "pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010676544s
    STEP: Saw pod success 01/05/23 10:25:25.628
    Jan  5 10:25:25.628: INFO: Pod "pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64" satisfied condition "Succeeded or Failed"
    Jan  5 10:25:25.633: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64 container secret-env-test: <nil>
    STEP: delete the pod 01/05/23 10:25:25.647
    Jan  5 10:25:25.658: INFO: Waiting for pod pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64 to disappear
    Jan  5 10:25:25.661: INFO: Pod pod-secrets-e8768d25-adbf-4cdf-99c5-b7fb170caa64 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 10:25:25.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9586" for this suite. 01/05/23 10:25:25.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:25:25.678
Jan  5 10:25:25.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename resourcequota 01/05/23 10:25:25.678
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:25.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:25.697
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 01/05/23 10:25:25.703
STEP: Ensuring ResourceQuota status is calculated 01/05/23 10:25:25.709
STEP: Creating a ResourceQuota with not best effort scope 01/05/23 10:25:27.714
STEP: Ensuring ResourceQuota status is calculated 01/05/23 10:25:27.719
STEP: Creating a best-effort pod 01/05/23 10:25:29.725
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/05/23 10:25:29.736
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/05/23 10:25:31.743
STEP: Deleting the pod 01/05/23 10:25:33.749
STEP: Ensuring resource quota status released the pod usage 01/05/23 10:25:33.757
STEP: Creating a not best-effort pod 01/05/23 10:25:35.762
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/05/23 10:25:35.772
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/05/23 10:25:37.787
STEP: Deleting the pod 01/05/23 10:25:39.793
STEP: Ensuring resource quota status released the pod usage 01/05/23 10:25:39.802
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 10:25:41.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1783" for this suite. 01/05/23 10:25:41.818
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":306,"skipped":5772,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.147 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:25:25.678
    Jan  5 10:25:25.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename resourcequota 01/05/23 10:25:25.678
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:25.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:25.697
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 01/05/23 10:25:25.703
    STEP: Ensuring ResourceQuota status is calculated 01/05/23 10:25:25.709
    STEP: Creating a ResourceQuota with not best effort scope 01/05/23 10:25:27.714
    STEP: Ensuring ResourceQuota status is calculated 01/05/23 10:25:27.719
    STEP: Creating a best-effort pod 01/05/23 10:25:29.725
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/05/23 10:25:29.736
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/05/23 10:25:31.743
    STEP: Deleting the pod 01/05/23 10:25:33.749
    STEP: Ensuring resource quota status released the pod usage 01/05/23 10:25:33.757
    STEP: Creating a not best-effort pod 01/05/23 10:25:35.762
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/05/23 10:25:35.772
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/05/23 10:25:37.787
    STEP: Deleting the pod 01/05/23 10:25:39.793
    STEP: Ensuring resource quota status released the pod usage 01/05/23 10:25:39.802
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 10:25:41.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1783" for this suite. 01/05/23 10:25:41.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:25:41.825
Jan  5 10:25:41.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubelet-test 01/05/23 10:25:41.826
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:41.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:41.846
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/05/23 10:25:41.869
Jan  5 10:25:41.869: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases2250bdb5-d77f-4027-940e-130c4fa1e5ad" in namespace "kubelet-test-2444" to be "completed"
Jan  5 10:25:41.877: INFO: Pod "agnhost-host-aliases2250bdb5-d77f-4027-940e-130c4fa1e5ad": Phase="Pending", Reason="", readiness=false. Elapsed: 8.130206ms
Jan  5 10:25:43.882: INFO: Pod "agnhost-host-aliases2250bdb5-d77f-4027-940e-130c4fa1e5ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012856243s
Jan  5 10:25:45.883: INFO: Pod "agnhost-host-aliases2250bdb5-d77f-4027-940e-130c4fa1e5ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013919738s
Jan  5 10:25:45.883: INFO: Pod "agnhost-host-aliases2250bdb5-d77f-4027-940e-130c4fa1e5ad" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  5 10:25:45.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2444" for this suite. 01/05/23 10:25:45.906
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":307,"skipped":5780,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:25:41.825
    Jan  5 10:25:41.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 10:25:41.826
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:41.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:41.846
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/05/23 10:25:41.869
    Jan  5 10:25:41.869: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases2250bdb5-d77f-4027-940e-130c4fa1e5ad" in namespace "kubelet-test-2444" to be "completed"
    Jan  5 10:25:41.877: INFO: Pod "agnhost-host-aliases2250bdb5-d77f-4027-940e-130c4fa1e5ad": Phase="Pending", Reason="", readiness=false. Elapsed: 8.130206ms
    Jan  5 10:25:43.882: INFO: Pod "agnhost-host-aliases2250bdb5-d77f-4027-940e-130c4fa1e5ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012856243s
    Jan  5 10:25:45.883: INFO: Pod "agnhost-host-aliases2250bdb5-d77f-4027-940e-130c4fa1e5ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013919738s
    Jan  5 10:25:45.883: INFO: Pod "agnhost-host-aliases2250bdb5-d77f-4027-940e-130c4fa1e5ad" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  5 10:25:45.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2444" for this suite. 01/05/23 10:25:45.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:25:45.915
Jan  5 10:25:45.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 10:25:45.916
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:45.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:45.938
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-63c7ece7-955d-420b-92b9-fb5c0f6ed7a9 01/05/23 10:25:45.944
STEP: Creating a pod to test consume configMaps 01/05/23 10:25:45.949
Jan  5 10:25:45.960: INFO: Waiting up to 5m0s for pod "pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6" in namespace "configmap-3274" to be "Succeeded or Failed"
Jan  5 10:25:45.971: INFO: Pod "pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.379328ms
Jan  5 10:25:47.978: INFO: Pod "pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017669549s
Jan  5 10:25:49.978: INFO: Pod "pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017925962s
STEP: Saw pod success 01/05/23 10:25:49.978
Jan  5 10:25:49.978: INFO: Pod "pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6" satisfied condition "Succeeded or Failed"
Jan  5 10:25:49.983: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6 container configmap-volume-test: <nil>
STEP: delete the pod 01/05/23 10:25:50.003
Jan  5 10:25:50.014: INFO: Waiting for pod pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6 to disappear
Jan  5 10:25:50.018: INFO: Pod pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 10:25:50.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3274" for this suite. 01/05/23 10:25:50.027
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":308,"skipped":5803,"failed":0}
------------------------------
â€¢ [4.116 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:25:45.915
    Jan  5 10:25:45.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 10:25:45.916
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:45.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:45.938
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-63c7ece7-955d-420b-92b9-fb5c0f6ed7a9 01/05/23 10:25:45.944
    STEP: Creating a pod to test consume configMaps 01/05/23 10:25:45.949
    Jan  5 10:25:45.960: INFO: Waiting up to 5m0s for pod "pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6" in namespace "configmap-3274" to be "Succeeded or Failed"
    Jan  5 10:25:45.971: INFO: Pod "pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.379328ms
    Jan  5 10:25:47.978: INFO: Pod "pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017669549s
    Jan  5 10:25:49.978: INFO: Pod "pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017925962s
    STEP: Saw pod success 01/05/23 10:25:49.978
    Jan  5 10:25:49.978: INFO: Pod "pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6" satisfied condition "Succeeded or Failed"
    Jan  5 10:25:49.983: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6 container configmap-volume-test: <nil>
    STEP: delete the pod 01/05/23 10:25:50.003
    Jan  5 10:25:50.014: INFO: Waiting for pod pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6 to disappear
    Jan  5 10:25:50.018: INFO: Pod pod-configmaps-2b4ba71a-55ca-4226-a1ab-02bd2da3bbc6 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 10:25:50.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3274" for this suite. 01/05/23 10:25:50.027
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:25:50.032
Jan  5 10:25:50.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename var-expansion 01/05/23 10:25:50.034
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:50.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:50.052
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 01/05/23 10:25:50.058
Jan  5 10:25:50.067: INFO: Waiting up to 2m0s for pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95" in namespace "var-expansion-4516" to be "running"
Jan  5 10:25:50.076: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 9.365721ms
Jan  5 10:25:52.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015031139s
Jan  5 10:25:54.090: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022982076s
Jan  5 10:25:56.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014788633s
Jan  5 10:25:58.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016028069s
Jan  5 10:26:00.091: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023934577s
Jan  5 10:26:02.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014688185s
Jan  5 10:26:04.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 14.015957154s
Jan  5 10:26:06.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 16.014663046s
Jan  5 10:26:08.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016118924s
Jan  5 10:26:10.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016416164s
Jan  5 10:26:12.081: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014508175s
Jan  5 10:26:14.084: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 24.016548207s
Jan  5 10:26:16.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 26.015397773s
Jan  5 10:26:18.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 28.015367703s
Jan  5 10:26:20.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 30.016195318s
Jan  5 10:26:22.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 32.015791197s
Jan  5 10:26:24.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 34.015721689s
Jan  5 10:26:26.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 36.016502663s
Jan  5 10:26:28.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014977597s
Jan  5 10:26:30.090: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 40.023010616s
Jan  5 10:26:32.086: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 42.018979036s
Jan  5 10:26:34.084: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 44.016875728s
Jan  5 10:26:36.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 46.016094431s
Jan  5 10:26:38.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 48.015903891s
Jan  5 10:26:40.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016238225s
Jan  5 10:26:42.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 52.015703461s
Jan  5 10:26:44.084: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 54.017176429s
Jan  5 10:26:46.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 56.014850425s
Jan  5 10:26:48.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 58.014614374s
Jan  5 10:26:50.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015957184s
Jan  5 10:26:52.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.015200121s
Jan  5 10:26:54.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.016125933s
Jan  5 10:26:56.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.015784322s
Jan  5 10:26:58.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.016536819s
Jan  5 10:27:00.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.016432944s
Jan  5 10:27:02.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.015997112s
Jan  5 10:27:04.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.016215246s
Jan  5 10:27:06.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.015382665s
Jan  5 10:27:08.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.015730111s
Jan  5 10:27:10.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.015179193s
Jan  5 10:27:12.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015297471s
Jan  5 10:27:14.095: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.027678035s
Jan  5 10:27:16.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.015069268s
Jan  5 10:27:18.089: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.021939725s
Jan  5 10:27:20.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.01481142s
Jan  5 10:27:22.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.014605844s
Jan  5 10:27:24.086: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.019129065s
Jan  5 10:27:26.090: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.02292359s
Jan  5 10:27:28.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.014669896s
Jan  5 10:27:30.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.015304136s
Jan  5 10:27:32.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.015663205s
Jan  5 10:27:34.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.015025096s
Jan  5 10:27:36.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.014804624s
Jan  5 10:27:38.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.015424798s
Jan  5 10:27:40.085: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.017846101s
Jan  5 10:27:42.086: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.019029212s
Jan  5 10:27:44.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.016017116s
Jan  5 10:27:46.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.016206619s
Jan  5 10:27:48.081: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.014454945s
Jan  5 10:27:50.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015731494s
Jan  5 10:27:50.087: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.020068054s
STEP: updating the pod 01/05/23 10:27:50.087
Jan  5 10:27:50.601: INFO: Successfully updated pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95"
STEP: waiting for pod running 01/05/23 10:27:50.601
Jan  5 10:27:50.601: INFO: Waiting up to 2m0s for pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95" in namespace "var-expansion-4516" to be "running"
Jan  5 10:27:50.627: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 25.682913ms
Jan  5 10:27:52.633: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Running", Reason="", readiness=true. Elapsed: 2.031916137s
Jan  5 10:27:52.633: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95" satisfied condition "running"
STEP: deleting the pod gracefully 01/05/23 10:27:52.633
Jan  5 10:27:52.634: INFO: Deleting pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95" in namespace "var-expansion-4516"
Jan  5 10:27:52.641: INFO: Wait up to 5m0s for pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 10:28:24.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4516" for this suite. 01/05/23 10:28:24.66
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":309,"skipped":5805,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.634 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:25:50.032
    Jan  5 10:25:50.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename var-expansion 01/05/23 10:25:50.034
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:25:50.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:25:50.052
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 01/05/23 10:25:50.058
    Jan  5 10:25:50.067: INFO: Waiting up to 2m0s for pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95" in namespace "var-expansion-4516" to be "running"
    Jan  5 10:25:50.076: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 9.365721ms
    Jan  5 10:25:52.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015031139s
    Jan  5 10:25:54.090: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022982076s
    Jan  5 10:25:56.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014788633s
    Jan  5 10:25:58.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016028069s
    Jan  5 10:26:00.091: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023934577s
    Jan  5 10:26:02.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014688185s
    Jan  5 10:26:04.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 14.015957154s
    Jan  5 10:26:06.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 16.014663046s
    Jan  5 10:26:08.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016118924s
    Jan  5 10:26:10.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016416164s
    Jan  5 10:26:12.081: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014508175s
    Jan  5 10:26:14.084: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 24.016548207s
    Jan  5 10:26:16.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 26.015397773s
    Jan  5 10:26:18.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 28.015367703s
    Jan  5 10:26:20.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 30.016195318s
    Jan  5 10:26:22.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 32.015791197s
    Jan  5 10:26:24.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 34.015721689s
    Jan  5 10:26:26.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 36.016502663s
    Jan  5 10:26:28.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014977597s
    Jan  5 10:26:30.090: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 40.023010616s
    Jan  5 10:26:32.086: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 42.018979036s
    Jan  5 10:26:34.084: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 44.016875728s
    Jan  5 10:26:36.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 46.016094431s
    Jan  5 10:26:38.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 48.015903891s
    Jan  5 10:26:40.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016238225s
    Jan  5 10:26:42.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 52.015703461s
    Jan  5 10:26:44.084: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 54.017176429s
    Jan  5 10:26:46.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 56.014850425s
    Jan  5 10:26:48.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 58.014614374s
    Jan  5 10:26:50.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015957184s
    Jan  5 10:26:52.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.015200121s
    Jan  5 10:26:54.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.016125933s
    Jan  5 10:26:56.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.015784322s
    Jan  5 10:26:58.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.016536819s
    Jan  5 10:27:00.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.016432944s
    Jan  5 10:27:02.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.015997112s
    Jan  5 10:27:04.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.016215246s
    Jan  5 10:27:06.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.015382665s
    Jan  5 10:27:08.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.015730111s
    Jan  5 10:27:10.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.015179193s
    Jan  5 10:27:12.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015297471s
    Jan  5 10:27:14.095: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.027678035s
    Jan  5 10:27:16.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.015069268s
    Jan  5 10:27:18.089: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.021939725s
    Jan  5 10:27:20.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.01481142s
    Jan  5 10:27:22.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.014605844s
    Jan  5 10:27:24.086: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.019129065s
    Jan  5 10:27:26.090: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.02292359s
    Jan  5 10:27:28.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.014669896s
    Jan  5 10:27:30.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.015304136s
    Jan  5 10:27:32.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.015663205s
    Jan  5 10:27:34.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.015025096s
    Jan  5 10:27:36.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.014804624s
    Jan  5 10:27:38.082: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.015424798s
    Jan  5 10:27:40.085: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.017846101s
    Jan  5 10:27:42.086: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.019029212s
    Jan  5 10:27:44.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.016017116s
    Jan  5 10:27:46.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.016206619s
    Jan  5 10:27:48.081: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.014454945s
    Jan  5 10:27:50.083: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015731494s
    Jan  5 10:27:50.087: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.020068054s
    STEP: updating the pod 01/05/23 10:27:50.087
    Jan  5 10:27:50.601: INFO: Successfully updated pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95"
    STEP: waiting for pod running 01/05/23 10:27:50.601
    Jan  5 10:27:50.601: INFO: Waiting up to 2m0s for pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95" in namespace "var-expansion-4516" to be "running"
    Jan  5 10:27:50.627: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Pending", Reason="", readiness=false. Elapsed: 25.682913ms
    Jan  5 10:27:52.633: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95": Phase="Running", Reason="", readiness=true. Elapsed: 2.031916137s
    Jan  5 10:27:52.633: INFO: Pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95" satisfied condition "running"
    STEP: deleting the pod gracefully 01/05/23 10:27:52.633
    Jan  5 10:27:52.634: INFO: Deleting pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95" in namespace "var-expansion-4516"
    Jan  5 10:27:52.641: INFO: Wait up to 5m0s for pod "var-expansion-3b9be46f-448d-435a-a2d2-b29eb2d8ec95" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 10:28:24.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4516" for this suite. 01/05/23 10:28:24.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:28:24.67
Jan  5 10:28:24.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename runtimeclass 01/05/23 10:28:24.671
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:28:24.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:28:24.712
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan  5 10:28:24.736: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3418 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  5 10:28:24.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3418" for this suite. 01/05/23 10:28:24.787
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":310,"skipped":5820,"failed":0}
------------------------------
â€¢ [0.123 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:28:24.67
    Jan  5 10:28:24.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 10:28:24.671
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:28:24.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:28:24.712
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan  5 10:28:24.736: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3418 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  5 10:28:24.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3418" for this suite. 01/05/23 10:28:24.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:28:24.795
Jan  5 10:28:24.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 10:28:24.796
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:28:24.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:28:24.819
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jan  5 10:28:24.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 10:28:27.365
Jan  5 10:28:27.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 create -f -'
Jan  5 10:28:28.012: INFO: stderr: ""
Jan  5 10:28:28.012: INFO: stdout: "e2e-test-crd-publish-openapi-9547-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  5 10:28:28.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 delete e2e-test-crd-publish-openapi-9547-crds test-cr'
Jan  5 10:28:28.109: INFO: stderr: ""
Jan  5 10:28:28.109: INFO: stdout: "e2e-test-crd-publish-openapi-9547-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan  5 10:28:28.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 apply -f -'
Jan  5 10:28:28.686: INFO: stderr: ""
Jan  5 10:28:28.686: INFO: stdout: "e2e-test-crd-publish-openapi-9547-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  5 10:28:28.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 delete e2e-test-crd-publish-openapi-9547-crds test-cr'
Jan  5 10:28:28.757: INFO: stderr: ""
Jan  5 10:28:28.757: INFO: stdout: "e2e-test-crd-publish-openapi-9547-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/05/23 10:28:28.757
Jan  5 10:28:28.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-555 explain e2e-test-crd-publish-openapi-9547-crds'
Jan  5 10:28:28.917: INFO: stderr: ""
Jan  5 10:28:28.917: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9547-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:28:31.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-555" for this suite. 01/05/23 10:28:31.527
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":311,"skipped":5832,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.738 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:28:24.795
    Jan  5 10:28:24.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 10:28:24.796
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:28:24.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:28:24.819
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jan  5 10:28:24.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 10:28:27.365
    Jan  5 10:28:27.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 create -f -'
    Jan  5 10:28:28.012: INFO: stderr: ""
    Jan  5 10:28:28.012: INFO: stdout: "e2e-test-crd-publish-openapi-9547-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan  5 10:28:28.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 delete e2e-test-crd-publish-openapi-9547-crds test-cr'
    Jan  5 10:28:28.109: INFO: stderr: ""
    Jan  5 10:28:28.109: INFO: stdout: "e2e-test-crd-publish-openapi-9547-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan  5 10:28:28.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 apply -f -'
    Jan  5 10:28:28.686: INFO: stderr: ""
    Jan  5 10:28:28.686: INFO: stdout: "e2e-test-crd-publish-openapi-9547-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan  5 10:28:28.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 delete e2e-test-crd-publish-openapi-9547-crds test-cr'
    Jan  5 10:28:28.757: INFO: stderr: ""
    Jan  5 10:28:28.757: INFO: stdout: "e2e-test-crd-publish-openapi-9547-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/05/23 10:28:28.757
    Jan  5 10:28:28.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=crd-publish-openapi-555 explain e2e-test-crd-publish-openapi-9547-crds'
    Jan  5 10:28:28.917: INFO: stderr: ""
    Jan  5 10:28:28.917: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9547-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:28:31.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-555" for this suite. 01/05/23 10:28:31.527
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:28:31.534
Jan  5 10:28:31.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename sched-preemption 01/05/23 10:28:31.534
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:28:31.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:28:31.56
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan  5 10:28:31.578: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 10:29:31.648: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:29:31.653
Jan  5 10:29:31.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename sched-preemption-path 01/05/23 10:29:31.654
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:29:31.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:29:31.673
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 01/05/23 10:29:31.678
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 10:29:31.678
Jan  5 10:29:31.687: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-498" to be "running"
Jan  5 10:29:31.698: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.295885ms
Jan  5 10:29:33.704: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.017378228s
Jan  5 10:29:33.704: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 10:29:33.709
Jan  5 10:29:33.721: INFO: found a healthy node: shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jan  5 10:29:47.877: INFO: pods created so far: [1 1 1]
Jan  5 10:29:47.877: INFO: length of pods created so far: 3
Jan  5 10:29:49.895: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jan  5 10:29:56.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-498" for this suite. 01/05/23 10:29:56.905
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan  5 10:29:56.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3534" for this suite. 01/05/23 10:29:56.958
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":312,"skipped":5835,"failed":0}
------------------------------
â€¢ [SLOW TEST] [85.510 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:28:31.534
    Jan  5 10:28:31.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename sched-preemption 01/05/23 10:28:31.534
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:28:31.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:28:31.56
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan  5 10:28:31.578: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 10:29:31.648: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:29:31.653
    Jan  5 10:29:31.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename sched-preemption-path 01/05/23 10:29:31.654
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:29:31.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:29:31.673
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 01/05/23 10:29:31.678
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 10:29:31.678
    Jan  5 10:29:31.687: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-498" to be "running"
    Jan  5 10:29:31.698: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.295885ms
    Jan  5 10:29:33.704: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.017378228s
    Jan  5 10:29:33.704: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 10:29:33.709
    Jan  5 10:29:33.721: INFO: found a healthy node: shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jan  5 10:29:47.877: INFO: pods created so far: [1 1 1]
    Jan  5 10:29:47.877: INFO: length of pods created so far: 3
    Jan  5 10:29:49.895: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jan  5 10:29:56.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-498" for this suite. 01/05/23 10:29:56.905
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 10:29:56.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-3534" for this suite. 01/05/23 10:29:56.958
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:29:57.044
Jan  5 10:29:57.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-probe 01/05/23 10:29:57.045
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:29:57.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:29:57.066
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 in namespace container-probe-6473 01/05/23 10:29:57.071
Jan  5 10:29:57.079: INFO: Waiting up to 5m0s for pod "liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9" in namespace "container-probe-6473" to be "not pending"
Jan  5 10:29:57.083: INFO: Pod "liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.874369ms
Jan  5 10:29:59.089: INFO: Pod "liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010392146s
Jan  5 10:29:59.089: INFO: Pod "liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9" satisfied condition "not pending"
Jan  5 10:29:59.089: INFO: Started pod liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 in namespace container-probe-6473
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 10:29:59.089
Jan  5 10:29:59.094: INFO: Initial restart count of pod liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 is 0
Jan  5 10:30:19.163: INFO: Restart count of pod container-probe-6473/liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 is now 1 (20.06859182s elapsed)
Jan  5 10:30:39.266: INFO: Restart count of pod container-probe-6473/liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 is now 2 (40.171609141s elapsed)
Jan  5 10:30:59.337: INFO: Restart count of pod container-probe-6473/liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 is now 3 (1m0.242742419s elapsed)
Jan  5 10:31:19.403: INFO: Restart count of pod container-probe-6473/liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 is now 4 (1m20.308368595s elapsed)
Jan  5 10:32:21.605: INFO: Restart count of pod container-probe-6473/liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 is now 5 (2m22.510828046s elapsed)
STEP: deleting the pod 01/05/23 10:32:21.605
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 10:32:21.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6473" for this suite. 01/05/23 10:32:21.633
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":313,"skipped":5835,"failed":0}
------------------------------
â€¢ [SLOW TEST] [144.597 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:29:57.044
    Jan  5 10:29:57.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-probe 01/05/23 10:29:57.045
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:29:57.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:29:57.066
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 in namespace container-probe-6473 01/05/23 10:29:57.071
    Jan  5 10:29:57.079: INFO: Waiting up to 5m0s for pod "liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9" in namespace "container-probe-6473" to be "not pending"
    Jan  5 10:29:57.083: INFO: Pod "liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.874369ms
    Jan  5 10:29:59.089: INFO: Pod "liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010392146s
    Jan  5 10:29:59.089: INFO: Pod "liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9" satisfied condition "not pending"
    Jan  5 10:29:59.089: INFO: Started pod liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 in namespace container-probe-6473
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 10:29:59.089
    Jan  5 10:29:59.094: INFO: Initial restart count of pod liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 is 0
    Jan  5 10:30:19.163: INFO: Restart count of pod container-probe-6473/liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 is now 1 (20.06859182s elapsed)
    Jan  5 10:30:39.266: INFO: Restart count of pod container-probe-6473/liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 is now 2 (40.171609141s elapsed)
    Jan  5 10:30:59.337: INFO: Restart count of pod container-probe-6473/liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 is now 3 (1m0.242742419s elapsed)
    Jan  5 10:31:19.403: INFO: Restart count of pod container-probe-6473/liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 is now 4 (1m20.308368595s elapsed)
    Jan  5 10:32:21.605: INFO: Restart count of pod container-probe-6473/liveness-2b632f7b-b1f3-48d8-87b3-c7bbadd9d3a9 is now 5 (2m22.510828046s elapsed)
    STEP: deleting the pod 01/05/23 10:32:21.605
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 10:32:21.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6473" for this suite. 01/05/23 10:32:21.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:32:21.641
Jan  5 10:32:21.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 10:32:21.642
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:32:21.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:32:21.662
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-329 01/05/23 10:32:21.667
STEP: creating replication controller nodeport-test in namespace services-329 01/05/23 10:32:21.685
I0105 10:32:21.691286      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-329, replica count: 2
I0105 10:32:24.743046      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 10:32:24.743: INFO: Creating new exec pod
Jan  5 10:32:24.756: INFO: Waiting up to 5m0s for pod "execpodxznx6" in namespace "services-329" to be "running"
Jan  5 10:32:24.760: INFO: Pod "execpodxznx6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.44365ms
Jan  5 10:32:26.766: INFO: Pod "execpodxznx6": Phase="Running", Reason="", readiness=true. Elapsed: 2.00996529s
Jan  5 10:32:26.766: INFO: Pod "execpodxznx6" satisfied condition "running"
Jan  5 10:32:27.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-329 exec execpodxznx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan  5 10:32:28.181: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan  5 10:32:28.181: INFO: stdout: ""
Jan  5 10:32:29.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-329 exec execpodxznx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan  5 10:32:29.605: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan  5 10:32:29.605: INFO: stdout: "nodeport-test-sgwv8"
Jan  5 10:32:29.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-329 exec execpodxznx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.117.154.112 80'
Jan  5 10:32:30.082: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.117.154.112 80\nConnection to 10.117.154.112 80 port [tcp/http] succeeded!\n"
Jan  5 10:32:30.082: INFO: stdout: "nodeport-test-4f4wq"
Jan  5 10:32:30.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-329 exec execpodxznx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.1.19 31782'
Jan  5 10:32:30.569: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.1.19 31782\nConnection to 10.250.1.19 31782 port [tcp/*] succeeded!\n"
Jan  5 10:32:30.569: INFO: stdout: "nodeport-test-4f4wq"
Jan  5 10:32:30.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-329 exec execpodxznx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.128 31782'
Jan  5 10:32:31.098: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.128 31782\nConnection to 10.250.0.128 31782 port [tcp/*] succeeded!\n"
Jan  5 10:32:31.098: INFO: stdout: "nodeport-test-4f4wq"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 10:32:31.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-329" for this suite. 01/05/23 10:32:31.108
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":314,"skipped":5843,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.473 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:32:21.641
    Jan  5 10:32:21.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 10:32:21.642
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:32:21.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:32:21.662
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-329 01/05/23 10:32:21.667
    STEP: creating replication controller nodeport-test in namespace services-329 01/05/23 10:32:21.685
    I0105 10:32:21.691286      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-329, replica count: 2
    I0105 10:32:24.743046      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 10:32:24.743: INFO: Creating new exec pod
    Jan  5 10:32:24.756: INFO: Waiting up to 5m0s for pod "execpodxznx6" in namespace "services-329" to be "running"
    Jan  5 10:32:24.760: INFO: Pod "execpodxznx6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.44365ms
    Jan  5 10:32:26.766: INFO: Pod "execpodxznx6": Phase="Running", Reason="", readiness=true. Elapsed: 2.00996529s
    Jan  5 10:32:26.766: INFO: Pod "execpodxznx6" satisfied condition "running"
    Jan  5 10:32:27.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-329 exec execpodxznx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan  5 10:32:28.181: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan  5 10:32:28.181: INFO: stdout: ""
    Jan  5 10:32:29.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-329 exec execpodxznx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan  5 10:32:29.605: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan  5 10:32:29.605: INFO: stdout: "nodeport-test-sgwv8"
    Jan  5 10:32:29.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-329 exec execpodxznx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.117.154.112 80'
    Jan  5 10:32:30.082: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.117.154.112 80\nConnection to 10.117.154.112 80 port [tcp/http] succeeded!\n"
    Jan  5 10:32:30.082: INFO: stdout: "nodeport-test-4f4wq"
    Jan  5 10:32:30.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-329 exec execpodxznx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.1.19 31782'
    Jan  5 10:32:30.569: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.1.19 31782\nConnection to 10.250.1.19 31782 port [tcp/*] succeeded!\n"
    Jan  5 10:32:30.569: INFO: stdout: "nodeport-test-4f4wq"
    Jan  5 10:32:30.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-329 exec execpodxznx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.128 31782'
    Jan  5 10:32:31.098: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.128 31782\nConnection to 10.250.0.128 31782 port [tcp/*] succeeded!\n"
    Jan  5 10:32:31.098: INFO: stdout: "nodeport-test-4f4wq"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 10:32:31.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-329" for this suite. 01/05/23 10:32:31.108
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:32:31.115
Jan  5 10:32:31.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename resourcequota 01/05/23 10:32:31.116
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:32:31.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:32:31.137
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 01/05/23 10:32:31.141
STEP: Counting existing ResourceQuota 01/05/23 10:32:36.147
STEP: Creating a ResourceQuota 01/05/23 10:32:41.153
STEP: Ensuring resource quota status is calculated 01/05/23 10:32:41.158
STEP: Creating a Secret 01/05/23 10:32:43.165
STEP: Ensuring resource quota status captures secret creation 01/05/23 10:32:43.175
STEP: Deleting a secret 01/05/23 10:32:45.181
STEP: Ensuring resource quota status released usage 01/05/23 10:32:45.191
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 10:32:47.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-978" for this suite. 01/05/23 10:32:47.208
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":315,"skipped":5843,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.098 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:32:31.115
    Jan  5 10:32:31.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename resourcequota 01/05/23 10:32:31.116
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:32:31.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:32:31.137
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 01/05/23 10:32:31.141
    STEP: Counting existing ResourceQuota 01/05/23 10:32:36.147
    STEP: Creating a ResourceQuota 01/05/23 10:32:41.153
    STEP: Ensuring resource quota status is calculated 01/05/23 10:32:41.158
    STEP: Creating a Secret 01/05/23 10:32:43.165
    STEP: Ensuring resource quota status captures secret creation 01/05/23 10:32:43.175
    STEP: Deleting a secret 01/05/23 10:32:45.181
    STEP: Ensuring resource quota status released usage 01/05/23 10:32:45.191
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 10:32:47.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-978" for this suite. 01/05/23 10:32:47.208
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:32:47.213
Jan  5 10:32:47.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:32:47.214
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:32:47.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:32:47.235
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-f3eec7b3-4817-48d5-9c2a-7036e79ddc3c 01/05/23 10:32:47.242
STEP: Creating a pod to test consume configMaps 01/05/23 10:32:47.247
Jan  5 10:32:47.256: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b" in namespace "projected-4657" to be "Succeeded or Failed"
Jan  5 10:32:47.260: INFO: Pod "pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.476181ms
Jan  5 10:32:49.266: INFO: Pod "pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b": Phase="Running", Reason="", readiness=false. Elapsed: 2.010363691s
Jan  5 10:32:51.267: INFO: Pod "pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011493064s
STEP: Saw pod success 01/05/23 10:32:51.267
Jan  5 10:32:51.267: INFO: Pod "pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b" satisfied condition "Succeeded or Failed"
Jan  5 10:32:51.272: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b container agnhost-container: <nil>
STEP: delete the pod 01/05/23 10:32:51.289
Jan  5 10:32:51.303: INFO: Waiting for pod pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b to disappear
Jan  5 10:32:51.310: INFO: Pod pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 10:32:51.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4657" for this suite. 01/05/23 10:32:51.323
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":316,"skipped":5847,"failed":0}
------------------------------
â€¢ [4.115 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:32:47.213
    Jan  5 10:32:47.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:32:47.214
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:32:47.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:32:47.235
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-f3eec7b3-4817-48d5-9c2a-7036e79ddc3c 01/05/23 10:32:47.242
    STEP: Creating a pod to test consume configMaps 01/05/23 10:32:47.247
    Jan  5 10:32:47.256: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b" in namespace "projected-4657" to be "Succeeded or Failed"
    Jan  5 10:32:47.260: INFO: Pod "pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.476181ms
    Jan  5 10:32:49.266: INFO: Pod "pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b": Phase="Running", Reason="", readiness=false. Elapsed: 2.010363691s
    Jan  5 10:32:51.267: INFO: Pod "pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011493064s
    STEP: Saw pod success 01/05/23 10:32:51.267
    Jan  5 10:32:51.267: INFO: Pod "pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b" satisfied condition "Succeeded or Failed"
    Jan  5 10:32:51.272: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 10:32:51.289
    Jan  5 10:32:51.303: INFO: Waiting for pod pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b to disappear
    Jan  5 10:32:51.310: INFO: Pod pod-projected-configmaps-43559f54-825d-49d3-a2d8-111a067cf39b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 10:32:51.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4657" for this suite. 01/05/23 10:32:51.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:32:51.329
Jan  5 10:32:51.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename resourcequota 01/05/23 10:32:51.33
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:32:51.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:32:51.35
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 01/05/23 10:32:51.355
STEP: Creating a ResourceQuota 01/05/23 10:32:56.361
STEP: Ensuring resource quota status is calculated 01/05/23 10:32:56.366
STEP: Creating a ReplicationController 01/05/23 10:32:58.373
STEP: Ensuring resource quota status captures replication controller creation 01/05/23 10:32:58.386
STEP: Deleting a ReplicationController 01/05/23 10:33:00.391
STEP: Ensuring resource quota status released usage 01/05/23 10:33:00.396
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 10:33:02.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3011" for this suite. 01/05/23 10:33:02.411
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":317,"skipped":5886,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.088 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:32:51.329
    Jan  5 10:32:51.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename resourcequota 01/05/23 10:32:51.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:32:51.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:32:51.35
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 01/05/23 10:32:51.355
    STEP: Creating a ResourceQuota 01/05/23 10:32:56.361
    STEP: Ensuring resource quota status is calculated 01/05/23 10:32:56.366
    STEP: Creating a ReplicationController 01/05/23 10:32:58.373
    STEP: Ensuring resource quota status captures replication controller creation 01/05/23 10:32:58.386
    STEP: Deleting a ReplicationController 01/05/23 10:33:00.391
    STEP: Ensuring resource quota status released usage 01/05/23 10:33:00.396
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 10:33:02.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3011" for this suite. 01/05/23 10:33:02.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:33:02.418
Jan  5 10:33:02.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pod-network-test 01/05/23 10:33:02.419
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:33:02.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:33:02.439
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-74 01/05/23 10:33:02.445
STEP: creating a selector 01/05/23 10:33:02.445
STEP: Creating the service pods in kubernetes 01/05/23 10:33:02.445
Jan  5 10:33:02.445: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  5 10:33:02.508: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-74" to be "running and ready"
Jan  5 10:33:02.516: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.87357ms
Jan  5 10:33:02.516: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:33:04.526: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01740167s
Jan  5 10:33:04.526: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:33:06.523: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014458674s
Jan  5 10:33:06.523: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:33:08.526: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017607597s
Jan  5 10:33:08.526: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:33:10.525: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01677883s
Jan  5 10:33:10.525: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:33:12.523: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014499921s
Jan  5 10:33:12.523: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:33:14.523: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014514822s
Jan  5 10:33:14.523: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:33:16.522: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013306344s
Jan  5 10:33:16.522: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:33:18.522: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013279499s
Jan  5 10:33:18.522: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:33:20.525: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.016397025s
Jan  5 10:33:20.525: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:33:22.522: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013176798s
Jan  5 10:33:22.522: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:33:24.524: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015082017s
Jan  5 10:33:24.524: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  5 10:33:24.524: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  5 10:33:24.530: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-74" to be "running and ready"
Jan  5 10:33:24.538: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.9915ms
Jan  5 10:33:24.538: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  5 10:33:24.538: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  5 10:33:24.544: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-74" to be "running and ready"
Jan  5 10:33:24.549: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.50208ms
Jan  5 10:33:24.549: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  5 10:33:24.549: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan  5 10:33:24.554: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-74" to be "running and ready"
Jan  5 10:33:24.562: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 8.143414ms
Jan  5 10:33:24.562: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan  5 10:33:24.562: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jan  5 10:33:24.567: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-74" to be "running and ready"
Jan  5 10:33:24.571: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.678109ms
Jan  5 10:33:24.571: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jan  5 10:33:24.571: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 01/05/23 10:33:24.577
Jan  5 10:33:24.591: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-74" to be "running"
Jan  5 10:33:24.596: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.856622ms
Jan  5 10:33:26.601: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009910951s
Jan  5 10:33:26.601: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  5 10:33:26.606: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-74" to be "running"
Jan  5 10:33:26.610: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.071467ms
Jan  5 10:33:26.610: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan  5 10:33:26.614: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jan  5 10:33:26.614: INFO: Going to poll 10.96.3.35 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jan  5 10:33:26.619: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.3.35 8081 | grep -v '^\s*$'] Namespace:pod-network-test-74 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:33:26.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:33:26.619: INFO: ExecWithOptions: Clientset creation
Jan  5 10:33:26.620: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-74/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.3.35+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 10:33:28.130: INFO: Found all 1 expected endpoints: [netserver-0]
Jan  5 10:33:28.130: INFO: Going to poll 10.96.1.161 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jan  5 10:33:28.134: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.1.161 8081 | grep -v '^\s*$'] Namespace:pod-network-test-74 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:33:28.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:33:28.135: INFO: ExecWithOptions: Clientset creation
Jan  5 10:33:28.135: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-74/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.1.161+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 10:33:29.511: INFO: Found all 1 expected endpoints: [netserver-1]
Jan  5 10:33:29.511: INFO: Going to poll 10.96.0.153 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jan  5 10:33:29.528: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.0.153 8081 | grep -v '^\s*$'] Namespace:pod-network-test-74 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:33:29.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:33:29.528: INFO: ExecWithOptions: Clientset creation
Jan  5 10:33:29.528: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-74/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.0.153+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 10:33:31.025: INFO: Found all 1 expected endpoints: [netserver-2]
Jan  5 10:33:31.025: INFO: Going to poll 10.96.2.119 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jan  5 10:33:31.030: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.2.119 8081 | grep -v '^\s*$'] Namespace:pod-network-test-74 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:33:31.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:33:31.031: INFO: ExecWithOptions: Clientset creation
Jan  5 10:33:31.031: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-74/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.2.119+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 10:33:32.461: INFO: Found all 1 expected endpoints: [netserver-3]
Jan  5 10:33:32.461: INFO: Going to poll 10.96.4.194 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jan  5 10:33:32.467: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.4.194 8081 | grep -v '^\s*$'] Namespace:pod-network-test-74 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:33:32.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:33:32.468: INFO: ExecWithOptions: Clientset creation
Jan  5 10:33:32.469: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-74/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.4.194+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 10:33:33.920: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan  5 10:33:33.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-74" for this suite. 01/05/23 10:33:33.93
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":318,"skipped":5904,"failed":0}
------------------------------
â€¢ [SLOW TEST] [31.517 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:33:02.418
    Jan  5 10:33:02.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pod-network-test 01/05/23 10:33:02.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:33:02.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:33:02.439
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-74 01/05/23 10:33:02.445
    STEP: creating a selector 01/05/23 10:33:02.445
    STEP: Creating the service pods in kubernetes 01/05/23 10:33:02.445
    Jan  5 10:33:02.445: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  5 10:33:02.508: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-74" to be "running and ready"
    Jan  5 10:33:02.516: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.87357ms
    Jan  5 10:33:02.516: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:33:04.526: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01740167s
    Jan  5 10:33:04.526: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:33:06.523: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014458674s
    Jan  5 10:33:06.523: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:33:08.526: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017607597s
    Jan  5 10:33:08.526: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:33:10.525: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01677883s
    Jan  5 10:33:10.525: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:33:12.523: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014499921s
    Jan  5 10:33:12.523: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:33:14.523: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014514822s
    Jan  5 10:33:14.523: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:33:16.522: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013306344s
    Jan  5 10:33:16.522: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:33:18.522: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013279499s
    Jan  5 10:33:18.522: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:33:20.525: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.016397025s
    Jan  5 10:33:20.525: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:33:22.522: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013176798s
    Jan  5 10:33:22.522: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:33:24.524: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015082017s
    Jan  5 10:33:24.524: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  5 10:33:24.524: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  5 10:33:24.530: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-74" to be "running and ready"
    Jan  5 10:33:24.538: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.9915ms
    Jan  5 10:33:24.538: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  5 10:33:24.538: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  5 10:33:24.544: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-74" to be "running and ready"
    Jan  5 10:33:24.549: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.50208ms
    Jan  5 10:33:24.549: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  5 10:33:24.549: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan  5 10:33:24.554: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-74" to be "running and ready"
    Jan  5 10:33:24.562: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 8.143414ms
    Jan  5 10:33:24.562: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan  5 10:33:24.562: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jan  5 10:33:24.567: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-74" to be "running and ready"
    Jan  5 10:33:24.571: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.678109ms
    Jan  5 10:33:24.571: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jan  5 10:33:24.571: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 01/05/23 10:33:24.577
    Jan  5 10:33:24.591: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-74" to be "running"
    Jan  5 10:33:24.596: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.856622ms
    Jan  5 10:33:26.601: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009910951s
    Jan  5 10:33:26.601: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  5 10:33:26.606: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-74" to be "running"
    Jan  5 10:33:26.610: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.071467ms
    Jan  5 10:33:26.610: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan  5 10:33:26.614: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Jan  5 10:33:26.614: INFO: Going to poll 10.96.3.35 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jan  5 10:33:26.619: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.3.35 8081 | grep -v '^\s*$'] Namespace:pod-network-test-74 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:33:26.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:33:26.619: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:33:26.620: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-74/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.3.35+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 10:33:28.130: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan  5 10:33:28.130: INFO: Going to poll 10.96.1.161 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jan  5 10:33:28.134: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.1.161 8081 | grep -v '^\s*$'] Namespace:pod-network-test-74 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:33:28.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:33:28.135: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:33:28.135: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-74/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.1.161+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 10:33:29.511: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan  5 10:33:29.511: INFO: Going to poll 10.96.0.153 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jan  5 10:33:29.528: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.0.153 8081 | grep -v '^\s*$'] Namespace:pod-network-test-74 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:33:29.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:33:29.528: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:33:29.528: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-74/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.0.153+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 10:33:31.025: INFO: Found all 1 expected endpoints: [netserver-2]
    Jan  5 10:33:31.025: INFO: Going to poll 10.96.2.119 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jan  5 10:33:31.030: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.2.119 8081 | grep -v '^\s*$'] Namespace:pod-network-test-74 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:33:31.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:33:31.031: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:33:31.031: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-74/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.2.119+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 10:33:32.461: INFO: Found all 1 expected endpoints: [netserver-3]
    Jan  5 10:33:32.461: INFO: Going to poll 10.96.4.194 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jan  5 10:33:32.467: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.4.194 8081 | grep -v '^\s*$'] Namespace:pod-network-test-74 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:33:32.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:33:32.468: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:33:32.469: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-74/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.4.194+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 10:33:33.920: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan  5 10:33:33.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-74" for this suite. 01/05/23 10:33:33.93
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:33:33.936
Jan  5 10:33:33.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:33:33.937
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:33:33.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:33:33.954
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-5b180867-493a-4dc4-8e57-f79db8b88bcc 01/05/23 10:33:33.96
STEP: Creating a pod to test consume secrets 01/05/23 10:33:33.965
Jan  5 10:33:33.977: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137" in namespace "projected-8815" to be "Succeeded or Failed"
Jan  5 10:33:33.983: INFO: Pod "pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137": Phase="Pending", Reason="", readiness=false. Elapsed: 5.816031ms
Jan  5 10:33:35.988: INFO: Pod "pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01134846s
Jan  5 10:33:37.989: INFO: Pod "pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012456335s
STEP: Saw pod success 01/05/23 10:33:37.989
Jan  5 10:33:37.990: INFO: Pod "pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137" satisfied condition "Succeeded or Failed"
Jan  5 10:33:37.993: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 10:33:38.01
Jan  5 10:33:38.021: INFO: Waiting for pod pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137 to disappear
Jan  5 10:33:38.025: INFO: Pod pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 10:33:38.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8815" for this suite. 01/05/23 10:33:38.035
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":319,"skipped":5907,"failed":0}
------------------------------
â€¢ [4.105 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:33:33.936
    Jan  5 10:33:33.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:33:33.937
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:33:33.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:33:33.954
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-5b180867-493a-4dc4-8e57-f79db8b88bcc 01/05/23 10:33:33.96
    STEP: Creating a pod to test consume secrets 01/05/23 10:33:33.965
    Jan  5 10:33:33.977: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137" in namespace "projected-8815" to be "Succeeded or Failed"
    Jan  5 10:33:33.983: INFO: Pod "pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137": Phase="Pending", Reason="", readiness=false. Elapsed: 5.816031ms
    Jan  5 10:33:35.988: INFO: Pod "pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01134846s
    Jan  5 10:33:37.989: INFO: Pod "pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012456335s
    STEP: Saw pod success 01/05/23 10:33:37.989
    Jan  5 10:33:37.990: INFO: Pod "pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137" satisfied condition "Succeeded or Failed"
    Jan  5 10:33:37.993: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 10:33:38.01
    Jan  5 10:33:38.021: INFO: Waiting for pod pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137 to disappear
    Jan  5 10:33:38.025: INFO: Pod pod-projected-secrets-a88e5162-0cc7-403e-b723-336a92710137 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 10:33:38.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8815" for this suite. 01/05/23 10:33:38.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:33:38.042
Jan  5 10:33:38.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename taint-single-pod 01/05/23 10:33:38.042
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:33:38.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:33:38.068
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan  5 10:33:38.075: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 10:34:38.141: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jan  5 10:34:38.146: INFO: Starting informer...
STEP: Starting pod... 01/05/23 10:34:38.146
Jan  5 10:34:38.363: INFO: Pod is running on shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c. Tainting Node
STEP: Trying to apply a taint on the Node 01/05/23 10:34:38.363
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 10:34:38.382
STEP: Waiting short time to make sure Pod is queued for deletion 01/05/23 10:34:38.389
Jan  5 10:34:38.389: INFO: Pod wasn't evicted. Proceeding
Jan  5 10:34:38.389: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 10:34:38.41
STEP: Waiting some time to make sure that toleration time passed. 01/05/23 10:34:38.416
Jan  5 10:35:53.416: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jan  5 10:35:53.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-541" for this suite. 01/05/23 10:35:53.426
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":320,"skipped":5925,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.391 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:33:38.042
    Jan  5 10:33:38.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename taint-single-pod 01/05/23 10:33:38.042
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:33:38.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:33:38.068
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jan  5 10:33:38.075: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 10:34:38.141: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jan  5 10:34:38.146: INFO: Starting informer...
    STEP: Starting pod... 01/05/23 10:34:38.146
    Jan  5 10:34:38.363: INFO: Pod is running on shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c. Tainting Node
    STEP: Trying to apply a taint on the Node 01/05/23 10:34:38.363
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 10:34:38.382
    STEP: Waiting short time to make sure Pod is queued for deletion 01/05/23 10:34:38.389
    Jan  5 10:34:38.389: INFO: Pod wasn't evicted. Proceeding
    Jan  5 10:34:38.389: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 10:34:38.41
    STEP: Waiting some time to make sure that toleration time passed. 01/05/23 10:34:38.416
    Jan  5 10:35:53.416: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 10:35:53.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-541" for this suite. 01/05/23 10:35:53.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:35:53.434
Jan  5 10:35:53.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 10:35:53.435
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:35:53.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:35:53.455
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-76/secret-test-f0eb4959-930b-43b4-b2ac-31cccbea3e1f 01/05/23 10:35:53.461
STEP: Creating a pod to test consume secrets 01/05/23 10:35:53.465
Jan  5 10:35:53.474: INFO: Waiting up to 5m0s for pod "pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a" in namespace "secrets-76" to be "Succeeded or Failed"
Jan  5 10:35:53.481: INFO: Pod "pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.55398ms
Jan  5 10:35:55.487: INFO: Pod "pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013133841s
Jan  5 10:35:57.487: INFO: Pod "pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012451226s
STEP: Saw pod success 01/05/23 10:35:57.487
Jan  5 10:35:57.487: INFO: Pod "pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a" satisfied condition "Succeeded or Failed"
Jan  5 10:35:57.492: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a container env-test: <nil>
STEP: delete the pod 01/05/23 10:35:57.515
Jan  5 10:35:57.539: INFO: Waiting for pod pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a to disappear
Jan  5 10:35:57.555: INFO: Pod pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan  5 10:35:57.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-76" for this suite. 01/05/23 10:35:57.565
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":321,"skipped":5953,"failed":0}
------------------------------
â€¢ [4.137 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:35:53.434
    Jan  5 10:35:53.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 10:35:53.435
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:35:53.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:35:53.455
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-76/secret-test-f0eb4959-930b-43b4-b2ac-31cccbea3e1f 01/05/23 10:35:53.461
    STEP: Creating a pod to test consume secrets 01/05/23 10:35:53.465
    Jan  5 10:35:53.474: INFO: Waiting up to 5m0s for pod "pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a" in namespace "secrets-76" to be "Succeeded or Failed"
    Jan  5 10:35:53.481: INFO: Pod "pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.55398ms
    Jan  5 10:35:55.487: INFO: Pod "pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013133841s
    Jan  5 10:35:57.487: INFO: Pod "pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012451226s
    STEP: Saw pod success 01/05/23 10:35:57.487
    Jan  5 10:35:57.487: INFO: Pod "pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a" satisfied condition "Succeeded or Failed"
    Jan  5 10:35:57.492: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a container env-test: <nil>
    STEP: delete the pod 01/05/23 10:35:57.515
    Jan  5 10:35:57.539: INFO: Waiting for pod pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a to disappear
    Jan  5 10:35:57.555: INFO: Pod pod-configmaps-b8876095-071c-49cf-8151-da68e5fb0b1a no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 10:35:57.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-76" for this suite. 01/05/23 10:35:57.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:35:57.571
Jan  5 10:35:57.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename dns 01/05/23 10:35:57.572
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:35:57.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:35:57.591
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/05/23 10:35:57.597
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/05/23 10:35:57.597
STEP: creating a pod to probe DNS 01/05/23 10:35:57.597
STEP: submitting the pod to kubernetes 01/05/23 10:35:57.598
Jan  5 10:35:57.609: INFO: Waiting up to 15m0s for pod "dns-test-d536d0f4-cbb8-4df9-969a-8628c108ab0f" in namespace "dns-9643" to be "running"
Jan  5 10:35:57.614: INFO: Pod "dns-test-d536d0f4-cbb8-4df9-969a-8628c108ab0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.893019ms
Jan  5 10:35:59.621: INFO: Pod "dns-test-d536d0f4-cbb8-4df9-969a-8628c108ab0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011572723s
Jan  5 10:35:59.621: INFO: Pod "dns-test-d536d0f4-cbb8-4df9-969a-8628c108ab0f" satisfied condition "running"
STEP: retrieving the pod 01/05/23 10:35:59.621
STEP: looking for the results for each expected name from probers 01/05/23 10:35:59.626
Jan  5 10:35:59.796: INFO: DNS probes using dns-9643/dns-test-d536d0f4-cbb8-4df9-969a-8628c108ab0f succeeded

STEP: deleting the pod 01/05/23 10:35:59.796
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 10:35:59.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9643" for this suite. 01/05/23 10:35:59.816
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":322,"skipped":5972,"failed":0}
------------------------------
â€¢ [2.251 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:35:57.571
    Jan  5 10:35:57.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename dns 01/05/23 10:35:57.572
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:35:57.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:35:57.591
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/05/23 10:35:57.597
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/05/23 10:35:57.597
    STEP: creating a pod to probe DNS 01/05/23 10:35:57.597
    STEP: submitting the pod to kubernetes 01/05/23 10:35:57.598
    Jan  5 10:35:57.609: INFO: Waiting up to 15m0s for pod "dns-test-d536d0f4-cbb8-4df9-969a-8628c108ab0f" in namespace "dns-9643" to be "running"
    Jan  5 10:35:57.614: INFO: Pod "dns-test-d536d0f4-cbb8-4df9-969a-8628c108ab0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.893019ms
    Jan  5 10:35:59.621: INFO: Pod "dns-test-d536d0f4-cbb8-4df9-969a-8628c108ab0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011572723s
    Jan  5 10:35:59.621: INFO: Pod "dns-test-d536d0f4-cbb8-4df9-969a-8628c108ab0f" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 10:35:59.621
    STEP: looking for the results for each expected name from probers 01/05/23 10:35:59.626
    Jan  5 10:35:59.796: INFO: DNS probes using dns-9643/dns-test-d536d0f4-cbb8-4df9-969a-8628c108ab0f succeeded

    STEP: deleting the pod 01/05/23 10:35:59.796
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 10:35:59.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9643" for this suite. 01/05/23 10:35:59.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:35:59.823
Jan  5 10:35:59.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:35:59.824
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:35:59.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:35:59.846
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 01/05/23 10:35:59.853
Jan  5 10:35:59.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08" in namespace "projected-5864" to be "Succeeded or Failed"
Jan  5 10:35:59.869: INFO: Pod "downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08": Phase="Pending", Reason="", readiness=false. Elapsed: 5.176496ms
Jan  5 10:36:01.876: INFO: Pod "downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012145212s
Jan  5 10:36:03.875: INFO: Pod "downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010946906s
STEP: Saw pod success 01/05/23 10:36:03.875
Jan  5 10:36:03.875: INFO: Pod "downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08" satisfied condition "Succeeded or Failed"
Jan  5 10:36:03.879: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08 container client-container: <nil>
STEP: delete the pod 01/05/23 10:36:03.896
Jan  5 10:36:03.906: INFO: Waiting for pod downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08 to disappear
Jan  5 10:36:03.910: INFO: Pod downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 10:36:03.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5864" for this suite. 01/05/23 10:36:03.92
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":323,"skipped":5994,"failed":0}
------------------------------
â€¢ [4.102 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:35:59.823
    Jan  5 10:35:59.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:35:59.824
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:35:59.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:35:59.846
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 01/05/23 10:35:59.853
    Jan  5 10:35:59.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08" in namespace "projected-5864" to be "Succeeded or Failed"
    Jan  5 10:35:59.869: INFO: Pod "downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08": Phase="Pending", Reason="", readiness=false. Elapsed: 5.176496ms
    Jan  5 10:36:01.876: INFO: Pod "downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012145212s
    Jan  5 10:36:03.875: INFO: Pod "downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010946906s
    STEP: Saw pod success 01/05/23 10:36:03.875
    Jan  5 10:36:03.875: INFO: Pod "downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08" satisfied condition "Succeeded or Failed"
    Jan  5 10:36:03.879: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08 container client-container: <nil>
    STEP: delete the pod 01/05/23 10:36:03.896
    Jan  5 10:36:03.906: INFO: Waiting for pod downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08 to disappear
    Jan  5 10:36:03.910: INFO: Pod downwardapi-volume-57a36fce-66cf-4289-9f1e-aedb1ffaba08 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 10:36:03.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5864" for this suite. 01/05/23 10:36:03.92
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:36:03.925
Jan  5 10:36:03.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename downward-api 01/05/23 10:36:03.926
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:36:03.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:36:03.946
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 01/05/23 10:36:03.953
Jan  5 10:36:03.963: INFO: Waiting up to 5m0s for pod "downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223" in namespace "downward-api-7304" to be "Succeeded or Failed"
Jan  5 10:36:03.978: INFO: Pod "downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223": Phase="Pending", Reason="", readiness=false. Elapsed: 14.939572ms
Jan  5 10:36:05.983: INFO: Pod "downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020056702s
Jan  5 10:36:07.983: INFO: Pod "downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020328943s
STEP: Saw pod success 01/05/23 10:36:07.983
Jan  5 10:36:07.983: INFO: Pod "downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223" satisfied condition "Succeeded or Failed"
Jan  5 10:36:07.988: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223 container dapi-container: <nil>
STEP: delete the pod 01/05/23 10:36:08.002
Jan  5 10:36:08.013: INFO: Waiting for pod downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223 to disappear
Jan  5 10:36:08.017: INFO: Pod downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  5 10:36:08.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7304" for this suite. 01/05/23 10:36:08.028
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":324,"skipped":5997,"failed":0}
------------------------------
â€¢ [4.109 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:36:03.925
    Jan  5 10:36:03.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename downward-api 01/05/23 10:36:03.926
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:36:03.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:36:03.946
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 01/05/23 10:36:03.953
    Jan  5 10:36:03.963: INFO: Waiting up to 5m0s for pod "downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223" in namespace "downward-api-7304" to be "Succeeded or Failed"
    Jan  5 10:36:03.978: INFO: Pod "downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223": Phase="Pending", Reason="", readiness=false. Elapsed: 14.939572ms
    Jan  5 10:36:05.983: INFO: Pod "downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020056702s
    Jan  5 10:36:07.983: INFO: Pod "downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020328943s
    STEP: Saw pod success 01/05/23 10:36:07.983
    Jan  5 10:36:07.983: INFO: Pod "downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223" satisfied condition "Succeeded or Failed"
    Jan  5 10:36:07.988: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 10:36:08.002
    Jan  5 10:36:08.013: INFO: Waiting for pod downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223 to disappear
    Jan  5 10:36:08.017: INFO: Pod downward-api-ace3d932-9e3a-44b0-b0c5-f13483bc0223 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  5 10:36:08.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7304" for this suite. 01/05/23 10:36:08.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:36:08.037
Jan  5 10:36:08.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 10:36:08.038
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:36:08.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:36:08.059
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 10:36:08.064
Jan  5 10:36:08.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-1075 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jan  5 10:36:08.149: INFO: stderr: ""
Jan  5 10:36:08.149: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/05/23 10:36:08.149
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jan  5 10:36:08.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-1075 delete pods e2e-test-httpd-pod'
Jan  5 10:36:10.609: INFO: stderr: ""
Jan  5 10:36:10.609: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 10:36:10.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1075" for this suite. 01/05/23 10:36:10.63
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":325,"skipped":6013,"failed":0}
------------------------------
â€¢ [2.600 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:36:08.037
    Jan  5 10:36:08.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 10:36:08.038
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:36:08.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:36:08.059
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 10:36:08.064
    Jan  5 10:36:08.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-1075 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jan  5 10:36:08.149: INFO: stderr: ""
    Jan  5 10:36:08.149: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/05/23 10:36:08.149
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jan  5 10:36:08.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-1075 delete pods e2e-test-httpd-pod'
    Jan  5 10:36:10.609: INFO: stderr: ""
    Jan  5 10:36:10.609: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 10:36:10.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1075" for this suite. 01/05/23 10:36:10.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:36:10.637
Jan  5 10:36:10.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename cronjob 01/05/23 10:36:10.638
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:36:10.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:36:10.662
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/05/23 10:36:10.668
STEP: Ensuring a job is scheduled 01/05/23 10:36:10.672
STEP: Ensuring exactly one is scheduled 01/05/23 10:37:00.679
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/05/23 10:37:00.684
STEP: Ensuring the job is replaced with a new one 01/05/23 10:37:00.689
STEP: Removing cronjob 01/05/23 10:38:00.695
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  5 10:38:00.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6719" for this suite. 01/05/23 10:38:00.709
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":326,"skipped":6022,"failed":0}
------------------------------
â€¢ [SLOW TEST] [110.081 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:36:10.637
    Jan  5 10:36:10.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename cronjob 01/05/23 10:36:10.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:36:10.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:36:10.662
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/05/23 10:36:10.668
    STEP: Ensuring a job is scheduled 01/05/23 10:36:10.672
    STEP: Ensuring exactly one is scheduled 01/05/23 10:37:00.679
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/05/23 10:37:00.684
    STEP: Ensuring the job is replaced with a new one 01/05/23 10:37:00.689
    STEP: Removing cronjob 01/05/23 10:38:00.695
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  5 10:38:00.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6719" for this suite. 01/05/23 10:38:00.709
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:38:00.718
Jan  5 10:38:00.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 10:38:00.72
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:00.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:00.746
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1311 01/05/23 10:38:00.752
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/05/23 10:38:00.766
STEP: creating service externalsvc in namespace services-1311 01/05/23 10:38:00.766
STEP: creating replication controller externalsvc in namespace services-1311 01/05/23 10:38:00.783
I0105 10:38:00.795228      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1311, replica count: 2
I0105 10:38:03.846226      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/05/23 10:38:03.851
Jan  5 10:38:03.867: INFO: Creating new exec pod
Jan  5 10:38:03.882: INFO: Waiting up to 5m0s for pod "execpodmtw2m" in namespace "services-1311" to be "running"
Jan  5 10:38:03.888: INFO: Pod "execpodmtw2m": Phase="Pending", Reason="", readiness=false. Elapsed: 5.802126ms
Jan  5 10:38:05.896: INFO: Pod "execpodmtw2m": Phase="Running", Reason="", readiness=true. Elapsed: 2.01452644s
Jan  5 10:38:05.896: INFO: Pod "execpodmtw2m" satisfied condition "running"
Jan  5 10:38:05.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-1311 exec execpodmtw2m -- /bin/sh -x -c nslookup clusterip-service.services-1311.svc.cluster.local'
Jan  5 10:38:06.497: INFO: stderr: "+ nslookup clusterip-service.services-1311.svc.cluster.local\n"
Jan  5 10:38:06.497: INFO: stdout: "Server:\t\t10.112.0.10\nAddress:\t10.112.0.10#53\n\nclusterip-service.services-1311.svc.cluster.local\tcanonical name = externalsvc.services-1311.svc.cluster.local.\nName:\texternalsvc.services-1311.svc.cluster.local\nAddress: 10.113.88.112\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1311, will wait for the garbage collector to delete the pods 01/05/23 10:38:06.497
Jan  5 10:38:06.558: INFO: Deleting ReplicationController externalsvc took: 5.992018ms
Jan  5 10:38:06.659: INFO: Terminating ReplicationController externalsvc pods took: 100.504475ms
Jan  5 10:38:08.973: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 10:38:08.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1311" for this suite. 01/05/23 10:38:09.007
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":327,"skipped":6025,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.294 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:38:00.718
    Jan  5 10:38:00.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 10:38:00.72
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:00.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:00.746
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1311 01/05/23 10:38:00.752
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/05/23 10:38:00.766
    STEP: creating service externalsvc in namespace services-1311 01/05/23 10:38:00.766
    STEP: creating replication controller externalsvc in namespace services-1311 01/05/23 10:38:00.783
    I0105 10:38:00.795228      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1311, replica count: 2
    I0105 10:38:03.846226      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/05/23 10:38:03.851
    Jan  5 10:38:03.867: INFO: Creating new exec pod
    Jan  5 10:38:03.882: INFO: Waiting up to 5m0s for pod "execpodmtw2m" in namespace "services-1311" to be "running"
    Jan  5 10:38:03.888: INFO: Pod "execpodmtw2m": Phase="Pending", Reason="", readiness=false. Elapsed: 5.802126ms
    Jan  5 10:38:05.896: INFO: Pod "execpodmtw2m": Phase="Running", Reason="", readiness=true. Elapsed: 2.01452644s
    Jan  5 10:38:05.896: INFO: Pod "execpodmtw2m" satisfied condition "running"
    Jan  5 10:38:05.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-1311 exec execpodmtw2m -- /bin/sh -x -c nslookup clusterip-service.services-1311.svc.cluster.local'
    Jan  5 10:38:06.497: INFO: stderr: "+ nslookup clusterip-service.services-1311.svc.cluster.local\n"
    Jan  5 10:38:06.497: INFO: stdout: "Server:\t\t10.112.0.10\nAddress:\t10.112.0.10#53\n\nclusterip-service.services-1311.svc.cluster.local\tcanonical name = externalsvc.services-1311.svc.cluster.local.\nName:\texternalsvc.services-1311.svc.cluster.local\nAddress: 10.113.88.112\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1311, will wait for the garbage collector to delete the pods 01/05/23 10:38:06.497
    Jan  5 10:38:06.558: INFO: Deleting ReplicationController externalsvc took: 5.992018ms
    Jan  5 10:38:06.659: INFO: Terminating ReplicationController externalsvc pods took: 100.504475ms
    Jan  5 10:38:08.973: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 10:38:08.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1311" for this suite. 01/05/23 10:38:09.007
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:38:09.014
Jan  5 10:38:09.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename job 01/05/23 10:38:09.014
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:09.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:09.035
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 01/05/23 10:38:09.044
STEP: Patching the Job 01/05/23 10:38:09.049
STEP: Watching for Job to be patched 01/05/23 10:38:09.061
Jan  5 10:38:09.065: INFO: Event ADDED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan  5 10:38:09.065: INFO: Event MODIFIED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan  5 10:38:09.065: INFO: Event MODIFIED found for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/05/23 10:38:09.065
STEP: Watching for Job to be updated 01/05/23 10:38:09.074
Jan  5 10:38:09.079: INFO: Event MODIFIED found for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 10:38:09.079: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/05/23 10:38:09.079
Jan  5 10:38:09.099: INFO: Job: e2e-xhbw7 as labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched]
STEP: Waiting for job to complete 01/05/23 10:38:09.099
STEP: Delete a job collection with a labelselector 01/05/23 10:38:19.105
STEP: Watching for Job to be deleted 01/05/23 10:38:19.112
Jan  5 10:38:19.116: INFO: Event MODIFIED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 10:38:19.116: INFO: Event MODIFIED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 10:38:19.116: INFO: Event MODIFIED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 10:38:19.116: INFO: Event MODIFIED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 10:38:19.117: INFO: Event MODIFIED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 10:38:19.117: INFO: Event DELETED found for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/05/23 10:38:19.117
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  5 10:38:19.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8262" for this suite. 01/05/23 10:38:19.136
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":328,"skipped":6057,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.130 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:38:09.014
    Jan  5 10:38:09.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename job 01/05/23 10:38:09.014
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:09.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:09.035
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 01/05/23 10:38:09.044
    STEP: Patching the Job 01/05/23 10:38:09.049
    STEP: Watching for Job to be patched 01/05/23 10:38:09.061
    Jan  5 10:38:09.065: INFO: Event ADDED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan  5 10:38:09.065: INFO: Event MODIFIED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan  5 10:38:09.065: INFO: Event MODIFIED found for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/05/23 10:38:09.065
    STEP: Watching for Job to be updated 01/05/23 10:38:09.074
    Jan  5 10:38:09.079: INFO: Event MODIFIED found for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 10:38:09.079: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/05/23 10:38:09.079
    Jan  5 10:38:09.099: INFO: Job: e2e-xhbw7 as labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched]
    STEP: Waiting for job to complete 01/05/23 10:38:09.099
    STEP: Delete a job collection with a labelselector 01/05/23 10:38:19.105
    STEP: Watching for Job to be deleted 01/05/23 10:38:19.112
    Jan  5 10:38:19.116: INFO: Event MODIFIED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 10:38:19.116: INFO: Event MODIFIED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 10:38:19.116: INFO: Event MODIFIED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 10:38:19.116: INFO: Event MODIFIED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 10:38:19.117: INFO: Event MODIFIED observed for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 10:38:19.117: INFO: Event DELETED found for Job e2e-xhbw7 in namespace job-8262 with labels: map[e2e-job-label:e2e-xhbw7 e2e-xhbw7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/05/23 10:38:19.117
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  5 10:38:19.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8262" for this suite. 01/05/23 10:38:19.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:38:19.145
Jan  5 10:38:19.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename resourcequota 01/05/23 10:38:19.145
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:19.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:19.167
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 01/05/23 10:38:19.173
STEP: Creating a ResourceQuota 01/05/23 10:38:24.178
STEP: Ensuring resource quota status is calculated 01/05/23 10:38:24.182
STEP: Creating a Pod that fits quota 01/05/23 10:38:26.188
STEP: Ensuring ResourceQuota status captures the pod usage 01/05/23 10:38:26.202
STEP: Not allowing a pod to be created that exceeds remaining quota 01/05/23 10:38:28.208
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/05/23 10:38:28.216
STEP: Ensuring a pod cannot update its resource requirements 01/05/23 10:38:28.224
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/05/23 10:38:28.23
STEP: Deleting the pod 01/05/23 10:38:30.236
STEP: Ensuring resource quota status released the pod usage 01/05/23 10:38:30.246
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 10:38:32.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6956" for this suite. 01/05/23 10:38:32.262
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":329,"skipped":6068,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.123 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:38:19.145
    Jan  5 10:38:19.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename resourcequota 01/05/23 10:38:19.145
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:19.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:19.167
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 01/05/23 10:38:19.173
    STEP: Creating a ResourceQuota 01/05/23 10:38:24.178
    STEP: Ensuring resource quota status is calculated 01/05/23 10:38:24.182
    STEP: Creating a Pod that fits quota 01/05/23 10:38:26.188
    STEP: Ensuring ResourceQuota status captures the pod usage 01/05/23 10:38:26.202
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/05/23 10:38:28.208
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/05/23 10:38:28.216
    STEP: Ensuring a pod cannot update its resource requirements 01/05/23 10:38:28.224
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/05/23 10:38:28.23
    STEP: Deleting the pod 01/05/23 10:38:30.236
    STEP: Ensuring resource quota status released the pod usage 01/05/23 10:38:30.246
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 10:38:32.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6956" for this suite. 01/05/23 10:38:32.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:38:32.268
Jan  5 10:38:32.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:38:32.269
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:32.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:32.287
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-975547c6-a138-4cbb-b9dd-0c0c8649c4f0 01/05/23 10:38:32.302
STEP: Creating configMap with name cm-test-opt-upd-f0a1c2bd-e4de-4dfa-98ea-633a19a1a273 01/05/23 10:38:32.308
STEP: Creating the pod 01/05/23 10:38:32.313
Jan  5 10:38:32.325: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-86a91cf8-2b67-4713-9499-1b1286bb88c3" in namespace "projected-9468" to be "running and ready"
Jan  5 10:38:32.330: INFO: Pod "pod-projected-configmaps-86a91cf8-2b67-4713-9499-1b1286bb88c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.782137ms
Jan  5 10:38:32.330: INFO: The phase of Pod pod-projected-configmaps-86a91cf8-2b67-4713-9499-1b1286bb88c3 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:38:34.337: INFO: Pod "pod-projected-configmaps-86a91cf8-2b67-4713-9499-1b1286bb88c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011577835s
Jan  5 10:38:34.337: INFO: The phase of Pod pod-projected-configmaps-86a91cf8-2b67-4713-9499-1b1286bb88c3 is Running (Ready = true)
Jan  5 10:38:34.337: INFO: Pod "pod-projected-configmaps-86a91cf8-2b67-4713-9499-1b1286bb88c3" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-975547c6-a138-4cbb-b9dd-0c0c8649c4f0 01/05/23 10:38:34.545
STEP: Updating configmap cm-test-opt-upd-f0a1c2bd-e4de-4dfa-98ea-633a19a1a273 01/05/23 10:38:34.551
STEP: Creating configMap with name cm-test-opt-create-68841a4a-af82-47d5-8448-3ddfae6593b6 01/05/23 10:38:34.557
STEP: waiting to observe update in volume 01/05/23 10:38:34.561
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 10:38:36.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9468" for this suite. 01/05/23 10:38:36.851
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":330,"skipped":6074,"failed":0}
------------------------------
â€¢ [4.588 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:38:32.268
    Jan  5 10:38:32.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:38:32.269
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:32.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:32.287
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-975547c6-a138-4cbb-b9dd-0c0c8649c4f0 01/05/23 10:38:32.302
    STEP: Creating configMap with name cm-test-opt-upd-f0a1c2bd-e4de-4dfa-98ea-633a19a1a273 01/05/23 10:38:32.308
    STEP: Creating the pod 01/05/23 10:38:32.313
    Jan  5 10:38:32.325: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-86a91cf8-2b67-4713-9499-1b1286bb88c3" in namespace "projected-9468" to be "running and ready"
    Jan  5 10:38:32.330: INFO: Pod "pod-projected-configmaps-86a91cf8-2b67-4713-9499-1b1286bb88c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.782137ms
    Jan  5 10:38:32.330: INFO: The phase of Pod pod-projected-configmaps-86a91cf8-2b67-4713-9499-1b1286bb88c3 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:38:34.337: INFO: Pod "pod-projected-configmaps-86a91cf8-2b67-4713-9499-1b1286bb88c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011577835s
    Jan  5 10:38:34.337: INFO: The phase of Pod pod-projected-configmaps-86a91cf8-2b67-4713-9499-1b1286bb88c3 is Running (Ready = true)
    Jan  5 10:38:34.337: INFO: Pod "pod-projected-configmaps-86a91cf8-2b67-4713-9499-1b1286bb88c3" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-975547c6-a138-4cbb-b9dd-0c0c8649c4f0 01/05/23 10:38:34.545
    STEP: Updating configmap cm-test-opt-upd-f0a1c2bd-e4de-4dfa-98ea-633a19a1a273 01/05/23 10:38:34.551
    STEP: Creating configMap with name cm-test-opt-create-68841a4a-af82-47d5-8448-3ddfae6593b6 01/05/23 10:38:34.557
    STEP: waiting to observe update in volume 01/05/23 10:38:34.561
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 10:38:36.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9468" for this suite. 01/05/23 10:38:36.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:38:36.857
Jan  5 10:38:36.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename proxy 01/05/23 10:38:36.857
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:36.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:36.877
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan  5 10:38:36.882: INFO: Creating pod...
Jan  5 10:38:36.891: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6652" to be "running"
Jan  5 10:38:36.896: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.689254ms
Jan  5 10:38:38.903: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.011548667s
Jan  5 10:38:38.903: INFO: Pod "agnhost" satisfied condition "running"
Jan  5 10:38:38.903: INFO: Creating service...
Jan  5 10:38:38.917: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/DELETE
Jan  5 10:38:38.955: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  5 10:38:38.955: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/GET
Jan  5 10:38:38.981: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan  5 10:38:38.981: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/HEAD
Jan  5 10:38:38.998: INFO: http.Client request:HEAD | StatusCode:200
Jan  5 10:38:38.998: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/OPTIONS
Jan  5 10:38:39.050: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  5 10:38:39.050: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/PATCH
Jan  5 10:38:39.060: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  5 10:38:39.060: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/POST
Jan  5 10:38:39.068: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  5 10:38:39.068: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/PUT
Jan  5 10:38:39.076: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  5 10:38:39.076: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/DELETE
Jan  5 10:38:39.086: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  5 10:38:39.086: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/GET
Jan  5 10:38:39.096: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan  5 10:38:39.096: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/HEAD
Jan  5 10:38:39.104: INFO: http.Client request:HEAD | StatusCode:200
Jan  5 10:38:39.104: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/OPTIONS
Jan  5 10:38:39.113: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  5 10:38:39.113: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/PATCH
Jan  5 10:38:39.121: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  5 10:38:39.121: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/POST
Jan  5 10:38:39.129: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  5 10:38:39.129: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/PUT
Jan  5 10:38:39.138: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan  5 10:38:39.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6652" for this suite. 01/05/23 10:38:39.147
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":331,"skipped":6084,"failed":0}
------------------------------
â€¢ [2.296 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:38:36.857
    Jan  5 10:38:36.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename proxy 01/05/23 10:38:36.857
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:36.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:36.877
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan  5 10:38:36.882: INFO: Creating pod...
    Jan  5 10:38:36.891: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6652" to be "running"
    Jan  5 10:38:36.896: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.689254ms
    Jan  5 10:38:38.903: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.011548667s
    Jan  5 10:38:38.903: INFO: Pod "agnhost" satisfied condition "running"
    Jan  5 10:38:38.903: INFO: Creating service...
    Jan  5 10:38:38.917: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/DELETE
    Jan  5 10:38:38.955: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  5 10:38:38.955: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/GET
    Jan  5 10:38:38.981: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan  5 10:38:38.981: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/HEAD
    Jan  5 10:38:38.998: INFO: http.Client request:HEAD | StatusCode:200
    Jan  5 10:38:38.998: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan  5 10:38:39.050: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  5 10:38:39.050: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/PATCH
    Jan  5 10:38:39.060: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  5 10:38:39.060: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/POST
    Jan  5 10:38:39.068: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  5 10:38:39.068: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/pods/agnhost/proxy/some/path/with/PUT
    Jan  5 10:38:39.076: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  5 10:38:39.076: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/DELETE
    Jan  5 10:38:39.086: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  5 10:38:39.086: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/GET
    Jan  5 10:38:39.096: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan  5 10:38:39.096: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/HEAD
    Jan  5 10:38:39.104: INFO: http.Client request:HEAD | StatusCode:200
    Jan  5 10:38:39.104: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/OPTIONS
    Jan  5 10:38:39.113: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  5 10:38:39.113: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/PATCH
    Jan  5 10:38:39.121: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  5 10:38:39.121: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/POST
    Jan  5 10:38:39.129: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  5 10:38:39.129: INFO: Starting http.Client for https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/proxy-6652/services/test-service/proxy/some/path/with/PUT
    Jan  5 10:38:39.138: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan  5 10:38:39.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6652" for this suite. 01/05/23 10:38:39.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:38:39.156
Jan  5 10:38:39.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 10:38:39.157
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:39.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:39.177
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 10:38:39.198
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:38:39.661
STEP: Deploying the webhook pod 01/05/23 10:38:39.668
STEP: Wait for the deployment to be ready 01/05/23 10:38:39.681
Jan  5 10:38:39.690: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/05/23 10:38:41.705
STEP: Verifying the service has paired with the endpoint 01/05/23 10:38:41.72
Jan  5 10:38:42.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/05/23 10:38:42.725
STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 10:38:42.726
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/05/23 10:38:42.854
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/05/23 10:38:43.866
STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 10:38:43.866
STEP: Having no error when timeout is longer than webhook latency 01/05/23 10:38:44.969
STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 10:38:44.969
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/05/23 10:38:50.157
STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 10:38:50.157
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:38:55.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7418" for this suite. 01/05/23 10:38:55.207
STEP: Destroying namespace "webhook-7418-markers" for this suite. 01/05/23 10:38:55.212
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":332,"skipped":6115,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.091 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:38:39.156
    Jan  5 10:38:39.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 10:38:39.157
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:39.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:39.177
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 10:38:39.198
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:38:39.661
    STEP: Deploying the webhook pod 01/05/23 10:38:39.668
    STEP: Wait for the deployment to be ready 01/05/23 10:38:39.681
    Jan  5 10:38:39.690: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/05/23 10:38:41.705
    STEP: Verifying the service has paired with the endpoint 01/05/23 10:38:41.72
    Jan  5 10:38:42.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/05/23 10:38:42.725
    STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 10:38:42.726
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/05/23 10:38:42.854
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/05/23 10:38:43.866
    STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 10:38:43.866
    STEP: Having no error when timeout is longer than webhook latency 01/05/23 10:38:44.969
    STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 10:38:44.969
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/05/23 10:38:50.157
    STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 10:38:50.157
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:38:55.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7418" for this suite. 01/05/23 10:38:55.207
    STEP: Destroying namespace "webhook-7418-markers" for this suite. 01/05/23 10:38:55.212
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:38:55.247
Jan  5 10:38:55.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 10:38:55.248
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:55.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:55.295
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jan  5 10:38:55.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 create -f -'
Jan  5 10:38:55.880: INFO: stderr: ""
Jan  5 10:38:55.880: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan  5 10:38:55.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 create -f -'
Jan  5 10:38:56.408: INFO: stderr: ""
Jan  5 10:38:56.408: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/05/23 10:38:56.408
Jan  5 10:38:57.414: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 10:38:57.414: INFO: Found 1 / 1
Jan  5 10:38:57.414: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  5 10:38:57.419: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 10:38:57.419: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  5 10:38:57.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 describe pod agnhost-primary-5nrg7'
Jan  5 10:38:57.509: INFO: stderr: ""
Jan  5 10:38:57.509: INFO: stdout: "Name:             agnhost-primary-5nrg7\nNamespace:        kubectl-772\nPriority:         0\nService Account:  default\nNode:             shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c/10.250.0.234\nStart Time:       Thu, 05 Jan 2023 10:38:55 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.96.4.6\nIPs:\n  IP:           10.96.4.6\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://a68992b526d278c5047f4eb720a43621eb7e2b30ff94650603083d125bb26882\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 05 Jan 2023 10:38:56 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bv868 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bv868:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-772/agnhost-primary-5nrg7 to shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan  5 10:38:57.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 describe rc agnhost-primary'
Jan  5 10:38:57.588: INFO: stderr: ""
Jan  5 10:38:57.588: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-772\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-5nrg7\n"
Jan  5 10:38:57.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 describe service agnhost-primary'
Jan  5 10:38:57.665: INFO: stderr: ""
Jan  5 10:38:57.665: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-772\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.119.247.114\nIPs:               10.119.247.114\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.96.4.6:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan  5 10:38:57.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 describe node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t'
Jan  5 10:38:57.794: INFO: stderr: ""
Jan  5 10:38:57.794: INFO: stdout: "Name:               shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=e1295f38-4b08-436d-8351-47b93c7ae368\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=prod1\n                    failure-domain.beta.kubernetes.io/zone=az1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t\n                    kubernetes.io/os=linux\n                    networking.gardener.cloud/node-local-dns-enabled=false\n                    node.kubernetes.io/instance-type=e1295f38-4b08-436d-8351-47b93c7ae368\n                    node.kubernetes.io/role=node\n                    topology.cinder.csi.openstack.org/zone=az1\n                    topology.kubernetes.io/region=prod1\n                    topology.kubernetes.io/zone=az1\n                    worker.garden.sapcloud.io/group=worker-omyby\n                    worker.gardener.cloud/cri-name=containerd\n                    worker.gardener.cloud/kubernetes-version=1.25.5\n                    worker.gardener.cloud/pool=worker-omyby\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: c1273b15433cc6a7a35db464a3459d914f0dd9156dac1b98c00639e7ea76e14f\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"c517aa67-e4fb-46cb-b23b-3799e373f1b9\"}\n                    io.cilium.network.ipv4-cilium-host: 10.96.3.32\n                    io.cilium.network.ipv4-health-ip: 10.96.3.13\n                    io.cilium.network.ipv4-pod-cidr: 10.96.3.0/24\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"kubernetes.io/arch\":\"amd64\",\"networking.gardener.cloud/node-local-dns-enabled\":\"false\",\"n...\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 05 Jan 2023 08:52:42 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 05 Jan 2023 10:38:54 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  FrequentContainerdRestart     False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  FrequentUnregisterNetDevice   False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  KernelDeadlock                False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  CorruptDockerOverlay2         False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentKubeletRestart        False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   NoFrequentDockerRestart         docker is functioning properly\n  NetworkUnavailable            False   Thu, 05 Jan 2023 08:53:36 +0000   Thu, 05 Jan 2023 08:53:36 +0000   CiliumIsUp                      Cilium is running on this node\n  MemoryPressure                False   Thu, 05 Jan 2023 10:38:53 +0000   Thu, 05 Jan 2023 08:52:42 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Thu, 05 Jan 2023 10:38:53 +0000   Thu, 05 Jan 2023 08:52:42 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Thu, 05 Jan 2023 10:38:53 +0000   Thu, 05 Jan 2023 08:52:42 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Thu, 05 Jan 2023 10:38:53 +0000   Thu, 05 Jan 2023 08:53:43 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.250.0.128\n  Hostname:    shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t\nCapacity:\n  cpu:                4\n  ephemeral-storage:  101430960Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16385532Ki\n  pods:               110\nAllocatable:\n  cpu:                3920m\n  ephemeral-storage:  98672037811\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15234556Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c517aa67e4fb46cbb23b3799e373f1b9\n  System UUID:                c517aa67-e4fb-46cb-b23b-3799e373f1b9\n  Boot ID:                    3500de07-f146-45ad-8d70-10370b65e341\n  Kernel Version:             5.15.0-53-generic\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.9-0ubuntu3.1\n  Kubelet Version:            v1.25.5\n  Kube-Proxy Version:         v1.25.5\nPodCIDR:                      10.96.3.0/24\nPodCIDRs:                     10.96.3.0/24\nProviderID:                   openstack:///c517aa67-e4fb-46cb-b23b-3799e373f1b9\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 apiserver-proxy-m4wgr                                      40m (1%)      0 (0%)      40Mi (0%)        1114Mi (7%)    106m\n  kube-system                 cilium-5qvr2                                               100m (2%)     0 (0%)      200Mi (1%)       1Gi (6%)       106m\n  kube-system                 csi-driver-node-nt6bm                                      37m (0%)      0 (0%)      106Mi (0%)       3272Mi (21%)   106m\n  kube-system                 kube-proxy-worker-omyby-v1.25.5-xvlqq                      20m (0%)      0 (0%)      64Mi (0%)        0 (0%)         106m\n  kube-system                 node-exporter-hckzs                                        50m (1%)      0 (0%)      50Mi (0%)        250Mi (1%)     106m\n  kube-system                 node-problem-detector-5ln6w                                20m (0%)      0 (0%)      20Mi (0%)        500Mi (3%)     106m\n  sonobuoy                    sonobuoy-e2e-job-2d2d23fb3e814b3d                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-m6xzl    0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                267m (6%)   0 (0%)\n  memory             480Mi (3%)  6160Mi (41%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Jan  5 10:38:57.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 describe namespace kubectl-772'
Jan  5 10:38:57.882: INFO: stderr: ""
Jan  5 10:38:57.882: INFO: stdout: "Name:         kubectl-772\nLabels:       e2e-framework=kubectl\n              e2e-run=2e867fcd-3821-44ab-8131-3ddf12f91b07\n              kubernetes.io/metadata.name=kubectl-772\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 10:38:57.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-772" for this suite. 01/05/23 10:38:57.892
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":333,"skipped":6115,"failed":0}
------------------------------
â€¢ [2.650 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:38:55.247
    Jan  5 10:38:55.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 10:38:55.248
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:55.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:55.295
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jan  5 10:38:55.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 create -f -'
    Jan  5 10:38:55.880: INFO: stderr: ""
    Jan  5 10:38:55.880: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan  5 10:38:55.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 create -f -'
    Jan  5 10:38:56.408: INFO: stderr: ""
    Jan  5 10:38:56.408: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/05/23 10:38:56.408
    Jan  5 10:38:57.414: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 10:38:57.414: INFO: Found 1 / 1
    Jan  5 10:38:57.414: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan  5 10:38:57.419: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 10:38:57.419: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  5 10:38:57.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 describe pod agnhost-primary-5nrg7'
    Jan  5 10:38:57.509: INFO: stderr: ""
    Jan  5 10:38:57.509: INFO: stdout: "Name:             agnhost-primary-5nrg7\nNamespace:        kubectl-772\nPriority:         0\nService Account:  default\nNode:             shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c/10.250.0.234\nStart Time:       Thu, 05 Jan 2023 10:38:55 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.96.4.6\nIPs:\n  IP:           10.96.4.6\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://a68992b526d278c5047f4eb720a43621eb7e2b30ff94650603083d125bb26882\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 05 Jan 2023 10:38:56 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bv868 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bv868:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-772/agnhost-primary-5nrg7 to shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Jan  5 10:38:57.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 describe rc agnhost-primary'
    Jan  5 10:38:57.588: INFO: stderr: ""
    Jan  5 10:38:57.588: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-772\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-5nrg7\n"
    Jan  5 10:38:57.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 describe service agnhost-primary'
    Jan  5 10:38:57.665: INFO: stderr: ""
    Jan  5 10:38:57.665: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-772\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.119.247.114\nIPs:               10.119.247.114\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.96.4.6:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan  5 10:38:57.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 describe node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t'
    Jan  5 10:38:57.794: INFO: stderr: ""
    Jan  5 10:38:57.794: INFO: stdout: "Name:               shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=e1295f38-4b08-436d-8351-47b93c7ae368\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=prod1\n                    failure-domain.beta.kubernetes.io/zone=az1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t\n                    kubernetes.io/os=linux\n                    networking.gardener.cloud/node-local-dns-enabled=false\n                    node.kubernetes.io/instance-type=e1295f38-4b08-436d-8351-47b93c7ae368\n                    node.kubernetes.io/role=node\n                    topology.cinder.csi.openstack.org/zone=az1\n                    topology.kubernetes.io/region=prod1\n                    topology.kubernetes.io/zone=az1\n                    worker.garden.sapcloud.io/group=worker-omyby\n                    worker.gardener.cloud/cri-name=containerd\n                    worker.gardener.cloud/kubernetes-version=1.25.5\n                    worker.gardener.cloud/pool=worker-omyby\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: c1273b15433cc6a7a35db464a3459d914f0dd9156dac1b98c00639e7ea76e14f\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"c517aa67-e4fb-46cb-b23b-3799e373f1b9\"}\n                    io.cilium.network.ipv4-cilium-host: 10.96.3.32\n                    io.cilium.network.ipv4-health-ip: 10.96.3.13\n                    io.cilium.network.ipv4-pod-cidr: 10.96.3.0/24\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"kubernetes.io/arch\":\"amd64\",\"networking.gardener.cloud/node-local-dns-enabled\":\"false\",\"n...\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 05 Jan 2023 08:52:42 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 05 Jan 2023 10:38:54 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  FrequentContainerdRestart     False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  FrequentUnregisterNetDevice   False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  KernelDeadlock                False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  CorruptDockerOverlay2         False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentKubeletRestart        False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Thu, 05 Jan 2023 10:33:56 +0000   Thu, 05 Jan 2023 08:53:41 +0000   NoFrequentDockerRestart         docker is functioning properly\n  NetworkUnavailable            False   Thu, 05 Jan 2023 08:53:36 +0000   Thu, 05 Jan 2023 08:53:36 +0000   CiliumIsUp                      Cilium is running on this node\n  MemoryPressure                False   Thu, 05 Jan 2023 10:38:53 +0000   Thu, 05 Jan 2023 08:52:42 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Thu, 05 Jan 2023 10:38:53 +0000   Thu, 05 Jan 2023 08:52:42 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Thu, 05 Jan 2023 10:38:53 +0000   Thu, 05 Jan 2023 08:52:42 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Thu, 05 Jan 2023 10:38:53 +0000   Thu, 05 Jan 2023 08:53:43 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.250.0.128\n  Hostname:    shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t\nCapacity:\n  cpu:                4\n  ephemeral-storage:  101430960Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16385532Ki\n  pods:               110\nAllocatable:\n  cpu:                3920m\n  ephemeral-storage:  98672037811\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15234556Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c517aa67e4fb46cbb23b3799e373f1b9\n  System UUID:                c517aa67-e4fb-46cb-b23b-3799e373f1b9\n  Boot ID:                    3500de07-f146-45ad-8d70-10370b65e341\n  Kernel Version:             5.15.0-53-generic\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.9-0ubuntu3.1\n  Kubelet Version:            v1.25.5\n  Kube-Proxy Version:         v1.25.5\nPodCIDR:                      10.96.3.0/24\nPodCIDRs:                     10.96.3.0/24\nProviderID:                   openstack:///c517aa67-e4fb-46cb-b23b-3799e373f1b9\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 apiserver-proxy-m4wgr                                      40m (1%)      0 (0%)      40Mi (0%)        1114Mi (7%)    106m\n  kube-system                 cilium-5qvr2                                               100m (2%)     0 (0%)      200Mi (1%)       1Gi (6%)       106m\n  kube-system                 csi-driver-node-nt6bm                                      37m (0%)      0 (0%)      106Mi (0%)       3272Mi (21%)   106m\n  kube-system                 kube-proxy-worker-omyby-v1.25.5-xvlqq                      20m (0%)      0 (0%)      64Mi (0%)        0 (0%)         106m\n  kube-system                 node-exporter-hckzs                                        50m (1%)      0 (0%)      50Mi (0%)        250Mi (1%)     106m\n  kube-system                 node-problem-detector-5ln6w                                20m (0%)      0 (0%)      20Mi (0%)        500Mi (3%)     106m\n  sonobuoy                    sonobuoy-e2e-job-2d2d23fb3e814b3d                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-m6xzl    0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                267m (6%)   0 (0%)\n  memory             480Mi (3%)  6160Mi (41%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
    Jan  5 10:38:57.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-772 describe namespace kubectl-772'
    Jan  5 10:38:57.882: INFO: stderr: ""
    Jan  5 10:38:57.882: INFO: stdout: "Name:         kubectl-772\nLabels:       e2e-framework=kubectl\n              e2e-run=2e867fcd-3821-44ab-8131-3ddf12f91b07\n              kubernetes.io/metadata.name=kubectl-772\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 10:38:57.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-772" for this suite. 01/05/23 10:38:57.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:38:57.898
Jan  5 10:38:57.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename dns 01/05/23 10:38:57.899
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:57.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:57.922
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/05/23 10:38:57.928
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2130.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2130.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/05/23 10:38:57.934
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2130.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2130.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/05/23 10:38:57.934
STEP: creating a pod to probe DNS 01/05/23 10:38:57.934
STEP: submitting the pod to kubernetes 01/05/23 10:38:57.934
Jan  5 10:38:57.949: INFO: Waiting up to 15m0s for pod "dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380" in namespace "dns-2130" to be "running"
Jan  5 10:38:57.956: INFO: Pod "dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380": Phase="Pending", Reason="", readiness=false. Elapsed: 6.794887ms
Jan  5 10:38:59.963: INFO: Pod "dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380": Phase="Running", Reason="", readiness=true. Elapsed: 2.01411922s
Jan  5 10:38:59.963: INFO: Pod "dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380" satisfied condition "running"
STEP: retrieving the pod 01/05/23 10:38:59.963
STEP: looking for the results for each expected name from probers 01/05/23 10:38:59.968
Jan  5 10:39:00.149: INFO: Unable to read jessie_hosts@dns-querier-2.dns-test-service-2.dns-2130.svc.cluster.local from pod dns-2130/dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380: the server could not find the requested resource (get pods dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380)
Jan  5 10:39:00.203: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-2130/dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380: the server could not find the requested resource (get pods dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380)
Jan  5 10:39:00.203: INFO: Lookups using dns-2130/dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380 failed for: [jessie_hosts@dns-querier-2.dns-test-service-2.dns-2130.svc.cluster.local jessie_hosts@dns-querier-2]

Jan  5 10:39:05.287: INFO: DNS probes using dns-2130/dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380 succeeded

STEP: deleting the pod 01/05/23 10:39:05.287
STEP: deleting the test headless service 01/05/23 10:39:05.299
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 10:39:05.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2130" for this suite. 01/05/23 10:39:05.323
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":334,"skipped":6125,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.429 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:38:57.898
    Jan  5 10:38:57.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename dns 01/05/23 10:38:57.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:38:57.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:38:57.922
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/05/23 10:38:57.928
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2130.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2130.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/05/23 10:38:57.934
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2130.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2130.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/05/23 10:38:57.934
    STEP: creating a pod to probe DNS 01/05/23 10:38:57.934
    STEP: submitting the pod to kubernetes 01/05/23 10:38:57.934
    Jan  5 10:38:57.949: INFO: Waiting up to 15m0s for pod "dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380" in namespace "dns-2130" to be "running"
    Jan  5 10:38:57.956: INFO: Pod "dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380": Phase="Pending", Reason="", readiness=false. Elapsed: 6.794887ms
    Jan  5 10:38:59.963: INFO: Pod "dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380": Phase="Running", Reason="", readiness=true. Elapsed: 2.01411922s
    Jan  5 10:38:59.963: INFO: Pod "dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 10:38:59.963
    STEP: looking for the results for each expected name from probers 01/05/23 10:38:59.968
    Jan  5 10:39:00.149: INFO: Unable to read jessie_hosts@dns-querier-2.dns-test-service-2.dns-2130.svc.cluster.local from pod dns-2130/dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380: the server could not find the requested resource (get pods dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380)
    Jan  5 10:39:00.203: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-2130/dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380: the server could not find the requested resource (get pods dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380)
    Jan  5 10:39:00.203: INFO: Lookups using dns-2130/dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380 failed for: [jessie_hosts@dns-querier-2.dns-test-service-2.dns-2130.svc.cluster.local jessie_hosts@dns-querier-2]

    Jan  5 10:39:05.287: INFO: DNS probes using dns-2130/dns-test-80d2db4d-8858-4030-9ec2-ddbe1a56d380 succeeded

    STEP: deleting the pod 01/05/23 10:39:05.287
    STEP: deleting the test headless service 01/05/23 10:39:05.299
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 10:39:05.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2130" for this suite. 01/05/23 10:39:05.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:39:05.329
Jan  5 10:39:05.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename pod-network-test 01/05/23 10:39:05.33
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:39:05.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:39:05.36
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-8254 01/05/23 10:39:05.366
STEP: creating a selector 01/05/23 10:39:05.366
STEP: Creating the service pods in kubernetes 01/05/23 10:39:05.366
Jan  5 10:39:05.366: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  5 10:39:05.423: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8254" to be "running and ready"
Jan  5 10:39:05.431: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.981547ms
Jan  5 10:39:05.431: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:39:07.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015222796s
Jan  5 10:39:07.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:39:09.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014514994s
Jan  5 10:39:09.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:39:11.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013457148s
Jan  5 10:39:11.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:39:13.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016633266s
Jan  5 10:39:13.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:39:15.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015895118s
Jan  5 10:39:15.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:39:17.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014182005s
Jan  5 10:39:17.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:39:19.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015992973s
Jan  5 10:39:19.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:39:21.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012564376s
Jan  5 10:39:21.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:39:23.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013579973s
Jan  5 10:39:23.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:39:25.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014911358s
Jan  5 10:39:25.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 10:39:27.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015056653s
Jan  5 10:39:27.439: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  5 10:39:27.439: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  5 10:39:27.444: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8254" to be "running and ready"
Jan  5 10:39:27.448: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.357257ms
Jan  5 10:39:27.448: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  5 10:39:27.448: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  5 10:39:27.452: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8254" to be "running and ready"
Jan  5 10:39:27.457: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.480647ms
Jan  5 10:39:27.457: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  5 10:39:27.457: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan  5 10:39:27.461: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8254" to be "running and ready"
Jan  5 10:39:27.465: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.430564ms
Jan  5 10:39:27.465: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan  5 10:39:27.465: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jan  5 10:39:27.468: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-8254" to be "running and ready"
Jan  5 10:39:27.472: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.63993ms
Jan  5 10:39:27.472: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jan  5 10:39:27.472: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 01/05/23 10:39:27.476
Jan  5 10:39:27.483: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8254" to be "running"
Jan  5 10:39:27.488: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.751061ms
Jan  5 10:39:29.494: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010741001s
Jan  5 10:39:29.494: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  5 10:39:29.503: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jan  5 10:39:29.504: INFO: Breadth first check of 10.96.3.110 on host 10.250.0.128...
Jan  5 10:39:29.508: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.4.166:9080/dial?request=hostname&protocol=http&host=10.96.3.110&port=8083&tries=1'] Namespace:pod-network-test-8254 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:39:29.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:39:29.509: INFO: ExecWithOptions: Clientset creation
Jan  5 10:39:29.509: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-8254/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.4.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.3.110%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 10:39:29.820: INFO: Waiting for responses: map[]
Jan  5 10:39:29.820: INFO: reached 10.96.3.110 after 0/1 tries
Jan  5 10:39:29.820: INFO: Breadth first check of 10.96.1.225 on host 10.250.1.19...
Jan  5 10:39:29.831: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.4.166:9080/dial?request=hostname&protocol=http&host=10.96.1.225&port=8083&tries=1'] Namespace:pod-network-test-8254 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:39:29.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:39:29.831: INFO: ExecWithOptions: Clientset creation
Jan  5 10:39:29.832: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-8254/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.4.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.1.225%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 10:39:30.289: INFO: Waiting for responses: map[]
Jan  5 10:39:30.289: INFO: reached 10.96.1.225 after 0/1 tries
Jan  5 10:39:30.289: INFO: Breadth first check of 10.96.0.197 on host 10.250.0.174...
Jan  5 10:39:30.294: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.4.166:9080/dial?request=hostname&protocol=http&host=10.96.0.197&port=8083&tries=1'] Namespace:pod-network-test-8254 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:39:30.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:39:30.294: INFO: ExecWithOptions: Clientset creation
Jan  5 10:39:30.294: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-8254/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.4.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.0.197%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 10:39:30.712: INFO: Waiting for responses: map[]
Jan  5 10:39:30.712: INFO: reached 10.96.0.197 after 0/1 tries
Jan  5 10:39:30.712: INFO: Breadth first check of 10.96.2.229 on host 10.250.2.138...
Jan  5 10:39:30.717: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.4.166:9080/dial?request=hostname&protocol=http&host=10.96.2.229&port=8083&tries=1'] Namespace:pod-network-test-8254 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:39:30.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:39:30.718: INFO: ExecWithOptions: Clientset creation
Jan  5 10:39:30.718: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-8254/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.4.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.2.229%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 10:39:31.196: INFO: Waiting for responses: map[]
Jan  5 10:39:31.196: INFO: reached 10.96.2.229 after 0/1 tries
Jan  5 10:39:31.196: INFO: Breadth first check of 10.96.4.236 on host 10.250.0.234...
Jan  5 10:39:31.202: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.4.166:9080/dial?request=hostname&protocol=http&host=10.96.4.236&port=8083&tries=1'] Namespace:pod-network-test-8254 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:39:31.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:39:31.202: INFO: ExecWithOptions: Clientset creation
Jan  5 10:39:31.202: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-8254/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.4.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.4.236%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 10:39:31.764: INFO: Waiting for responses: map[]
Jan  5 10:39:31.764: INFO: reached 10.96.4.236 after 0/1 tries
Jan  5 10:39:31.764: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan  5 10:39:31.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8254" for this suite. 01/05/23 10:39:31.773
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":335,"skipped":6138,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.450 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:39:05.329
    Jan  5 10:39:05.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename pod-network-test 01/05/23 10:39:05.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:39:05.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:39:05.36
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-8254 01/05/23 10:39:05.366
    STEP: creating a selector 01/05/23 10:39:05.366
    STEP: Creating the service pods in kubernetes 01/05/23 10:39:05.366
    Jan  5 10:39:05.366: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  5 10:39:05.423: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8254" to be "running and ready"
    Jan  5 10:39:05.431: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.981547ms
    Jan  5 10:39:05.431: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:39:07.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015222796s
    Jan  5 10:39:07.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:39:09.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014514994s
    Jan  5 10:39:09.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:39:11.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013457148s
    Jan  5 10:39:11.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:39:13.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016633266s
    Jan  5 10:39:13.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:39:15.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015895118s
    Jan  5 10:39:15.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:39:17.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014182005s
    Jan  5 10:39:17.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:39:19.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015992973s
    Jan  5 10:39:19.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:39:21.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012564376s
    Jan  5 10:39:21.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:39:23.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013579973s
    Jan  5 10:39:23.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:39:25.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014911358s
    Jan  5 10:39:25.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 10:39:27.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015056653s
    Jan  5 10:39:27.439: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  5 10:39:27.439: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  5 10:39:27.444: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8254" to be "running and ready"
    Jan  5 10:39:27.448: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.357257ms
    Jan  5 10:39:27.448: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  5 10:39:27.448: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  5 10:39:27.452: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8254" to be "running and ready"
    Jan  5 10:39:27.457: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.480647ms
    Jan  5 10:39:27.457: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  5 10:39:27.457: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan  5 10:39:27.461: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8254" to be "running and ready"
    Jan  5 10:39:27.465: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.430564ms
    Jan  5 10:39:27.465: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan  5 10:39:27.465: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jan  5 10:39:27.468: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-8254" to be "running and ready"
    Jan  5 10:39:27.472: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.63993ms
    Jan  5 10:39:27.472: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jan  5 10:39:27.472: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 01/05/23 10:39:27.476
    Jan  5 10:39:27.483: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8254" to be "running"
    Jan  5 10:39:27.488: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.751061ms
    Jan  5 10:39:29.494: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010741001s
    Jan  5 10:39:29.494: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  5 10:39:29.503: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Jan  5 10:39:29.504: INFO: Breadth first check of 10.96.3.110 on host 10.250.0.128...
    Jan  5 10:39:29.508: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.4.166:9080/dial?request=hostname&protocol=http&host=10.96.3.110&port=8083&tries=1'] Namespace:pod-network-test-8254 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:39:29.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:39:29.509: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:39:29.509: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-8254/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.4.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.3.110%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 10:39:29.820: INFO: Waiting for responses: map[]
    Jan  5 10:39:29.820: INFO: reached 10.96.3.110 after 0/1 tries
    Jan  5 10:39:29.820: INFO: Breadth first check of 10.96.1.225 on host 10.250.1.19...
    Jan  5 10:39:29.831: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.4.166:9080/dial?request=hostname&protocol=http&host=10.96.1.225&port=8083&tries=1'] Namespace:pod-network-test-8254 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:39:29.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:39:29.831: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:39:29.832: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-8254/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.4.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.1.225%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 10:39:30.289: INFO: Waiting for responses: map[]
    Jan  5 10:39:30.289: INFO: reached 10.96.1.225 after 0/1 tries
    Jan  5 10:39:30.289: INFO: Breadth first check of 10.96.0.197 on host 10.250.0.174...
    Jan  5 10:39:30.294: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.4.166:9080/dial?request=hostname&protocol=http&host=10.96.0.197&port=8083&tries=1'] Namespace:pod-network-test-8254 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:39:30.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:39:30.294: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:39:30.294: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-8254/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.4.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.0.197%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 10:39:30.712: INFO: Waiting for responses: map[]
    Jan  5 10:39:30.712: INFO: reached 10.96.0.197 after 0/1 tries
    Jan  5 10:39:30.712: INFO: Breadth first check of 10.96.2.229 on host 10.250.2.138...
    Jan  5 10:39:30.717: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.4.166:9080/dial?request=hostname&protocol=http&host=10.96.2.229&port=8083&tries=1'] Namespace:pod-network-test-8254 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:39:30.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:39:30.718: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:39:30.718: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-8254/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.4.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.2.229%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 10:39:31.196: INFO: Waiting for responses: map[]
    Jan  5 10:39:31.196: INFO: reached 10.96.2.229 after 0/1 tries
    Jan  5 10:39:31.196: INFO: Breadth first check of 10.96.4.236 on host 10.250.0.234...
    Jan  5 10:39:31.202: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.4.166:9080/dial?request=hostname&protocol=http&host=10.96.4.236&port=8083&tries=1'] Namespace:pod-network-test-8254 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:39:31.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:39:31.202: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:39:31.202: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/pod-network-test-8254/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.4.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.4.236%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 10:39:31.764: INFO: Waiting for responses: map[]
    Jan  5 10:39:31.764: INFO: reached 10.96.4.236 after 0/1 tries
    Jan  5 10:39:31.764: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan  5 10:39:31.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8254" for this suite. 01/05/23 10:39:31.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:39:31.779
Jan  5 10:39:31.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 10:39:31.78
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:39:31.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:39:31.798
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-6c356f39-02e8-4c1d-81b2-91bc99b7fa8f 01/05/23 10:39:31.804
STEP: Creating a pod to test consume secrets 01/05/23 10:39:31.808
Jan  5 10:39:31.822: INFO: Waiting up to 5m0s for pod "pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4" in namespace "secrets-6765" to be "Succeeded or Failed"
Jan  5 10:39:31.828: INFO: Pod "pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.812999ms
Jan  5 10:39:33.834: INFO: Pod "pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012173858s
Jan  5 10:39:35.833: INFO: Pod "pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011546729s
STEP: Saw pod success 01/05/23 10:39:35.833
Jan  5 10:39:35.833: INFO: Pod "pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4" satisfied condition "Succeeded or Failed"
Jan  5 10:39:35.838: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 10:39:35.854
Jan  5 10:39:35.866: INFO: Waiting for pod pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4 to disappear
Jan  5 10:39:35.871: INFO: Pod pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 10:39:35.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6765" for this suite. 01/05/23 10:39:35.881
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":336,"skipped":6153,"failed":0}
------------------------------
â€¢ [4.108 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:39:31.779
    Jan  5 10:39:31.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 10:39:31.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:39:31.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:39:31.798
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-6c356f39-02e8-4c1d-81b2-91bc99b7fa8f 01/05/23 10:39:31.804
    STEP: Creating a pod to test consume secrets 01/05/23 10:39:31.808
    Jan  5 10:39:31.822: INFO: Waiting up to 5m0s for pod "pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4" in namespace "secrets-6765" to be "Succeeded or Failed"
    Jan  5 10:39:31.828: INFO: Pod "pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.812999ms
    Jan  5 10:39:33.834: INFO: Pod "pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012173858s
    Jan  5 10:39:35.833: INFO: Pod "pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011546729s
    STEP: Saw pod success 01/05/23 10:39:35.833
    Jan  5 10:39:35.833: INFO: Pod "pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4" satisfied condition "Succeeded or Failed"
    Jan  5 10:39:35.838: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 10:39:35.854
    Jan  5 10:39:35.866: INFO: Waiting for pod pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4 to disappear
    Jan  5 10:39:35.871: INFO: Pod pod-secrets-7f9262c2-b28d-4baa-92b0-b0971bbd0fd4 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 10:39:35.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6765" for this suite. 01/05/23 10:39:35.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:39:35.89
Jan  5 10:39:35.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 10:39:35.892
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:39:35.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:39:35.912
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 10:39:35.93
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:39:36.291
STEP: Deploying the webhook pod 01/05/23 10:39:36.299
STEP: Wait for the deployment to be ready 01/05/23 10:39:36.308
Jan  5 10:39:36.316: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 10:39:38.331
STEP: Verifying the service has paired with the endpoint 01/05/23 10:39:38.343
Jan  5 10:39:39.344: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/05/23 10:39:39.349
STEP: create a configmap that should be updated by the webhook 01/05/23 10:39:39.475
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:39:39.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4365" for this suite. 01/05/23 10:39:39.72
STEP: Destroying namespace "webhook-4365-markers" for this suite. 01/05/23 10:39:39.725
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":337,"skipped":6164,"failed":0}
------------------------------
â€¢ [3.874 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:39:35.89
    Jan  5 10:39:35.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 10:39:35.892
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:39:35.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:39:35.912
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 10:39:35.93
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:39:36.291
    STEP: Deploying the webhook pod 01/05/23 10:39:36.299
    STEP: Wait for the deployment to be ready 01/05/23 10:39:36.308
    Jan  5 10:39:36.316: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 10:39:38.331
    STEP: Verifying the service has paired with the endpoint 01/05/23 10:39:38.343
    Jan  5 10:39:39.344: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/05/23 10:39:39.349
    STEP: create a configmap that should be updated by the webhook 01/05/23 10:39:39.475
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:39:39.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4365" for this suite. 01/05/23 10:39:39.72
    STEP: Destroying namespace "webhook-4365-markers" for this suite. 01/05/23 10:39:39.725
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:39:39.772
Jan  5 10:39:39.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename disruption 01/05/23 10:39:39.772
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:39:39.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:39:39.797
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 01/05/23 10:39:39.804
STEP: Waiting for the pdb to be processed 01/05/23 10:39:39.809
STEP: First trying to evict a pod which shouldn't be evictable 01/05/23 10:39:41.823
STEP: Waiting for all pods to be running 01/05/23 10:39:41.823
Jan  5 10:39:41.826: INFO: pods: 0 < 3
STEP: locating a running pod 01/05/23 10:39:43.832
STEP: Updating the pdb to allow a pod to be evicted 01/05/23 10:39:43.843
STEP: Waiting for the pdb to be processed 01/05/23 10:39:43.852
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/05/23 10:39:45.86
STEP: Waiting for all pods to be running 01/05/23 10:39:45.861
STEP: Waiting for the pdb to observed all healthy pods 01/05/23 10:39:45.869
STEP: Patching the pdb to disallow a pod to be evicted 01/05/23 10:39:45.89
STEP: Waiting for the pdb to be processed 01/05/23 10:39:45.908
STEP: Waiting for all pods to be running 01/05/23 10:39:47.917
STEP: locating a running pod 01/05/23 10:39:47.922
STEP: Deleting the pdb to allow a pod to be evicted 01/05/23 10:39:47.932
STEP: Waiting for the pdb to be deleted 01/05/23 10:39:47.937
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/05/23 10:39:47.941
STEP: Waiting for all pods to be running 01/05/23 10:39:47.941
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  5 10:39:47.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4437" for this suite. 01/05/23 10:39:47.963
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":338,"skipped":6257,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.252 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:39:39.772
    Jan  5 10:39:39.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename disruption 01/05/23 10:39:39.772
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:39:39.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:39:39.797
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 01/05/23 10:39:39.804
    STEP: Waiting for the pdb to be processed 01/05/23 10:39:39.809
    STEP: First trying to evict a pod which shouldn't be evictable 01/05/23 10:39:41.823
    STEP: Waiting for all pods to be running 01/05/23 10:39:41.823
    Jan  5 10:39:41.826: INFO: pods: 0 < 3
    STEP: locating a running pod 01/05/23 10:39:43.832
    STEP: Updating the pdb to allow a pod to be evicted 01/05/23 10:39:43.843
    STEP: Waiting for the pdb to be processed 01/05/23 10:39:43.852
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/05/23 10:39:45.86
    STEP: Waiting for all pods to be running 01/05/23 10:39:45.861
    STEP: Waiting for the pdb to observed all healthy pods 01/05/23 10:39:45.869
    STEP: Patching the pdb to disallow a pod to be evicted 01/05/23 10:39:45.89
    STEP: Waiting for the pdb to be processed 01/05/23 10:39:45.908
    STEP: Waiting for all pods to be running 01/05/23 10:39:47.917
    STEP: locating a running pod 01/05/23 10:39:47.922
    STEP: Deleting the pdb to allow a pod to be evicted 01/05/23 10:39:47.932
    STEP: Waiting for the pdb to be deleted 01/05/23 10:39:47.937
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/05/23 10:39:47.941
    STEP: Waiting for all pods to be running 01/05/23 10:39:47.941
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  5 10:39:47.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4437" for this suite. 01/05/23 10:39:47.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:39:48.025
Jan  5 10:39:48.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename statefulset 01/05/23 10:39:48.026
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:39:48.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:39:48.054
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5786 01/05/23 10:39:48.06
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jan  5 10:39:48.075: INFO: Found 0 stateful pods, waiting for 1
Jan  5 10:39:58.082: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/05/23 10:39:58.09
W0105 10:39:58.096241      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  5 10:39:58.104: INFO: Found 1 stateful pods, waiting for 2
Jan  5 10:40:08.112: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 10:40:08.112: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/05/23 10:40:08.121
STEP: Delete all of the StatefulSets 01/05/23 10:40:08.124
STEP: Verify that StatefulSets have been deleted 01/05/23 10:40:08.131
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 10:40:08.176: INFO: Deleting all statefulset in ns statefulset-5786
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 10:40:08.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5786" for this suite. 01/05/23 10:40:08.205
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":339,"skipped":6268,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.185 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:39:48.025
    Jan  5 10:39:48.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename statefulset 01/05/23 10:39:48.026
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:39:48.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:39:48.054
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5786 01/05/23 10:39:48.06
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jan  5 10:39:48.075: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 10:39:58.082: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/05/23 10:39:58.09
    W0105 10:39:58.096241      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  5 10:39:58.104: INFO: Found 1 stateful pods, waiting for 2
    Jan  5 10:40:08.112: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 10:40:08.112: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/05/23 10:40:08.121
    STEP: Delete all of the StatefulSets 01/05/23 10:40:08.124
    STEP: Verify that StatefulSets have been deleted 01/05/23 10:40:08.131
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 10:40:08.176: INFO: Deleting all statefulset in ns statefulset-5786
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 10:40:08.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5786" for this suite. 01/05/23 10:40:08.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:40:08.211
Jan  5 10:40:08.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename projected 01/05/23 10:40:08.212
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:08.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:08.233
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-c26ebbf4-1c8b-4a49-88c1-5d494163ac41 01/05/23 10:40:08.246
STEP: Creating a pod to test consume configMaps 01/05/23 10:40:08.251
Jan  5 10:40:08.260: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03" in namespace "projected-2487" to be "Succeeded or Failed"
Jan  5 10:40:08.265: INFO: Pod "pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.930757ms
Jan  5 10:40:10.271: INFO: Pod "pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010994454s
Jan  5 10:40:12.272: INFO: Pod "pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011814181s
STEP: Saw pod success 01/05/23 10:40:12.272
Jan  5 10:40:12.272: INFO: Pod "pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03" satisfied condition "Succeeded or Failed"
Jan  5 10:40:12.276: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 10:40:12.293
Jan  5 10:40:12.303: INFO: Waiting for pod pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03 to disappear
Jan  5 10:40:12.307: INFO: Pod pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 10:40:12.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2487" for this suite. 01/05/23 10:40:12.317
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":340,"skipped":6273,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:40:08.211
    Jan  5 10:40:08.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename projected 01/05/23 10:40:08.212
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:08.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:08.233
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-c26ebbf4-1c8b-4a49-88c1-5d494163ac41 01/05/23 10:40:08.246
    STEP: Creating a pod to test consume configMaps 01/05/23 10:40:08.251
    Jan  5 10:40:08.260: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03" in namespace "projected-2487" to be "Succeeded or Failed"
    Jan  5 10:40:08.265: INFO: Pod "pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.930757ms
    Jan  5 10:40:10.271: INFO: Pod "pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010994454s
    Jan  5 10:40:12.272: INFO: Pod "pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011814181s
    STEP: Saw pod success 01/05/23 10:40:12.272
    Jan  5 10:40:12.272: INFO: Pod "pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03" satisfied condition "Succeeded or Failed"
    Jan  5 10:40:12.276: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 10:40:12.293
    Jan  5 10:40:12.303: INFO: Waiting for pod pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03 to disappear
    Jan  5 10:40:12.307: INFO: Pod pod-projected-configmaps-27dc0884-d3a4-4cc1-831c-c3f00dda7f03 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 10:40:12.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2487" for this suite. 01/05/23 10:40:12.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:40:12.325
Jan  5 10:40:12.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 10:40:12.325
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:12.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:12.346
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 01/05/23 10:40:12.352
Jan  5 10:40:12.361: INFO: Waiting up to 5m0s for pod "pod-36480e7f-effc-47f4-889a-5e27d4b3bc03" in namespace "emptydir-6065" to be "Succeeded or Failed"
Jan  5 10:40:12.370: INFO: Pod "pod-36480e7f-effc-47f4-889a-5e27d4b3bc03": Phase="Pending", Reason="", readiness=false. Elapsed: 8.67937ms
Jan  5 10:40:14.377: INFO: Pod "pod-36480e7f-effc-47f4-889a-5e27d4b3bc03": Phase="Running", Reason="", readiness=false. Elapsed: 2.015796568s
Jan  5 10:40:16.404: INFO: Pod "pod-36480e7f-effc-47f4-889a-5e27d4b3bc03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042547228s
STEP: Saw pod success 01/05/23 10:40:16.404
Jan  5 10:40:16.404: INFO: Pod "pod-36480e7f-effc-47f4-889a-5e27d4b3bc03" satisfied condition "Succeeded or Failed"
Jan  5 10:40:16.408: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-36480e7f-effc-47f4-889a-5e27d4b3bc03 container test-container: <nil>
STEP: delete the pod 01/05/23 10:40:16.421
Jan  5 10:40:16.432: INFO: Waiting for pod pod-36480e7f-effc-47f4-889a-5e27d4b3bc03 to disappear
Jan  5 10:40:16.436: INFO: Pod pod-36480e7f-effc-47f4-889a-5e27d4b3bc03 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 10:40:16.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6065" for this suite. 01/05/23 10:40:16.445
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":341,"skipped":6315,"failed":0}
------------------------------
â€¢ [4.125 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:40:12.325
    Jan  5 10:40:12.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 10:40:12.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:12.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:12.346
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 01/05/23 10:40:12.352
    Jan  5 10:40:12.361: INFO: Waiting up to 5m0s for pod "pod-36480e7f-effc-47f4-889a-5e27d4b3bc03" in namespace "emptydir-6065" to be "Succeeded or Failed"
    Jan  5 10:40:12.370: INFO: Pod "pod-36480e7f-effc-47f4-889a-5e27d4b3bc03": Phase="Pending", Reason="", readiness=false. Elapsed: 8.67937ms
    Jan  5 10:40:14.377: INFO: Pod "pod-36480e7f-effc-47f4-889a-5e27d4b3bc03": Phase="Running", Reason="", readiness=false. Elapsed: 2.015796568s
    Jan  5 10:40:16.404: INFO: Pod "pod-36480e7f-effc-47f4-889a-5e27d4b3bc03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042547228s
    STEP: Saw pod success 01/05/23 10:40:16.404
    Jan  5 10:40:16.404: INFO: Pod "pod-36480e7f-effc-47f4-889a-5e27d4b3bc03" satisfied condition "Succeeded or Failed"
    Jan  5 10:40:16.408: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-36480e7f-effc-47f4-889a-5e27d4b3bc03 container test-container: <nil>
    STEP: delete the pod 01/05/23 10:40:16.421
    Jan  5 10:40:16.432: INFO: Waiting for pod pod-36480e7f-effc-47f4-889a-5e27d4b3bc03 to disappear
    Jan  5 10:40:16.436: INFO: Pod pod-36480e7f-effc-47f4-889a-5e27d4b3bc03 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 10:40:16.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6065" for this suite. 01/05/23 10:40:16.445
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:40:16.45
Jan  5 10:40:16.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename endpointslice 01/05/23 10:40:16.451
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:16.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:16.473
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 01/05/23 10:40:21.556
STEP: referencing matching pods with named port 01/05/23 10:40:26.566
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/05/23 10:40:31.577
STEP: recreating EndpointSlices after they've been deleted 01/05/23 10:40:36.587
Jan  5 10:40:36.608: INFO: EndpointSlice for Service endpointslice-8930/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan  5 10:40:46.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8930" for this suite. 01/05/23 10:40:46.629
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":342,"skipped":6316,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.185 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:40:16.45
    Jan  5 10:40:16.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename endpointslice 01/05/23 10:40:16.451
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:16.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:16.473
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 01/05/23 10:40:21.556
    STEP: referencing matching pods with named port 01/05/23 10:40:26.566
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/05/23 10:40:31.577
    STEP: recreating EndpointSlices after they've been deleted 01/05/23 10:40:36.587
    Jan  5 10:40:36.608: INFO: EndpointSlice for Service endpointslice-8930/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan  5 10:40:46.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8930" for this suite. 01/05/23 10:40:46.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:40:46.637
Jan  5 10:40:46.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename endpointslice 01/05/23 10:40:46.638
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:46.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:46.659
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jan  5 10:40:46.676: INFO: Endpoints addresses: [10.90.157.191] , ports: [443]
Jan  5 10:40:46.677: INFO: EndpointSlices addresses: [10.90.157.191] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan  5 10:40:46.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7934" for this suite. 01/05/23 10:40:46.684
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":343,"skipped":6329,"failed":0}
------------------------------
â€¢ [0.053 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:40:46.637
    Jan  5 10:40:46.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename endpointslice 01/05/23 10:40:46.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:46.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:46.659
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jan  5 10:40:46.676: INFO: Endpoints addresses: [10.90.157.191] , ports: [443]
    Jan  5 10:40:46.677: INFO: EndpointSlices addresses: [10.90.157.191] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan  5 10:40:46.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7934" for this suite. 01/05/23 10:40:46.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:40:46.691
Jan  5 10:40:46.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename security-context-test 01/05/23 10:40:46.692
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:46.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:46.763
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jan  5 10:40:46.782: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-b0c2b84f-4275-458b-876b-792269250c3d" in namespace "security-context-test-1027" to be "Succeeded or Failed"
Jan  5 10:40:46.786: INFO: Pod "busybox-readonly-false-b0c2b84f-4275-458b-876b-792269250c3d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029898ms
Jan  5 10:40:48.792: INFO: Pod "busybox-readonly-false-b0c2b84f-4275-458b-876b-792269250c3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009664463s
Jan  5 10:40:50.794: INFO: Pod "busybox-readonly-false-b0c2b84f-4275-458b-876b-792269250c3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011661647s
Jan  5 10:40:50.794: INFO: Pod "busybox-readonly-false-b0c2b84f-4275-458b-876b-792269250c3d" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  5 10:40:50.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1027" for this suite. 01/05/23 10:40:50.803
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":344,"skipped":6349,"failed":0}
------------------------------
â€¢ [4.117 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:40:46.691
    Jan  5 10:40:46.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename security-context-test 01/05/23 10:40:46.692
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:46.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:46.763
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jan  5 10:40:46.782: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-b0c2b84f-4275-458b-876b-792269250c3d" in namespace "security-context-test-1027" to be "Succeeded or Failed"
    Jan  5 10:40:46.786: INFO: Pod "busybox-readonly-false-b0c2b84f-4275-458b-876b-792269250c3d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029898ms
    Jan  5 10:40:48.792: INFO: Pod "busybox-readonly-false-b0c2b84f-4275-458b-876b-792269250c3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009664463s
    Jan  5 10:40:50.794: INFO: Pod "busybox-readonly-false-b0c2b84f-4275-458b-876b-792269250c3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011661647s
    Jan  5 10:40:50.794: INFO: Pod "busybox-readonly-false-b0c2b84f-4275-458b-876b-792269250c3d" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  5 10:40:50.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1027" for this suite. 01/05/23 10:40:50.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:40:50.811
Jan  5 10:40:50.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 10:40:50.812
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:50.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:50.833
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/05/23 10:40:50.839
Jan  5 10:40:50.848: INFO: Waiting up to 5m0s for pod "pod-43a39077-0ed0-49f7-9335-655b0a2552dd" in namespace "emptydir-4606" to be "Succeeded or Failed"
Jan  5 10:40:50.860: INFO: Pod "pod-43a39077-0ed0-49f7-9335-655b0a2552dd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.857112ms
Jan  5 10:40:52.866: INFO: Pod "pod-43a39077-0ed0-49f7-9335-655b0a2552dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017406373s
Jan  5 10:40:54.866: INFO: Pod "pod-43a39077-0ed0-49f7-9335-655b0a2552dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017478358s
STEP: Saw pod success 01/05/23 10:40:54.866
Jan  5 10:40:54.866: INFO: Pod "pod-43a39077-0ed0-49f7-9335-655b0a2552dd" satisfied condition "Succeeded or Failed"
Jan  5 10:40:54.871: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-43a39077-0ed0-49f7-9335-655b0a2552dd container test-container: <nil>
STEP: delete the pod 01/05/23 10:40:54.924
Jan  5 10:40:54.936: INFO: Waiting for pod pod-43a39077-0ed0-49f7-9335-655b0a2552dd to disappear
Jan  5 10:40:54.940: INFO: Pod pod-43a39077-0ed0-49f7-9335-655b0a2552dd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 10:40:54.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4606" for this suite. 01/05/23 10:40:54.95
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":345,"skipped":6399,"failed":0}
------------------------------
â€¢ [4.145 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:40:50.811
    Jan  5 10:40:50.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 10:40:50.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:50.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:50.833
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/05/23 10:40:50.839
    Jan  5 10:40:50.848: INFO: Waiting up to 5m0s for pod "pod-43a39077-0ed0-49f7-9335-655b0a2552dd" in namespace "emptydir-4606" to be "Succeeded or Failed"
    Jan  5 10:40:50.860: INFO: Pod "pod-43a39077-0ed0-49f7-9335-655b0a2552dd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.857112ms
    Jan  5 10:40:52.866: INFO: Pod "pod-43a39077-0ed0-49f7-9335-655b0a2552dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017406373s
    Jan  5 10:40:54.866: INFO: Pod "pod-43a39077-0ed0-49f7-9335-655b0a2552dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017478358s
    STEP: Saw pod success 01/05/23 10:40:54.866
    Jan  5 10:40:54.866: INFO: Pod "pod-43a39077-0ed0-49f7-9335-655b0a2552dd" satisfied condition "Succeeded or Failed"
    Jan  5 10:40:54.871: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod pod-43a39077-0ed0-49f7-9335-655b0a2552dd container test-container: <nil>
    STEP: delete the pod 01/05/23 10:40:54.924
    Jan  5 10:40:54.936: INFO: Waiting for pod pod-43a39077-0ed0-49f7-9335-655b0a2552dd to disappear
    Jan  5 10:40:54.940: INFO: Pod pod-43a39077-0ed0-49f7-9335-655b0a2552dd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 10:40:54.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4606" for this suite. 01/05/23 10:40:54.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:40:54.964
Jan  5 10:40:54.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 10:40:54.965
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:54.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:54.988
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 01/05/23 10:40:54.995
Jan  5 10:40:55.005: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-86efb88c-b462-4b51-ad6c-beb4f870282d" in namespace "emptydir-8145" to be "running"
Jan  5 10:40:55.113: INFO: Pod "pod-sharedvolume-86efb88c-b462-4b51-ad6c-beb4f870282d": Phase="Pending", Reason="", readiness=false. Elapsed: 108.364208ms
Jan  5 10:40:57.119: INFO: Pod "pod-sharedvolume-86efb88c-b462-4b51-ad6c-beb4f870282d": Phase="Running", Reason="", readiness=false. Elapsed: 2.114479652s
Jan  5 10:40:57.119: INFO: Pod "pod-sharedvolume-86efb88c-b462-4b51-ad6c-beb4f870282d" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/05/23 10:40:57.119
Jan  5 10:40:57.120: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8145 PodName:pod-sharedvolume-86efb88c-b462-4b51-ad6c-beb4f870282d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 10:40:57.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
Jan  5 10:40:57.120: INFO: ExecWithOptions: Clientset creation
Jan  5 10:40:57.121: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/emptydir-8145/pods/pod-sharedvolume-86efb88c-b462-4b51-ad6c-beb4f870282d/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan  5 10:40:57.523: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 10:40:57.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8145" for this suite. 01/05/23 10:40:57.537
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":346,"skipped":6436,"failed":0}
------------------------------
â€¢ [2.579 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:40:54.964
    Jan  5 10:40:54.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 10:40:54.965
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:54.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:54.988
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 01/05/23 10:40:54.995
    Jan  5 10:40:55.005: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-86efb88c-b462-4b51-ad6c-beb4f870282d" in namespace "emptydir-8145" to be "running"
    Jan  5 10:40:55.113: INFO: Pod "pod-sharedvolume-86efb88c-b462-4b51-ad6c-beb4f870282d": Phase="Pending", Reason="", readiness=false. Elapsed: 108.364208ms
    Jan  5 10:40:57.119: INFO: Pod "pod-sharedvolume-86efb88c-b462-4b51-ad6c-beb4f870282d": Phase="Running", Reason="", readiness=false. Elapsed: 2.114479652s
    Jan  5 10:40:57.119: INFO: Pod "pod-sharedvolume-86efb88c-b462-4b51-ad6c-beb4f870282d" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/05/23 10:40:57.119
    Jan  5 10:40:57.120: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8145 PodName:pod-sharedvolume-86efb88c-b462-4b51-ad6c-beb4f870282d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 10:40:57.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    Jan  5 10:40:57.120: INFO: ExecWithOptions: Clientset creation
    Jan  5 10:40:57.121: INFO: ExecWithOptions: execute(POST https://api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io:443/api/v1/namespaces/emptydir-8145/pods/pod-sharedvolume-86efb88c-b462-4b51-ad6c-beb4f870282d/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan  5 10:40:57.523: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 10:40:57.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8145" for this suite. 01/05/23 10:40:57.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:40:57.548
Jan  5 10:40:57.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename kubectl 01/05/23 10:40:57.549
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:57.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:57.574
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 01/05/23 10:40:57.58
Jan  5 10:40:57.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-8412 api-versions'
Jan  5 10:40:57.649: INFO: stderr: ""
Jan  5 10:40:57.649: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 10:40:57.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8412" for this suite. 01/05/23 10:40:57.658
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":347,"skipped":6473,"failed":0}
------------------------------
â€¢ [0.116 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:40:57.548
    Jan  5 10:40:57.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename kubectl 01/05/23 10:40:57.549
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:57.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:57.574
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 01/05/23 10:40:57.58
    Jan  5 10:40:57.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=kubectl-8412 api-versions'
    Jan  5 10:40:57.649: INFO: stderr: ""
    Jan  5 10:40:57.649: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 10:40:57.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8412" for this suite. 01/05/23 10:40:57.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:40:57.665
Jan  5 10:40:57.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename emptydir 01/05/23 10:40:57.665
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:57.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:57.689
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 01/05/23 10:40:57.695
Jan  5 10:40:57.706: INFO: Waiting up to 5m0s for pod "pod-0f219791-16fc-4334-a87c-a3e50eb885d6" in namespace "emptydir-2867" to be "Succeeded or Failed"
Jan  5 10:40:57.713: INFO: Pod "pod-0f219791-16fc-4334-a87c-a3e50eb885d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.629111ms
Jan  5 10:40:59.718: INFO: Pod "pod-0f219791-16fc-4334-a87c-a3e50eb885d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012348583s
Jan  5 10:41:01.720: INFO: Pod "pod-0f219791-16fc-4334-a87c-a3e50eb885d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014186467s
STEP: Saw pod success 01/05/23 10:41:01.72
Jan  5 10:41:01.720: INFO: Pod "pod-0f219791-16fc-4334-a87c-a3e50eb885d6" satisfied condition "Succeeded or Failed"
Jan  5 10:41:01.724: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-0f219791-16fc-4334-a87c-a3e50eb885d6 container test-container: <nil>
STEP: delete the pod 01/05/23 10:41:01.747
Jan  5 10:41:01.757: INFO: Waiting for pod pod-0f219791-16fc-4334-a87c-a3e50eb885d6 to disappear
Jan  5 10:41:01.761: INFO: Pod pod-0f219791-16fc-4334-a87c-a3e50eb885d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 10:41:01.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2867" for this suite. 01/05/23 10:41:01.769
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":348,"skipped":6484,"failed":0}
------------------------------
â€¢ [4.109 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:40:57.665
    Jan  5 10:40:57.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename emptydir 01/05/23 10:40:57.665
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:40:57.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:40:57.689
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/05/23 10:40:57.695
    Jan  5 10:40:57.706: INFO: Waiting up to 5m0s for pod "pod-0f219791-16fc-4334-a87c-a3e50eb885d6" in namespace "emptydir-2867" to be "Succeeded or Failed"
    Jan  5 10:40:57.713: INFO: Pod "pod-0f219791-16fc-4334-a87c-a3e50eb885d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.629111ms
    Jan  5 10:40:59.718: INFO: Pod "pod-0f219791-16fc-4334-a87c-a3e50eb885d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012348583s
    Jan  5 10:41:01.720: INFO: Pod "pod-0f219791-16fc-4334-a87c-a3e50eb885d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014186467s
    STEP: Saw pod success 01/05/23 10:41:01.72
    Jan  5 10:41:01.720: INFO: Pod "pod-0f219791-16fc-4334-a87c-a3e50eb885d6" satisfied condition "Succeeded or Failed"
    Jan  5 10:41:01.724: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-0f219791-16fc-4334-a87c-a3e50eb885d6 container test-container: <nil>
    STEP: delete the pod 01/05/23 10:41:01.747
    Jan  5 10:41:01.757: INFO: Waiting for pod pod-0f219791-16fc-4334-a87c-a3e50eb885d6 to disappear
    Jan  5 10:41:01.761: INFO: Pod pod-0f219791-16fc-4334-a87c-a3e50eb885d6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 10:41:01.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2867" for this suite. 01/05/23 10:41:01.769
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:41:01.774
Jan  5 10:41:01.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 10:41:01.775
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:01.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:01.8
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 01/05/23 10:41:01.81
STEP: watching for the Service to be added 01/05/23 10:41:01.825
Jan  5 10:41:01.832: INFO: Found Service test-service-jdxlf in namespace services-4567 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan  5 10:41:01.832: INFO: Service test-service-jdxlf created
STEP: Getting /status 01/05/23 10:41:01.832
Jan  5 10:41:01.836: INFO: Service test-service-jdxlf has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/05/23 10:41:01.836
STEP: watching for the Service to be patched 01/05/23 10:41:01.843
Jan  5 10:41:01.845: INFO: observed Service test-service-jdxlf in namespace services-4567 with annotations: map[] & LoadBalancer: {[]}
Jan  5 10:41:01.845: INFO: Found Service test-service-jdxlf in namespace services-4567 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan  5 10:41:01.845: INFO: Service test-service-jdxlf has service status patched
STEP: updating the ServiceStatus 01/05/23 10:41:01.845
Jan  5 10:41:01.854: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/05/23 10:41:01.854
Jan  5 10:41:01.859: INFO: Observed Service test-service-jdxlf in namespace services-4567 with annotations: map[] & Conditions: {[]}
Jan  5 10:41:01.859: INFO: Observed event: &Service{ObjectMeta:{test-service-jdxlf  services-4567  c0b73780-8e9b-4358-af4c-59b58e2741bd 48988 0 2023-01-05 10:41:01 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-05 10:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-05 10:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.121.46.192,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.121.46.192],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan  5 10:41:01.859: INFO: Found Service test-service-jdxlf in namespace services-4567 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  5 10:41:01.859: INFO: Service test-service-jdxlf has service status updated
STEP: patching the service 01/05/23 10:41:01.859
STEP: watching for the Service to be patched 01/05/23 10:41:01.871
Jan  5 10:41:01.876: INFO: observed Service test-service-jdxlf in namespace services-4567 with labels: map[test-service-static:true]
Jan  5 10:41:01.876: INFO: observed Service test-service-jdxlf in namespace services-4567 with labels: map[test-service-static:true]
Jan  5 10:41:01.877: INFO: observed Service test-service-jdxlf in namespace services-4567 with labels: map[test-service-static:true]
Jan  5 10:41:01.877: INFO: Found Service test-service-jdxlf in namespace services-4567 with labels: map[test-service:patched test-service-static:true]
Jan  5 10:41:01.877: INFO: Service test-service-jdxlf patched
STEP: deleting the service 01/05/23 10:41:01.877
STEP: watching for the Service to be deleted 01/05/23 10:41:01.891
Jan  5 10:41:01.894: INFO: Observed event: ADDED
Jan  5 10:41:01.894: INFO: Observed event: MODIFIED
Jan  5 10:41:01.894: INFO: Observed event: MODIFIED
Jan  5 10:41:01.895: INFO: Observed event: MODIFIED
Jan  5 10:41:01.895: INFO: Found Service test-service-jdxlf in namespace services-4567 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan  5 10:41:01.895: INFO: Service test-service-jdxlf deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 10:41:01.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4567" for this suite. 01/05/23 10:41:01.903
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":349,"skipped":6484,"failed":0}
------------------------------
â€¢ [0.134 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:41:01.774
    Jan  5 10:41:01.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 10:41:01.775
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:01.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:01.8
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 01/05/23 10:41:01.81
    STEP: watching for the Service to be added 01/05/23 10:41:01.825
    Jan  5 10:41:01.832: INFO: Found Service test-service-jdxlf in namespace services-4567 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan  5 10:41:01.832: INFO: Service test-service-jdxlf created
    STEP: Getting /status 01/05/23 10:41:01.832
    Jan  5 10:41:01.836: INFO: Service test-service-jdxlf has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/05/23 10:41:01.836
    STEP: watching for the Service to be patched 01/05/23 10:41:01.843
    Jan  5 10:41:01.845: INFO: observed Service test-service-jdxlf in namespace services-4567 with annotations: map[] & LoadBalancer: {[]}
    Jan  5 10:41:01.845: INFO: Found Service test-service-jdxlf in namespace services-4567 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan  5 10:41:01.845: INFO: Service test-service-jdxlf has service status patched
    STEP: updating the ServiceStatus 01/05/23 10:41:01.845
    Jan  5 10:41:01.854: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/05/23 10:41:01.854
    Jan  5 10:41:01.859: INFO: Observed Service test-service-jdxlf in namespace services-4567 with annotations: map[] & Conditions: {[]}
    Jan  5 10:41:01.859: INFO: Observed event: &Service{ObjectMeta:{test-service-jdxlf  services-4567  c0b73780-8e9b-4358-af4c-59b58e2741bd 48988 0 2023-01-05 10:41:01 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-05 10:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-05 10:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.121.46.192,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.121.46.192],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan  5 10:41:01.859: INFO: Found Service test-service-jdxlf in namespace services-4567 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  5 10:41:01.859: INFO: Service test-service-jdxlf has service status updated
    STEP: patching the service 01/05/23 10:41:01.859
    STEP: watching for the Service to be patched 01/05/23 10:41:01.871
    Jan  5 10:41:01.876: INFO: observed Service test-service-jdxlf in namespace services-4567 with labels: map[test-service-static:true]
    Jan  5 10:41:01.876: INFO: observed Service test-service-jdxlf in namespace services-4567 with labels: map[test-service-static:true]
    Jan  5 10:41:01.877: INFO: observed Service test-service-jdxlf in namespace services-4567 with labels: map[test-service-static:true]
    Jan  5 10:41:01.877: INFO: Found Service test-service-jdxlf in namespace services-4567 with labels: map[test-service:patched test-service-static:true]
    Jan  5 10:41:01.877: INFO: Service test-service-jdxlf patched
    STEP: deleting the service 01/05/23 10:41:01.877
    STEP: watching for the Service to be deleted 01/05/23 10:41:01.891
    Jan  5 10:41:01.894: INFO: Observed event: ADDED
    Jan  5 10:41:01.894: INFO: Observed event: MODIFIED
    Jan  5 10:41:01.894: INFO: Observed event: MODIFIED
    Jan  5 10:41:01.895: INFO: Observed event: MODIFIED
    Jan  5 10:41:01.895: INFO: Found Service test-service-jdxlf in namespace services-4567 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan  5 10:41:01.895: INFO: Service test-service-jdxlf deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 10:41:01.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4567" for this suite. 01/05/23 10:41:01.903
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:41:01.908
Jan  5 10:41:01.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 10:41:01.909
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:01.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:01.928
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-67697aa7-8399-4949-a292-2dc32e349e47 01/05/23 10:41:01.934
STEP: Creating a pod to test consume secrets 01/05/23 10:41:01.938
Jan  5 10:41:01.947: INFO: Waiting up to 5m0s for pod "pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42" in namespace "secrets-1674" to be "Succeeded or Failed"
Jan  5 10:41:01.955: INFO: Pod "pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42": Phase="Pending", Reason="", readiness=false. Elapsed: 8.295156ms
Jan  5 10:41:03.961: INFO: Pod "pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014128917s
Jan  5 10:41:05.960: INFO: Pod "pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013055554s
STEP: Saw pod success 01/05/23 10:41:05.96
Jan  5 10:41:05.960: INFO: Pod "pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42" satisfied condition "Succeeded or Failed"
Jan  5 10:41:05.964: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 10:41:05.977
Jan  5 10:41:05.994: INFO: Waiting for pod pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42 to disappear
Jan  5 10:41:05.997: INFO: Pod pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 10:41:05.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1674" for this suite. 01/05/23 10:41:06.007
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":350,"skipped":6494,"failed":0}
------------------------------
â€¢ [4.104 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:41:01.908
    Jan  5 10:41:01.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 10:41:01.909
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:01.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:01.928
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-67697aa7-8399-4949-a292-2dc32e349e47 01/05/23 10:41:01.934
    STEP: Creating a pod to test consume secrets 01/05/23 10:41:01.938
    Jan  5 10:41:01.947: INFO: Waiting up to 5m0s for pod "pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42" in namespace "secrets-1674" to be "Succeeded or Failed"
    Jan  5 10:41:01.955: INFO: Pod "pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42": Phase="Pending", Reason="", readiness=false. Elapsed: 8.295156ms
    Jan  5 10:41:03.961: INFO: Pod "pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014128917s
    Jan  5 10:41:05.960: INFO: Pod "pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013055554s
    STEP: Saw pod success 01/05/23 10:41:05.96
    Jan  5 10:41:05.960: INFO: Pod "pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42" satisfied condition "Succeeded or Failed"
    Jan  5 10:41:05.964: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r pod pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 10:41:05.977
    Jan  5 10:41:05.994: INFO: Waiting for pod pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42 to disappear
    Jan  5 10:41:05.997: INFO: Pod pod-secrets-9d059b5a-40b3-4a9d-9159-9bae992a3d42 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 10:41:05.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1674" for this suite. 01/05/23 10:41:06.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:41:06.014
Jan  5 10:41:06.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename container-probe 01/05/23 10:41:06.015
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:06.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:06.035
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf in namespace container-probe-4175 01/05/23 10:41:06.041
Jan  5 10:41:06.050: INFO: Waiting up to 5m0s for pod "liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf" in namespace "container-probe-4175" to be "not pending"
Jan  5 10:41:06.059: INFO: Pod "liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.389512ms
Jan  5 10:41:08.065: INFO: Pod "liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf": Phase="Running", Reason="", readiness=true. Elapsed: 2.01516448s
Jan  5 10:41:08.065: INFO: Pod "liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf" satisfied condition "not pending"
Jan  5 10:41:08.065: INFO: Started pod liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf in namespace container-probe-4175
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 10:41:08.066
Jan  5 10:41:08.070: INFO: Initial restart count of pod liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf is 0
Jan  5 10:41:28.139: INFO: Restart count of pod container-probe-4175/liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf is now 1 (20.06894005s elapsed)
STEP: deleting the pod 01/05/23 10:41:28.139
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 10:41:28.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4175" for this suite. 01/05/23 10:41:28.161
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":351,"skipped":6509,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.153 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:41:06.014
    Jan  5 10:41:06.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename container-probe 01/05/23 10:41:06.015
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:06.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:06.035
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf in namespace container-probe-4175 01/05/23 10:41:06.041
    Jan  5 10:41:06.050: INFO: Waiting up to 5m0s for pod "liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf" in namespace "container-probe-4175" to be "not pending"
    Jan  5 10:41:06.059: INFO: Pod "liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.389512ms
    Jan  5 10:41:08.065: INFO: Pod "liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf": Phase="Running", Reason="", readiness=true. Elapsed: 2.01516448s
    Jan  5 10:41:08.065: INFO: Pod "liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf" satisfied condition "not pending"
    Jan  5 10:41:08.065: INFO: Started pod liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf in namespace container-probe-4175
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 10:41:08.066
    Jan  5 10:41:08.070: INFO: Initial restart count of pod liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf is 0
    Jan  5 10:41:28.139: INFO: Restart count of pod container-probe-4175/liveness-f9a9dc2b-6407-46fd-a72c-58f6bbe35eaf is now 1 (20.06894005s elapsed)
    STEP: deleting the pod 01/05/23 10:41:28.139
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 10:41:28.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4175" for this suite. 01/05/23 10:41:28.161
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:41:28.167
Jan  5 10:41:28.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename svcaccounts 01/05/23 10:41:28.168
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:28.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:28.197
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  01/05/23 10:41:28.203
Jan  5 10:41:28.213: INFO: Waiting up to 5m0s for pod "test-pod-6ea328e2-1386-4781-b28f-8a2427040912" in namespace "svcaccounts-5789" to be "Succeeded or Failed"
Jan  5 10:41:28.217: INFO: Pod "test-pod-6ea328e2-1386-4781-b28f-8a2427040912": Phase="Pending", Reason="", readiness=false. Elapsed: 3.40805ms
Jan  5 10:41:30.223: INFO: Pod "test-pod-6ea328e2-1386-4781-b28f-8a2427040912": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009695897s
Jan  5 10:41:32.226: INFO: Pod "test-pod-6ea328e2-1386-4781-b28f-8a2427040912": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012249402s
STEP: Saw pod success 01/05/23 10:41:32.226
Jan  5 10:41:32.226: INFO: Pod "test-pod-6ea328e2-1386-4781-b28f-8a2427040912" satisfied condition "Succeeded or Failed"
Jan  5 10:41:32.234: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod test-pod-6ea328e2-1386-4781-b28f-8a2427040912 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 10:41:32.246
Jan  5 10:41:32.257: INFO: Waiting for pod test-pod-6ea328e2-1386-4781-b28f-8a2427040912 to disappear
Jan  5 10:41:32.261: INFO: Pod test-pod-6ea328e2-1386-4781-b28f-8a2427040912 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  5 10:41:32.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5789" for this suite. 01/05/23 10:41:32.269
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":352,"skipped":6512,"failed":0}
------------------------------
â€¢ [4.107 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:41:28.167
    Jan  5 10:41:28.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 10:41:28.168
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:28.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:28.197
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  01/05/23 10:41:28.203
    Jan  5 10:41:28.213: INFO: Waiting up to 5m0s for pod "test-pod-6ea328e2-1386-4781-b28f-8a2427040912" in namespace "svcaccounts-5789" to be "Succeeded or Failed"
    Jan  5 10:41:28.217: INFO: Pod "test-pod-6ea328e2-1386-4781-b28f-8a2427040912": Phase="Pending", Reason="", readiness=false. Elapsed: 3.40805ms
    Jan  5 10:41:30.223: INFO: Pod "test-pod-6ea328e2-1386-4781-b28f-8a2427040912": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009695897s
    Jan  5 10:41:32.226: INFO: Pod "test-pod-6ea328e2-1386-4781-b28f-8a2427040912": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012249402s
    STEP: Saw pod success 01/05/23 10:41:32.226
    Jan  5 10:41:32.226: INFO: Pod "test-pod-6ea328e2-1386-4781-b28f-8a2427040912" satisfied condition "Succeeded or Failed"
    Jan  5 10:41:32.234: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c pod test-pod-6ea328e2-1386-4781-b28f-8a2427040912 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 10:41:32.246
    Jan  5 10:41:32.257: INFO: Waiting for pod test-pod-6ea328e2-1386-4781-b28f-8a2427040912 to disappear
    Jan  5 10:41:32.261: INFO: Pod test-pod-6ea328e2-1386-4781-b28f-8a2427040912 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  5 10:41:32.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5789" for this suite. 01/05/23 10:41:32.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:41:32.275
Jan  5 10:41:32.275: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename webhook 01/05/23 10:41:32.276
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:32.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:32.294
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 10:41:32.311
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:41:32.582
STEP: Deploying the webhook pod 01/05/23 10:41:32.591
STEP: Wait for the deployment to be ready 01/05/23 10:41:32.627
Jan  5 10:41:32.640: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 10:41:34.656
STEP: Verifying the service has paired with the endpoint 01/05/23 10:41:34.67
Jan  5 10:41:35.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 01/05/23 10:41:35.676
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/05/23 10:41:35.679
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/05/23 10:41:35.679
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/05/23 10:41:35.679
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/05/23 10:41:35.682
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/05/23 10:41:35.682
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/05/23 10:41:35.684
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 10:41:35.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6421" for this suite. 01/05/23 10:41:35.692
STEP: Destroying namespace "webhook-6421-markers" for this suite. 01/05/23 10:41:35.699
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":353,"skipped":6522,"failed":0}
------------------------------
â€¢ [3.456 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:41:32.275
    Jan  5 10:41:32.275: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename webhook 01/05/23 10:41:32.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:32.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:32.294
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 10:41:32.311
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 10:41:32.582
    STEP: Deploying the webhook pod 01/05/23 10:41:32.591
    STEP: Wait for the deployment to be ready 01/05/23 10:41:32.627
    Jan  5 10:41:32.640: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 10:41:34.656
    STEP: Verifying the service has paired with the endpoint 01/05/23 10:41:34.67
    Jan  5 10:41:35.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 01/05/23 10:41:35.676
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/05/23 10:41:35.679
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/05/23 10:41:35.679
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/05/23 10:41:35.679
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/05/23 10:41:35.682
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/05/23 10:41:35.682
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/05/23 10:41:35.684
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 10:41:35.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6421" for this suite. 01/05/23 10:41:35.692
    STEP: Destroying namespace "webhook-6421-markers" for this suite. 01/05/23 10:41:35.699
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:41:35.732
Jan  5 10:41:35.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename svcaccounts 01/05/23 10:41:35.733
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:35.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:35.759
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jan  5 10:41:35.782: INFO: created pod pod-service-account-defaultsa
Jan  5 10:41:35.782: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan  5 10:41:35.789: INFO: created pod pod-service-account-mountsa
Jan  5 10:41:35.789: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan  5 10:41:35.798: INFO: created pod pod-service-account-nomountsa
Jan  5 10:41:35.798: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan  5 10:41:35.805: INFO: created pod pod-service-account-defaultsa-mountspec
Jan  5 10:41:35.805: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan  5 10:41:35.824: INFO: created pod pod-service-account-mountsa-mountspec
Jan  5 10:41:35.824: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan  5 10:41:35.832: INFO: created pod pod-service-account-nomountsa-mountspec
Jan  5 10:41:35.832: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan  5 10:41:35.838: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan  5 10:41:35.838: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan  5 10:41:35.845: INFO: created pod pod-service-account-mountsa-nomountspec
Jan  5 10:41:35.845: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan  5 10:41:35.855: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan  5 10:41:35.855: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  5 10:41:35.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7011" for this suite. 01/05/23 10:41:35.865
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":354,"skipped":6522,"failed":0}
------------------------------
â€¢ [0.142 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:41:35.732
    Jan  5 10:41:35.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 10:41:35.733
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:35.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:35.759
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jan  5 10:41:35.782: INFO: created pod pod-service-account-defaultsa
    Jan  5 10:41:35.782: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan  5 10:41:35.789: INFO: created pod pod-service-account-mountsa
    Jan  5 10:41:35.789: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan  5 10:41:35.798: INFO: created pod pod-service-account-nomountsa
    Jan  5 10:41:35.798: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan  5 10:41:35.805: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan  5 10:41:35.805: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan  5 10:41:35.824: INFO: created pod pod-service-account-mountsa-mountspec
    Jan  5 10:41:35.824: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan  5 10:41:35.832: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan  5 10:41:35.832: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan  5 10:41:35.838: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan  5 10:41:35.838: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan  5 10:41:35.845: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan  5 10:41:35.845: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan  5 10:41:35.855: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan  5 10:41:35.855: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  5 10:41:35.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7011" for this suite. 01/05/23 10:41:35.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:41:35.875
Jan  5 10:41:35.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename services 01/05/23 10:41:35.876
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:35.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:35.903
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5789 01/05/23 10:41:35.911
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/05/23 10:41:35.933
STEP: creating service externalsvc in namespace services-5789 01/05/23 10:41:35.933
STEP: creating replication controller externalsvc in namespace services-5789 01/05/23 10:41:35.948
I0105 10:41:35.954062      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5789, replica count: 2
I0105 10:41:39.005043      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/05/23 10:41:39.01
Jan  5 10:41:39.029: INFO: Creating new exec pod
Jan  5 10:41:39.039: INFO: Waiting up to 5m0s for pod "execpod9n2gv" in namespace "services-5789" to be "running"
Jan  5 10:41:39.045: INFO: Pod "execpod9n2gv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029816ms
Jan  5 10:41:41.051: INFO: Pod "execpod9n2gv": Phase="Running", Reason="", readiness=true. Elapsed: 2.011914158s
Jan  5 10:41:41.051: INFO: Pod "execpod9n2gv" satisfied condition "running"
Jan  5 10:41:41.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-5789 exec execpod9n2gv -- /bin/sh -x -c nslookup nodeport-service.services-5789.svc.cluster.local'
Jan  5 10:41:41.598: INFO: stderr: "+ nslookup nodeport-service.services-5789.svc.cluster.local\n"
Jan  5 10:41:41.598: INFO: stdout: "Server:\t\t10.112.0.10\nAddress:\t10.112.0.10#53\n\nnodeport-service.services-5789.svc.cluster.local\tcanonical name = externalsvc.services-5789.svc.cluster.local.\nName:\texternalsvc.services-5789.svc.cluster.local\nAddress: 10.113.135.57\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5789, will wait for the garbage collector to delete the pods 01/05/23 10:41:41.598
Jan  5 10:41:41.661: INFO: Deleting ReplicationController externalsvc took: 6.038212ms
Jan  5 10:41:41.761: INFO: Terminating ReplicationController externalsvc pods took: 100.28833ms
Jan  5 10:41:43.576: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 10:41:43.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5789" for this suite. 01/05/23 10:41:43.597
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":355,"skipped":6547,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.727 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:41:35.875
    Jan  5 10:41:35.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename services 01/05/23 10:41:35.876
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:35.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:35.903
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-5789 01/05/23 10:41:35.911
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/05/23 10:41:35.933
    STEP: creating service externalsvc in namespace services-5789 01/05/23 10:41:35.933
    STEP: creating replication controller externalsvc in namespace services-5789 01/05/23 10:41:35.948
    I0105 10:41:35.954062      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5789, replica count: 2
    I0105 10:41:39.005043      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/05/23 10:41:39.01
    Jan  5 10:41:39.029: INFO: Creating new exec pod
    Jan  5 10:41:39.039: INFO: Waiting up to 5m0s for pod "execpod9n2gv" in namespace "services-5789" to be "running"
    Jan  5 10:41:39.045: INFO: Pod "execpod9n2gv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029816ms
    Jan  5 10:41:41.051: INFO: Pod "execpod9n2gv": Phase="Running", Reason="", readiness=true. Elapsed: 2.011914158s
    Jan  5 10:41:41.051: INFO: Pod "execpod9n2gv" satisfied condition "running"
    Jan  5 10:41:41.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1415592784 --namespace=services-5789 exec execpod9n2gv -- /bin/sh -x -c nslookup nodeport-service.services-5789.svc.cluster.local'
    Jan  5 10:41:41.598: INFO: stderr: "+ nslookup nodeport-service.services-5789.svc.cluster.local\n"
    Jan  5 10:41:41.598: INFO: stdout: "Server:\t\t10.112.0.10\nAddress:\t10.112.0.10#53\n\nnodeport-service.services-5789.svc.cluster.local\tcanonical name = externalsvc.services-5789.svc.cluster.local.\nName:\texternalsvc.services-5789.svc.cluster.local\nAddress: 10.113.135.57\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-5789, will wait for the garbage collector to delete the pods 01/05/23 10:41:41.598
    Jan  5 10:41:41.661: INFO: Deleting ReplicationController externalsvc took: 6.038212ms
    Jan  5 10:41:41.761: INFO: Terminating ReplicationController externalsvc pods took: 100.28833ms
    Jan  5 10:41:43.576: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 10:41:43.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5789" for this suite. 01/05/23 10:41:43.597
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:41:43.602
Jan  5 10:41:43.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename configmap 01/05/23 10:41:43.603
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:43.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:43.629
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-f2a8420b-2c82-40f0-a2ad-cd60999155f0 01/05/23 10:41:43.635
STEP: Creating a pod to test consume configMaps 01/05/23 10:41:43.64
Jan  5 10:41:43.650: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b" in namespace "configmap-9462" to be "Succeeded or Failed"
Jan  5 10:41:43.654: INFO: Pod "pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.750883ms
Jan  5 10:41:45.662: INFO: Pod "pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01180814s
Jan  5 10:41:47.661: INFO: Pod "pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011558993s
STEP: Saw pod success 01/05/23 10:41:47.661
Jan  5 10:41:47.661: INFO: Pod "pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b" satisfied condition "Succeeded or Failed"
Jan  5 10:41:47.665: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b container agnhost-container: <nil>
STEP: delete the pod 01/05/23 10:41:47.685
Jan  5 10:41:47.697: INFO: Waiting for pod pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b to disappear
Jan  5 10:41:47.701: INFO: Pod pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 10:41:47.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9462" for this suite. 01/05/23 10:41:47.709
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":356,"skipped":6547,"failed":0}
------------------------------
â€¢ [4.113 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:41:43.602
    Jan  5 10:41:43.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename configmap 01/05/23 10:41:43.603
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:43.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:43.629
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-f2a8420b-2c82-40f0-a2ad-cd60999155f0 01/05/23 10:41:43.635
    STEP: Creating a pod to test consume configMaps 01/05/23 10:41:43.64
    Jan  5 10:41:43.650: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b" in namespace "configmap-9462" to be "Succeeded or Failed"
    Jan  5 10:41:43.654: INFO: Pod "pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.750883ms
    Jan  5 10:41:45.662: INFO: Pod "pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01180814s
    Jan  5 10:41:47.661: INFO: Pod "pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011558993s
    STEP: Saw pod success 01/05/23 10:41:47.661
    Jan  5 10:41:47.661: INFO: Pod "pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b" satisfied condition "Succeeded or Failed"
    Jan  5 10:41:47.665: INFO: Trying to get logs from node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv pod pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 10:41:47.685
    Jan  5 10:41:47.697: INFO: Waiting for pod pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b to disappear
    Jan  5 10:41:47.701: INFO: Pod pod-configmaps-ad2552cb-9754-4669-a0c3-70bb78e51b9b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 10:41:47.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9462" for this suite. 01/05/23 10:41:47.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:41:47.716
Jan  5 10:41:47.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename secrets 01/05/23 10:41:47.717
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:47.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:47.739
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-e0965025-3c5b-4e6c-8b7d-eb332f96b4b4 01/05/23 10:41:47.753
STEP: Creating secret with name s-test-opt-upd-2053997f-5ab0-458a-a484-9df6afd3c501 01/05/23 10:41:47.759
STEP: Creating the pod 01/05/23 10:41:47.763
Jan  5 10:41:47.773: INFO: Waiting up to 5m0s for pod "pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350" in namespace "secrets-6702" to be "running and ready"
Jan  5 10:41:47.777: INFO: Pod "pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350": Phase="Pending", Reason="", readiness=false. Elapsed: 4.182564ms
Jan  5 10:41:47.777: INFO: The phase of Pod pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:41:49.785: INFO: Pod "pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350": Phase="Running", Reason="", readiness=true. Elapsed: 2.011353882s
Jan  5 10:41:49.785: INFO: The phase of Pod pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350 is Running (Ready = true)
Jan  5 10:41:49.785: INFO: Pod "pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-e0965025-3c5b-4e6c-8b7d-eb332f96b4b4 01/05/23 10:41:49.991
STEP: Updating secret s-test-opt-upd-2053997f-5ab0-458a-a484-9df6afd3c501 01/05/23 10:41:49.996
STEP: Creating secret with name s-test-opt-create-644b126c-f162-4228-8279-ae15032d8859 01/05/23 10:41:50.002
STEP: waiting to observe update in volume 01/05/23 10:41:50.006
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 10:41:54.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6702" for this suite. 01/05/23 10:41:54.299
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":357,"skipped":6575,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.591 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:41:47.716
    Jan  5 10:41:47.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename secrets 01/05/23 10:41:47.717
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:47.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:47.739
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-e0965025-3c5b-4e6c-8b7d-eb332f96b4b4 01/05/23 10:41:47.753
    STEP: Creating secret with name s-test-opt-upd-2053997f-5ab0-458a-a484-9df6afd3c501 01/05/23 10:41:47.759
    STEP: Creating the pod 01/05/23 10:41:47.763
    Jan  5 10:41:47.773: INFO: Waiting up to 5m0s for pod "pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350" in namespace "secrets-6702" to be "running and ready"
    Jan  5 10:41:47.777: INFO: Pod "pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350": Phase="Pending", Reason="", readiness=false. Elapsed: 4.182564ms
    Jan  5 10:41:47.777: INFO: The phase of Pod pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:41:49.785: INFO: Pod "pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350": Phase="Running", Reason="", readiness=true. Elapsed: 2.011353882s
    Jan  5 10:41:49.785: INFO: The phase of Pod pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350 is Running (Ready = true)
    Jan  5 10:41:49.785: INFO: Pod "pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-e0965025-3c5b-4e6c-8b7d-eb332f96b4b4 01/05/23 10:41:49.991
    STEP: Updating secret s-test-opt-upd-2053997f-5ab0-458a-a484-9df6afd3c501 01/05/23 10:41:49.996
    STEP: Creating secret with name s-test-opt-create-644b126c-f162-4228-8279-ae15032d8859 01/05/23 10:41:50.002
    STEP: waiting to observe update in volume 01/05/23 10:41:50.006
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 10:41:54.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6702" for this suite. 01/05/23 10:41:54.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:41:54.31
Jan  5 10:41:54.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename deployment 01/05/23 10:41:54.311
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:54.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:54.33
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan  5 10:41:54.352: INFO: Creating simple deployment test-new-deployment
Jan  5 10:41:54.366: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 01/05/23 10:41:56.382
STEP: updating a scale subresource 01/05/23 10:41:56.386
STEP: verifying the deployment Spec.Replicas was modified 01/05/23 10:41:56.392
STEP: Patch a scale subresource 01/05/23 10:41:56.397
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 10:41:56.422: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1592  9c1b582a-5be7-423b-873b-12ef40763944 49587 3 2023-01-05 10:41:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-05 10:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b89998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 10:41:55 +0000 UTC,LastTransitionTime:2023-01-05 10:41:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-05 10:41:55 +0000 UTC,LastTransitionTime:2023-01-05 10:41:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  5 10:41:56.451: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1592  3ecd8269-3d98-4117-9450-fec9b08d6c13 49591 2 2023-01-05 10:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 9c1b582a-5be7-423b-873b-12ef40763944 0xc002b89dd7 0xc002b89dd8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 10:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c1b582a-5be7-423b-873b-12ef40763944\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:41:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b89e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 10:41:56.456: INFO: Pod "test-new-deployment-845c8977d9-fzjq7" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-fzjq7 test-new-deployment-845c8977d9- deployment-1592  3c6b2c59-59ae-4cac-b39e-13a889dc329c 49575 0 2023-01-05 10:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 3ecd8269-3d98-4117-9450-fec9b08d6c13 0xc004d8e217 0xc004d8e218}] [] [{kube-controller-manager Update v1 2023-01-05 10:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ecd8269-3d98-4117-9450-fec9b08d6c13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:41:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jgnbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jgnbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.95,StartTime:2023-01-05 10:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:41:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b3a6584ccb017f0096d1d60e16d622c2746fd165350295ac57698a5520dfdf09,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 10:41:56.456: INFO: Pod "test-new-deployment-845c8977d9-vl69n" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-vl69n test-new-deployment-845c8977d9- deployment-1592  d49b7cc6-0d7c-4ef4-b6b4-e813823693e3 49593 0 2023-01-05 10:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 3ecd8269-3d98-4117-9450-fec9b08d6c13 0xc004d8e3f0 0xc004d8e3f1}] [] [{kube-controller-manager Update v1 2023-01-05 10:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ecd8269-3d98-4117-9450-fec9b08d6c13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:41:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5l6gj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5l6gj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:,StartTime:2023-01-05 10:41:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 10:41:56.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1592" for this suite. 01/05/23 10:41:56.468
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":358,"skipped":6603,"failed":0}
------------------------------
â€¢ [2.166 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:41:54.31
    Jan  5 10:41:54.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename deployment 01/05/23 10:41:54.311
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:54.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:54.33
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan  5 10:41:54.352: INFO: Creating simple deployment test-new-deployment
    Jan  5 10:41:54.366: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 01/05/23 10:41:56.382
    STEP: updating a scale subresource 01/05/23 10:41:56.386
    STEP: verifying the deployment Spec.Replicas was modified 01/05/23 10:41:56.392
    STEP: Patch a scale subresource 01/05/23 10:41:56.397
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 10:41:56.422: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-1592  9c1b582a-5be7-423b-873b-12ef40763944 49587 3 2023-01-05 10:41:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-05 10:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b89998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 10:41:55 +0000 UTC,LastTransitionTime:2023-01-05 10:41:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-05 10:41:55 +0000 UTC,LastTransitionTime:2023-01-05 10:41:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  5 10:41:56.451: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1592  3ecd8269-3d98-4117-9450-fec9b08d6c13 49591 2 2023-01-05 10:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 9c1b582a-5be7-423b-873b-12ef40763944 0xc002b89dd7 0xc002b89dd8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 10:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c1b582a-5be7-423b-873b-12ef40763944\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 10:41:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b89e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 10:41:56.456: INFO: Pod "test-new-deployment-845c8977d9-fzjq7" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-fzjq7 test-new-deployment-845c8977d9- deployment-1592  3c6b2c59-59ae-4cac-b39e-13a889dc329c 49575 0 2023-01-05 10:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 3ecd8269-3d98-4117-9450-fec9b08d6c13 0xc004d8e217 0xc004d8e218}] [] [{kube-controller-manager Update v1 2023-01-05 10:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ecd8269-3d98-4117-9450-fec9b08d6c13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:41:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jgnbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jgnbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.138,PodIP:10.96.2.95,StartTime:2023-01-05 10:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 10:41:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b3a6584ccb017f0096d1d60e16d622c2746fd165350295ac57698a5520dfdf09,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 10:41:56.456: INFO: Pod "test-new-deployment-845c8977d9-vl69n" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-vl69n test-new-deployment-845c8977d9- deployment-1592  d49b7cc6-0d7c-4ef4-b6b4-e813823693e3 49593 0 2023-01-05 10:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 3ecd8269-3d98-4117-9450-fec9b08d6c13 0xc004d8e3f0 0xc004d8e3f1}] [] [{kube-controller-manager Update v1 2023-01-05 10:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ecd8269-3d98-4117-9450-fec9b08d6c13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 10:41:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5l6gj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.k8s-cncf-conf.kd500770.internal.prod.gardener.get-cloud.io,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5l6gj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 10:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.19,PodIP:,StartTime:2023-01-05 10:41:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 10:41:56.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1592" for this suite. 01/05/23 10:41:56.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:41:56.477
Jan  5 10:41:56.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename replication-controller 01/05/23 10:41:56.478
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:56.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:56.534
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 01/05/23 10:41:56.54
Jan  5 10:41:56.548: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-4452" to be "running and ready"
Jan  5 10:41:56.562: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 14.210431ms
Jan  5 10:41:56.563: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan  5 10:41:58.567: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.019201217s
Jan  5 10:41:58.567: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan  5 10:41:58.568: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/05/23 10:41:58.571
STEP: Then the orphan pod is adopted 01/05/23 10:41:58.575
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  5 10:41:59.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4452" for this suite. 01/05/23 10:41:59.647
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":359,"skipped":6611,"failed":0}
------------------------------
â€¢ [3.176 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:41:56.477
    Jan  5 10:41:56.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename replication-controller 01/05/23 10:41:56.478
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:56.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:56.534
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/05/23 10:41:56.54
    Jan  5 10:41:56.548: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-4452" to be "running and ready"
    Jan  5 10:41:56.562: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 14.210431ms
    Jan  5 10:41:56.563: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 10:41:58.567: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.019201217s
    Jan  5 10:41:58.567: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan  5 10:41:58.568: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/05/23 10:41:58.571
    STEP: Then the orphan pod is adopted 01/05/23 10:41:58.575
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  5 10:41:59.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4452" for this suite. 01/05/23 10:41:59.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:41:59.655
Jan  5 10:41:59.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename sched-pred 01/05/23 10:41:59.656
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:59.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:59.68
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan  5 10:41:59.685: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  5 10:41:59.702: INFO: Waiting for terminating namespaces to be deleted...
Jan  5 10:41:59.707: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t before test
Jan  5 10:41:59.721: INFO: test-new-deployment-845c8977d9-kwlp4 from deployment-1592 started at 2023-01-05 10:41:56 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.721: INFO: 	Container httpd ready: false, restart count 0
Jan  5 10:41:59.721: INFO: apiserver-proxy-m4wgr from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.721: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:41:59.721: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:41:59.721: INFO: cilium-5qvr2 from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.721: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:41:59.721: INFO: csi-driver-node-nt6bm from kube-system started at 2023-01-05 08:52:50 +0000 UTC (3 container statuses recorded)
Jan  5 10:41:59.721: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:41:59.721: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:41:59.721: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:41:59.721: INFO: kube-proxy-worker-omyby-v1.25.5-xvlqq from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.721: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:41:59.721: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:41:59.721: INFO: node-exporter-hckzs from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.721: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:41:59.721: INFO: node-problem-detector-5ln6w from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.721: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:41:59.721: INFO: sonobuoy-e2e-job-2d2d23fb3e814b3d from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.721: INFO: 	Container e2e ready: true, restart count 0
Jan  5 10:41:59.721: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:41:59.721: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-m6xzl from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.721: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:41:59.721: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:41:59.721: INFO: pod-service-account-mountsa-mountspec from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.721: INFO: 	Container token-test ready: false, restart count 0
Jan  5 10:41:59.721: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r before test
Jan  5 10:41:59.736: INFO: test-new-deployment-845c8977d9-vl69n from deployment-1592 started at 2023-01-05 10:41:56 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.736: INFO: 	Container httpd ready: false, restart count 0
Jan  5 10:41:59.736: INFO: apiserver-proxy-4w6xb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.736: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:41:59.736: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:41:59.736: INFO: cilium-cjv9l from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.736: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:41:59.736: INFO: cilium-operator-6bf67c77c6-r9hbq from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.736: INFO: 	Container cilium-operator ready: true, restart count 0
Jan  5 10:41:59.736: INFO: csi-driver-node-r2l8x from kube-system started at 2023-01-05 08:52:30 +0000 UTC (3 container statuses recorded)
Jan  5 10:41:59.736: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:41:59.736: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:41:59.736: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:41:59.736: INFO: kube-proxy-worker-omyby-v1.25.5-fz64v from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.736: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:41:59.736: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:41:59.736: INFO: node-exporter-7hcrb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.736: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:41:59.736: INFO: node-problem-detector-l85nm from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.736: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:41:59.736: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-xghsj from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.736: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:41:59.736: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:41:59.736: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.736: INFO: 	Container token-test ready: false, restart count 0
Jan  5 10:41:59.736: INFO: pod-service-account-mountsa from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.736: INFO: 	Container token-test ready: false, restart count 0
Jan  5 10:41:59.736: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 before test
Jan  5 10:41:59.750: INFO: test-new-deployment-845c8977d9-cnn9x from deployment-1592 started at 2023-01-05 10:41:56 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container httpd ready: false, restart count 0
Jan  5 10:41:59.750: INFO: apiserver-proxy-fnbwn from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:41:59.750: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:41:59.750: INFO: blackbox-exporter-c866d5696-wkcmf from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan  5 10:41:59.750: INFO: blackbox-exporter-c866d5696-z5dx5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan  5 10:41:59.750: INFO: cilium-operator-6bf67c77c6-mmgs4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container cilium-operator ready: true, restart count 0
Jan  5 10:41:59.750: INFO: cilium-x98hw from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:41:59.750: INFO: coredns-7594945774-g79vz from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container coredns ready: true, restart count 0
Jan  5 10:41:59.750: INFO: coredns-7594945774-wghlc from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container coredns ready: true, restart count 0
Jan  5 10:41:59.750: INFO: csi-driver-node-wcklx from kube-system started at 2023-01-05 08:52:24 +0000 UTC (3 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:41:59.750: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:41:59.750: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:41:59.750: INFO: kube-proxy-worker-vot5k-v1.25.5-86pmg from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:41:59.750: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:41:59.750: INFO: metrics-server-b58b76d9c-jzq89 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container metrics-server ready: true, restart count 0
Jan  5 10:41:59.750: INFO: metrics-server-b58b76d9c-kwfd5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container metrics-server ready: true, restart count 0
Jan  5 10:41:59.750: INFO: node-exporter-xlnb4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:41:59.750: INFO: node-problem-detector-bzpk7 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:41:59.750: INFO: vpn-shoot-69957f7fdd-rmrkr from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container vpn-shoot ready: true, restart count 0
Jan  5 10:41:59.750: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-mw7c6 from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.750: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:41:59.750: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:41:59.750: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv before test
Jan  5 10:41:59.764: INFO: test-new-deployment-845c8977d9-fzjq7 from deployment-1592 started at 2023-01-05 10:41:54 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.764: INFO: 	Container httpd ready: true, restart count 0
Jan  5 10:41:59.764: INFO: apiserver-proxy-4j65b from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.764: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:41:59.764: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:41:59.764: INFO: cilium-57wqw from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.764: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:41:59.764: INFO: csi-driver-node-rj5cq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (3 container statuses recorded)
Jan  5 10:41:59.764: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:41:59.764: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:41:59.764: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:41:59.764: INFO: kube-proxy-worker-vot5k-v1.25.5-pg2pl from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.764: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:41:59.764: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:41:59.764: INFO: node-exporter-25qsq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.764: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:41:59.764: INFO: node-problem-detector-7xm42 from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.764: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:41:59.764: INFO: pod-adoption from replication-controller-4452 started at 2023-01-05 10:41:56 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.764: INFO: 	Container pod-adoption ready: true, restart count 0
Jan  5 10:41:59.764: INFO: sonobuoy from sonobuoy started at 2023-01-05 09:06:38 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.764: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  5 10:41:59.764: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-qvcts from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.764: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:41:59.764: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:41:59.764: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c before test
Jan  5 10:41:59.780: INFO: apiserver-proxy-6plt4 from kube-system started at 2023-01-05 10:15:05 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.780: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:41:59.780: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:41:59.780: INFO: cilium-zljff from kube-system started at 2023-01-05 10:15:05 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.780: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:41:59.780: INFO: csi-driver-node-gznwc from kube-system started at 2023-01-05 10:15:05 +0000 UTC (3 container statuses recorded)
Jan  5 10:41:59.780: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:41:59.780: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:41:59.780: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:41:59.780: INFO: kube-proxy-worker-vot5k-v1.25.5-5kqql from kube-system started at 2023-01-05 10:15:05 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.780: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:41:59.780: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:41:59.780: INFO: node-exporter-bn94z from kube-system started at 2023-01-05 10:15:05 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.780: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:41:59.780: INFO: node-problem-detector-2xncr from kube-system started at 2023-01-05 10:15:05 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.780: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:41:59.780: INFO: pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350 from secrets-6702 started at 2023-01-05 10:41:47 +0000 UTC (3 container statuses recorded)
Jan  5 10:41:59.780: INFO: 	Container creates-volume-test ready: true, restart count 0
Jan  5 10:41:59.780: INFO: 	Container dels-volume-test ready: true, restart count 0
Jan  5 10:41:59.780: INFO: 	Container upds-volume-test ready: true, restart count 0
Jan  5 10:41:59.780: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-rp2vl from sonobuoy started at 2023-01-05 10:15:05 +0000 UTC (2 container statuses recorded)
Jan  5 10:41:59.780: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Jan  5 10:41:59.780: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:41:59.780: INFO: pod-service-account-defaultsa from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.780: INFO: 	Container token-test ready: false, restart count 0
Jan  5 10:41:59.780: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
Jan  5 10:41:59.780: INFO: 	Container token-test ready: false, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 10:41:59.78
Jan  5 10:41:59.789: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9858" to be "running"
Jan  5 10:41:59.793: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.089741ms
Jan  5 10:42:01.798: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009107243s
Jan  5 10:42:01.798: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 10:42:01.803
STEP: Trying to apply a random label on the found node. 01/05/23 10:42:01.835
STEP: verifying the node has the label kubernetes.io/e2e-16b55192-e96d-4c5d-9487-ac0d63b6df8f 42 01/05/23 10:42:01.857
STEP: Trying to relaunch the pod, now with labels. 01/05/23 10:42:01.865
Jan  5 10:42:01.894: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9858" to be "not pending"
Jan  5 10:42:01.899: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 5.168842ms
Jan  5 10:42:03.906: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.011810093s
Jan  5 10:42:03.906: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-16b55192-e96d-4c5d-9487-ac0d63b6df8f off the node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv 01/05/23 10:42:03.91
STEP: verifying the node doesn't have the label kubernetes.io/e2e-16b55192-e96d-4c5d-9487-ac0d63b6df8f 01/05/23 10:42:03.927
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan  5 10:42:03.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9858" for this suite. 01/05/23 10:42:03.947
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":360,"skipped":6621,"failed":0}
------------------------------
â€¢ [4.303 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:41:59.655
    Jan  5 10:41:59.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename sched-pred 01/05/23 10:41:59.656
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:41:59.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:41:59.68
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan  5 10:41:59.685: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  5 10:41:59.702: INFO: Waiting for terminating namespaces to be deleted...
    Jan  5 10:41:59.707: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t before test
    Jan  5 10:41:59.721: INFO: test-new-deployment-845c8977d9-kwlp4 from deployment-1592 started at 2023-01-05 10:41:56 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.721: INFO: 	Container httpd ready: false, restart count 0
    Jan  5 10:41:59.721: INFO: apiserver-proxy-m4wgr from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.721: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: cilium-5qvr2 from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.721: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: csi-driver-node-nt6bm from kube-system started at 2023-01-05 08:52:50 +0000 UTC (3 container statuses recorded)
    Jan  5 10:41:59.721: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: kube-proxy-worker-omyby-v1.25.5-xvlqq from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.721: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: node-exporter-hckzs from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.721: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: node-problem-detector-5ln6w from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.721: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: sonobuoy-e2e-job-2d2d23fb3e814b3d from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.721: INFO: 	Container e2e ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-m6xzl from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.721: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:41:59.721: INFO: pod-service-account-mountsa-mountspec from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.721: INFO: 	Container token-test ready: false, restart count 0
    Jan  5 10:41:59.721: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r before test
    Jan  5 10:41:59.736: INFO: test-new-deployment-845c8977d9-vl69n from deployment-1592 started at 2023-01-05 10:41:56 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.736: INFO: 	Container httpd ready: false, restart count 0
    Jan  5 10:41:59.736: INFO: apiserver-proxy-4w6xb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.736: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: cilium-cjv9l from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.736: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: cilium-operator-6bf67c77c6-r9hbq from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.736: INFO: 	Container cilium-operator ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: csi-driver-node-r2l8x from kube-system started at 2023-01-05 08:52:30 +0000 UTC (3 container statuses recorded)
    Jan  5 10:41:59.736: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: kube-proxy-worker-omyby-v1.25.5-fz64v from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.736: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: node-exporter-7hcrb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.736: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: node-problem-detector-l85nm from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.736: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-xghsj from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.736: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:41:59.736: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.736: INFO: 	Container token-test ready: false, restart count 0
    Jan  5 10:41:59.736: INFO: pod-service-account-mountsa from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.736: INFO: 	Container token-test ready: false, restart count 0
    Jan  5 10:41:59.736: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 before test
    Jan  5 10:41:59.750: INFO: test-new-deployment-845c8977d9-cnn9x from deployment-1592 started at 2023-01-05 10:41:56 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container httpd ready: false, restart count 0
    Jan  5 10:41:59.750: INFO: apiserver-proxy-fnbwn from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: blackbox-exporter-c866d5696-wkcmf from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: blackbox-exporter-c866d5696-z5dx5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: cilium-operator-6bf67c77c6-mmgs4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container cilium-operator ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: cilium-x98hw from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: coredns-7594945774-g79vz from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container coredns ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: coredns-7594945774-wghlc from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container coredns ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: csi-driver-node-wcklx from kube-system started at 2023-01-05 08:52:24 +0000 UTC (3 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: kube-proxy-worker-vot5k-v1.25.5-86pmg from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: metrics-server-b58b76d9c-jzq89 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: metrics-server-b58b76d9c-kwfd5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: node-exporter-xlnb4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: node-problem-detector-bzpk7 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: vpn-shoot-69957f7fdd-rmrkr from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container vpn-shoot ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-mw7c6 from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.750: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:41:59.750: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv before test
    Jan  5 10:41:59.764: INFO: test-new-deployment-845c8977d9-fzjq7 from deployment-1592 started at 2023-01-05 10:41:54 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.764: INFO: 	Container httpd ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: apiserver-proxy-4j65b from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.764: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: cilium-57wqw from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.764: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: csi-driver-node-rj5cq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (3 container statuses recorded)
    Jan  5 10:41:59.764: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: kube-proxy-worker-vot5k-v1.25.5-pg2pl from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.764: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: node-exporter-25qsq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.764: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: node-problem-detector-7xm42 from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.764: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: pod-adoption from replication-controller-4452 started at 2023-01-05 10:41:56 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.764: INFO: 	Container pod-adoption ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: sonobuoy from sonobuoy started at 2023-01-05 09:06:38 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.764: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-qvcts from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.764: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:41:59.764: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c before test
    Jan  5 10:41:59.780: INFO: apiserver-proxy-6plt4 from kube-system started at 2023-01-05 10:15:05 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.780: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: cilium-zljff from kube-system started at 2023-01-05 10:15:05 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.780: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: csi-driver-node-gznwc from kube-system started at 2023-01-05 10:15:05 +0000 UTC (3 container statuses recorded)
    Jan  5 10:41:59.780: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: kube-proxy-worker-vot5k-v1.25.5-5kqql from kube-system started at 2023-01-05 10:15:05 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.780: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: node-exporter-bn94z from kube-system started at 2023-01-05 10:15:05 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.780: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: node-problem-detector-2xncr from kube-system started at 2023-01-05 10:15:05 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.780: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: pod-secrets-28de92fd-732b-4345-89a5-64bd29ab0350 from secrets-6702 started at 2023-01-05 10:41:47 +0000 UTC (3 container statuses recorded)
    Jan  5 10:41:59.780: INFO: 	Container creates-volume-test ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: 	Container dels-volume-test ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: 	Container upds-volume-test ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-rp2vl from sonobuoy started at 2023-01-05 10:15:05 +0000 UTC (2 container statuses recorded)
    Jan  5 10:41:59.780: INFO: 	Container sonobuoy-worker ready: false, restart count 9
    Jan  5 10:41:59.780: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:41:59.780: INFO: pod-service-account-defaultsa from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.780: INFO: 	Container token-test ready: false, restart count 0
    Jan  5 10:41:59.780: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
    Jan  5 10:41:59.780: INFO: 	Container token-test ready: false, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 10:41:59.78
    Jan  5 10:41:59.789: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9858" to be "running"
    Jan  5 10:41:59.793: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.089741ms
    Jan  5 10:42:01.798: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009107243s
    Jan  5 10:42:01.798: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 10:42:01.803
    STEP: Trying to apply a random label on the found node. 01/05/23 10:42:01.835
    STEP: verifying the node has the label kubernetes.io/e2e-16b55192-e96d-4c5d-9487-ac0d63b6df8f 42 01/05/23 10:42:01.857
    STEP: Trying to relaunch the pod, now with labels. 01/05/23 10:42:01.865
    Jan  5 10:42:01.894: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9858" to be "not pending"
    Jan  5 10:42:01.899: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 5.168842ms
    Jan  5 10:42:03.906: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.011810093s
    Jan  5 10:42:03.906: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-16b55192-e96d-4c5d-9487-ac0d63b6df8f off the node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv 01/05/23 10:42:03.91
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-16b55192-e96d-4c5d-9487-ac0d63b6df8f 01/05/23 10:42:03.927
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 10:42:03.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9858" for this suite. 01/05/23 10:42:03.947
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:42:03.96
Jan  5 10:42:03.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename sched-pred 01/05/23 10:42:03.961
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:42:03.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:42:03.984
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan  5 10:42:03.998: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  5 10:42:04.015: INFO: Waiting for terminating namespaces to be deleted...
Jan  5 10:42:04.022: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t before test
Jan  5 10:42:04.040: INFO: apiserver-proxy-m4wgr from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.040: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:42:04.040: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:42:04.040: INFO: cilium-5qvr2 from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.040: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:42:04.040: INFO: csi-driver-node-nt6bm from kube-system started at 2023-01-05 08:52:50 +0000 UTC (3 container statuses recorded)
Jan  5 10:42:04.040: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:42:04.040: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:42:04.040: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:42:04.040: INFO: kube-proxy-worker-omyby-v1.25.5-xvlqq from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.040: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:42:04.040: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:42:04.040: INFO: node-exporter-hckzs from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.040: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:42:04.040: INFO: node-problem-detector-5ln6w from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.040: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:42:04.040: INFO: sonobuoy-e2e-job-2d2d23fb3e814b3d from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.040: INFO: 	Container e2e ready: true, restart count 0
Jan  5 10:42:04.040: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:42:04.040: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-m6xzl from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.040: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:42:04.040: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:42:04.040: INFO: pod-service-account-mountsa-mountspec from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.040: INFO: 	Container token-test ready: false, restart count 0
Jan  5 10:42:04.040: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r before test
Jan  5 10:42:04.060: INFO: apiserver-proxy-4w6xb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.060: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:42:04.060: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:42:04.060: INFO: cilium-cjv9l from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.060: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:42:04.060: INFO: cilium-operator-6bf67c77c6-r9hbq from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.060: INFO: 	Container cilium-operator ready: true, restart count 0
Jan  5 10:42:04.060: INFO: csi-driver-node-r2l8x from kube-system started at 2023-01-05 08:52:30 +0000 UTC (3 container statuses recorded)
Jan  5 10:42:04.060: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:42:04.060: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:42:04.060: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:42:04.060: INFO: kube-proxy-worker-omyby-v1.25.5-fz64v from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.060: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:42:04.060: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:42:04.060: INFO: node-exporter-7hcrb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.060: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:42:04.060: INFO: node-problem-detector-l85nm from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.060: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:42:04.060: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-xghsj from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.060: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:42:04.060: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:42:04.060: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.060: INFO: 	Container token-test ready: false, restart count 0
Jan  5 10:42:04.060: INFO: pod-service-account-mountsa from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.060: INFO: 	Container token-test ready: false, restart count 0
Jan  5 10:42:04.060: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 before test
Jan  5 10:42:04.091: INFO: apiserver-proxy-fnbwn from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:42:04.091: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:42:04.091: INFO: blackbox-exporter-c866d5696-wkcmf from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan  5 10:42:04.091: INFO: blackbox-exporter-c866d5696-z5dx5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan  5 10:42:04.091: INFO: cilium-operator-6bf67c77c6-mmgs4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container cilium-operator ready: true, restart count 0
Jan  5 10:42:04.091: INFO: cilium-x98hw from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:42:04.091: INFO: coredns-7594945774-g79vz from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container coredns ready: true, restart count 0
Jan  5 10:42:04.091: INFO: coredns-7594945774-wghlc from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container coredns ready: true, restart count 0
Jan  5 10:42:04.091: INFO: csi-driver-node-wcklx from kube-system started at 2023-01-05 08:52:24 +0000 UTC (3 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:42:04.091: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:42:04.091: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:42:04.091: INFO: kube-proxy-worker-vot5k-v1.25.5-86pmg from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:42:04.091: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:42:04.091: INFO: metrics-server-b58b76d9c-jzq89 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container metrics-server ready: true, restart count 0
Jan  5 10:42:04.091: INFO: metrics-server-b58b76d9c-kwfd5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container metrics-server ready: true, restart count 0
Jan  5 10:42:04.091: INFO: node-exporter-xlnb4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:42:04.091: INFO: node-problem-detector-bzpk7 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:42:04.091: INFO: vpn-shoot-69957f7fdd-rmrkr from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container vpn-shoot ready: true, restart count 0
Jan  5 10:42:04.091: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-mw7c6 from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.091: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:42:04.091: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:42:04.091: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv before test
Jan  5 10:42:04.102: INFO: apiserver-proxy-4j65b from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.102: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:42:04.102: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:42:04.102: INFO: cilium-57wqw from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.102: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:42:04.102: INFO: csi-driver-node-rj5cq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (3 container statuses recorded)
Jan  5 10:42:04.102: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:42:04.102: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:42:04.102: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:42:04.102: INFO: kube-proxy-worker-vot5k-v1.25.5-pg2pl from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.102: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:42:04.102: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:42:04.103: INFO: node-exporter-25qsq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.103: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:42:04.103: INFO: node-problem-detector-7xm42 from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.103: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:42:04.103: INFO: pod-adoption from replication-controller-4452 started at 2023-01-05 10:41:56 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.103: INFO: 	Container pod-adoption ready: true, restart count 0
Jan  5 10:42:04.103: INFO: with-labels from sched-pred-9858 started at 2023-01-05 10:42:01 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.103: INFO: 	Container with-labels ready: true, restart count 0
Jan  5 10:42:04.103: INFO: sonobuoy from sonobuoy started at 2023-01-05 09:06:38 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.103: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  5 10:42:04.103: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-qvcts from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.103: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 10:42:04.103: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:42:04.103: INFO: 
Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c before test
Jan  5 10:42:04.117: INFO: apiserver-proxy-6plt4 from kube-system started at 2023-01-05 10:15:05 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.117: INFO: 	Container proxy ready: true, restart count 0
Jan  5 10:42:04.117: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 10:42:04.117: INFO: cilium-zljff from kube-system started at 2023-01-05 10:15:05 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.117: INFO: 	Container cilium-agent ready: true, restart count 0
Jan  5 10:42:04.117: INFO: csi-driver-node-gznwc from kube-system started at 2023-01-05 10:15:05 +0000 UTC (3 container statuses recorded)
Jan  5 10:42:04.117: INFO: 	Container csi-driver ready: true, restart count 0
Jan  5 10:42:04.117: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan  5 10:42:04.117: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  5 10:42:04.117: INFO: kube-proxy-worker-vot5k-v1.25.5-5kqql from kube-system started at 2023-01-05 10:15:05 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.117: INFO: 	Container conntrack-fix ready: true, restart count 0
Jan  5 10:42:04.117: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 10:42:04.117: INFO: node-exporter-bn94z from kube-system started at 2023-01-05 10:15:05 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.117: INFO: 	Container node-exporter ready: true, restart count 0
Jan  5 10:42:04.117: INFO: node-problem-detector-2xncr from kube-system started at 2023-01-05 10:15:05 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.117: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan  5 10:42:04.117: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-rp2vl from sonobuoy started at 2023-01-05 10:15:05 +0000 UTC (2 container statuses recorded)
Jan  5 10:42:04.117: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Jan  5 10:42:04.117: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 10:42:04.117: INFO: pod-service-account-defaultsa from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.117: INFO: 	Container token-test ready: false, restart count 0
Jan  5 10:42:04.117: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
Jan  5 10:42:04.117: INFO: 	Container token-test ready: false, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 10:42:04.117
Jan  5 10:42:04.126: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1541" to be "running"
Jan  5 10:42:04.168: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 41.335559ms
Jan  5 10:42:06.175: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.048877538s
Jan  5 10:42:06.175: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 10:42:06.18
STEP: Trying to apply a random label on the found node. 01/05/23 10:42:06.194
STEP: verifying the node has the label kubernetes.io/e2e-73e53617-9df6-4f90-ac55-757e7e7c0dbb 95 01/05/23 10:42:06.208
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/05/23 10:42:06.214
Jan  5 10:42:06.226: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1541" to be "not pending"
Jan  5 10:42:06.230: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.099218ms
Jan  5 10:42:08.236: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010487621s
Jan  5 10:42:08.236: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.0.234 on the node which pod4 resides and expect not scheduled 01/05/23 10:42:08.236
Jan  5 10:42:08.246: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1541" to be "not pending"
Jan  5 10:42:08.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.47006ms
Jan  5 10:42:10.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011408927s
Jan  5 10:42:12.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010360141s
Jan  5 10:42:14.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010571378s
Jan  5 10:42:16.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010279201s
Jan  5 10:42:18.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010654974s
Jan  5 10:42:20.263: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016435588s
Jan  5 10:42:22.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010609634s
Jan  5 10:42:24.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011341217s
Jan  5 10:42:26.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009894284s
Jan  5 10:42:28.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.011175532s
Jan  5 10:42:30.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010324207s
Jan  5 10:42:32.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009425962s
Jan  5 10:42:34.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009379978s
Jan  5 10:42:36.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010659751s
Jan  5 10:42:38.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011574692s
Jan  5 10:42:40.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010383836s
Jan  5 10:42:42.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.010295691s
Jan  5 10:42:44.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009522603s
Jan  5 10:42:46.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010243102s
Jan  5 10:42:48.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010877605s
Jan  5 10:42:50.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010943162s
Jan  5 10:42:52.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010038866s
Jan  5 10:42:54.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009168238s
Jan  5 10:42:56.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.012342168s
Jan  5 10:42:58.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011390579s
Jan  5 10:43:00.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011188617s
Jan  5 10:43:02.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010022575s
Jan  5 10:43:04.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00883816s
Jan  5 10:43:06.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009776993s
Jan  5 10:43:08.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011654674s
Jan  5 10:43:10.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010411452s
Jan  5 10:43:12.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009364487s
Jan  5 10:43:14.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011492152s
Jan  5 10:43:16.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010444872s
Jan  5 10:43:18.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.00983873s
Jan  5 10:43:20.264: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.018117118s
Jan  5 10:43:22.259: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.012448055s
Jan  5 10:43:24.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009622604s
Jan  5 10:43:26.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009929826s
Jan  5 10:43:28.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009993927s
Jan  5 10:43:30.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009721181s
Jan  5 10:43:32.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009394241s
Jan  5 10:43:34.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009810327s
Jan  5 10:43:36.280: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.03395341s
Jan  5 10:43:38.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009777496s
Jan  5 10:43:40.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.012321666s
Jan  5 10:43:42.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010341467s
Jan  5 10:43:44.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011497677s
Jan  5 10:43:46.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010007436s
Jan  5 10:43:48.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010215866s
Jan  5 10:43:50.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011565155s
Jan  5 10:43:52.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010655653s
Jan  5 10:43:54.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.011321092s
Jan  5 10:43:56.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010134921s
Jan  5 10:43:58.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011150265s
Jan  5 10:44:00.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.010101322s
Jan  5 10:44:02.260: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013366452s
Jan  5 10:44:04.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011072974s
Jan  5 10:44:06.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010838689s
Jan  5 10:44:08.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011082885s
Jan  5 10:44:10.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.01000553s
Jan  5 10:44:12.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.010237275s
Jan  5 10:44:14.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.010870312s
Jan  5 10:44:16.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.011184368s
Jan  5 10:44:18.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.011111553s
Jan  5 10:44:20.294: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.047394579s
Jan  5 10:44:22.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.010466401s
Jan  5 10:44:24.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.010004366s
Jan  5 10:44:26.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010673814s
Jan  5 10:44:28.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01136891s
Jan  5 10:44:30.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.011238068s
Jan  5 10:44:32.260: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.013994351s
Jan  5 10:44:34.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.010238066s
Jan  5 10:44:36.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.011124844s
Jan  5 10:44:38.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.011132043s
Jan  5 10:44:40.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.010355793s
Jan  5 10:44:42.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.010753553s
Jan  5 10:44:44.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.072452813s
Jan  5 10:44:46.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.010042798s
Jan  5 10:44:48.281: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.035068131s
Jan  5 10:44:50.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.011097751s
Jan  5 10:44:52.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.010414112s
Jan  5 10:44:54.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.011316569s
Jan  5 10:44:56.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009713249s
Jan  5 10:44:58.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.010767404s
Jan  5 10:45:00.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.010347199s
Jan  5 10:45:02.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.01101315s
Jan  5 10:45:04.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.011248848s
Jan  5 10:45:06.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.010127493s
Jan  5 10:45:08.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009613929s
Jan  5 10:45:10.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.010862682s
Jan  5 10:45:12.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009957969s
Jan  5 10:45:14.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.010218367s
Jan  5 10:45:16.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.011613181s
Jan  5 10:45:18.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.010586974s
Jan  5 10:45:20.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.010258626s
Jan  5 10:45:22.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.010129984s
Jan  5 10:45:24.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.01169667s
Jan  5 10:45:26.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.010355175s
Jan  5 10:45:28.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.010718017s
Jan  5 10:45:30.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.01150376s
Jan  5 10:45:32.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.026088305s
Jan  5 10:45:34.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009952913s
Jan  5 10:45:36.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.011366684s
Jan  5 10:45:38.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.009481587s
Jan  5 10:45:40.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.010580093s
Jan  5 10:45:42.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.010381019s
Jan  5 10:45:44.261: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.015152923s
Jan  5 10:45:46.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009843245s
Jan  5 10:45:48.260: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.01421994s
Jan  5 10:45:50.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.011185009s
Jan  5 10:45:52.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.009718194s
Jan  5 10:45:54.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.010161282s
Jan  5 10:45:56.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010342746s
Jan  5 10:45:58.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.011971959s
Jan  5 10:46:00.262: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.015501877s
Jan  5 10:46:02.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.010942692s
Jan  5 10:46:04.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009286431s
Jan  5 10:46:06.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.010089715s
Jan  5 10:46:08.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.010800463s
Jan  5 10:46:10.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.011688922s
Jan  5 10:46:12.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.010455165s
Jan  5 10:46:14.259: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.013085242s
Jan  5 10:46:16.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009960889s
Jan  5 10:46:18.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.011793908s
Jan  5 10:46:20.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.011569401s
Jan  5 10:46:22.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.010048956s
Jan  5 10:46:24.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.011173352s
Jan  5 10:46:26.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009675108s
Jan  5 10:46:28.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.009344828s
Jan  5 10:46:30.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.011122522s
Jan  5 10:46:32.260: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.013934477s
Jan  5 10:46:34.259: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.012740659s
Jan  5 10:46:36.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.010348684s
Jan  5 10:46:38.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.011374058s
Jan  5 10:46:40.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.01089784s
Jan  5 10:46:42.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.009882248s
Jan  5 10:46:44.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.011711177s
Jan  5 10:46:46.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.010362882s
Jan  5 10:46:48.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.011385376s
Jan  5 10:46:50.261: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.014691747s
Jan  5 10:46:52.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.010185419s
Jan  5 10:46:54.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.011699666s
Jan  5 10:46:56.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010580453s
Jan  5 10:46:58.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.011048365s
Jan  5 10:47:00.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.010594398s
Jan  5 10:47:02.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.010564479s
Jan  5 10:47:04.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.011406954s
Jan  5 10:47:06.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.010868073s
Jan  5 10:47:08.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009689557s
Jan  5 10:47:08.259: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.013198038s
STEP: removing the label kubernetes.io/e2e-73e53617-9df6-4f90-ac55-757e7e7c0dbb off the node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c 01/05/23 10:47:08.259
STEP: verifying the node doesn't have the label kubernetes.io/e2e-73e53617-9df6-4f90-ac55-757e7e7c0dbb 01/05/23 10:47:08.28
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan  5 10:47:08.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1541" for this suite. 01/05/23 10:47:08.304
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":361,"skipped":6688,"failed":0}
------------------------------
â€¢ [SLOW TEST] [304.352 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:42:03.96
    Jan  5 10:42:03.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename sched-pred 01/05/23 10:42:03.961
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:42:03.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:42:03.984
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan  5 10:42:03.998: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  5 10:42:04.015: INFO: Waiting for terminating namespaces to be deleted...
    Jan  5 10:42:04.022: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-l9p9t before test
    Jan  5 10:42:04.040: INFO: apiserver-proxy-m4wgr from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.040: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: cilium-5qvr2 from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.040: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: csi-driver-node-nt6bm from kube-system started at 2023-01-05 08:52:50 +0000 UTC (3 container statuses recorded)
    Jan  5 10:42:04.040: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: kube-proxy-worker-omyby-v1.25.5-xvlqq from kube-system started at 2023-01-05 08:52:50 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.040: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: node-exporter-hckzs from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.040: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: node-problem-detector-5ln6w from kube-system started at 2023-01-05 08:52:50 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.040: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: sonobuoy-e2e-job-2d2d23fb3e814b3d from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.040: INFO: 	Container e2e ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-m6xzl from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.040: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:42:04.040: INFO: pod-service-account-mountsa-mountspec from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.040: INFO: 	Container token-test ready: false, restart count 0
    Jan  5 10:42:04.040: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-omyby-z1-6cd58-t947r before test
    Jan  5 10:42:04.060: INFO: apiserver-proxy-4w6xb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.060: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: cilium-cjv9l from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.060: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: cilium-operator-6bf67c77c6-r9hbq from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.060: INFO: 	Container cilium-operator ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: csi-driver-node-r2l8x from kube-system started at 2023-01-05 08:52:30 +0000 UTC (3 container statuses recorded)
    Jan  5 10:42:04.060: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: kube-proxy-worker-omyby-v1.25.5-fz64v from kube-system started at 2023-01-05 08:52:30 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.060: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: node-exporter-7hcrb from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.060: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: node-problem-detector-l85nm from kube-system started at 2023-01-05 08:52:30 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.060: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-xghsj from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.060: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:42:04.060: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.060: INFO: 	Container token-test ready: false, restart count 0
    Jan  5 10:42:04.060: INFO: pod-service-account-mountsa from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.060: INFO: 	Container token-test ready: false, restart count 0
    Jan  5 10:42:04.060: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-7gn22 before test
    Jan  5 10:42:04.091: INFO: apiserver-proxy-fnbwn from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: blackbox-exporter-c866d5696-wkcmf from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: blackbox-exporter-c866d5696-z5dx5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: cilium-operator-6bf67c77c6-mmgs4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container cilium-operator ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: cilium-x98hw from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: coredns-7594945774-g79vz from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container coredns ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: coredns-7594945774-wghlc from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container coredns ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: csi-driver-node-wcklx from kube-system started at 2023-01-05 08:52:24 +0000 UTC (3 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: kube-proxy-worker-vot5k-v1.25.5-86pmg from kube-system started at 2023-01-05 08:52:24 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: metrics-server-b58b76d9c-jzq89 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: metrics-server-b58b76d9c-kwfd5 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: node-exporter-xlnb4 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: node-problem-detector-bzpk7 from kube-system started at 2023-01-05 08:52:24 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: vpn-shoot-69957f7fdd-rmrkr from kube-system started at 2023-01-05 08:53:12 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container vpn-shoot ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-mw7c6 from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.091: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:42:04.091: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-8pbvv before test
    Jan  5 10:42:04.102: INFO: apiserver-proxy-4j65b from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.102: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:42:04.102: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:42:04.102: INFO: cilium-57wqw from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.102: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:42:04.102: INFO: csi-driver-node-rj5cq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (3 container statuses recorded)
    Jan  5 10:42:04.102: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:42:04.102: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:42:04.102: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:42:04.102: INFO: kube-proxy-worker-vot5k-v1.25.5-pg2pl from kube-system started at 2023-01-05 08:52:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.102: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:42:04.102: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:42:04.103: INFO: node-exporter-25qsq from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.103: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:42:04.103: INFO: node-problem-detector-7xm42 from kube-system started at 2023-01-05 08:52:42 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.103: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:42:04.103: INFO: pod-adoption from replication-controller-4452 started at 2023-01-05 10:41:56 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.103: INFO: 	Container pod-adoption ready: true, restart count 0
    Jan  5 10:42:04.103: INFO: with-labels from sched-pred-9858 started at 2023-01-05 10:42:01 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.103: INFO: 	Container with-labels ready: true, restart count 0
    Jan  5 10:42:04.103: INFO: sonobuoy from sonobuoy started at 2023-01-05 09:06:38 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.103: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  5 10:42:04.103: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-qvcts from sonobuoy started at 2023-01-05 09:06:42 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.103: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 10:42:04.103: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:42:04.103: INFO: 
    Logging pods the apiserver thinks is on node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c before test
    Jan  5 10:42:04.117: INFO: apiserver-proxy-6plt4 from kube-system started at 2023-01-05 10:15:05 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.117: INFO: 	Container proxy ready: true, restart count 0
    Jan  5 10:42:04.117: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 10:42:04.117: INFO: cilium-zljff from kube-system started at 2023-01-05 10:15:05 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.117: INFO: 	Container cilium-agent ready: true, restart count 0
    Jan  5 10:42:04.117: INFO: csi-driver-node-gznwc from kube-system started at 2023-01-05 10:15:05 +0000 UTC (3 container statuses recorded)
    Jan  5 10:42:04.117: INFO: 	Container csi-driver ready: true, restart count 0
    Jan  5 10:42:04.117: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan  5 10:42:04.117: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  5 10:42:04.117: INFO: kube-proxy-worker-vot5k-v1.25.5-5kqql from kube-system started at 2023-01-05 10:15:05 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.117: INFO: 	Container conntrack-fix ready: true, restart count 0
    Jan  5 10:42:04.117: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 10:42:04.117: INFO: node-exporter-bn94z from kube-system started at 2023-01-05 10:15:05 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.117: INFO: 	Container node-exporter ready: true, restart count 0
    Jan  5 10:42:04.117: INFO: node-problem-detector-2xncr from kube-system started at 2023-01-05 10:15:05 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.117: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan  5 10:42:04.117: INFO: sonobuoy-systemd-logs-daemon-set-fbdc0534336b4c0c-rp2vl from sonobuoy started at 2023-01-05 10:15:05 +0000 UTC (2 container statuses recorded)
    Jan  5 10:42:04.117: INFO: 	Container sonobuoy-worker ready: false, restart count 9
    Jan  5 10:42:04.117: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 10:42:04.117: INFO: pod-service-account-defaultsa from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.117: INFO: 	Container token-test ready: false, restart count 0
    Jan  5 10:42:04.117: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-7011 started at 2023-01-05 10:41:35 +0000 UTC (1 container statuses recorded)
    Jan  5 10:42:04.117: INFO: 	Container token-test ready: false, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 10:42:04.117
    Jan  5 10:42:04.126: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1541" to be "running"
    Jan  5 10:42:04.168: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 41.335559ms
    Jan  5 10:42:06.175: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.048877538s
    Jan  5 10:42:06.175: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 10:42:06.18
    STEP: Trying to apply a random label on the found node. 01/05/23 10:42:06.194
    STEP: verifying the node has the label kubernetes.io/e2e-73e53617-9df6-4f90-ac55-757e7e7c0dbb 95 01/05/23 10:42:06.208
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/05/23 10:42:06.214
    Jan  5 10:42:06.226: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1541" to be "not pending"
    Jan  5 10:42:06.230: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.099218ms
    Jan  5 10:42:08.236: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010487621s
    Jan  5 10:42:08.236: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.0.234 on the node which pod4 resides and expect not scheduled 01/05/23 10:42:08.236
    Jan  5 10:42:08.246: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1541" to be "not pending"
    Jan  5 10:42:08.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.47006ms
    Jan  5 10:42:10.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011408927s
    Jan  5 10:42:12.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010360141s
    Jan  5 10:42:14.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010571378s
    Jan  5 10:42:16.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010279201s
    Jan  5 10:42:18.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010654974s
    Jan  5 10:42:20.263: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016435588s
    Jan  5 10:42:22.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010609634s
    Jan  5 10:42:24.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011341217s
    Jan  5 10:42:26.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009894284s
    Jan  5 10:42:28.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.011175532s
    Jan  5 10:42:30.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010324207s
    Jan  5 10:42:32.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009425962s
    Jan  5 10:42:34.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009379978s
    Jan  5 10:42:36.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010659751s
    Jan  5 10:42:38.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011574692s
    Jan  5 10:42:40.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010383836s
    Jan  5 10:42:42.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.010295691s
    Jan  5 10:42:44.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009522603s
    Jan  5 10:42:46.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010243102s
    Jan  5 10:42:48.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010877605s
    Jan  5 10:42:50.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010943162s
    Jan  5 10:42:52.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010038866s
    Jan  5 10:42:54.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009168238s
    Jan  5 10:42:56.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.012342168s
    Jan  5 10:42:58.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011390579s
    Jan  5 10:43:00.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011188617s
    Jan  5 10:43:02.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010022575s
    Jan  5 10:43:04.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00883816s
    Jan  5 10:43:06.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009776993s
    Jan  5 10:43:08.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011654674s
    Jan  5 10:43:10.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010411452s
    Jan  5 10:43:12.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009364487s
    Jan  5 10:43:14.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011492152s
    Jan  5 10:43:16.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010444872s
    Jan  5 10:43:18.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.00983873s
    Jan  5 10:43:20.264: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.018117118s
    Jan  5 10:43:22.259: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.012448055s
    Jan  5 10:43:24.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009622604s
    Jan  5 10:43:26.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009929826s
    Jan  5 10:43:28.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009993927s
    Jan  5 10:43:30.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009721181s
    Jan  5 10:43:32.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009394241s
    Jan  5 10:43:34.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009810327s
    Jan  5 10:43:36.280: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.03395341s
    Jan  5 10:43:38.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009777496s
    Jan  5 10:43:40.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.012321666s
    Jan  5 10:43:42.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010341467s
    Jan  5 10:43:44.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011497677s
    Jan  5 10:43:46.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010007436s
    Jan  5 10:43:48.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010215866s
    Jan  5 10:43:50.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011565155s
    Jan  5 10:43:52.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010655653s
    Jan  5 10:43:54.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.011321092s
    Jan  5 10:43:56.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010134921s
    Jan  5 10:43:58.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011150265s
    Jan  5 10:44:00.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.010101322s
    Jan  5 10:44:02.260: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013366452s
    Jan  5 10:44:04.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011072974s
    Jan  5 10:44:06.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010838689s
    Jan  5 10:44:08.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011082885s
    Jan  5 10:44:10.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.01000553s
    Jan  5 10:44:12.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.010237275s
    Jan  5 10:44:14.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.010870312s
    Jan  5 10:44:16.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.011184368s
    Jan  5 10:44:18.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.011111553s
    Jan  5 10:44:20.294: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.047394579s
    Jan  5 10:44:22.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.010466401s
    Jan  5 10:44:24.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.010004366s
    Jan  5 10:44:26.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010673814s
    Jan  5 10:44:28.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01136891s
    Jan  5 10:44:30.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.011238068s
    Jan  5 10:44:32.260: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.013994351s
    Jan  5 10:44:34.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.010238066s
    Jan  5 10:44:36.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.011124844s
    Jan  5 10:44:38.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.011132043s
    Jan  5 10:44:40.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.010355793s
    Jan  5 10:44:42.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.010753553s
    Jan  5 10:44:44.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.072452813s
    Jan  5 10:44:46.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.010042798s
    Jan  5 10:44:48.281: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.035068131s
    Jan  5 10:44:50.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.011097751s
    Jan  5 10:44:52.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.010414112s
    Jan  5 10:44:54.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.011316569s
    Jan  5 10:44:56.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009713249s
    Jan  5 10:44:58.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.010767404s
    Jan  5 10:45:00.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.010347199s
    Jan  5 10:45:02.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.01101315s
    Jan  5 10:45:04.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.011248848s
    Jan  5 10:45:06.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.010127493s
    Jan  5 10:45:08.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009613929s
    Jan  5 10:45:10.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.010862682s
    Jan  5 10:45:12.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009957969s
    Jan  5 10:45:14.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.010218367s
    Jan  5 10:45:16.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.011613181s
    Jan  5 10:45:18.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.010586974s
    Jan  5 10:45:20.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.010258626s
    Jan  5 10:45:22.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.010129984s
    Jan  5 10:45:24.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.01169667s
    Jan  5 10:45:26.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.010355175s
    Jan  5 10:45:28.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.010718017s
    Jan  5 10:45:30.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.01150376s
    Jan  5 10:45:32.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.026088305s
    Jan  5 10:45:34.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009952913s
    Jan  5 10:45:36.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.011366684s
    Jan  5 10:45:38.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.009481587s
    Jan  5 10:45:40.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.010580093s
    Jan  5 10:45:42.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.010381019s
    Jan  5 10:45:44.261: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.015152923s
    Jan  5 10:45:46.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009843245s
    Jan  5 10:45:48.260: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.01421994s
    Jan  5 10:45:50.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.011185009s
    Jan  5 10:45:52.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.009718194s
    Jan  5 10:45:54.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.010161282s
    Jan  5 10:45:56.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010342746s
    Jan  5 10:45:58.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.011971959s
    Jan  5 10:46:00.262: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.015501877s
    Jan  5 10:46:02.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.010942692s
    Jan  5 10:46:04.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009286431s
    Jan  5 10:46:06.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.010089715s
    Jan  5 10:46:08.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.010800463s
    Jan  5 10:46:10.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.011688922s
    Jan  5 10:46:12.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.010455165s
    Jan  5 10:46:14.259: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.013085242s
    Jan  5 10:46:16.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009960889s
    Jan  5 10:46:18.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.011793908s
    Jan  5 10:46:20.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.011569401s
    Jan  5 10:46:22.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.010048956s
    Jan  5 10:46:24.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.011173352s
    Jan  5 10:46:26.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009675108s
    Jan  5 10:46:28.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.009344828s
    Jan  5 10:46:30.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.011122522s
    Jan  5 10:46:32.260: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.013934477s
    Jan  5 10:46:34.259: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.012740659s
    Jan  5 10:46:36.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.010348684s
    Jan  5 10:46:38.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.011374058s
    Jan  5 10:46:40.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.01089784s
    Jan  5 10:46:42.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.009882248s
    Jan  5 10:46:44.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.011711177s
    Jan  5 10:46:46.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.010362882s
    Jan  5 10:46:48.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.011385376s
    Jan  5 10:46:50.261: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.014691747s
    Jan  5 10:46:52.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.010185419s
    Jan  5 10:46:54.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.011699666s
    Jan  5 10:46:56.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010580453s
    Jan  5 10:46:58.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.011048365s
    Jan  5 10:47:00.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.010594398s
    Jan  5 10:47:02.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.010564479s
    Jan  5 10:47:04.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.011406954s
    Jan  5 10:47:06.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.010868073s
    Jan  5 10:47:08.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009689557s
    Jan  5 10:47:08.259: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.013198038s
    STEP: removing the label kubernetes.io/e2e-73e53617-9df6-4f90-ac55-757e7e7c0dbb off the node shoot--kd500770--k8s-cncf-conf-worker-vot5k-z1-8689b-mvv9c 01/05/23 10:47:08.259
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-73e53617-9df6-4f90-ac55-757e7e7c0dbb 01/05/23 10:47:08.28
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 10:47:08.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1541" for this suite. 01/05/23 10:47:08.304
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 10:47:08.312
Jan  5 10:47:08.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
STEP: Building a namespace api object, basename lease-test 01/05/23 10:47:08.313
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:47:08.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:47:08.337
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jan  5 10:47:08.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8444" for this suite. 01/05/23 10:47:08.426
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":362,"skipped":6693,"failed":0}
------------------------------
â€¢ [0.120 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 10:47:08.312
    Jan  5 10:47:08.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1415592784
    STEP: Building a namespace api object, basename lease-test 01/05/23 10:47:08.313
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 10:47:08.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 10:47:08.337
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jan  5 10:47:08.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-8444" for this suite. 01/05/23 10:47:08.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Jan  5 10:47:08.434: INFO: Running AfterSuite actions on all nodes
Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jan  5 10:47:08.434: INFO: Running AfterSuite actions on node 1
Jan  5 10:47:08.434: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan  5 10:47:08.434: INFO: Running AfterSuite actions on all nodes
    Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jan  5 10:47:08.434: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan  5 10:47:08.434: INFO: Running AfterSuite actions on node 1
    Jan  5 10:47:08.434: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.047 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 6012.353 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h40m12.56098192s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

