I0903 20:38:14.087060      24 e2e.go:116] Starting e2e run "1f4750ba-5505-4208-8172-9446f89360af" on Ginkgo node 1
Sep  3 20:38:14.111: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1662237493 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Sep  3 20:38:14.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
E0903 20:38:14.415397      24 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0903 20:38:14.415397      24 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Sep  3 20:38:14.415: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  3 20:38:14.441: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  3 20:38:14.513: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  3 20:38:14.513: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep  3 20:38:14.513: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  3 20:38:14.528: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kindnet' (0 seconds elapsed)
Sep  3 20:38:14.528: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep  3 20:38:14.528: INFO: e2e test version: v1.25.0
Sep  3 20:38:14.530: INFO: kube-apiserver version: v1.25.0
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Sep  3 20:38:14.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 20:38:14.535: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.123 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Sep  3 20:38:14.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    E0903 20:38:14.415397      24 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Sep  3 20:38:14.415: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Sep  3 20:38:14.441: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Sep  3 20:38:14.513: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Sep  3 20:38:14.513: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
    Sep  3 20:38:14.513: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Sep  3 20:38:14.528: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kindnet' (0 seconds elapsed)
    Sep  3 20:38:14.528: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Sep  3 20:38:14.528: INFO: e2e test version: v1.25.0
    Sep  3 20:38:14.530: INFO: kube-apiserver version: v1.25.0
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Sep  3 20:38:14.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 20:38:14.535: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:38:14.593
Sep  3 20:38:14.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename job 09/03/22 20:38:14.594
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:14.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:14.62
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 09/03/22 20:38:14.627
STEP: Ensure pods equal to paralellism count is attached to the job 09/03/22 20:38:14.688
STEP: patching /status 09/03/22 20:38:24.699
STEP: updating /status 09/03/22 20:38:24.71
STEP: get /status 09/03/22 20:38:24.731
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Sep  3 20:38:24.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4432" for this suite. 09/03/22 20:38:24.737
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":1,"skipped":22,"failed":0}
------------------------------
• [SLOW TEST] [10.149 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:38:14.593
    Sep  3 20:38:14.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename job 09/03/22 20:38:14.594
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:14.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:14.62
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 09/03/22 20:38:14.627
    STEP: Ensure pods equal to paralellism count is attached to the job 09/03/22 20:38:14.688
    STEP: patching /status 09/03/22 20:38:24.699
    STEP: updating /status 09/03/22 20:38:24.71
    STEP: get /status 09/03/22 20:38:24.731
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Sep  3 20:38:24.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4432" for this suite. 09/03/22 20:38:24.737
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:38:24.743
Sep  3 20:38:24.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename podtemplate 09/03/22 20:38:24.745
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:24.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:24.816
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 09/03/22 20:38:24.818
Sep  3 20:38:24.823: INFO: created test-podtemplate-1
Sep  3 20:38:24.838: INFO: created test-podtemplate-2
Sep  3 20:38:24.850: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 09/03/22 20:38:24.85
STEP: delete collection of pod templates 09/03/22 20:38:24.856
Sep  3 20:38:24.856: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 09/03/22 20:38:24.874
Sep  3 20:38:24.874: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Sep  3 20:38:24.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-334" for this suite. 09/03/22 20:38:24.881
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":2,"skipped":23,"failed":0}
------------------------------
• [0.143 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:38:24.743
    Sep  3 20:38:24.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename podtemplate 09/03/22 20:38:24.745
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:24.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:24.816
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 09/03/22 20:38:24.818
    Sep  3 20:38:24.823: INFO: created test-podtemplate-1
    Sep  3 20:38:24.838: INFO: created test-podtemplate-2
    Sep  3 20:38:24.850: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 09/03/22 20:38:24.85
    STEP: delete collection of pod templates 09/03/22 20:38:24.856
    Sep  3 20:38:24.856: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 09/03/22 20:38:24.874
    Sep  3 20:38:24.874: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Sep  3 20:38:24.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-334" for this suite. 09/03/22 20:38:24.881
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:38:24.886
Sep  3 20:38:24.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename security-context-test 09/03/22 20:38:24.887
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:24.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:24.924
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Sep  3 20:38:24.944: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518" in namespace "security-context-test-1606" to be "Succeeded or Failed"
Sep  3 20:38:24.958: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518": Phase="Pending", Reason="", readiness=false. Elapsed: 13.648134ms
Sep  3 20:38:27.120: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518": Phase="Pending", Reason="", readiness=false. Elapsed: 2.175704671s
Sep  3 20:38:28.964: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019715823s
Sep  3 20:38:30.961: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017186354s
Sep  3 20:38:32.961: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518": Phase="Running", Reason="", readiness=false. Elapsed: 8.016559783s
Sep  3 20:38:34.961: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.016937414s
Sep  3 20:38:34.961: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Sep  3 20:38:34.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1606" for this suite. 09/03/22 20:38:34.978
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":3,"skipped":24,"failed":0}
------------------------------
• [SLOW TEST] [10.095 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:38:24.886
    Sep  3 20:38:24.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename security-context-test 09/03/22 20:38:24.887
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:24.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:24.924
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Sep  3 20:38:24.944: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518" in namespace "security-context-test-1606" to be "Succeeded or Failed"
    Sep  3 20:38:24.958: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518": Phase="Pending", Reason="", readiness=false. Elapsed: 13.648134ms
    Sep  3 20:38:27.120: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518": Phase="Pending", Reason="", readiness=false. Elapsed: 2.175704671s
    Sep  3 20:38:28.964: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019715823s
    Sep  3 20:38:30.961: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017186354s
    Sep  3 20:38:32.961: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518": Phase="Running", Reason="", readiness=false. Elapsed: 8.016559783s
    Sep  3 20:38:34.961: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.016937414s
    Sep  3 20:38:34.961: INFO: Pod "alpine-nnp-false-c6009179-ce6a-4dfc-b2e3-915365497518" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Sep  3 20:38:34.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1606" for this suite. 09/03/22 20:38:34.978
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:38:34.983
Sep  3 20:38:34.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename podtemplate 09/03/22 20:38:34.984
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:34.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:34.995
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Sep  3 20:38:35.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3243" for this suite. 09/03/22 20:38:35.017
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":4,"skipped":24,"failed":0}
------------------------------
• [0.038 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:38:34.983
    Sep  3 20:38:34.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename podtemplate 09/03/22 20:38:34.984
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:34.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:34.995
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Sep  3 20:38:35.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3243" for this suite. 09/03/22 20:38:35.017
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:38:35.022
Sep  3 20:38:35.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 20:38:35.023
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:35.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:35.035
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 09/03/22 20:38:35.038
Sep  3 20:38:35.045: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-e070bc9e-e879-4aed-8dc8-3d2be5e4aece" in namespace "emptydir-8763" to be "running"
Sep  3 20:38:35.054: INFO: Pod "pod-sharedvolume-e070bc9e-e879-4aed-8dc8-3d2be5e4aece": Phase="Pending", Reason="", readiness=false. Elapsed: 8.749822ms
Sep  3 20:38:37.056: INFO: Pod "pod-sharedvolume-e070bc9e-e879-4aed-8dc8-3d2be5e4aece": Phase="Running", Reason="", readiness=false. Elapsed: 2.011508659s
Sep  3 20:38:37.056: INFO: Pod "pod-sharedvolume-e070bc9e-e879-4aed-8dc8-3d2be5e4aece" satisfied condition "running"
STEP: Reading file content from the nginx-container 09/03/22 20:38:37.056
Sep  3 20:38:37.056: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8763 PodName:pod-sharedvolume-e070bc9e-e879-4aed-8dc8-3d2be5e4aece ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 20:38:37.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 20:38:37.057: INFO: ExecWithOptions: Clientset creation
Sep  3 20:38:37.057: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8763/pods/pod-sharedvolume-e070bc9e-e879-4aed-8dc8-3d2be5e4aece/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Sep  3 20:38:37.136: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 20:38:37.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8763" for this suite. 09/03/22 20:38:37.139
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":5,"skipped":26,"failed":0}
------------------------------
• [2.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:38:35.022
    Sep  3 20:38:35.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 20:38:35.023
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:35.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:35.035
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 09/03/22 20:38:35.038
    Sep  3 20:38:35.045: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-e070bc9e-e879-4aed-8dc8-3d2be5e4aece" in namespace "emptydir-8763" to be "running"
    Sep  3 20:38:35.054: INFO: Pod "pod-sharedvolume-e070bc9e-e879-4aed-8dc8-3d2be5e4aece": Phase="Pending", Reason="", readiness=false. Elapsed: 8.749822ms
    Sep  3 20:38:37.056: INFO: Pod "pod-sharedvolume-e070bc9e-e879-4aed-8dc8-3d2be5e4aece": Phase="Running", Reason="", readiness=false. Elapsed: 2.011508659s
    Sep  3 20:38:37.056: INFO: Pod "pod-sharedvolume-e070bc9e-e879-4aed-8dc8-3d2be5e4aece" satisfied condition "running"
    STEP: Reading file content from the nginx-container 09/03/22 20:38:37.056
    Sep  3 20:38:37.056: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8763 PodName:pod-sharedvolume-e070bc9e-e879-4aed-8dc8-3d2be5e4aece ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 20:38:37.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 20:38:37.057: INFO: ExecWithOptions: Clientset creation
    Sep  3 20:38:37.057: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8763/pods/pod-sharedvolume-e070bc9e-e879-4aed-8dc8-3d2be5e4aece/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Sep  3 20:38:37.136: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 20:38:37.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8763" for this suite. 09/03/22 20:38:37.139
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:38:37.143
Sep  3 20:38:37.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 20:38:37.143
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:37.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:37.154
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 09/03/22 20:38:37.156
Sep  3 20:38:37.161: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd" in namespace "projected-2386" to be "Succeeded or Failed"
Sep  3 20:38:37.174: INFO: Pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.160032ms
Sep  3 20:38:39.188: INFO: Pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027197497s
Sep  3 20:38:41.183: INFO: Pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022528812s
Sep  3 20:38:43.182: INFO: Pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021788737s
Sep  3 20:38:45.182: INFO: Pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021269763s
STEP: Saw pod success 09/03/22 20:38:45.182
Sep  3 20:38:45.182: INFO: Pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd" satisfied condition "Succeeded or Failed"
Sep  3 20:38:45.184: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd container client-container: <nil>
STEP: delete the pod 09/03/22 20:38:45.201
Sep  3 20:38:45.208: INFO: Waiting for pod downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd to disappear
Sep  3 20:38:45.217: INFO: Pod downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Sep  3 20:38:45.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2386" for this suite. 09/03/22 20:38:45.22
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":6,"skipped":26,"failed":0}
------------------------------
• [SLOW TEST] [8.080 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:38:37.143
    Sep  3 20:38:37.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 20:38:37.143
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:37.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:37.154
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 09/03/22 20:38:37.156
    Sep  3 20:38:37.161: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd" in namespace "projected-2386" to be "Succeeded or Failed"
    Sep  3 20:38:37.174: INFO: Pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.160032ms
    Sep  3 20:38:39.188: INFO: Pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027197497s
    Sep  3 20:38:41.183: INFO: Pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022528812s
    Sep  3 20:38:43.182: INFO: Pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021788737s
    Sep  3 20:38:45.182: INFO: Pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021269763s
    STEP: Saw pod success 09/03/22 20:38:45.182
    Sep  3 20:38:45.182: INFO: Pod "downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd" satisfied condition "Succeeded or Failed"
    Sep  3 20:38:45.184: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd container client-container: <nil>
    STEP: delete the pod 09/03/22 20:38:45.201
    Sep  3 20:38:45.208: INFO: Waiting for pod downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd to disappear
    Sep  3 20:38:45.217: INFO: Pod downwardapi-volume-3bb57490-e71a-4569-ae24-7cd3ee6b15cd no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Sep  3 20:38:45.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2386" for this suite. 09/03/22 20:38:45.22
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:38:45.229
Sep  3 20:38:45.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename podtemplate 09/03/22 20:38:45.23
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:45.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:45.246
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 09/03/22 20:38:45.248
STEP: Replace a pod template 09/03/22 20:38:45.251
Sep  3 20:38:45.256: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Sep  3 20:38:45.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7032" for this suite. 09/03/22 20:38:45.259
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":7,"skipped":50,"failed":0}
------------------------------
• [0.033 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:38:45.229
    Sep  3 20:38:45.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename podtemplate 09/03/22 20:38:45.23
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:45.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:45.246
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 09/03/22 20:38:45.248
    STEP: Replace a pod template 09/03/22 20:38:45.251
    Sep  3 20:38:45.256: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Sep  3 20:38:45.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-7032" for this suite. 09/03/22 20:38:45.259
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:38:45.263
Sep  3 20:38:45.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 20:38:45.264
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:45.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:45.275
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 20:38:45.284
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 20:38:46.009
STEP: Deploying the webhook pod 09/03/22 20:38:46.019
STEP: Wait for the deployment to be ready 09/03/22 20:38:46.03
Sep  3 20:38:46.047: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  3 20:38:48.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 20, 38, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 20, 38, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 20, 38, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 20, 38, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/03/22 20:38:50.062
STEP: Verifying the service has paired with the endpoint 09/03/22 20:38:50.071
Sep  3 20:38:51.072: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 09/03/22 20:38:51.074
STEP: create a pod 09/03/22 20:38:51.087
Sep  3 20:38:51.091: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7942" to be "running"
Sep  3 20:38:51.095: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.578809ms
Sep  3 20:38:53.097: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006108939s
Sep  3 20:38:55.098: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006170101s
Sep  3 20:38:55.098: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 09/03/22 20:38:55.098
Sep  3 20:38:55.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=webhook-7942 attach --namespace=webhook-7942 to-be-attached-pod -i -c=container1'
Sep  3 20:38:55.228: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 20:38:55.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7942" for this suite. 09/03/22 20:38:55.239
STEP: Destroying namespace "webhook-7942-markers" for this suite. 09/03/22 20:38:55.244
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":8,"skipped":73,"failed":0}
------------------------------
• [SLOW TEST] [10.084 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:38:45.263
    Sep  3 20:38:45.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 20:38:45.264
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:45.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:45.275
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 20:38:45.284
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 20:38:46.009
    STEP: Deploying the webhook pod 09/03/22 20:38:46.019
    STEP: Wait for the deployment to be ready 09/03/22 20:38:46.03
    Sep  3 20:38:46.047: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  3 20:38:48.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 20, 38, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 20, 38, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 20, 38, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 20, 38, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/03/22 20:38:50.062
    STEP: Verifying the service has paired with the endpoint 09/03/22 20:38:50.071
    Sep  3 20:38:51.072: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 09/03/22 20:38:51.074
    STEP: create a pod 09/03/22 20:38:51.087
    Sep  3 20:38:51.091: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7942" to be "running"
    Sep  3 20:38:51.095: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.578809ms
    Sep  3 20:38:53.097: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006108939s
    Sep  3 20:38:55.098: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006170101s
    Sep  3 20:38:55.098: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 09/03/22 20:38:55.098
    Sep  3 20:38:55.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=webhook-7942 attach --namespace=webhook-7942 to-be-attached-pod -i -c=container1'
    Sep  3 20:38:55.228: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 20:38:55.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7942" for this suite. 09/03/22 20:38:55.239
    STEP: Destroying namespace "webhook-7942-markers" for this suite. 09/03/22 20:38:55.244
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:38:55.347
Sep  3 20:38:55.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 20:38:55.349
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:55.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:55.405
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 09/03/22 20:38:55.413
Sep  3 20:38:55.414: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1262 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 09/03/22 20:38:55.569
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 20:38:55.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1262" for this suite. 09/03/22 20:38:55.768
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":9,"skipped":77,"failed":0}
------------------------------
• [0.429 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:38:55.347
    Sep  3 20:38:55.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 20:38:55.349
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:55.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:55.405
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 09/03/22 20:38:55.413
    Sep  3 20:38:55.414: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1262 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 09/03/22 20:38:55.569
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 20:38:55.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1262" for this suite. 09/03/22 20:38:55.768
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:38:55.777
Sep  3 20:38:55.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 20:38:55.779
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:55.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:55.798
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/03/22 20:38:55.801
Sep  3 20:38:55.806: INFO: Waiting up to 5m0s for pod "pod-4879463c-c869-4c1a-8232-680a354e64dc" in namespace "emptydir-3207" to be "Succeeded or Failed"
Sep  3 20:38:55.813: INFO: Pod "pod-4879463c-c869-4c1a-8232-680a354e64dc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.819098ms
Sep  3 20:38:57.817: INFO: Pod "pod-4879463c-c869-4c1a-8232-680a354e64dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010458956s
Sep  3 20:38:59.818: INFO: Pod "pod-4879463c-c869-4c1a-8232-680a354e64dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011767713s
STEP: Saw pod success 09/03/22 20:38:59.819
Sep  3 20:38:59.819: INFO: Pod "pod-4879463c-c869-4c1a-8232-680a354e64dc" satisfied condition "Succeeded or Failed"
Sep  3 20:38:59.822: INFO: Trying to get logs from node kind-worker2 pod pod-4879463c-c869-4c1a-8232-680a354e64dc container test-container: <nil>
STEP: delete the pod 09/03/22 20:38:59.826
Sep  3 20:38:59.833: INFO: Waiting for pod pod-4879463c-c869-4c1a-8232-680a354e64dc to disappear
Sep  3 20:38:59.835: INFO: Pod pod-4879463c-c869-4c1a-8232-680a354e64dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 20:38:59.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3207" for this suite. 09/03/22 20:38:59.84
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":10,"skipped":81,"failed":0}
------------------------------
• [4.074 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:38:55.777
    Sep  3 20:38:55.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 20:38:55.779
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:55.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:55.798
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/03/22 20:38:55.801
    Sep  3 20:38:55.806: INFO: Waiting up to 5m0s for pod "pod-4879463c-c869-4c1a-8232-680a354e64dc" in namespace "emptydir-3207" to be "Succeeded or Failed"
    Sep  3 20:38:55.813: INFO: Pod "pod-4879463c-c869-4c1a-8232-680a354e64dc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.819098ms
    Sep  3 20:38:57.817: INFO: Pod "pod-4879463c-c869-4c1a-8232-680a354e64dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010458956s
    Sep  3 20:38:59.818: INFO: Pod "pod-4879463c-c869-4c1a-8232-680a354e64dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011767713s
    STEP: Saw pod success 09/03/22 20:38:59.819
    Sep  3 20:38:59.819: INFO: Pod "pod-4879463c-c869-4c1a-8232-680a354e64dc" satisfied condition "Succeeded or Failed"
    Sep  3 20:38:59.822: INFO: Trying to get logs from node kind-worker2 pod pod-4879463c-c869-4c1a-8232-680a354e64dc container test-container: <nil>
    STEP: delete the pod 09/03/22 20:38:59.826
    Sep  3 20:38:59.833: INFO: Waiting for pod pod-4879463c-c869-4c1a-8232-680a354e64dc to disappear
    Sep  3 20:38:59.835: INFO: Pod pod-4879463c-c869-4c1a-8232-680a354e64dc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 20:38:59.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3207" for this suite. 09/03/22 20:38:59.84
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:38:59.86
Sep  3 20:38:59.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename daemonsets 09/03/22 20:38:59.863
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:59.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:59.899
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Sep  3 20:38:59.921: INFO: Create a RollingUpdate DaemonSet
Sep  3 20:38:59.926: INFO: Check that daemon pods launch on every node of the cluster
Sep  3 20:38:59.935: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:38:59.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 20:38:59.938: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 20:39:00.950: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:39:00.960: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 20:39:00.960: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 20:39:01.948: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:39:01.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 20:39:01.957: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 20:39:02.943: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:39:02.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 20:39:02.948: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 20:39:03.942: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:39:03.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 20:39:03.945: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 20:39:04.948: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:39:04.956: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 20:39:04.956: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 20:39:05.993: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:39:06.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 20:39:06.002: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 20:39:06.942: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:39:06.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  3 20:39:06.945: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Sep  3 20:39:06.945: INFO: Update the DaemonSet to trigger a rollout
Sep  3 20:39:06.950: INFO: Updating DaemonSet daemon-set
Sep  3 20:39:10.961: INFO: Roll back the DaemonSet before rollout is complete
Sep  3 20:39:10.966: INFO: Updating DaemonSet daemon-set
Sep  3 20:39:10.966: INFO: Make sure DaemonSet rollback is complete
Sep  3 20:39:10.970: INFO: Wrong image for pod: daemon-set-9v9pt. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Sep  3 20:39:10.970: INFO: Pod daemon-set-9v9pt is not available
Sep  3 20:39:10.973: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:39:11.980: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:39:12.979: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:39:13.979: INFO: Pod daemon-set-prnm5 is not available
Sep  3 20:39:13.985: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 09/03/22 20:39:13.99
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1957, will wait for the garbage collector to delete the pods 09/03/22 20:39:13.99
Sep  3 20:39:14.046: INFO: Deleting DaemonSet.extensions daemon-set took: 3.66871ms
Sep  3 20:39:14.147: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.850578ms
Sep  3 20:39:15.850: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 20:39:15.850: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  3 20:39:15.856: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1143"},"items":null}

Sep  3 20:39:15.872: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1144"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Sep  3 20:39:15.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1957" for this suite. 09/03/22 20:39:15.885
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":11,"skipped":112,"failed":0}
------------------------------
• [SLOW TEST] [16.028 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:38:59.86
    Sep  3 20:38:59.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename daemonsets 09/03/22 20:38:59.863
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:38:59.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:38:59.899
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Sep  3 20:38:59.921: INFO: Create a RollingUpdate DaemonSet
    Sep  3 20:38:59.926: INFO: Check that daemon pods launch on every node of the cluster
    Sep  3 20:38:59.935: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:38:59.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 20:38:59.938: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 20:39:00.950: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:39:00.960: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 20:39:00.960: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 20:39:01.948: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:39:01.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 20:39:01.957: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 20:39:02.943: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:39:02.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 20:39:02.948: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 20:39:03.942: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:39:03.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 20:39:03.945: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 20:39:04.948: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:39:04.956: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 20:39:04.956: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 20:39:05.993: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:39:06.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 20:39:06.002: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 20:39:06.942: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:39:06.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  3 20:39:06.945: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Sep  3 20:39:06.945: INFO: Update the DaemonSet to trigger a rollout
    Sep  3 20:39:06.950: INFO: Updating DaemonSet daemon-set
    Sep  3 20:39:10.961: INFO: Roll back the DaemonSet before rollout is complete
    Sep  3 20:39:10.966: INFO: Updating DaemonSet daemon-set
    Sep  3 20:39:10.966: INFO: Make sure DaemonSet rollback is complete
    Sep  3 20:39:10.970: INFO: Wrong image for pod: daemon-set-9v9pt. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Sep  3 20:39:10.970: INFO: Pod daemon-set-9v9pt is not available
    Sep  3 20:39:10.973: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:39:11.980: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:39:12.979: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:39:13.979: INFO: Pod daemon-set-prnm5 is not available
    Sep  3 20:39:13.985: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 09/03/22 20:39:13.99
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1957, will wait for the garbage collector to delete the pods 09/03/22 20:39:13.99
    Sep  3 20:39:14.046: INFO: Deleting DaemonSet.extensions daemon-set took: 3.66871ms
    Sep  3 20:39:14.147: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.850578ms
    Sep  3 20:39:15.850: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 20:39:15.850: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  3 20:39:15.856: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1143"},"items":null}

    Sep  3 20:39:15.872: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1144"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 20:39:15.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1957" for this suite. 09/03/22 20:39:15.885
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:39:15.888
Sep  3 20:39:15.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 20:39:15.889
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:39:15.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:39:15.909
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 09/03/22 20:39:15.913
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 20:39:15.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4041" for this suite. 09/03/22 20:39:15.917
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":12,"skipped":114,"failed":0}
------------------------------
• [0.032 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:39:15.888
    Sep  3 20:39:15.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 20:39:15.889
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:39:15.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:39:15.909
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 09/03/22 20:39:15.913
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 20:39:15.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4041" for this suite. 09/03/22 20:39:15.917
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:39:15.92
Sep  3 20:39:15.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 20:39:15.921
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:39:15.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:39:15.933
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-0c2ff253-21af-4709-958b-313bac6a8362 09/03/22 20:39:15.936
STEP: Creating a pod to test consume secrets 09/03/22 20:39:15.938
Sep  3 20:39:15.943: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d" in namespace "projected-7848" to be "Succeeded or Failed"
Sep  3 20:39:15.945: INFO: Pod "pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.804405ms
Sep  3 20:39:17.948: INFO: Pod "pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00473162s
Sep  3 20:39:19.948: INFO: Pod "pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004588026s
STEP: Saw pod success 09/03/22 20:39:19.948
Sep  3 20:39:19.949: INFO: Pod "pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d" satisfied condition "Succeeded or Failed"
Sep  3 20:39:19.951: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d container secret-volume-test: <nil>
STEP: delete the pod 09/03/22 20:39:19.955
Sep  3 20:39:19.961: INFO: Waiting for pod pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d to disappear
Sep  3 20:39:19.965: INFO: Pod pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Sep  3 20:39:19.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7848" for this suite. 09/03/22 20:39:19.967
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":13,"skipped":114,"failed":0}
------------------------------
• [4.051 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:39:15.92
    Sep  3 20:39:15.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 20:39:15.921
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:39:15.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:39:15.933
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-0c2ff253-21af-4709-958b-313bac6a8362 09/03/22 20:39:15.936
    STEP: Creating a pod to test consume secrets 09/03/22 20:39:15.938
    Sep  3 20:39:15.943: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d" in namespace "projected-7848" to be "Succeeded or Failed"
    Sep  3 20:39:15.945: INFO: Pod "pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.804405ms
    Sep  3 20:39:17.948: INFO: Pod "pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00473162s
    Sep  3 20:39:19.948: INFO: Pod "pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004588026s
    STEP: Saw pod success 09/03/22 20:39:19.948
    Sep  3 20:39:19.949: INFO: Pod "pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d" satisfied condition "Succeeded or Failed"
    Sep  3 20:39:19.951: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d container secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 20:39:19.955
    Sep  3 20:39:19.961: INFO: Waiting for pod pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d to disappear
    Sep  3 20:39:19.965: INFO: Pod pod-projected-secrets-8882a2ea-c8a7-4bdd-bad1-e822f027987d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Sep  3 20:39:19.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7848" for this suite. 09/03/22 20:39:19.967
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:39:19.978
Sep  3 20:39:19.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename var-expansion 09/03/22 20:39:19.979
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:39:19.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:39:19.994
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 09/03/22 20:39:19.998
Sep  3 20:39:20.006: INFO: Waiting up to 2m0s for pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7" in namespace "var-expansion-6790" to be "running"
Sep  3 20:39:20.026: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.405853ms
Sep  3 20:39:22.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023667871s
Sep  3 20:39:24.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022100974s
Sep  3 20:39:26.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023468684s
Sep  3 20:39:28.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022294485s
Sep  3 20:39:30.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02307539s
Sep  3 20:39:32.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023352593s
Sep  3 20:39:34.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.023303595s
Sep  3 20:39:36.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023054937s
Sep  3 20:39:38.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.023534328s
Sep  3 20:39:40.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.022461719s
Sep  3 20:39:42.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.023824011s
Sep  3 20:39:44.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 24.023177389s
Sep  3 20:39:46.033: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.027292338s
Sep  3 20:39:48.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 28.023039177s
Sep  3 20:39:50.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 30.023587821s
Sep  3 20:39:52.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.022794513s
Sep  3 20:39:54.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 34.023515201s
Sep  3 20:39:56.031: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 36.024389689s
Sep  3 20:39:58.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 38.02203807s
Sep  3 20:40:00.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 40.023268869s
Sep  3 20:40:02.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021895721s
Sep  3 20:40:04.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 44.023454778s
Sep  3 20:40:06.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 46.022671831s
Sep  3 20:40:08.031: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024413733s
Sep  3 20:40:10.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 50.022545664s
Sep  3 20:40:12.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 52.022428772s
Sep  3 20:40:14.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 54.022957273s
Sep  3 20:40:16.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 56.023249704s
Sep  3 20:40:18.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 58.023681225s
Sep  3 20:40:20.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.023023942s
Sep  3 20:40:22.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023591464s
Sep  3 20:40:24.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023102607s
Sep  3 20:40:26.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.023194964s
Sep  3 20:40:28.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.023509423s
Sep  3 20:40:30.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.022831676s
Sep  3 20:40:32.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.022054119s
Sep  3 20:40:34.031: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.024544263s
Sep  3 20:40:36.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.022793285s
Sep  3 20:40:38.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.022206213s
Sep  3 20:40:40.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.022910438s
Sep  3 20:40:42.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.023022169s
Sep  3 20:40:44.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.022703099s
Sep  3 20:40:46.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.023521334s
Sep  3 20:40:48.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.023749435s
Sep  3 20:40:50.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.023052066s
Sep  3 20:40:52.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.02334041s
Sep  3 20:40:54.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.022888944s
Sep  3 20:40:56.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.02269723s
Sep  3 20:40:58.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.023014413s
Sep  3 20:41:00.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.023233496s
Sep  3 20:41:02.031: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.024889474s
Sep  3 20:41:04.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.02312028s
Sep  3 20:41:06.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.022310238s
Sep  3 20:41:08.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.022903997s
Sep  3 20:41:10.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.023251455s
Sep  3 20:41:12.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.022588406s
Sep  3 20:41:14.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.022782486s
Sep  3 20:41:16.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.023662833s
Sep  3 20:41:18.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.022826359s
Sep  3 20:41:20.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022647903s
Sep  3 20:41:20.033: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.026553527s
STEP: updating the pod 09/03/22 20:41:20.033
Sep  3 20:41:20.545: INFO: Successfully updated pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7"
STEP: waiting for pod running 09/03/22 20:41:20.545
Sep  3 20:41:20.546: INFO: Waiting up to 2m0s for pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7" in namespace "var-expansion-6790" to be "running"
Sep  3 20:41:20.551: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.523334ms
Sep  3 20:41:22.555: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009487971s
Sep  3 20:41:22.555: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7" satisfied condition "running"
STEP: deleting the pod gracefully 09/03/22 20:41:22.555
Sep  3 20:41:22.555: INFO: Deleting pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7" in namespace "var-expansion-6790"
Sep  3 20:41:22.561: INFO: Wait up to 5m0s for pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Sep  3 20:41:54.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6790" for this suite. 09/03/22 20:41:54.576
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":14,"skipped":115,"failed":0}
------------------------------
• [SLOW TEST] [154.601 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:39:19.978
    Sep  3 20:39:19.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename var-expansion 09/03/22 20:39:19.979
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:39:19.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:39:19.994
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 09/03/22 20:39:19.998
    Sep  3 20:39:20.006: INFO: Waiting up to 2m0s for pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7" in namespace "var-expansion-6790" to be "running"
    Sep  3 20:39:20.026: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.405853ms
    Sep  3 20:39:22.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023667871s
    Sep  3 20:39:24.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022100974s
    Sep  3 20:39:26.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023468684s
    Sep  3 20:39:28.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022294485s
    Sep  3 20:39:30.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02307539s
    Sep  3 20:39:32.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023352593s
    Sep  3 20:39:34.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.023303595s
    Sep  3 20:39:36.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023054937s
    Sep  3 20:39:38.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.023534328s
    Sep  3 20:39:40.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.022461719s
    Sep  3 20:39:42.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.023824011s
    Sep  3 20:39:44.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 24.023177389s
    Sep  3 20:39:46.033: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.027292338s
    Sep  3 20:39:48.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 28.023039177s
    Sep  3 20:39:50.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 30.023587821s
    Sep  3 20:39:52.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.022794513s
    Sep  3 20:39:54.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 34.023515201s
    Sep  3 20:39:56.031: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 36.024389689s
    Sep  3 20:39:58.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 38.02203807s
    Sep  3 20:40:00.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 40.023268869s
    Sep  3 20:40:02.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021895721s
    Sep  3 20:40:04.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 44.023454778s
    Sep  3 20:40:06.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 46.022671831s
    Sep  3 20:40:08.031: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024413733s
    Sep  3 20:40:10.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 50.022545664s
    Sep  3 20:40:12.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 52.022428772s
    Sep  3 20:40:14.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 54.022957273s
    Sep  3 20:40:16.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 56.023249704s
    Sep  3 20:40:18.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 58.023681225s
    Sep  3 20:40:20.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.023023942s
    Sep  3 20:40:22.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023591464s
    Sep  3 20:40:24.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023102607s
    Sep  3 20:40:26.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.023194964s
    Sep  3 20:40:28.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.023509423s
    Sep  3 20:40:30.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.022831676s
    Sep  3 20:40:32.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.022054119s
    Sep  3 20:40:34.031: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.024544263s
    Sep  3 20:40:36.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.022793285s
    Sep  3 20:40:38.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.022206213s
    Sep  3 20:40:40.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.022910438s
    Sep  3 20:40:42.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.023022169s
    Sep  3 20:40:44.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.022703099s
    Sep  3 20:40:46.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.023521334s
    Sep  3 20:40:48.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.023749435s
    Sep  3 20:40:50.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.023052066s
    Sep  3 20:40:52.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.02334041s
    Sep  3 20:40:54.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.022888944s
    Sep  3 20:40:56.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.02269723s
    Sep  3 20:40:58.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.023014413s
    Sep  3 20:41:00.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.023233496s
    Sep  3 20:41:02.031: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.024889474s
    Sep  3 20:41:04.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.02312028s
    Sep  3 20:41:06.028: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.022310238s
    Sep  3 20:41:08.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.022903997s
    Sep  3 20:41:10.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.023251455s
    Sep  3 20:41:12.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.022588406s
    Sep  3 20:41:14.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.022782486s
    Sep  3 20:41:16.030: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.023662833s
    Sep  3 20:41:18.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.022826359s
    Sep  3 20:41:20.029: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022647903s
    Sep  3 20:41:20.033: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.026553527s
    STEP: updating the pod 09/03/22 20:41:20.033
    Sep  3 20:41:20.545: INFO: Successfully updated pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7"
    STEP: waiting for pod running 09/03/22 20:41:20.545
    Sep  3 20:41:20.546: INFO: Waiting up to 2m0s for pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7" in namespace "var-expansion-6790" to be "running"
    Sep  3 20:41:20.551: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.523334ms
    Sep  3 20:41:22.555: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009487971s
    Sep  3 20:41:22.555: INFO: Pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7" satisfied condition "running"
    STEP: deleting the pod gracefully 09/03/22 20:41:22.555
    Sep  3 20:41:22.555: INFO: Deleting pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7" in namespace "var-expansion-6790"
    Sep  3 20:41:22.561: INFO: Wait up to 5m0s for pod "var-expansion-d6b0b409-beef-4157-a0de-96beca0ff8f7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Sep  3 20:41:54.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6790" for this suite. 09/03/22 20:41:54.576
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:41:54.582
Sep  3 20:41:54.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename containers 09/03/22 20:41:54.583
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:41:54.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:41:54.594
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 09/03/22 20:41:54.597
Sep  3 20:41:54.602: INFO: Waiting up to 5m0s for pod "client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47" in namespace "containers-7767" to be "Succeeded or Failed"
Sep  3 20:41:54.604: INFO: Pod "client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47": Phase="Pending", Reason="", readiness=false. Elapsed: 1.72071ms
Sep  3 20:41:56.607: INFO: Pod "client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005249949s
Sep  3 20:41:58.607: INFO: Pod "client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005208268s
STEP: Saw pod success 09/03/22 20:41:58.607
Sep  3 20:41:58.607: INFO: Pod "client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47" satisfied condition "Succeeded or Failed"
Sep  3 20:41:58.609: INFO: Trying to get logs from node kind-worker2 pod client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47 container agnhost-container: <nil>
STEP: delete the pod 09/03/22 20:41:58.62
Sep  3 20:41:58.626: INFO: Waiting for pod client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47 to disappear
Sep  3 20:41:58.628: INFO: Pod client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Sep  3 20:41:58.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7767" for this suite. 09/03/22 20:41:58.63
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":15,"skipped":125,"failed":0}
------------------------------
• [4.051 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:41:54.582
    Sep  3 20:41:54.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename containers 09/03/22 20:41:54.583
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:41:54.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:41:54.594
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 09/03/22 20:41:54.597
    Sep  3 20:41:54.602: INFO: Waiting up to 5m0s for pod "client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47" in namespace "containers-7767" to be "Succeeded or Failed"
    Sep  3 20:41:54.604: INFO: Pod "client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47": Phase="Pending", Reason="", readiness=false. Elapsed: 1.72071ms
    Sep  3 20:41:56.607: INFO: Pod "client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005249949s
    Sep  3 20:41:58.607: INFO: Pod "client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005208268s
    STEP: Saw pod success 09/03/22 20:41:58.607
    Sep  3 20:41:58.607: INFO: Pod "client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47" satisfied condition "Succeeded or Failed"
    Sep  3 20:41:58.609: INFO: Trying to get logs from node kind-worker2 pod client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47 container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 20:41:58.62
    Sep  3 20:41:58.626: INFO: Waiting for pod client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47 to disappear
    Sep  3 20:41:58.628: INFO: Pod client-containers-a8f08d1c-bab5-474a-a2c3-b7df9dc4ea47 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Sep  3 20:41:58.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7767" for this suite. 09/03/22 20:41:58.63
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:41:58.637
Sep  3 20:41:58.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 20:41:58.638
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:41:58.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:41:58.65
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-28d3789d-5375-49d6-a3ac-3e6b190aa304 09/03/22 20:41:58.653
STEP: Creating a pod to test consume configMaps 09/03/22 20:41:58.656
Sep  3 20:41:58.662: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215" in namespace "projected-439" to be "Succeeded or Failed"
Sep  3 20:41:58.664: INFO: Pod "pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.319413ms
Sep  3 20:42:00.667: INFO: Pod "pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005424283s
Sep  3 20:42:02.670: INFO: Pod "pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008059242s
STEP: Saw pod success 09/03/22 20:42:02.67
Sep  3 20:42:02.670: INFO: Pod "pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215" satisfied condition "Succeeded or Failed"
Sep  3 20:42:02.673: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215 container agnhost-container: <nil>
STEP: delete the pod 09/03/22 20:42:02.677
Sep  3 20:42:02.684: INFO: Waiting for pod pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215 to disappear
Sep  3 20:42:02.687: INFO: Pod pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Sep  3 20:42:02.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-439" for this suite. 09/03/22 20:42:02.689
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":16,"skipped":127,"failed":0}
------------------------------
• [4.055 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:41:58.637
    Sep  3 20:41:58.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 20:41:58.638
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:41:58.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:41:58.65
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-28d3789d-5375-49d6-a3ac-3e6b190aa304 09/03/22 20:41:58.653
    STEP: Creating a pod to test consume configMaps 09/03/22 20:41:58.656
    Sep  3 20:41:58.662: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215" in namespace "projected-439" to be "Succeeded or Failed"
    Sep  3 20:41:58.664: INFO: Pod "pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.319413ms
    Sep  3 20:42:00.667: INFO: Pod "pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005424283s
    Sep  3 20:42:02.670: INFO: Pod "pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008059242s
    STEP: Saw pod success 09/03/22 20:42:02.67
    Sep  3 20:42:02.670: INFO: Pod "pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215" satisfied condition "Succeeded or Failed"
    Sep  3 20:42:02.673: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215 container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 20:42:02.677
    Sep  3 20:42:02.684: INFO: Waiting for pod pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215 to disappear
    Sep  3 20:42:02.687: INFO: Pod pod-projected-configmaps-7d6dc63b-d805-43cb-96bf-822036e66215 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Sep  3 20:42:02.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-439" for this suite. 09/03/22 20:42:02.689
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:42:02.707
Sep  3 20:42:02.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 20:42:02.709
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:42:02.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:42:02.728
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/03/22 20:42:02.73
Sep  3 20:42:02.736: INFO: Waiting up to 5m0s for pod "pod-42fd24de-aae3-4e8b-bc35-559c285b5632" in namespace "emptydir-5123" to be "Succeeded or Failed"
Sep  3 20:42:02.742: INFO: Pod "pod-42fd24de-aae3-4e8b-bc35-559c285b5632": Phase="Pending", Reason="", readiness=false. Elapsed: 6.231532ms
Sep  3 20:42:04.754: INFO: Pod "pod-42fd24de-aae3-4e8b-bc35-559c285b5632": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017672636s
Sep  3 20:42:06.745: INFO: Pod "pod-42fd24de-aae3-4e8b-bc35-559c285b5632": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008738935s
STEP: Saw pod success 09/03/22 20:42:06.745
Sep  3 20:42:06.745: INFO: Pod "pod-42fd24de-aae3-4e8b-bc35-559c285b5632" satisfied condition "Succeeded or Failed"
Sep  3 20:42:06.747: INFO: Trying to get logs from node kind-worker2 pod pod-42fd24de-aae3-4e8b-bc35-559c285b5632 container test-container: <nil>
STEP: delete the pod 09/03/22 20:42:06.751
Sep  3 20:42:06.757: INFO: Waiting for pod pod-42fd24de-aae3-4e8b-bc35-559c285b5632 to disappear
Sep  3 20:42:06.759: INFO: Pod pod-42fd24de-aae3-4e8b-bc35-559c285b5632 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 20:42:06.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5123" for this suite. 09/03/22 20:42:06.762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":17,"skipped":196,"failed":0}
------------------------------
• [4.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:42:02.707
    Sep  3 20:42:02.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 20:42:02.709
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:42:02.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:42:02.728
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/03/22 20:42:02.73
    Sep  3 20:42:02.736: INFO: Waiting up to 5m0s for pod "pod-42fd24de-aae3-4e8b-bc35-559c285b5632" in namespace "emptydir-5123" to be "Succeeded or Failed"
    Sep  3 20:42:02.742: INFO: Pod "pod-42fd24de-aae3-4e8b-bc35-559c285b5632": Phase="Pending", Reason="", readiness=false. Elapsed: 6.231532ms
    Sep  3 20:42:04.754: INFO: Pod "pod-42fd24de-aae3-4e8b-bc35-559c285b5632": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017672636s
    Sep  3 20:42:06.745: INFO: Pod "pod-42fd24de-aae3-4e8b-bc35-559c285b5632": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008738935s
    STEP: Saw pod success 09/03/22 20:42:06.745
    Sep  3 20:42:06.745: INFO: Pod "pod-42fd24de-aae3-4e8b-bc35-559c285b5632" satisfied condition "Succeeded or Failed"
    Sep  3 20:42:06.747: INFO: Trying to get logs from node kind-worker2 pod pod-42fd24de-aae3-4e8b-bc35-559c285b5632 container test-container: <nil>
    STEP: delete the pod 09/03/22 20:42:06.751
    Sep  3 20:42:06.757: INFO: Waiting for pod pod-42fd24de-aae3-4e8b-bc35-559c285b5632 to disappear
    Sep  3 20:42:06.759: INFO: Pod pod-42fd24de-aae3-4e8b-bc35-559c285b5632 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 20:42:06.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5123" for this suite. 09/03/22 20:42:06.762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:42:06.784
Sep  3 20:42:06.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 20:42:06.786
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:42:06.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:42:06.815
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 20:42:06.84
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 20:42:07.785
STEP: Deploying the webhook pod 09/03/22 20:42:07.79
STEP: Wait for the deployment to be ready 09/03/22 20:42:07.799
Sep  3 20:42:07.808: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 20:42:09.815
STEP: Verifying the service has paired with the endpoint 09/03/22 20:42:09.83
Sep  3 20:42:10.831: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Sep  3 20:42:10.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9695-crds.webhook.example.com via the AdmissionRegistration API 09/03/22 20:42:11.349
STEP: Creating a custom resource while v1 is storage version 09/03/22 20:42:11.362
STEP: Patching Custom Resource Definition to set v2 as storage 09/03/22 20:42:13.422
STEP: Patching the custom resource while v2 is storage version 09/03/22 20:42:13.438
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 20:42:14.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1422" for this suite. 09/03/22 20:42:14.047
STEP: Destroying namespace "webhook-1422-markers" for this suite. 09/03/22 20:42:14.05
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":18,"skipped":215,"failed":0}
------------------------------
• [SLOW TEST] [7.335 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:42:06.784
    Sep  3 20:42:06.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 20:42:06.786
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:42:06.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:42:06.815
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 20:42:06.84
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 20:42:07.785
    STEP: Deploying the webhook pod 09/03/22 20:42:07.79
    STEP: Wait for the deployment to be ready 09/03/22 20:42:07.799
    Sep  3 20:42:07.808: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 20:42:09.815
    STEP: Verifying the service has paired with the endpoint 09/03/22 20:42:09.83
    Sep  3 20:42:10.831: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Sep  3 20:42:10.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9695-crds.webhook.example.com via the AdmissionRegistration API 09/03/22 20:42:11.349
    STEP: Creating a custom resource while v1 is storage version 09/03/22 20:42:11.362
    STEP: Patching Custom Resource Definition to set v2 as storage 09/03/22 20:42:13.422
    STEP: Patching the custom resource while v2 is storage version 09/03/22 20:42:13.438
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 20:42:14.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1422" for this suite. 09/03/22 20:42:14.047
    STEP: Destroying namespace "webhook-1422-markers" for this suite. 09/03/22 20:42:14.05
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:42:14.131
Sep  3 20:42:14.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename resourcequota 09/03/22 20:42:14.132
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:42:14.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:42:14.189
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 09/03/22 20:42:14.191
STEP: Getting a ResourceQuota 09/03/22 20:42:14.201
STEP: Updating a ResourceQuota 09/03/22 20:42:14.223
STEP: Verifying a ResourceQuota was modified 09/03/22 20:42:14.229
STEP: Deleting a ResourceQuota 09/03/22 20:42:14.234
STEP: Verifying the deleted ResourceQuota 09/03/22 20:42:14.244
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Sep  3 20:42:14.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2930" for this suite. 09/03/22 20:42:14.272
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":19,"skipped":227,"failed":0}
------------------------------
• [0.175 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:42:14.131
    Sep  3 20:42:14.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename resourcequota 09/03/22 20:42:14.132
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:42:14.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:42:14.189
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 09/03/22 20:42:14.191
    STEP: Getting a ResourceQuota 09/03/22 20:42:14.201
    STEP: Updating a ResourceQuota 09/03/22 20:42:14.223
    STEP: Verifying a ResourceQuota was modified 09/03/22 20:42:14.229
    STEP: Deleting a ResourceQuota 09/03/22 20:42:14.234
    STEP: Verifying the deleted ResourceQuota 09/03/22 20:42:14.244
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Sep  3 20:42:14.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2930" for this suite. 09/03/22 20:42:14.272
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:42:14.341
Sep  3 20:42:14.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename job 09/03/22 20:42:14.345
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:42:14.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:42:14.434
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 09/03/22 20:42:14.44
STEP: Ensuring active pods == parallelism 09/03/22 20:42:14.458
STEP: delete a job 09/03/22 20:42:16.463
STEP: deleting Job.batch foo in namespace job-975, will wait for the garbage collector to delete the pods 09/03/22 20:42:16.463
Sep  3 20:42:16.519: INFO: Deleting Job.batch foo took: 3.447702ms
Sep  3 20:42:16.620: INFO: Terminating Job.batch foo pods took: 100.107165ms
STEP: Ensuring job was deleted 09/03/22 20:42:49.42
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Sep  3 20:42:49.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-975" for this suite. 09/03/22 20:42:49.428
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":20,"skipped":249,"failed":0}
------------------------------
• [SLOW TEST] [35.094 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:42:14.341
    Sep  3 20:42:14.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename job 09/03/22 20:42:14.345
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:42:14.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:42:14.434
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 09/03/22 20:42:14.44
    STEP: Ensuring active pods == parallelism 09/03/22 20:42:14.458
    STEP: delete a job 09/03/22 20:42:16.463
    STEP: deleting Job.batch foo in namespace job-975, will wait for the garbage collector to delete the pods 09/03/22 20:42:16.463
    Sep  3 20:42:16.519: INFO: Deleting Job.batch foo took: 3.447702ms
    Sep  3 20:42:16.620: INFO: Terminating Job.batch foo pods took: 100.107165ms
    STEP: Ensuring job was deleted 09/03/22 20:42:49.42
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Sep  3 20:42:49.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-975" for this suite. 09/03/22 20:42:49.428
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:42:49.44
Sep  3 20:42:49.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename security-context 09/03/22 20:42:49.442
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:42:49.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:42:49.483
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/03/22 20:42:49.49
Sep  3 20:42:49.498: INFO: Waiting up to 5m0s for pod "security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca" in namespace "security-context-8211" to be "Succeeded or Failed"
Sep  3 20:42:49.506: INFO: Pod "security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca": Phase="Pending", Reason="", readiness=false. Elapsed: 7.990504ms
Sep  3 20:42:51.512: INFO: Pod "security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013807351s
Sep  3 20:42:53.511: INFO: Pod "security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012269495s
STEP: Saw pod success 09/03/22 20:42:53.511
Sep  3 20:42:53.511: INFO: Pod "security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca" satisfied condition "Succeeded or Failed"
Sep  3 20:42:53.513: INFO: Trying to get logs from node kind-worker2 pod security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca container test-container: <nil>
STEP: delete the pod 09/03/22 20:42:53.518
Sep  3 20:42:53.524: INFO: Waiting for pod security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca to disappear
Sep  3 20:42:53.527: INFO: Pod security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Sep  3 20:42:53.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8211" for this suite. 09/03/22 20:42:53.53
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":21,"skipped":263,"failed":0}
------------------------------
• [4.093 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:42:49.44
    Sep  3 20:42:49.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename security-context 09/03/22 20:42:49.442
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:42:49.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:42:49.483
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/03/22 20:42:49.49
    Sep  3 20:42:49.498: INFO: Waiting up to 5m0s for pod "security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca" in namespace "security-context-8211" to be "Succeeded or Failed"
    Sep  3 20:42:49.506: INFO: Pod "security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca": Phase="Pending", Reason="", readiness=false. Elapsed: 7.990504ms
    Sep  3 20:42:51.512: INFO: Pod "security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013807351s
    Sep  3 20:42:53.511: INFO: Pod "security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012269495s
    STEP: Saw pod success 09/03/22 20:42:53.511
    Sep  3 20:42:53.511: INFO: Pod "security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca" satisfied condition "Succeeded or Failed"
    Sep  3 20:42:53.513: INFO: Trying to get logs from node kind-worker2 pod security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca container test-container: <nil>
    STEP: delete the pod 09/03/22 20:42:53.518
    Sep  3 20:42:53.524: INFO: Waiting for pod security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca to disappear
    Sep  3 20:42:53.527: INFO: Pod security-context-b82fa44f-2e88-4161-982d-c04d1399c3ca no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Sep  3 20:42:53.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-8211" for this suite. 09/03/22 20:42:53.53
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:42:53.536
Sep  3 20:42:53.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename replicaset 09/03/22 20:42:53.538
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:42:53.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:42:53.55
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Sep  3 20:42:53.552: INFO: Creating ReplicaSet my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a
Sep  3 20:42:53.561: INFO: Pod name my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a: Found 0 pods out of 1
Sep  3 20:42:58.567: INFO: Pod name my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a: Found 1 pods out of 1
Sep  3 20:42:58.567: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a" is running
Sep  3 20:42:58.567: INFO: Waiting up to 5m0s for pod "my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a-b67jh" in namespace "replicaset-8583" to be "running"
Sep  3 20:42:58.568: INFO: Pod "my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a-b67jh": Phase="Running", Reason="", readiness=true. Elapsed: 1.590302ms
Sep  3 20:42:58.568: INFO: Pod "my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a-b67jh" satisfied condition "running"
Sep  3 20:42:58.568: INFO: Pod "my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a-b67jh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 20:42:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 20:42:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 20:42:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 20:42:53 +0000 UTC Reason: Message:}])
Sep  3 20:42:58.569: INFO: Trying to dial the pod
Sep  3 20:43:03.580: INFO: Controller my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a: Got expected result from replica 1 [my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a-b67jh]: "my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a-b67jh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Sep  3 20:43:03.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8583" for this suite. 09/03/22 20:43:03.582
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":22,"skipped":272,"failed":0}
------------------------------
• [SLOW TEST] [10.050 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:42:53.536
    Sep  3 20:42:53.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename replicaset 09/03/22 20:42:53.538
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:42:53.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:42:53.55
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Sep  3 20:42:53.552: INFO: Creating ReplicaSet my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a
    Sep  3 20:42:53.561: INFO: Pod name my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a: Found 0 pods out of 1
    Sep  3 20:42:58.567: INFO: Pod name my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a: Found 1 pods out of 1
    Sep  3 20:42:58.567: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a" is running
    Sep  3 20:42:58.567: INFO: Waiting up to 5m0s for pod "my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a-b67jh" in namespace "replicaset-8583" to be "running"
    Sep  3 20:42:58.568: INFO: Pod "my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a-b67jh": Phase="Running", Reason="", readiness=true. Elapsed: 1.590302ms
    Sep  3 20:42:58.568: INFO: Pod "my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a-b67jh" satisfied condition "running"
    Sep  3 20:42:58.568: INFO: Pod "my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a-b67jh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 20:42:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 20:42:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 20:42:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 20:42:53 +0000 UTC Reason: Message:}])
    Sep  3 20:42:58.569: INFO: Trying to dial the pod
    Sep  3 20:43:03.580: INFO: Controller my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a: Got expected result from replica 1 [my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a-b67jh]: "my-hostname-basic-699046da-2c37-496e-8152-f7925acc8b5a-b67jh", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Sep  3 20:43:03.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8583" for this suite. 09/03/22 20:43:03.582
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:43:03.586
Sep  3 20:43:03.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename subpath 09/03/22 20:43:03.588
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:03.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:03.6
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/03/22 20:43:03.602
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-kmz6 09/03/22 20:43:03.608
STEP: Creating a pod to test atomic-volume-subpath 09/03/22 20:43:03.608
Sep  3 20:43:03.617: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kmz6" in namespace "subpath-4100" to be "Succeeded or Failed"
Sep  3 20:43:03.625: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.279812ms
Sep  3 20:43:05.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 2.010371901s
Sep  3 20:43:07.627: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 4.009992084s
Sep  3 20:43:09.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 6.010880269s
Sep  3 20:43:11.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 8.01086678s
Sep  3 20:43:13.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 10.010294079s
Sep  3 20:43:15.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 12.010034978s
Sep  3 20:43:17.630: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 14.012724387s
Sep  3 20:43:19.627: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 16.009891669s
Sep  3 20:43:21.629: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 18.011858414s
Sep  3 20:43:23.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 20.010399453s
Sep  3 20:43:25.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=false. Elapsed: 22.010340394s
Sep  3 20:43:27.627: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009545642s
STEP: Saw pod success 09/03/22 20:43:27.627
Sep  3 20:43:27.628: INFO: Pod "pod-subpath-test-configmap-kmz6" satisfied condition "Succeeded or Failed"
Sep  3 20:43:27.629: INFO: Trying to get logs from node kind-worker2 pod pod-subpath-test-configmap-kmz6 container test-container-subpath-configmap-kmz6: <nil>
STEP: delete the pod 09/03/22 20:43:27.634
Sep  3 20:43:27.641: INFO: Waiting for pod pod-subpath-test-configmap-kmz6 to disappear
Sep  3 20:43:27.643: INFO: Pod pod-subpath-test-configmap-kmz6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kmz6 09/03/22 20:43:27.643
Sep  3 20:43:27.643: INFO: Deleting pod "pod-subpath-test-configmap-kmz6" in namespace "subpath-4100"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Sep  3 20:43:27.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4100" for this suite. 09/03/22 20:43:27.647
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":23,"skipped":284,"failed":0}
------------------------------
• [SLOW TEST] [24.063 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:43:03.586
    Sep  3 20:43:03.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename subpath 09/03/22 20:43:03.588
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:03.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:03.6
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/03/22 20:43:03.602
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-kmz6 09/03/22 20:43:03.608
    STEP: Creating a pod to test atomic-volume-subpath 09/03/22 20:43:03.608
    Sep  3 20:43:03.617: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kmz6" in namespace "subpath-4100" to be "Succeeded or Failed"
    Sep  3 20:43:03.625: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.279812ms
    Sep  3 20:43:05.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 2.010371901s
    Sep  3 20:43:07.627: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 4.009992084s
    Sep  3 20:43:09.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 6.010880269s
    Sep  3 20:43:11.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 8.01086678s
    Sep  3 20:43:13.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 10.010294079s
    Sep  3 20:43:15.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 12.010034978s
    Sep  3 20:43:17.630: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 14.012724387s
    Sep  3 20:43:19.627: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 16.009891669s
    Sep  3 20:43:21.629: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 18.011858414s
    Sep  3 20:43:23.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=true. Elapsed: 20.010399453s
    Sep  3 20:43:25.628: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Running", Reason="", readiness=false. Elapsed: 22.010340394s
    Sep  3 20:43:27.627: INFO: Pod "pod-subpath-test-configmap-kmz6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009545642s
    STEP: Saw pod success 09/03/22 20:43:27.627
    Sep  3 20:43:27.628: INFO: Pod "pod-subpath-test-configmap-kmz6" satisfied condition "Succeeded or Failed"
    Sep  3 20:43:27.629: INFO: Trying to get logs from node kind-worker2 pod pod-subpath-test-configmap-kmz6 container test-container-subpath-configmap-kmz6: <nil>
    STEP: delete the pod 09/03/22 20:43:27.634
    Sep  3 20:43:27.641: INFO: Waiting for pod pod-subpath-test-configmap-kmz6 to disappear
    Sep  3 20:43:27.643: INFO: Pod pod-subpath-test-configmap-kmz6 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-kmz6 09/03/22 20:43:27.643
    Sep  3 20:43:27.643: INFO: Deleting pod "pod-subpath-test-configmap-kmz6" in namespace "subpath-4100"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Sep  3 20:43:27.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4100" for this suite. 09/03/22 20:43:27.647
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:43:27.652
Sep  3 20:43:27.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 20:43:27.653
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:27.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:27.673
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 09/03/22 20:43:27.675
Sep  3 20:43:27.680: INFO: Waiting up to 5m0s for pod "labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8" in namespace "projected-733" to be "running and ready"
Sep  3 20:43:27.683: INFO: Pod "labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.414506ms
Sep  3 20:43:27.683: INFO: The phase of Pod labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:43:29.686: INFO: Pod "labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005744991s
Sep  3 20:43:29.686: INFO: The phase of Pod labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8 is Running (Ready = true)
Sep  3 20:43:29.686: INFO: Pod "labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8" satisfied condition "running and ready"
Sep  3 20:43:30.201: INFO: Successfully updated pod "labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Sep  3 20:43:34.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-733" for this suite. 09/03/22 20:43:34.223
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":24,"skipped":291,"failed":0}
------------------------------
• [SLOW TEST] [6.575 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:43:27.652
    Sep  3 20:43:27.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 20:43:27.653
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:27.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:27.673
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 09/03/22 20:43:27.675
    Sep  3 20:43:27.680: INFO: Waiting up to 5m0s for pod "labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8" in namespace "projected-733" to be "running and ready"
    Sep  3 20:43:27.683: INFO: Pod "labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.414506ms
    Sep  3 20:43:27.683: INFO: The phase of Pod labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:43:29.686: INFO: Pod "labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005744991s
    Sep  3 20:43:29.686: INFO: The phase of Pod labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8 is Running (Ready = true)
    Sep  3 20:43:29.686: INFO: Pod "labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8" satisfied condition "running and ready"
    Sep  3 20:43:30.201: INFO: Successfully updated pod "labelsupdatefd6bfad2-c3ca-4150-8dd8-391fcb69d1d8"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Sep  3 20:43:34.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-733" for this suite. 09/03/22 20:43:34.223
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:43:34.239
Sep  3 20:43:34.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 20:43:34.24
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:34.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:34.256
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 09/03/22 20:43:34.258
Sep  3 20:43:34.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep  3 20:43:34.370: INFO: stderr: ""
Sep  3 20:43:34.370: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 09/03/22 20:43:34.37
Sep  3 20:43:34.370: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep  3 20:43:34.371: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1003" to be "running and ready, or succeeded"
Sep  3 20:43:34.377: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.438012ms
Sep  3 20:43:34.377: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'kind-worker2' to be 'Running' but was 'Pending'
Sep  3 20:43:36.380: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009082268s
Sep  3 20:43:36.380: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep  3 20:43:36.380: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 09/03/22 20:43:36.38
Sep  3 20:43:36.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 logs logs-generator logs-generator'
Sep  3 20:43:36.460: INFO: stderr: ""
Sep  3 20:43:36.460: INFO: stdout: "I0903 20:43:34.994906       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/ffh6 317\nI0903 20:43:35.195048       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/s4nq 304\nI0903 20:43:35.394975       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/mlg 569\nI0903 20:43:35.595200       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/xmln 249\nI0903 20:43:35.795595       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/g4n 419\nI0903 20:43:35.995758       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/fx7g 457\nI0903 20:43:36.195048       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/q29b 300\nI0903 20:43:36.395456       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z69d 499\n"
STEP: limiting log lines 09/03/22 20:43:36.46
Sep  3 20:43:36.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 logs logs-generator logs-generator --tail=1'
Sep  3 20:43:36.540: INFO: stderr: ""
Sep  3 20:43:36.540: INFO: stdout: "I0903 20:43:36.395456       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z69d 499\n"
Sep  3 20:43:36.541: INFO: got output "I0903 20:43:36.395456       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z69d 499\n"
STEP: limiting log bytes 09/03/22 20:43:36.541
Sep  3 20:43:36.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 logs logs-generator logs-generator --limit-bytes=1'
Sep  3 20:43:36.605: INFO: stderr: ""
Sep  3 20:43:36.605: INFO: stdout: "I"
Sep  3 20:43:36.605: INFO: got output "I"
STEP: exposing timestamps 09/03/22 20:43:36.605
Sep  3 20:43:36.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 logs logs-generator logs-generator --tail=1 --timestamps'
Sep  3 20:43:36.685: INFO: stderr: ""
Sep  3 20:43:36.685: INFO: stdout: "2022-09-03T20:43:36.595126358Z I0903 20:43:36.594960       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/rwk 290\n"
Sep  3 20:43:36.685: INFO: got output "2022-09-03T20:43:36.595126358Z I0903 20:43:36.594960       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/rwk 290\n"
STEP: restricting to a time range 09/03/22 20:43:36.685
Sep  3 20:43:39.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 logs logs-generator logs-generator --since=1s'
Sep  3 20:43:39.290: INFO: stderr: ""
Sep  3 20:43:39.290: INFO: stdout: "I0903 20:43:38.395167       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/fkn 395\nI0903 20:43:38.595358       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/4vfj 400\nI0903 20:43:38.795847       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/mm6q 476\nI0903 20:43:38.995028       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/mpbd 280\nI0903 20:43:39.195380       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/ddsd 483\n"
Sep  3 20:43:39.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 logs logs-generator logs-generator --since=24h'
Sep  3 20:43:39.420: INFO: stderr: ""
Sep  3 20:43:39.420: INFO: stdout: "I0903 20:43:34.994906       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/ffh6 317\nI0903 20:43:35.195048       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/s4nq 304\nI0903 20:43:35.394975       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/mlg 569\nI0903 20:43:35.595200       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/xmln 249\nI0903 20:43:35.795595       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/g4n 419\nI0903 20:43:35.995758       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/fx7g 457\nI0903 20:43:36.195048       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/q29b 300\nI0903 20:43:36.395456       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z69d 499\nI0903 20:43:36.594960       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/rwk 290\nI0903 20:43:36.795392       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/tz6n 323\nI0903 20:43:36.995733       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/t46 434\nI0903 20:43:37.195064       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/bcc 584\nI0903 20:43:37.395466       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/wxpw 540\nI0903 20:43:37.595905       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/nzh 586\nI0903 20:43:37.795277       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/7bsh 573\nI0903 20:43:37.995453       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/wwj 364\nI0903 20:43:38.195853       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/tn5 439\nI0903 20:43:38.395167       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/fkn 395\nI0903 20:43:38.595358       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/4vfj 400\nI0903 20:43:38.795847       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/mm6q 476\nI0903 20:43:38.995028       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/mpbd 280\nI0903 20:43:39.195380       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/ddsd 483\nI0903 20:43:39.395725       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/266b 289\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Sep  3 20:43:39.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 delete pod logs-generator'
Sep  3 20:43:40.485: INFO: stderr: ""
Sep  3 20:43:40.485: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 20:43:40.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1003" for this suite. 09/03/22 20:43:40.488
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":25,"skipped":335,"failed":0}
------------------------------
• [SLOW TEST] [6.255 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:43:34.239
    Sep  3 20:43:34.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 20:43:34.24
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:34.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:34.256
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 09/03/22 20:43:34.258
    Sep  3 20:43:34.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Sep  3 20:43:34.370: INFO: stderr: ""
    Sep  3 20:43:34.370: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 09/03/22 20:43:34.37
    Sep  3 20:43:34.370: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Sep  3 20:43:34.371: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1003" to be "running and ready, or succeeded"
    Sep  3 20:43:34.377: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.438012ms
    Sep  3 20:43:34.377: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'kind-worker2' to be 'Running' but was 'Pending'
    Sep  3 20:43:36.380: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009082268s
    Sep  3 20:43:36.380: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Sep  3 20:43:36.380: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 09/03/22 20:43:36.38
    Sep  3 20:43:36.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 logs logs-generator logs-generator'
    Sep  3 20:43:36.460: INFO: stderr: ""
    Sep  3 20:43:36.460: INFO: stdout: "I0903 20:43:34.994906       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/ffh6 317\nI0903 20:43:35.195048       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/s4nq 304\nI0903 20:43:35.394975       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/mlg 569\nI0903 20:43:35.595200       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/xmln 249\nI0903 20:43:35.795595       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/g4n 419\nI0903 20:43:35.995758       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/fx7g 457\nI0903 20:43:36.195048       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/q29b 300\nI0903 20:43:36.395456       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z69d 499\n"
    STEP: limiting log lines 09/03/22 20:43:36.46
    Sep  3 20:43:36.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 logs logs-generator logs-generator --tail=1'
    Sep  3 20:43:36.540: INFO: stderr: ""
    Sep  3 20:43:36.540: INFO: stdout: "I0903 20:43:36.395456       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z69d 499\n"
    Sep  3 20:43:36.541: INFO: got output "I0903 20:43:36.395456       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z69d 499\n"
    STEP: limiting log bytes 09/03/22 20:43:36.541
    Sep  3 20:43:36.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 logs logs-generator logs-generator --limit-bytes=1'
    Sep  3 20:43:36.605: INFO: stderr: ""
    Sep  3 20:43:36.605: INFO: stdout: "I"
    Sep  3 20:43:36.605: INFO: got output "I"
    STEP: exposing timestamps 09/03/22 20:43:36.605
    Sep  3 20:43:36.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 logs logs-generator logs-generator --tail=1 --timestamps'
    Sep  3 20:43:36.685: INFO: stderr: ""
    Sep  3 20:43:36.685: INFO: stdout: "2022-09-03T20:43:36.595126358Z I0903 20:43:36.594960       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/rwk 290\n"
    Sep  3 20:43:36.685: INFO: got output "2022-09-03T20:43:36.595126358Z I0903 20:43:36.594960       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/rwk 290\n"
    STEP: restricting to a time range 09/03/22 20:43:36.685
    Sep  3 20:43:39.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 logs logs-generator logs-generator --since=1s'
    Sep  3 20:43:39.290: INFO: stderr: ""
    Sep  3 20:43:39.290: INFO: stdout: "I0903 20:43:38.395167       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/fkn 395\nI0903 20:43:38.595358       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/4vfj 400\nI0903 20:43:38.795847       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/mm6q 476\nI0903 20:43:38.995028       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/mpbd 280\nI0903 20:43:39.195380       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/ddsd 483\n"
    Sep  3 20:43:39.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 logs logs-generator logs-generator --since=24h'
    Sep  3 20:43:39.420: INFO: stderr: ""
    Sep  3 20:43:39.420: INFO: stdout: "I0903 20:43:34.994906       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/ffh6 317\nI0903 20:43:35.195048       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/s4nq 304\nI0903 20:43:35.394975       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/mlg 569\nI0903 20:43:35.595200       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/xmln 249\nI0903 20:43:35.795595       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/g4n 419\nI0903 20:43:35.995758       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/fx7g 457\nI0903 20:43:36.195048       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/q29b 300\nI0903 20:43:36.395456       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/z69d 499\nI0903 20:43:36.594960       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/rwk 290\nI0903 20:43:36.795392       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/tz6n 323\nI0903 20:43:36.995733       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/t46 434\nI0903 20:43:37.195064       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/bcc 584\nI0903 20:43:37.395466       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/wxpw 540\nI0903 20:43:37.595905       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/nzh 586\nI0903 20:43:37.795277       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/7bsh 573\nI0903 20:43:37.995453       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/wwj 364\nI0903 20:43:38.195853       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/tn5 439\nI0903 20:43:38.395167       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/fkn 395\nI0903 20:43:38.595358       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/4vfj 400\nI0903 20:43:38.795847       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/mm6q 476\nI0903 20:43:38.995028       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/mpbd 280\nI0903 20:43:39.195380       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/ddsd 483\nI0903 20:43:39.395725       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/266b 289\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Sep  3 20:43:39.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1003 delete pod logs-generator'
    Sep  3 20:43:40.485: INFO: stderr: ""
    Sep  3 20:43:40.485: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 20:43:40.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1003" for this suite. 09/03/22 20:43:40.488
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:43:40.495
Sep  3 20:43:40.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 20:43:40.496
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:40.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:40.515
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 09/03/22 20:43:40.517
Sep  3 20:43:40.522: INFO: Waiting up to 5m0s for pod "pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241" in namespace "emptydir-8353" to be "Succeeded or Failed"
Sep  3 20:43:40.524: INFO: Pod "pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258308ms
Sep  3 20:43:42.527: INFO: Pod "pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005634501s
Sep  3 20:43:44.528: INFO: Pod "pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006012751s
STEP: Saw pod success 09/03/22 20:43:44.528
Sep  3 20:43:44.528: INFO: Pod "pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241" satisfied condition "Succeeded or Failed"
Sep  3 20:43:44.530: INFO: Trying to get logs from node kind-worker2 pod pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241 container test-container: <nil>
STEP: delete the pod 09/03/22 20:43:44.534
Sep  3 20:43:44.539: INFO: Waiting for pod pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241 to disappear
Sep  3 20:43:44.541: INFO: Pod pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 20:43:44.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8353" for this suite. 09/03/22 20:43:44.544
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":26,"skipped":373,"failed":0}
------------------------------
• [4.051 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:43:40.495
    Sep  3 20:43:40.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 20:43:40.496
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:40.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:40.515
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 09/03/22 20:43:40.517
    Sep  3 20:43:40.522: INFO: Waiting up to 5m0s for pod "pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241" in namespace "emptydir-8353" to be "Succeeded or Failed"
    Sep  3 20:43:40.524: INFO: Pod "pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258308ms
    Sep  3 20:43:42.527: INFO: Pod "pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005634501s
    Sep  3 20:43:44.528: INFO: Pod "pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006012751s
    STEP: Saw pod success 09/03/22 20:43:44.528
    Sep  3 20:43:44.528: INFO: Pod "pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241" satisfied condition "Succeeded or Failed"
    Sep  3 20:43:44.530: INFO: Trying to get logs from node kind-worker2 pod pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241 container test-container: <nil>
    STEP: delete the pod 09/03/22 20:43:44.534
    Sep  3 20:43:44.539: INFO: Waiting for pod pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241 to disappear
    Sep  3 20:43:44.541: INFO: Pod pod-7a80ebfc-ac3b-4ece-8e6a-437a8e645241 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 20:43:44.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8353" for this suite. 09/03/22 20:43:44.544
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:43:44.551
Sep  3 20:43:44.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename replicaset 09/03/22 20:43:44.552
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:44.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:44.572
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 09/03/22 20:43:44.576
STEP: Verify that the required pods have come up. 09/03/22 20:43:44.58
Sep  3 20:43:44.582: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  3 20:43:49.585: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/03/22 20:43:49.585
STEP: Getting /status 09/03/22 20:43:49.585
Sep  3 20:43:49.589: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 09/03/22 20:43:49.589
Sep  3 20:43:49.596: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 09/03/22 20:43:49.596
Sep  3 20:43:49.600: INFO: Observed &ReplicaSet event: ADDED
Sep  3 20:43:49.600: INFO: Observed &ReplicaSet event: MODIFIED
Sep  3 20:43:49.600: INFO: Observed &ReplicaSet event: MODIFIED
Sep  3 20:43:49.600: INFO: Observed &ReplicaSet event: MODIFIED
Sep  3 20:43:49.600: INFO: Found replicaset test-rs in namespace replicaset-7631 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  3 20:43:49.601: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 09/03/22 20:43:49.601
Sep  3 20:43:49.601: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  3 20:43:49.605: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 09/03/22 20:43:49.605
Sep  3 20:43:49.607: INFO: Observed &ReplicaSet event: ADDED
Sep  3 20:43:49.607: INFO: Observed &ReplicaSet event: MODIFIED
Sep  3 20:43:49.607: INFO: Observed &ReplicaSet event: MODIFIED
Sep  3 20:43:49.608: INFO: Observed &ReplicaSet event: MODIFIED
Sep  3 20:43:49.608: INFO: Observed replicaset test-rs in namespace replicaset-7631 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  3 20:43:49.608: INFO: Observed &ReplicaSet event: MODIFIED
Sep  3 20:43:49.608: INFO: Found replicaset test-rs in namespace replicaset-7631 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Sep  3 20:43:49.608: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Sep  3 20:43:49.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7631" for this suite. 09/03/22 20:43:49.612
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":27,"skipped":377,"failed":0}
------------------------------
• [SLOW TEST] [5.065 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:43:44.551
    Sep  3 20:43:44.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename replicaset 09/03/22 20:43:44.552
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:44.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:44.572
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 09/03/22 20:43:44.576
    STEP: Verify that the required pods have come up. 09/03/22 20:43:44.58
    Sep  3 20:43:44.582: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  3 20:43:49.585: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/03/22 20:43:49.585
    STEP: Getting /status 09/03/22 20:43:49.585
    Sep  3 20:43:49.589: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 09/03/22 20:43:49.589
    Sep  3 20:43:49.596: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 09/03/22 20:43:49.596
    Sep  3 20:43:49.600: INFO: Observed &ReplicaSet event: ADDED
    Sep  3 20:43:49.600: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  3 20:43:49.600: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  3 20:43:49.600: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  3 20:43:49.600: INFO: Found replicaset test-rs in namespace replicaset-7631 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  3 20:43:49.601: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 09/03/22 20:43:49.601
    Sep  3 20:43:49.601: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  3 20:43:49.605: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 09/03/22 20:43:49.605
    Sep  3 20:43:49.607: INFO: Observed &ReplicaSet event: ADDED
    Sep  3 20:43:49.607: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  3 20:43:49.607: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  3 20:43:49.608: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  3 20:43:49.608: INFO: Observed replicaset test-rs in namespace replicaset-7631 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  3 20:43:49.608: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  3 20:43:49.608: INFO: Found replicaset test-rs in namespace replicaset-7631 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Sep  3 20:43:49.608: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Sep  3 20:43:49.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7631" for this suite. 09/03/22 20:43:49.612
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:43:49.618
Sep  3 20:43:49.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/03/22 20:43:49.619
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:49.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:49.634
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 09/03/22 20:43:49.636
STEP: Creating hostNetwork=false pod 09/03/22 20:43:49.637
Sep  3 20:43:49.644: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1738" to be "running and ready"
Sep  3 20:43:49.649: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.539818ms
Sep  3 20:43:49.649: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:43:51.652: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008580622s
Sep  3 20:43:51.652: INFO: The phase of Pod test-pod is Running (Ready = true)
Sep  3 20:43:51.652: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 09/03/22 20:43:51.654
Sep  3 20:43:51.658: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1738" to be "running and ready"
Sep  3 20:43:51.661: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.851807ms
Sep  3 20:43:51.661: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:43:53.666: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008104424s
Sep  3 20:43:53.666: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Sep  3 20:43:53.666: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 09/03/22 20:43:53.669
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/03/22 20:43:53.669
Sep  3 20:43:53.669: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 20:43:53.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 20:43:53.670: INFO: ExecWithOptions: Clientset creation
Sep  3 20:43:53.670: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  3 20:43:53.742: INFO: Exec stderr: ""
Sep  3 20:43:53.742: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 20:43:53.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 20:43:53.743: INFO: ExecWithOptions: Clientset creation
Sep  3 20:43:53.743: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  3 20:43:53.826: INFO: Exec stderr: ""
Sep  3 20:43:53.827: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 20:43:53.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 20:43:53.828: INFO: ExecWithOptions: Clientset creation
Sep  3 20:43:53.828: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  3 20:43:53.897: INFO: Exec stderr: ""
Sep  3 20:43:53.898: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 20:43:53.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 20:43:53.898: INFO: ExecWithOptions: Clientset creation
Sep  3 20:43:53.898: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  3 20:43:53.977: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/03/22 20:43:53.977
Sep  3 20:43:53.977: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 20:43:53.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 20:43:53.978: INFO: ExecWithOptions: Clientset creation
Sep  3 20:43:53.978: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  3 20:43:54.045: INFO: Exec stderr: ""
Sep  3 20:43:54.045: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 20:43:54.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 20:43:54.046: INFO: ExecWithOptions: Clientset creation
Sep  3 20:43:54.046: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  3 20:43:54.115: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/03/22 20:43:54.115
Sep  3 20:43:54.115: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 20:43:54.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 20:43:54.116: INFO: ExecWithOptions: Clientset creation
Sep  3 20:43:54.116: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  3 20:43:54.235: INFO: Exec stderr: ""
Sep  3 20:43:54.235: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 20:43:54.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 20:43:54.236: INFO: ExecWithOptions: Clientset creation
Sep  3 20:43:54.236: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  3 20:43:54.301: INFO: Exec stderr: ""
Sep  3 20:43:54.301: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 20:43:54.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 20:43:54.302: INFO: ExecWithOptions: Clientset creation
Sep  3 20:43:54.302: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  3 20:43:54.361: INFO: Exec stderr: ""
Sep  3 20:43:54.362: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 20:43:54.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 20:43:54.362: INFO: ExecWithOptions: Clientset creation
Sep  3 20:43:54.362: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  3 20:43:54.423: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Sep  3 20:43:54.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1738" for this suite. 09/03/22 20:43:54.426
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":28,"skipped":397,"failed":0}
------------------------------
• [4.813 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:43:49.618
    Sep  3 20:43:49.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/03/22 20:43:49.619
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:49.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:49.634
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 09/03/22 20:43:49.636
    STEP: Creating hostNetwork=false pod 09/03/22 20:43:49.637
    Sep  3 20:43:49.644: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1738" to be "running and ready"
    Sep  3 20:43:49.649: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.539818ms
    Sep  3 20:43:49.649: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:43:51.652: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008580622s
    Sep  3 20:43:51.652: INFO: The phase of Pod test-pod is Running (Ready = true)
    Sep  3 20:43:51.652: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 09/03/22 20:43:51.654
    Sep  3 20:43:51.658: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1738" to be "running and ready"
    Sep  3 20:43:51.661: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.851807ms
    Sep  3 20:43:51.661: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:43:53.666: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008104424s
    Sep  3 20:43:53.666: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Sep  3 20:43:53.666: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 09/03/22 20:43:53.669
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/03/22 20:43:53.669
    Sep  3 20:43:53.669: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 20:43:53.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 20:43:53.670: INFO: ExecWithOptions: Clientset creation
    Sep  3 20:43:53.670: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  3 20:43:53.742: INFO: Exec stderr: ""
    Sep  3 20:43:53.742: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 20:43:53.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 20:43:53.743: INFO: ExecWithOptions: Clientset creation
    Sep  3 20:43:53.743: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  3 20:43:53.826: INFO: Exec stderr: ""
    Sep  3 20:43:53.827: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 20:43:53.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 20:43:53.828: INFO: ExecWithOptions: Clientset creation
    Sep  3 20:43:53.828: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  3 20:43:53.897: INFO: Exec stderr: ""
    Sep  3 20:43:53.898: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 20:43:53.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 20:43:53.898: INFO: ExecWithOptions: Clientset creation
    Sep  3 20:43:53.898: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  3 20:43:53.977: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/03/22 20:43:53.977
    Sep  3 20:43:53.977: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 20:43:53.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 20:43:53.978: INFO: ExecWithOptions: Clientset creation
    Sep  3 20:43:53.978: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  3 20:43:54.045: INFO: Exec stderr: ""
    Sep  3 20:43:54.045: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 20:43:54.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 20:43:54.046: INFO: ExecWithOptions: Clientset creation
    Sep  3 20:43:54.046: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  3 20:43:54.115: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/03/22 20:43:54.115
    Sep  3 20:43:54.115: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 20:43:54.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 20:43:54.116: INFO: ExecWithOptions: Clientset creation
    Sep  3 20:43:54.116: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  3 20:43:54.235: INFO: Exec stderr: ""
    Sep  3 20:43:54.235: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 20:43:54.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 20:43:54.236: INFO: ExecWithOptions: Clientset creation
    Sep  3 20:43:54.236: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  3 20:43:54.301: INFO: Exec stderr: ""
    Sep  3 20:43:54.301: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 20:43:54.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 20:43:54.302: INFO: ExecWithOptions: Clientset creation
    Sep  3 20:43:54.302: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  3 20:43:54.361: INFO: Exec stderr: ""
    Sep  3 20:43:54.362: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1738 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 20:43:54.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 20:43:54.362: INFO: ExecWithOptions: Clientset creation
    Sep  3 20:43:54.362: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  3 20:43:54.423: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Sep  3 20:43:54.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-1738" for this suite. 09/03/22 20:43:54.426
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:43:54.433
Sep  3 20:43:54.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 20:43:54.434
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:54.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:54.454
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-9caf742f-99f5-4222-b891-8843dd0a0425 09/03/22 20:43:54.458
STEP: Creating secret with name s-test-opt-upd-865d14d9-2b2c-4554-9ecd-8a439dd3ba69 09/03/22 20:43:54.461
STEP: Creating the pod 09/03/22 20:43:54.464
Sep  3 20:43:54.469: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-15b34311-9f16-4d0c-8b65-81044bc70196" in namespace "projected-1086" to be "running and ready"
Sep  3 20:43:54.471: INFO: Pod "pod-projected-secrets-15b34311-9f16-4d0c-8b65-81044bc70196": Phase="Pending", Reason="", readiness=false. Elapsed: 2.300806ms
Sep  3 20:43:54.472: INFO: The phase of Pod pod-projected-secrets-15b34311-9f16-4d0c-8b65-81044bc70196 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:43:56.475: INFO: Pod "pod-projected-secrets-15b34311-9f16-4d0c-8b65-81044bc70196": Phase="Running", Reason="", readiness=true. Elapsed: 2.006148119s
Sep  3 20:43:56.475: INFO: The phase of Pod pod-projected-secrets-15b34311-9f16-4d0c-8b65-81044bc70196 is Running (Ready = true)
Sep  3 20:43:56.475: INFO: Pod "pod-projected-secrets-15b34311-9f16-4d0c-8b65-81044bc70196" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-9caf742f-99f5-4222-b891-8843dd0a0425 09/03/22 20:43:56.491
STEP: Updating secret s-test-opt-upd-865d14d9-2b2c-4554-9ecd-8a439dd3ba69 09/03/22 20:43:56.493
STEP: Creating secret with name s-test-opt-create-11c5b20f-3674-44c9-969a-c1e1f00e9e3b 09/03/22 20:43:56.496
STEP: waiting to observe update in volume 09/03/22 20:43:56.499
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Sep  3 20:43:58.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1086" for this suite. 09/03/22 20:43:58.522
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":29,"skipped":423,"failed":0}
------------------------------
• [4.093 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:43:54.433
    Sep  3 20:43:54.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 20:43:54.434
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:54.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:54.454
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-9caf742f-99f5-4222-b891-8843dd0a0425 09/03/22 20:43:54.458
    STEP: Creating secret with name s-test-opt-upd-865d14d9-2b2c-4554-9ecd-8a439dd3ba69 09/03/22 20:43:54.461
    STEP: Creating the pod 09/03/22 20:43:54.464
    Sep  3 20:43:54.469: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-15b34311-9f16-4d0c-8b65-81044bc70196" in namespace "projected-1086" to be "running and ready"
    Sep  3 20:43:54.471: INFO: Pod "pod-projected-secrets-15b34311-9f16-4d0c-8b65-81044bc70196": Phase="Pending", Reason="", readiness=false. Elapsed: 2.300806ms
    Sep  3 20:43:54.472: INFO: The phase of Pod pod-projected-secrets-15b34311-9f16-4d0c-8b65-81044bc70196 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:43:56.475: INFO: Pod "pod-projected-secrets-15b34311-9f16-4d0c-8b65-81044bc70196": Phase="Running", Reason="", readiness=true. Elapsed: 2.006148119s
    Sep  3 20:43:56.475: INFO: The phase of Pod pod-projected-secrets-15b34311-9f16-4d0c-8b65-81044bc70196 is Running (Ready = true)
    Sep  3 20:43:56.475: INFO: Pod "pod-projected-secrets-15b34311-9f16-4d0c-8b65-81044bc70196" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-9caf742f-99f5-4222-b891-8843dd0a0425 09/03/22 20:43:56.491
    STEP: Updating secret s-test-opt-upd-865d14d9-2b2c-4554-9ecd-8a439dd3ba69 09/03/22 20:43:56.493
    STEP: Creating secret with name s-test-opt-create-11c5b20f-3674-44c9-969a-c1e1f00e9e3b 09/03/22 20:43:56.496
    STEP: waiting to observe update in volume 09/03/22 20:43:56.499
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Sep  3 20:43:58.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1086" for this suite. 09/03/22 20:43:58.522
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:43:58.536
Sep  3 20:43:58.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 20:43:58.537
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:58.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:58.548
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-73b9457c-c5d4-4e09-912f-8172b9a8d66e 09/03/22 20:43:58.549
STEP: Creating a pod to test consume secrets 09/03/22 20:43:58.552
Sep  3 20:43:58.557: INFO: Waiting up to 5m0s for pod "pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93" in namespace "secrets-6133" to be "Succeeded or Failed"
Sep  3 20:43:58.566: INFO: Pod "pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93": Phase="Pending", Reason="", readiness=false. Elapsed: 8.231921ms
Sep  3 20:44:00.568: INFO: Pod "pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010841391s
Sep  3 20:44:02.569: INFO: Pod "pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012038541s
STEP: Saw pod success 09/03/22 20:44:02.569
Sep  3 20:44:02.570: INFO: Pod "pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93" satisfied condition "Succeeded or Failed"
Sep  3 20:44:02.571: INFO: Trying to get logs from node kind-worker pod pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93 container secret-volume-test: <nil>
STEP: delete the pod 09/03/22 20:44:02.585
Sep  3 20:44:02.590: INFO: Waiting for pod pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93 to disappear
Sep  3 20:44:02.592: INFO: Pod pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Sep  3 20:44:02.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6133" for this suite. 09/03/22 20:44:02.594
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":30,"skipped":466,"failed":0}
------------------------------
• [4.061 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:43:58.536
    Sep  3 20:43:58.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 20:43:58.537
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:43:58.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:43:58.548
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-73b9457c-c5d4-4e09-912f-8172b9a8d66e 09/03/22 20:43:58.549
    STEP: Creating a pod to test consume secrets 09/03/22 20:43:58.552
    Sep  3 20:43:58.557: INFO: Waiting up to 5m0s for pod "pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93" in namespace "secrets-6133" to be "Succeeded or Failed"
    Sep  3 20:43:58.566: INFO: Pod "pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93": Phase="Pending", Reason="", readiness=false. Elapsed: 8.231921ms
    Sep  3 20:44:00.568: INFO: Pod "pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010841391s
    Sep  3 20:44:02.569: INFO: Pod "pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012038541s
    STEP: Saw pod success 09/03/22 20:44:02.569
    Sep  3 20:44:02.570: INFO: Pod "pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93" satisfied condition "Succeeded or Failed"
    Sep  3 20:44:02.571: INFO: Trying to get logs from node kind-worker pod pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93 container secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 20:44:02.585
    Sep  3 20:44:02.590: INFO: Waiting for pod pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93 to disappear
    Sep  3 20:44:02.592: INFO: Pod pod-secrets-a3999863-413b-4320-a079-19f66bdd2a93 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 20:44:02.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6133" for this suite. 09/03/22 20:44:02.594
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:44:02.598
Sep  3 20:44:02.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename statefulset 09/03/22 20:44:02.599
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:44:02.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:44:02.612
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7066 09/03/22 20:44:02.614
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 09/03/22 20:44:02.618
Sep  3 20:44:02.627: INFO: Found 0 stateful pods, waiting for 3
Sep  3 20:44:12.630: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 20:44:12.630: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 20:44:12.630: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 09/03/22 20:44:12.636
Sep  3 20:44:12.652: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/03/22 20:44:12.652
STEP: Not applying an update when the partition is greater than the number of replicas 09/03/22 20:44:22.668
STEP: Performing a canary update 09/03/22 20:44:22.668
Sep  3 20:44:22.683: INFO: Updating stateful set ss2
Sep  3 20:44:22.687: INFO: Waiting for Pod statefulset-7066/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 09/03/22 20:44:32.694
Sep  3 20:44:32.830: INFO: Found 2 stateful pods, waiting for 3
Sep  3 20:44:42.833: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 20:44:42.833: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 20:44:42.833: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 09/03/22 20:44:42.837
Sep  3 20:44:42.852: INFO: Updating stateful set ss2
Sep  3 20:44:42.868: INFO: Waiting for Pod statefulset-7066/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Sep  3 20:44:52.889: INFO: Updating stateful set ss2
Sep  3 20:44:52.904: INFO: Waiting for StatefulSet statefulset-7066/ss2 to complete update
Sep  3 20:44:52.905: INFO: Waiting for Pod statefulset-7066/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Sep  3 20:45:02.916: INFO: Deleting all statefulset in ns statefulset-7066
Sep  3 20:45:02.917: INFO: Scaling statefulset ss2 to 0
Sep  3 20:45:12.931: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 20:45:12.936: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Sep  3 20:45:12.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7066" for this suite. 09/03/22 20:45:12.969
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":31,"skipped":469,"failed":0}
------------------------------
• [SLOW TEST] [70.377 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:44:02.598
    Sep  3 20:44:02.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename statefulset 09/03/22 20:44:02.599
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:44:02.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:44:02.612
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7066 09/03/22 20:44:02.614
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 09/03/22 20:44:02.618
    Sep  3 20:44:02.627: INFO: Found 0 stateful pods, waiting for 3
    Sep  3 20:44:12.630: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 20:44:12.630: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 20:44:12.630: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 09/03/22 20:44:12.636
    Sep  3 20:44:12.652: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/03/22 20:44:12.652
    STEP: Not applying an update when the partition is greater than the number of replicas 09/03/22 20:44:22.668
    STEP: Performing a canary update 09/03/22 20:44:22.668
    Sep  3 20:44:22.683: INFO: Updating stateful set ss2
    Sep  3 20:44:22.687: INFO: Waiting for Pod statefulset-7066/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 09/03/22 20:44:32.694
    Sep  3 20:44:32.830: INFO: Found 2 stateful pods, waiting for 3
    Sep  3 20:44:42.833: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 20:44:42.833: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 20:44:42.833: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 09/03/22 20:44:42.837
    Sep  3 20:44:42.852: INFO: Updating stateful set ss2
    Sep  3 20:44:42.868: INFO: Waiting for Pod statefulset-7066/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Sep  3 20:44:52.889: INFO: Updating stateful set ss2
    Sep  3 20:44:52.904: INFO: Waiting for StatefulSet statefulset-7066/ss2 to complete update
    Sep  3 20:44:52.905: INFO: Waiting for Pod statefulset-7066/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Sep  3 20:45:02.916: INFO: Deleting all statefulset in ns statefulset-7066
    Sep  3 20:45:02.917: INFO: Scaling statefulset ss2 to 0
    Sep  3 20:45:12.931: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  3 20:45:12.936: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Sep  3 20:45:12.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7066" for this suite. 09/03/22 20:45:12.969
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:45:12.976
Sep  3 20:45:12.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename job 09/03/22 20:45:12.977
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:45:12.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:45:12.995
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 09/03/22 20:45:13.001
STEP: Patching the Job 09/03/22 20:45:13.004
STEP: Watching for Job to be patched 09/03/22 20:45:13.022
Sep  3 20:45:13.025: INFO: Event ADDED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  3 20:45:13.025: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  3 20:45:13.026: INFO: Event MODIFIED found for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 09/03/22 20:45:13.026
STEP: Watching for Job to be updated 09/03/22 20:45:13.031
Sep  3 20:45:13.034: INFO: Event MODIFIED found for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  3 20:45:13.034: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 09/03/22 20:45:13.034
Sep  3 20:45:13.037: INFO: Job: e2e-mvp24 as labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched]
STEP: Waiting for job to complete 09/03/22 20:45:13.037
STEP: Delete a job collection with a labelselector 09/03/22 20:45:23.04
STEP: Watching for Job to be deleted 09/03/22 20:45:23.046
Sep  3 20:45:23.047: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  3 20:45:23.048: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  3 20:45:23.048: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  3 20:45:23.048: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  3 20:45:23.048: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  3 20:45:23.049: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  3 20:45:23.049: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  3 20:45:23.049: INFO: Event DELETED found for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 09/03/22 20:45:23.049
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Sep  3 20:45:23.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-806" for this suite. 09/03/22 20:45:23.066
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":32,"skipped":469,"failed":0}
------------------------------
• [SLOW TEST] [10.099 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:45:12.976
    Sep  3 20:45:12.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename job 09/03/22 20:45:12.977
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:45:12.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:45:12.995
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 09/03/22 20:45:13.001
    STEP: Patching the Job 09/03/22 20:45:13.004
    STEP: Watching for Job to be patched 09/03/22 20:45:13.022
    Sep  3 20:45:13.025: INFO: Event ADDED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  3 20:45:13.025: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  3 20:45:13.026: INFO: Event MODIFIED found for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 09/03/22 20:45:13.026
    STEP: Watching for Job to be updated 09/03/22 20:45:13.031
    Sep  3 20:45:13.034: INFO: Event MODIFIED found for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  3 20:45:13.034: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 09/03/22 20:45:13.034
    Sep  3 20:45:13.037: INFO: Job: e2e-mvp24 as labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched]
    STEP: Waiting for job to complete 09/03/22 20:45:13.037
    STEP: Delete a job collection with a labelselector 09/03/22 20:45:23.04
    STEP: Watching for Job to be deleted 09/03/22 20:45:23.046
    Sep  3 20:45:23.047: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  3 20:45:23.048: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  3 20:45:23.048: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  3 20:45:23.048: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  3 20:45:23.048: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  3 20:45:23.049: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  3 20:45:23.049: INFO: Event MODIFIED observed for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  3 20:45:23.049: INFO: Event DELETED found for Job e2e-mvp24 in namespace job-806 with labels: map[e2e-job-label:e2e-mvp24 e2e-mvp24:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 09/03/22 20:45:23.049
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Sep  3 20:45:23.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-806" for this suite. 09/03/22 20:45:23.066
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:45:23.076
Sep  3 20:45:23.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename endpointslice 09/03/22 20:45:23.076
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:45:23.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:45:23.145
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Sep  3 20:45:23.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6892" for this suite. 09/03/22 20:45:23.267
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":33,"skipped":484,"failed":0}
------------------------------
• [0.200 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:45:23.076
    Sep  3 20:45:23.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename endpointslice 09/03/22 20:45:23.076
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:45:23.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:45:23.145
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Sep  3 20:45:23.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6892" for this suite. 09/03/22 20:45:23.267
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:45:23.281
Sep  3 20:45:23.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename var-expansion 09/03/22 20:45:23.282
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:45:23.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:45:23.318
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 09/03/22 20:45:23.327
Sep  3 20:45:23.342: INFO: Waiting up to 5m0s for pod "var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2" in namespace "var-expansion-2127" to be "Succeeded or Failed"
Sep  3 20:45:23.354: INFO: Pod "var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.907126ms
Sep  3 20:45:25.357: INFO: Pod "var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015217737s
Sep  3 20:45:27.362: INFO: Pod "var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020375717s
STEP: Saw pod success 09/03/22 20:45:27.362
Sep  3 20:45:27.363: INFO: Pod "var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2" satisfied condition "Succeeded or Failed"
Sep  3 20:45:27.370: INFO: Trying to get logs from node kind-worker2 pod var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2 container dapi-container: <nil>
STEP: delete the pod 09/03/22 20:45:27.375
Sep  3 20:45:27.381: INFO: Waiting for pod var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2 to disappear
Sep  3 20:45:27.383: INFO: Pod var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Sep  3 20:45:27.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2127" for this suite. 09/03/22 20:45:27.386
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":34,"skipped":487,"failed":0}
------------------------------
• [4.108 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:45:23.281
    Sep  3 20:45:23.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename var-expansion 09/03/22 20:45:23.282
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:45:23.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:45:23.318
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 09/03/22 20:45:23.327
    Sep  3 20:45:23.342: INFO: Waiting up to 5m0s for pod "var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2" in namespace "var-expansion-2127" to be "Succeeded or Failed"
    Sep  3 20:45:23.354: INFO: Pod "var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.907126ms
    Sep  3 20:45:25.357: INFO: Pod "var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015217737s
    Sep  3 20:45:27.362: INFO: Pod "var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020375717s
    STEP: Saw pod success 09/03/22 20:45:27.362
    Sep  3 20:45:27.363: INFO: Pod "var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2" satisfied condition "Succeeded or Failed"
    Sep  3 20:45:27.370: INFO: Trying to get logs from node kind-worker2 pod var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2 container dapi-container: <nil>
    STEP: delete the pod 09/03/22 20:45:27.375
    Sep  3 20:45:27.381: INFO: Waiting for pod var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2 to disappear
    Sep  3 20:45:27.383: INFO: Pod var-expansion-ddbfbe06-3912-4b85-8cc1-2c0c049ec3e2 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Sep  3 20:45:27.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2127" for this suite. 09/03/22 20:45:27.386
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:45:27.392
Sep  3 20:45:27.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 20:45:27.394
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:45:27.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:45:27.409
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 09/03/22 20:45:27.411
STEP: listing secrets in all namespaces to ensure that there are more than zero 09/03/22 20:45:27.414
STEP: patching the secret 09/03/22 20:45:27.416
STEP: deleting the secret using a LabelSelector 09/03/22 20:45:27.421
STEP: listing secrets in all namespaces, searching for label name and value in patch 09/03/22 20:45:27.425
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Sep  3 20:45:27.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6795" for this suite. 09/03/22 20:45:27.429
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":35,"skipped":497,"failed":0}
------------------------------
• [0.040 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:45:27.392
    Sep  3 20:45:27.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 20:45:27.394
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:45:27.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:45:27.409
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 09/03/22 20:45:27.411
    STEP: listing secrets in all namespaces to ensure that there are more than zero 09/03/22 20:45:27.414
    STEP: patching the secret 09/03/22 20:45:27.416
    STEP: deleting the secret using a LabelSelector 09/03/22 20:45:27.421
    STEP: listing secrets in all namespaces, searching for label name and value in patch 09/03/22 20:45:27.425
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 20:45:27.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6795" for this suite. 09/03/22 20:45:27.429
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:45:27.433
Sep  3 20:45:27.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 20:45:27.434
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:45:27.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:45:27.449
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Sep  3 20:45:27.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/03/22 20:45:29.678
Sep  3 20:45:29.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-7982 --namespace=crd-publish-openapi-7982 create -f -'
Sep  3 20:45:30.408: INFO: stderr: ""
Sep  3 20:45:30.408: INFO: stdout: "e2e-test-crd-publish-openapi-1619-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  3 20:45:30.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-7982 --namespace=crd-publish-openapi-7982 delete e2e-test-crd-publish-openapi-1619-crds test-cr'
Sep  3 20:45:30.492: INFO: stderr: ""
Sep  3 20:45:30.492: INFO: stdout: "e2e-test-crd-publish-openapi-1619-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep  3 20:45:30.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-7982 --namespace=crd-publish-openapi-7982 apply -f -'
Sep  3 20:45:30.718: INFO: stderr: ""
Sep  3 20:45:30.718: INFO: stdout: "e2e-test-crd-publish-openapi-1619-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  3 20:45:30.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-7982 --namespace=crd-publish-openapi-7982 delete e2e-test-crd-publish-openapi-1619-crds test-cr'
Sep  3 20:45:30.804: INFO: stderr: ""
Sep  3 20:45:30.804: INFO: stdout: "e2e-test-crd-publish-openapi-1619-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/03/22 20:45:30.804
Sep  3 20:45:30.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-7982 explain e2e-test-crd-publish-openapi-1619-crds'
Sep  3 20:45:31.008: INFO: stderr: ""
Sep  3 20:45:31.008: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1619-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 20:45:33.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7982" for this suite. 09/03/22 20:45:33.365
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":36,"skipped":517,"failed":0}
------------------------------
• [SLOW TEST] [5.936 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:45:27.433
    Sep  3 20:45:27.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 20:45:27.434
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:45:27.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:45:27.449
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Sep  3 20:45:27.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/03/22 20:45:29.678
    Sep  3 20:45:29.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-7982 --namespace=crd-publish-openapi-7982 create -f -'
    Sep  3 20:45:30.408: INFO: stderr: ""
    Sep  3 20:45:30.408: INFO: stdout: "e2e-test-crd-publish-openapi-1619-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  3 20:45:30.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-7982 --namespace=crd-publish-openapi-7982 delete e2e-test-crd-publish-openapi-1619-crds test-cr'
    Sep  3 20:45:30.492: INFO: stderr: ""
    Sep  3 20:45:30.492: INFO: stdout: "e2e-test-crd-publish-openapi-1619-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Sep  3 20:45:30.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-7982 --namespace=crd-publish-openapi-7982 apply -f -'
    Sep  3 20:45:30.718: INFO: stderr: ""
    Sep  3 20:45:30.718: INFO: stdout: "e2e-test-crd-publish-openapi-1619-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  3 20:45:30.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-7982 --namespace=crd-publish-openapi-7982 delete e2e-test-crd-publish-openapi-1619-crds test-cr'
    Sep  3 20:45:30.804: INFO: stderr: ""
    Sep  3 20:45:30.804: INFO: stdout: "e2e-test-crd-publish-openapi-1619-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/03/22 20:45:30.804
    Sep  3 20:45:30.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-7982 explain e2e-test-crd-publish-openapi-1619-crds'
    Sep  3 20:45:31.008: INFO: stderr: ""
    Sep  3 20:45:31.008: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1619-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 20:45:33.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7982" for this suite. 09/03/22 20:45:33.365
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:45:33.378
Sep  3 20:45:33.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-probe 09/03/22 20:45:33.378
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:45:33.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:45:33.394
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-da7d17d0-7096-4268-b541-bb2d34b40c91 in namespace container-probe-1045 09/03/22 20:45:33.397
Sep  3 20:45:33.402: INFO: Waiting up to 5m0s for pod "busybox-da7d17d0-7096-4268-b541-bb2d34b40c91" in namespace "container-probe-1045" to be "not pending"
Sep  3 20:45:33.406: INFO: Pod "busybox-da7d17d0-7096-4268-b541-bb2d34b40c91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042609ms
Sep  3 20:45:35.409: INFO: Pod "busybox-da7d17d0-7096-4268-b541-bb2d34b40c91": Phase="Running", Reason="", readiness=true. Elapsed: 2.006760283s
Sep  3 20:45:35.409: INFO: Pod "busybox-da7d17d0-7096-4268-b541-bb2d34b40c91" satisfied condition "not pending"
Sep  3 20:45:35.409: INFO: Started pod busybox-da7d17d0-7096-4268-b541-bb2d34b40c91 in namespace container-probe-1045
STEP: checking the pod's current state and verifying that restartCount is present 09/03/22 20:45:35.409
Sep  3 20:45:35.411: INFO: Initial restart count of pod busybox-da7d17d0-7096-4268-b541-bb2d34b40c91 is 0
STEP: deleting the pod 09/03/22 20:49:35.816
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Sep  3 20:49:35.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1045" for this suite. 09/03/22 20:49:35.836
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":37,"skipped":544,"failed":0}
------------------------------
• [SLOW TEST] [242.462 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:45:33.378
    Sep  3 20:45:33.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-probe 09/03/22 20:45:33.378
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:45:33.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:45:33.394
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-da7d17d0-7096-4268-b541-bb2d34b40c91 in namespace container-probe-1045 09/03/22 20:45:33.397
    Sep  3 20:45:33.402: INFO: Waiting up to 5m0s for pod "busybox-da7d17d0-7096-4268-b541-bb2d34b40c91" in namespace "container-probe-1045" to be "not pending"
    Sep  3 20:45:33.406: INFO: Pod "busybox-da7d17d0-7096-4268-b541-bb2d34b40c91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042609ms
    Sep  3 20:45:35.409: INFO: Pod "busybox-da7d17d0-7096-4268-b541-bb2d34b40c91": Phase="Running", Reason="", readiness=true. Elapsed: 2.006760283s
    Sep  3 20:45:35.409: INFO: Pod "busybox-da7d17d0-7096-4268-b541-bb2d34b40c91" satisfied condition "not pending"
    Sep  3 20:45:35.409: INFO: Started pod busybox-da7d17d0-7096-4268-b541-bb2d34b40c91 in namespace container-probe-1045
    STEP: checking the pod's current state and verifying that restartCount is present 09/03/22 20:45:35.409
    Sep  3 20:45:35.411: INFO: Initial restart count of pod busybox-da7d17d0-7096-4268-b541-bb2d34b40c91 is 0
    STEP: deleting the pod 09/03/22 20:49:35.816
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Sep  3 20:49:35.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1045" for this suite. 09/03/22 20:49:35.836
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:49:35.842
Sep  3 20:49:35.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename var-expansion 09/03/22 20:49:35.843
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:49:35.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:49:35.855
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Sep  3 20:49:35.861: INFO: Waiting up to 2m0s for pod "var-expansion-d90dedc0-8221-4455-abbd-7521be396ecd" in namespace "var-expansion-6663" to be "container 0 failed with reason CreateContainerConfigError"
Sep  3 20:49:35.868: INFO: Pod "var-expansion-d90dedc0-8221-4455-abbd-7521be396ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.499424ms
Sep  3 20:49:37.871: INFO: Pod "var-expansion-d90dedc0-8221-4455-abbd-7521be396ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009425214s
Sep  3 20:49:37.871: INFO: Pod "var-expansion-d90dedc0-8221-4455-abbd-7521be396ecd" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  3 20:49:37.871: INFO: Deleting pod "var-expansion-d90dedc0-8221-4455-abbd-7521be396ecd" in namespace "var-expansion-6663"
Sep  3 20:49:37.875: INFO: Wait up to 5m0s for pod "var-expansion-d90dedc0-8221-4455-abbd-7521be396ecd" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Sep  3 20:49:39.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6663" for this suite. 09/03/22 20:49:39.891
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":38,"skipped":552,"failed":0}
------------------------------
• [4.053 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:49:35.842
    Sep  3 20:49:35.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename var-expansion 09/03/22 20:49:35.843
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:49:35.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:49:35.855
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Sep  3 20:49:35.861: INFO: Waiting up to 2m0s for pod "var-expansion-d90dedc0-8221-4455-abbd-7521be396ecd" in namespace "var-expansion-6663" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  3 20:49:35.868: INFO: Pod "var-expansion-d90dedc0-8221-4455-abbd-7521be396ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.499424ms
    Sep  3 20:49:37.871: INFO: Pod "var-expansion-d90dedc0-8221-4455-abbd-7521be396ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009425214s
    Sep  3 20:49:37.871: INFO: Pod "var-expansion-d90dedc0-8221-4455-abbd-7521be396ecd" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  3 20:49:37.871: INFO: Deleting pod "var-expansion-d90dedc0-8221-4455-abbd-7521be396ecd" in namespace "var-expansion-6663"
    Sep  3 20:49:37.875: INFO: Wait up to 5m0s for pod "var-expansion-d90dedc0-8221-4455-abbd-7521be396ecd" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Sep  3 20:49:39.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6663" for this suite. 09/03/22 20:49:39.891
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:49:39.908
Sep  3 20:49:39.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 20:49:39.909
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:49:39.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:49:39.923
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 09/03/22 20:49:39.924
Sep  3 20:49:39.929: INFO: Waiting up to 5m0s for pod "pod-1ec65b1b-f617-4868-a529-73432ac7589d" in namespace "emptydir-7473" to be "Succeeded or Failed"
Sep  3 20:49:39.931: INFO: Pod "pod-1ec65b1b-f617-4868-a529-73432ac7589d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.834407ms
Sep  3 20:49:41.935: INFO: Pod "pod-1ec65b1b-f617-4868-a529-73432ac7589d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005842s
Sep  3 20:49:43.934: INFO: Pod "pod-1ec65b1b-f617-4868-a529-73432ac7589d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004748326s
STEP: Saw pod success 09/03/22 20:49:43.934
Sep  3 20:49:43.934: INFO: Pod "pod-1ec65b1b-f617-4868-a529-73432ac7589d" satisfied condition "Succeeded or Failed"
Sep  3 20:49:43.936: INFO: Trying to get logs from node kind-worker2 pod pod-1ec65b1b-f617-4868-a529-73432ac7589d container test-container: <nil>
STEP: delete the pod 09/03/22 20:49:43.957
Sep  3 20:49:43.963: INFO: Waiting for pod pod-1ec65b1b-f617-4868-a529-73432ac7589d to disappear
Sep  3 20:49:43.966: INFO: Pod pod-1ec65b1b-f617-4868-a529-73432ac7589d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 20:49:43.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7473" for this suite. 09/03/22 20:49:43.972
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":39,"skipped":608,"failed":0}
------------------------------
• [4.072 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:49:39.908
    Sep  3 20:49:39.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 20:49:39.909
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:49:39.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:49:39.923
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/03/22 20:49:39.924
    Sep  3 20:49:39.929: INFO: Waiting up to 5m0s for pod "pod-1ec65b1b-f617-4868-a529-73432ac7589d" in namespace "emptydir-7473" to be "Succeeded or Failed"
    Sep  3 20:49:39.931: INFO: Pod "pod-1ec65b1b-f617-4868-a529-73432ac7589d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.834407ms
    Sep  3 20:49:41.935: INFO: Pod "pod-1ec65b1b-f617-4868-a529-73432ac7589d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005842s
    Sep  3 20:49:43.934: INFO: Pod "pod-1ec65b1b-f617-4868-a529-73432ac7589d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004748326s
    STEP: Saw pod success 09/03/22 20:49:43.934
    Sep  3 20:49:43.934: INFO: Pod "pod-1ec65b1b-f617-4868-a529-73432ac7589d" satisfied condition "Succeeded or Failed"
    Sep  3 20:49:43.936: INFO: Trying to get logs from node kind-worker2 pod pod-1ec65b1b-f617-4868-a529-73432ac7589d container test-container: <nil>
    STEP: delete the pod 09/03/22 20:49:43.957
    Sep  3 20:49:43.963: INFO: Waiting for pod pod-1ec65b1b-f617-4868-a529-73432ac7589d to disappear
    Sep  3 20:49:43.966: INFO: Pod pod-1ec65b1b-f617-4868-a529-73432ac7589d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 20:49:43.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7473" for this suite. 09/03/22 20:49:43.972
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:49:43.981
Sep  3 20:49:43.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename statefulset 09/03/22 20:49:43.983
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:49:43.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:49:43.998
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-477 09/03/22 20:49:44
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 09/03/22 20:49:44.005
Sep  3 20:49:44.018: INFO: Found 0 stateful pods, waiting for 3
Sep  3 20:49:54.023: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 20:49:54.023: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 20:49:54.023: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 20:49:54.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-477 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 20:49:54.184: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 20:49:54.184: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 20:49:54.184: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 09/03/22 20:50:04.197
Sep  3 20:50:04.213: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/03/22 20:50:04.213
STEP: Updating Pods in reverse ordinal order 09/03/22 20:50:14.226
Sep  3 20:50:14.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-477 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 20:50:14.376: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 20:50:14.376: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 20:50:14.376: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 09/03/22 20:50:24.389
Sep  3 20:50:24.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-477 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 20:50:24.523: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 20:50:24.523: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 20:50:24.523: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 20:50:34.548: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 09/03/22 20:50:44.57
Sep  3 20:50:44.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-477 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 20:50:44.717: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 20:50:44.717: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 20:50:44.717: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Sep  3 20:50:54.730: INFO: Deleting all statefulset in ns statefulset-477
Sep  3 20:50:54.731: INFO: Scaling statefulset ss2 to 0
Sep  3 20:51:04.744: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 20:51:04.747: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Sep  3 20:51:04.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-477" for this suite. 09/03/22 20:51:04.765
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":40,"skipped":612,"failed":0}
------------------------------
• [SLOW TEST] [80.788 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:49:43.981
    Sep  3 20:49:43.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename statefulset 09/03/22 20:49:43.983
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:49:43.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:49:43.998
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-477 09/03/22 20:49:44
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 09/03/22 20:49:44.005
    Sep  3 20:49:44.018: INFO: Found 0 stateful pods, waiting for 3
    Sep  3 20:49:54.023: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 20:49:54.023: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 20:49:54.023: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 20:49:54.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-477 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  3 20:49:54.184: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  3 20:49:54.184: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  3 20:49:54.184: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 09/03/22 20:50:04.197
    Sep  3 20:50:04.213: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/03/22 20:50:04.213
    STEP: Updating Pods in reverse ordinal order 09/03/22 20:50:14.226
    Sep  3 20:50:14.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-477 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  3 20:50:14.376: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  3 20:50:14.376: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  3 20:50:14.376: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 09/03/22 20:50:24.389
    Sep  3 20:50:24.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-477 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  3 20:50:24.523: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  3 20:50:24.523: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  3 20:50:24.523: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  3 20:50:34.548: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 09/03/22 20:50:44.57
    Sep  3 20:50:44.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-477 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  3 20:50:44.717: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  3 20:50:44.717: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  3 20:50:44.717: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Sep  3 20:50:54.730: INFO: Deleting all statefulset in ns statefulset-477
    Sep  3 20:50:54.731: INFO: Scaling statefulset ss2 to 0
    Sep  3 20:51:04.744: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  3 20:51:04.747: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Sep  3 20:51:04.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-477" for this suite. 09/03/22 20:51:04.765
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:51:04.769
Sep  3 20:51:04.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 20:51:04.771
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:04.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:04.784
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-34c1fe71-7bc1-4a6f-b938-e4d8c3bfd8cd 09/03/22 20:51:04.787
STEP: Creating a pod to test consume configMaps 09/03/22 20:51:04.789
Sep  3 20:51:04.799: INFO: Waiting up to 5m0s for pod "pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94" in namespace "configmap-3362" to be "Succeeded or Failed"
Sep  3 20:51:04.805: INFO: Pod "pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94": Phase="Pending", Reason="", readiness=false. Elapsed: 5.300307ms
Sep  3 20:51:06.808: INFO: Pod "pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008033014s
Sep  3 20:51:08.808: INFO: Pod "pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008152418s
STEP: Saw pod success 09/03/22 20:51:08.808
Sep  3 20:51:08.808: INFO: Pod "pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94" satisfied condition "Succeeded or Failed"
Sep  3 20:51:08.810: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94 container agnhost-container: <nil>
STEP: delete the pod 09/03/22 20:51:08.814
Sep  3 20:51:08.820: INFO: Waiting for pod pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94 to disappear
Sep  3 20:51:08.822: INFO: Pod pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 20:51:08.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3362" for this suite. 09/03/22 20:51:08.824
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":41,"skipped":612,"failed":0}
------------------------------
• [4.058 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:51:04.769
    Sep  3 20:51:04.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 20:51:04.771
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:04.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:04.784
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-34c1fe71-7bc1-4a6f-b938-e4d8c3bfd8cd 09/03/22 20:51:04.787
    STEP: Creating a pod to test consume configMaps 09/03/22 20:51:04.789
    Sep  3 20:51:04.799: INFO: Waiting up to 5m0s for pod "pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94" in namespace "configmap-3362" to be "Succeeded or Failed"
    Sep  3 20:51:04.805: INFO: Pod "pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94": Phase="Pending", Reason="", readiness=false. Elapsed: 5.300307ms
    Sep  3 20:51:06.808: INFO: Pod "pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008033014s
    Sep  3 20:51:08.808: INFO: Pod "pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008152418s
    STEP: Saw pod success 09/03/22 20:51:08.808
    Sep  3 20:51:08.808: INFO: Pod "pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94" satisfied condition "Succeeded or Failed"
    Sep  3 20:51:08.810: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94 container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 20:51:08.814
    Sep  3 20:51:08.820: INFO: Waiting for pod pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94 to disappear
    Sep  3 20:51:08.822: INFO: Pod pod-configmaps-83e53706-9a96-4962-9071-aeacc9257d94 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 20:51:08.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3362" for this suite. 09/03/22 20:51:08.824
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:51:08.83
Sep  3 20:51:08.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename proxy 09/03/22 20:51:08.831
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:08.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:08.842
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Sep  3 20:51:08.844: INFO: Creating pod...
Sep  3 20:51:08.850: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1642" to be "running"
Sep  3 20:51:08.854: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.475406ms
Sep  3 20:51:10.857: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007406861s
Sep  3 20:51:10.857: INFO: Pod "agnhost" satisfied condition "running"
Sep  3 20:51:10.857: INFO: Creating service...
Sep  3 20:51:10.865: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=DELETE
Sep  3 20:51:10.887: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  3 20:51:10.887: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=OPTIONS
Sep  3 20:51:10.895: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  3 20:51:10.896: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=PATCH
Sep  3 20:51:10.906: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  3 20:51:10.906: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=POST
Sep  3 20:51:10.916: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  3 20:51:10.916: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=PUT
Sep  3 20:51:10.926: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  3 20:51:10.926: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=DELETE
Sep  3 20:51:10.936: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  3 20:51:10.936: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=OPTIONS
Sep  3 20:51:10.941: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  3 20:51:10.942: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=PATCH
Sep  3 20:51:10.951: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  3 20:51:10.951: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=POST
Sep  3 20:51:10.955: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  3 20:51:10.955: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=PUT
Sep  3 20:51:10.960: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  3 20:51:10.960: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=GET
Sep  3 20:51:10.962: INFO: http.Client request:GET StatusCode:301
Sep  3 20:51:10.962: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=GET
Sep  3 20:51:10.966: INFO: http.Client request:GET StatusCode:301
Sep  3 20:51:10.966: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=HEAD
Sep  3 20:51:10.972: INFO: http.Client request:HEAD StatusCode:301
Sep  3 20:51:10.972: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=HEAD
Sep  3 20:51:10.996: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Sep  3 20:51:10.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1642" for this suite. 09/03/22 20:51:11.019
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":42,"skipped":613,"failed":0}
------------------------------
• [2.209 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:51:08.83
    Sep  3 20:51:08.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename proxy 09/03/22 20:51:08.831
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:08.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:08.842
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Sep  3 20:51:08.844: INFO: Creating pod...
    Sep  3 20:51:08.850: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1642" to be "running"
    Sep  3 20:51:08.854: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.475406ms
    Sep  3 20:51:10.857: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007406861s
    Sep  3 20:51:10.857: INFO: Pod "agnhost" satisfied condition "running"
    Sep  3 20:51:10.857: INFO: Creating service...
    Sep  3 20:51:10.865: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=DELETE
    Sep  3 20:51:10.887: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  3 20:51:10.887: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=OPTIONS
    Sep  3 20:51:10.895: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  3 20:51:10.896: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=PATCH
    Sep  3 20:51:10.906: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  3 20:51:10.906: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=POST
    Sep  3 20:51:10.916: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  3 20:51:10.916: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=PUT
    Sep  3 20:51:10.926: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  3 20:51:10.926: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=DELETE
    Sep  3 20:51:10.936: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  3 20:51:10.936: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Sep  3 20:51:10.941: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  3 20:51:10.942: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=PATCH
    Sep  3 20:51:10.951: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  3 20:51:10.951: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=POST
    Sep  3 20:51:10.955: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  3 20:51:10.955: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=PUT
    Sep  3 20:51:10.960: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  3 20:51:10.960: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=GET
    Sep  3 20:51:10.962: INFO: http.Client request:GET StatusCode:301
    Sep  3 20:51:10.962: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=GET
    Sep  3 20:51:10.966: INFO: http.Client request:GET StatusCode:301
    Sep  3 20:51:10.966: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/pods/agnhost/proxy?method=HEAD
    Sep  3 20:51:10.972: INFO: http.Client request:HEAD StatusCode:301
    Sep  3 20:51:10.972: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1642/services/e2e-proxy-test-service/proxy?method=HEAD
    Sep  3 20:51:10.996: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Sep  3 20:51:10.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-1642" for this suite. 09/03/22 20:51:11.019
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:51:11.047
Sep  3 20:51:11.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 20:51:11.054
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:11.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:11.086
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1662 09/03/22 20:51:11.091
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/03/22 20:51:11.102
STEP: creating service externalsvc in namespace services-1662 09/03/22 20:51:11.103
STEP: creating replication controller externalsvc in namespace services-1662 09/03/22 20:51:11.112
I0903 20:51:11.117386      24 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1662, replica count: 2
I0903 20:51:14.168243      24 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 09/03/22 20:51:14.171
Sep  3 20:51:14.188: INFO: Creating new exec pod
Sep  3 20:51:14.193: INFO: Waiting up to 5m0s for pod "execpodg9qm5" in namespace "services-1662" to be "running"
Sep  3 20:51:14.205: INFO: Pod "execpodg9qm5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.722039ms
Sep  3 20:51:16.232: INFO: Pod "execpodg9qm5": Phase="Running", Reason="", readiness=true. Elapsed: 2.038112701s
Sep  3 20:51:16.232: INFO: Pod "execpodg9qm5" satisfied condition "running"
Sep  3 20:51:16.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1662 exec execpodg9qm5 -- /bin/sh -x -c nslookup clusterip-service.services-1662.svc.cluster.local'
Sep  3 20:51:16.592: INFO: stderr: "+ nslookup clusterip-service.services-1662.svc.cluster.local\n"
Sep  3 20:51:16.592: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-1662.svc.cluster.local\tcanonical name = externalsvc.services-1662.svc.cluster.local.\nName:\texternalsvc.services-1662.svc.cluster.local\nAddress: 10.96.85.183\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1662, will wait for the garbage collector to delete the pods 09/03/22 20:51:16.592
Sep  3 20:51:16.651: INFO: Deleting ReplicationController externalsvc took: 6.09012ms
Sep  3 20:51:16.752: INFO: Terminating ReplicationController externalsvc pods took: 100.800332ms
Sep  3 20:51:18.762: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 20:51:18.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1662" for this suite. 09/03/22 20:51:18.779
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":43,"skipped":628,"failed":0}
------------------------------
• [SLOW TEST] [7.739 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:51:11.047
    Sep  3 20:51:11.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 20:51:11.054
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:11.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:11.086
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1662 09/03/22 20:51:11.091
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/03/22 20:51:11.102
    STEP: creating service externalsvc in namespace services-1662 09/03/22 20:51:11.103
    STEP: creating replication controller externalsvc in namespace services-1662 09/03/22 20:51:11.112
    I0903 20:51:11.117386      24 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1662, replica count: 2
    I0903 20:51:14.168243      24 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 09/03/22 20:51:14.171
    Sep  3 20:51:14.188: INFO: Creating new exec pod
    Sep  3 20:51:14.193: INFO: Waiting up to 5m0s for pod "execpodg9qm5" in namespace "services-1662" to be "running"
    Sep  3 20:51:14.205: INFO: Pod "execpodg9qm5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.722039ms
    Sep  3 20:51:16.232: INFO: Pod "execpodg9qm5": Phase="Running", Reason="", readiness=true. Elapsed: 2.038112701s
    Sep  3 20:51:16.232: INFO: Pod "execpodg9qm5" satisfied condition "running"
    Sep  3 20:51:16.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1662 exec execpodg9qm5 -- /bin/sh -x -c nslookup clusterip-service.services-1662.svc.cluster.local'
    Sep  3 20:51:16.592: INFO: stderr: "+ nslookup clusterip-service.services-1662.svc.cluster.local\n"
    Sep  3 20:51:16.592: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-1662.svc.cluster.local\tcanonical name = externalsvc.services-1662.svc.cluster.local.\nName:\texternalsvc.services-1662.svc.cluster.local\nAddress: 10.96.85.183\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1662, will wait for the garbage collector to delete the pods 09/03/22 20:51:16.592
    Sep  3 20:51:16.651: INFO: Deleting ReplicationController externalsvc took: 6.09012ms
    Sep  3 20:51:16.752: INFO: Terminating ReplicationController externalsvc pods took: 100.800332ms
    Sep  3 20:51:18.762: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 20:51:18.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1662" for this suite. 09/03/22 20:51:18.779
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:51:18.789
Sep  3 20:51:18.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename events 09/03/22 20:51:18.79
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:18.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:18.8
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 09/03/22 20:51:18.803
STEP: listing all events in all namespaces 09/03/22 20:51:18.805
STEP: patching the test event 09/03/22 20:51:18.81
STEP: fetching the test event 09/03/22 20:51:18.814
STEP: updating the test event 09/03/22 20:51:18.816
STEP: getting the test event 09/03/22 20:51:18.822
STEP: deleting the test event 09/03/22 20:51:18.824
STEP: listing all events in all namespaces 09/03/22 20:51:18.828
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Sep  3 20:51:18.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5393" for this suite. 09/03/22 20:51:18.834
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":44,"skipped":637,"failed":0}
------------------------------
• [0.049 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:51:18.789
    Sep  3 20:51:18.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename events 09/03/22 20:51:18.79
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:18.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:18.8
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 09/03/22 20:51:18.803
    STEP: listing all events in all namespaces 09/03/22 20:51:18.805
    STEP: patching the test event 09/03/22 20:51:18.81
    STEP: fetching the test event 09/03/22 20:51:18.814
    STEP: updating the test event 09/03/22 20:51:18.816
    STEP: getting the test event 09/03/22 20:51:18.822
    STEP: deleting the test event 09/03/22 20:51:18.824
    STEP: listing all events in all namespaces 09/03/22 20:51:18.828
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Sep  3 20:51:18.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5393" for this suite. 09/03/22 20:51:18.834
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:51:18.849
Sep  3 20:51:18.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 20:51:18.85
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:18.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:18.863
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 20:51:18.872
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 20:51:19.584
STEP: Deploying the webhook pod 09/03/22 20:51:19.589
STEP: Wait for the deployment to be ready 09/03/22 20:51:19.596
Sep  3 20:51:19.604: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 20:51:21.61
STEP: Verifying the service has paired with the endpoint 09/03/22 20:51:21.616
Sep  3 20:51:22.616: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 09/03/22 20:51:22.677
STEP: Creating a configMap that does not comply to the validation webhook rules 09/03/22 20:51:22.748
STEP: Deleting the collection of validation webhooks 09/03/22 20:51:22.802
STEP: Creating a configMap that does not comply to the validation webhook rules 09/03/22 20:51:22.828
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 20:51:22.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6539" for this suite. 09/03/22 20:51:22.837
STEP: Destroying namespace "webhook-6539-markers" for this suite. 09/03/22 20:51:22.84
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":45,"skipped":727,"failed":0}
------------------------------
• [4.079 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:51:18.849
    Sep  3 20:51:18.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 20:51:18.85
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:18.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:18.863
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 20:51:18.872
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 20:51:19.584
    STEP: Deploying the webhook pod 09/03/22 20:51:19.589
    STEP: Wait for the deployment to be ready 09/03/22 20:51:19.596
    Sep  3 20:51:19.604: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 20:51:21.61
    STEP: Verifying the service has paired with the endpoint 09/03/22 20:51:21.616
    Sep  3 20:51:22.616: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 09/03/22 20:51:22.677
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/03/22 20:51:22.748
    STEP: Deleting the collection of validation webhooks 09/03/22 20:51:22.802
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/03/22 20:51:22.828
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 20:51:22.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6539" for this suite. 09/03/22 20:51:22.837
    STEP: Destroying namespace "webhook-6539-markers" for this suite. 09/03/22 20:51:22.84
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:51:22.928
Sep  3 20:51:22.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 20:51:22.93
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:22.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:22.952
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 09/03/22 20:51:22.955
Sep  3 20:51:22.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-7601 create -f -'
Sep  3 20:51:23.579: INFO: stderr: ""
Sep  3 20:51:23.579: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/03/22 20:51:23.579
Sep  3 20:51:24.584: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 20:51:24.584: INFO: Found 0 / 1
Sep  3 20:51:25.583: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 20:51:25.583: INFO: Found 1 / 1
Sep  3 20:51:25.583: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 09/03/22 20:51:25.583
Sep  3 20:51:25.585: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 20:51:25.585: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  3 20:51:25.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-7601 patch pod agnhost-primary-r2wm7 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  3 20:51:25.667: INFO: stderr: ""
Sep  3 20:51:25.667: INFO: stdout: "pod/agnhost-primary-r2wm7 patched\n"
STEP: checking annotations 09/03/22 20:51:25.667
Sep  3 20:51:25.669: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 20:51:25.669: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 20:51:25.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7601" for this suite. 09/03/22 20:51:25.672
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":46,"skipped":727,"failed":0}
------------------------------
• [2.747 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:51:22.928
    Sep  3 20:51:22.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 20:51:22.93
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:22.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:22.952
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 09/03/22 20:51:22.955
    Sep  3 20:51:22.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-7601 create -f -'
    Sep  3 20:51:23.579: INFO: stderr: ""
    Sep  3 20:51:23.579: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/03/22 20:51:23.579
    Sep  3 20:51:24.584: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  3 20:51:24.584: INFO: Found 0 / 1
    Sep  3 20:51:25.583: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  3 20:51:25.583: INFO: Found 1 / 1
    Sep  3 20:51:25.583: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 09/03/22 20:51:25.583
    Sep  3 20:51:25.585: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  3 20:51:25.585: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  3 20:51:25.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-7601 patch pod agnhost-primary-r2wm7 -p {"metadata":{"annotations":{"x":"y"}}}'
    Sep  3 20:51:25.667: INFO: stderr: ""
    Sep  3 20:51:25.667: INFO: stdout: "pod/agnhost-primary-r2wm7 patched\n"
    STEP: checking annotations 09/03/22 20:51:25.667
    Sep  3 20:51:25.669: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  3 20:51:25.669: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 20:51:25.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7601" for this suite. 09/03/22 20:51:25.672
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:51:25.68
Sep  3 20:51:25.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-lifecycle-hook 09/03/22 20:51:25.682
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:25.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:25.708
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 09/03/22 20:51:25.714
Sep  3 20:51:25.719: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9707" to be "running and ready"
Sep  3 20:51:25.722: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.551309ms
Sep  3 20:51:25.722: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:51:27.725: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005618795s
Sep  3 20:51:27.725: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  3 20:51:27.725: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 09/03/22 20:51:27.727
Sep  3 20:51:27.730: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9707" to be "running and ready"
Sep  3 20:51:27.733: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.273911ms
Sep  3 20:51:27.733: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:51:29.736: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005836283s
Sep  3 20:51:29.736: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Sep  3 20:51:29.736: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/03/22 20:51:29.737
Sep  3 20:51:29.742: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  3 20:51:29.744: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  3 20:51:31.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  3 20:51:31.748: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  3 20:51:33.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  3 20:51:33.747: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 09/03/22 20:51:33.748
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Sep  3 20:51:33.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9707" for this suite. 09/03/22 20:51:33.763
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":47,"skipped":747,"failed":0}
------------------------------
• [SLOW TEST] [8.086 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:51:25.68
    Sep  3 20:51:25.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/03/22 20:51:25.682
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:25.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:25.708
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 09/03/22 20:51:25.714
    Sep  3 20:51:25.719: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9707" to be "running and ready"
    Sep  3 20:51:25.722: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.551309ms
    Sep  3 20:51:25.722: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:51:27.725: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005618795s
    Sep  3 20:51:27.725: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  3 20:51:27.725: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 09/03/22 20:51:27.727
    Sep  3 20:51:27.730: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9707" to be "running and ready"
    Sep  3 20:51:27.733: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.273911ms
    Sep  3 20:51:27.733: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:51:29.736: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005836283s
    Sep  3 20:51:29.736: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Sep  3 20:51:29.736: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/03/22 20:51:29.737
    Sep  3 20:51:29.742: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  3 20:51:29.744: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  3 20:51:31.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  3 20:51:31.748: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  3 20:51:33.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  3 20:51:33.747: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 09/03/22 20:51:33.748
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Sep  3 20:51:33.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9707" for this suite. 09/03/22 20:51:33.763
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:51:33.768
Sep  3 20:51:33.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename var-expansion 09/03/22 20:51:33.769
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:33.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:33.781
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 09/03/22 20:51:33.784
Sep  3 20:51:33.790: INFO: Waiting up to 5m0s for pod "var-expansion-9ad8d2d3-7796-4c27-a414-962094827535" in namespace "var-expansion-584" to be "Succeeded or Failed"
Sep  3 20:51:33.795: INFO: Pod "var-expansion-9ad8d2d3-7796-4c27-a414-962094827535": Phase="Pending", Reason="", readiness=false. Elapsed: 5.199017ms
Sep  3 20:51:35.801: INFO: Pod "var-expansion-9ad8d2d3-7796-4c27-a414-962094827535": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0111456s
Sep  3 20:51:37.798: INFO: Pod "var-expansion-9ad8d2d3-7796-4c27-a414-962094827535": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008215653s
STEP: Saw pod success 09/03/22 20:51:37.798
Sep  3 20:51:37.798: INFO: Pod "var-expansion-9ad8d2d3-7796-4c27-a414-962094827535" satisfied condition "Succeeded or Failed"
Sep  3 20:51:37.800: INFO: Trying to get logs from node kind-worker2 pod var-expansion-9ad8d2d3-7796-4c27-a414-962094827535 container dapi-container: <nil>
STEP: delete the pod 09/03/22 20:51:37.809
Sep  3 20:51:37.814: INFO: Waiting for pod var-expansion-9ad8d2d3-7796-4c27-a414-962094827535 to disappear
Sep  3 20:51:37.816: INFO: Pod var-expansion-9ad8d2d3-7796-4c27-a414-962094827535 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Sep  3 20:51:37.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-584" for this suite. 09/03/22 20:51:37.819
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":48,"skipped":756,"failed":0}
------------------------------
• [4.054 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:51:33.768
    Sep  3 20:51:33.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename var-expansion 09/03/22 20:51:33.769
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:33.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:33.781
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 09/03/22 20:51:33.784
    Sep  3 20:51:33.790: INFO: Waiting up to 5m0s for pod "var-expansion-9ad8d2d3-7796-4c27-a414-962094827535" in namespace "var-expansion-584" to be "Succeeded or Failed"
    Sep  3 20:51:33.795: INFO: Pod "var-expansion-9ad8d2d3-7796-4c27-a414-962094827535": Phase="Pending", Reason="", readiness=false. Elapsed: 5.199017ms
    Sep  3 20:51:35.801: INFO: Pod "var-expansion-9ad8d2d3-7796-4c27-a414-962094827535": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0111456s
    Sep  3 20:51:37.798: INFO: Pod "var-expansion-9ad8d2d3-7796-4c27-a414-962094827535": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008215653s
    STEP: Saw pod success 09/03/22 20:51:37.798
    Sep  3 20:51:37.798: INFO: Pod "var-expansion-9ad8d2d3-7796-4c27-a414-962094827535" satisfied condition "Succeeded or Failed"
    Sep  3 20:51:37.800: INFO: Trying to get logs from node kind-worker2 pod var-expansion-9ad8d2d3-7796-4c27-a414-962094827535 container dapi-container: <nil>
    STEP: delete the pod 09/03/22 20:51:37.809
    Sep  3 20:51:37.814: INFO: Waiting for pod var-expansion-9ad8d2d3-7796-4c27-a414-962094827535 to disappear
    Sep  3 20:51:37.816: INFO: Pod var-expansion-9ad8d2d3-7796-4c27-a414-962094827535 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Sep  3 20:51:37.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-584" for this suite. 09/03/22 20:51:37.819
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:51:37.823
Sep  3 20:51:37.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename sched-preemption 09/03/22 20:51:37.824
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:37.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:37.841
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Sep  3 20:51:37.850: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  3 20:52:37.866: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:52:37.869
Sep  3 20:52:37.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename sched-preemption-path 09/03/22 20:52:37.87
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:52:37.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:52:37.882
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 09/03/22 20:52:37.885
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/03/22 20:52:37.886
Sep  3 20:52:37.890: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7930" to be "running"
Sep  3 20:52:37.893: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.603809ms
Sep  3 20:52:39.900: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009687712s
Sep  3 20:52:41.896: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.005550014s
Sep  3 20:52:41.896: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/03/22 20:52:41.898
Sep  3 20:52:41.904: INFO: found a healthy node: kind-worker2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Sep  3 20:52:51.962: INFO: pods created so far: [1 1 1]
Sep  3 20:52:51.962: INFO: length of pods created so far: 3
Sep  3 20:52:53.970: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Sep  3 20:53:00.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7930" for this suite. 09/03/22 20:53:00.973
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Sep  3 20:53:00.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6291" for this suite. 09/03/22 20:53:00.997
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":49,"skipped":776,"failed":0}
------------------------------
• [SLOW TEST] [83.195 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:51:37.823
    Sep  3 20:51:37.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename sched-preemption 09/03/22 20:51:37.824
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:51:37.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:51:37.841
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Sep  3 20:51:37.850: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  3 20:52:37.866: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:52:37.869
    Sep  3 20:52:37.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename sched-preemption-path 09/03/22 20:52:37.87
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:52:37.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:52:37.882
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 09/03/22 20:52:37.885
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/03/22 20:52:37.886
    Sep  3 20:52:37.890: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7930" to be "running"
    Sep  3 20:52:37.893: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.603809ms
    Sep  3 20:52:39.900: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009687712s
    Sep  3 20:52:41.896: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.005550014s
    Sep  3 20:52:41.896: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/03/22 20:52:41.898
    Sep  3 20:52:41.904: INFO: found a healthy node: kind-worker2
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Sep  3 20:52:51.962: INFO: pods created so far: [1 1 1]
    Sep  3 20:52:51.962: INFO: length of pods created so far: 3
    Sep  3 20:52:53.970: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Sep  3 20:53:00.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-7930" for this suite. 09/03/22 20:53:00.973
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 20:53:00.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6291" for this suite. 09/03/22 20:53:00.997
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:53:01.018
Sep  3 20:53:01.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename security-context-test 09/03/22 20:53:01.019
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:01.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:01.03
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Sep  3 20:53:01.036: INFO: Waiting up to 5m0s for pod "busybox-user-65534-2ec662df-8a8d-4e28-a420-6e34b3c3aac4" in namespace "security-context-test-3106" to be "Succeeded or Failed"
Sep  3 20:53:01.039: INFO: Pod "busybox-user-65534-2ec662df-8a8d-4e28-a420-6e34b3c3aac4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459312ms
Sep  3 20:53:03.042: INFO: Pod "busybox-user-65534-2ec662df-8a8d-4e28-a420-6e34b3c3aac4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0055948s
Sep  3 20:53:05.042: INFO: Pod "busybox-user-65534-2ec662df-8a8d-4e28-a420-6e34b3c3aac4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005864768s
Sep  3 20:53:07.044: INFO: Pod "busybox-user-65534-2ec662df-8a8d-4e28-a420-6e34b3c3aac4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00754719s
Sep  3 20:53:07.044: INFO: Pod "busybox-user-65534-2ec662df-8a8d-4e28-a420-6e34b3c3aac4" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Sep  3 20:53:07.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3106" for this suite. 09/03/22 20:53:07.047
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":50,"skipped":776,"failed":0}
------------------------------
• [SLOW TEST] [6.034 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:53:01.018
    Sep  3 20:53:01.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename security-context-test 09/03/22 20:53:01.019
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:01.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:01.03
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Sep  3 20:53:01.036: INFO: Waiting up to 5m0s for pod "busybox-user-65534-2ec662df-8a8d-4e28-a420-6e34b3c3aac4" in namespace "security-context-test-3106" to be "Succeeded or Failed"
    Sep  3 20:53:01.039: INFO: Pod "busybox-user-65534-2ec662df-8a8d-4e28-a420-6e34b3c3aac4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459312ms
    Sep  3 20:53:03.042: INFO: Pod "busybox-user-65534-2ec662df-8a8d-4e28-a420-6e34b3c3aac4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0055948s
    Sep  3 20:53:05.042: INFO: Pod "busybox-user-65534-2ec662df-8a8d-4e28-a420-6e34b3c3aac4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005864768s
    Sep  3 20:53:07.044: INFO: Pod "busybox-user-65534-2ec662df-8a8d-4e28-a420-6e34b3c3aac4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00754719s
    Sep  3 20:53:07.044: INFO: Pod "busybox-user-65534-2ec662df-8a8d-4e28-a420-6e34b3c3aac4" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Sep  3 20:53:07.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3106" for this suite. 09/03/22 20:53:07.047
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:53:07.058
Sep  3 20:53:07.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pods 09/03/22 20:53:07.06
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:07.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:07.099
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Sep  3 20:53:07.112: INFO: Waiting up to 5m0s for pod "server-envvars-67c26651-4e25-4bce-83e2-85e5d7a1e8d1" in namespace "pods-656" to be "running and ready"
Sep  3 20:53:07.126: INFO: Pod "server-envvars-67c26651-4e25-4bce-83e2-85e5d7a1e8d1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.840056ms
Sep  3 20:53:07.126: INFO: The phase of Pod server-envvars-67c26651-4e25-4bce-83e2-85e5d7a1e8d1 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:53:09.129: INFO: Pod "server-envvars-67c26651-4e25-4bce-83e2-85e5d7a1e8d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017255085s
Sep  3 20:53:09.129: INFO: The phase of Pod server-envvars-67c26651-4e25-4bce-83e2-85e5d7a1e8d1 is Running (Ready = true)
Sep  3 20:53:09.129: INFO: Pod "server-envvars-67c26651-4e25-4bce-83e2-85e5d7a1e8d1" satisfied condition "running and ready"
Sep  3 20:53:09.159: INFO: Waiting up to 5m0s for pod "client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47" in namespace "pods-656" to be "Succeeded or Failed"
Sep  3 20:53:09.164: INFO: Pod "client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.251318ms
Sep  3 20:53:11.166: INFO: Pod "client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006911544s
Sep  3 20:53:13.167: INFO: Pod "client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007277532s
STEP: Saw pod success 09/03/22 20:53:13.167
Sep  3 20:53:13.167: INFO: Pod "client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47" satisfied condition "Succeeded or Failed"
Sep  3 20:53:13.169: INFO: Trying to get logs from node kind-worker2 pod client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47 container env3cont: <nil>
STEP: delete the pod 09/03/22 20:53:13.198
Sep  3 20:53:13.205: INFO: Waiting for pod client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47 to disappear
Sep  3 20:53:13.207: INFO: Pod client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Sep  3 20:53:13.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-656" for this suite. 09/03/22 20:53:13.21
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":51,"skipped":787,"failed":0}
------------------------------
• [SLOW TEST] [6.156 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:53:07.058
    Sep  3 20:53:07.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pods 09/03/22 20:53:07.06
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:07.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:07.099
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Sep  3 20:53:07.112: INFO: Waiting up to 5m0s for pod "server-envvars-67c26651-4e25-4bce-83e2-85e5d7a1e8d1" in namespace "pods-656" to be "running and ready"
    Sep  3 20:53:07.126: INFO: Pod "server-envvars-67c26651-4e25-4bce-83e2-85e5d7a1e8d1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.840056ms
    Sep  3 20:53:07.126: INFO: The phase of Pod server-envvars-67c26651-4e25-4bce-83e2-85e5d7a1e8d1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:53:09.129: INFO: Pod "server-envvars-67c26651-4e25-4bce-83e2-85e5d7a1e8d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017255085s
    Sep  3 20:53:09.129: INFO: The phase of Pod server-envvars-67c26651-4e25-4bce-83e2-85e5d7a1e8d1 is Running (Ready = true)
    Sep  3 20:53:09.129: INFO: Pod "server-envvars-67c26651-4e25-4bce-83e2-85e5d7a1e8d1" satisfied condition "running and ready"
    Sep  3 20:53:09.159: INFO: Waiting up to 5m0s for pod "client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47" in namespace "pods-656" to be "Succeeded or Failed"
    Sep  3 20:53:09.164: INFO: Pod "client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.251318ms
    Sep  3 20:53:11.166: INFO: Pod "client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006911544s
    Sep  3 20:53:13.167: INFO: Pod "client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007277532s
    STEP: Saw pod success 09/03/22 20:53:13.167
    Sep  3 20:53:13.167: INFO: Pod "client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47" satisfied condition "Succeeded or Failed"
    Sep  3 20:53:13.169: INFO: Trying to get logs from node kind-worker2 pod client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47 container env3cont: <nil>
    STEP: delete the pod 09/03/22 20:53:13.198
    Sep  3 20:53:13.205: INFO: Waiting for pod client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47 to disappear
    Sep  3 20:53:13.207: INFO: Pod client-envvars-13590434-ef7a-4f93-a972-8393d7f12d47 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Sep  3 20:53:13.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-656" for this suite. 09/03/22 20:53:13.21
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:53:13.216
Sep  3 20:53:13.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename svcaccounts 09/03/22 20:53:13.217
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:13.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:13.233
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Sep  3 20:53:13.244: INFO: Waiting up to 5m0s for pod "pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6" in namespace "svcaccounts-5311" to be "running"
Sep  3 20:53:13.247: INFO: Pod "pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.649409ms
Sep  3 20:53:15.254: INFO: Pod "pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009654578s
Sep  3 20:53:17.250: INFO: Pod "pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6": Phase="Running", Reason="", readiness=true. Elapsed: 4.006251812s
Sep  3 20:53:17.251: INFO: Pod "pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6" satisfied condition "running"
STEP: reading a file in the container 09/03/22 20:53:17.251
Sep  3 20:53:17.251: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5311 pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 09/03/22 20:53:17.391
Sep  3 20:53:17.391: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5311 pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 09/03/22 20:53:17.553
Sep  3 20:53:17.553: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5311 pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Sep  3 20:53:17.684: INFO: Got root ca configmap in namespace "svcaccounts-5311"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Sep  3 20:53:17.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5311" for this suite. 09/03/22 20:53:17.688
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":52,"skipped":807,"failed":0}
------------------------------
• [4.481 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:53:13.216
    Sep  3 20:53:13.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename svcaccounts 09/03/22 20:53:13.217
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:13.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:13.233
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Sep  3 20:53:13.244: INFO: Waiting up to 5m0s for pod "pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6" in namespace "svcaccounts-5311" to be "running"
    Sep  3 20:53:13.247: INFO: Pod "pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.649409ms
    Sep  3 20:53:15.254: INFO: Pod "pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009654578s
    Sep  3 20:53:17.250: INFO: Pod "pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6": Phase="Running", Reason="", readiness=true. Elapsed: 4.006251812s
    Sep  3 20:53:17.251: INFO: Pod "pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6" satisfied condition "running"
    STEP: reading a file in the container 09/03/22 20:53:17.251
    Sep  3 20:53:17.251: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5311 pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 09/03/22 20:53:17.391
    Sep  3 20:53:17.391: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5311 pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 09/03/22 20:53:17.553
    Sep  3 20:53:17.553: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5311 pod-service-account-d7c7dc70-9ae9-48d3-bdd8-66b8040486f6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Sep  3 20:53:17.684: INFO: Got root ca configmap in namespace "svcaccounts-5311"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Sep  3 20:53:17.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5311" for this suite. 09/03/22 20:53:17.688
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:53:17.71
Sep  3 20:53:17.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 20:53:17.711
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:17.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:17.731
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-9388 09/03/22 20:53:17.733
STEP: creating replication controller nodeport-test in namespace services-9388 09/03/22 20:53:17.749
I0903 20:53:17.761360      24 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9388, replica count: 2
I0903 20:53:20.813069      24 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 20:53:20.813: INFO: Creating new exec pod
Sep  3 20:53:20.817: INFO: Waiting up to 5m0s for pod "execpodtxf2k" in namespace "services-9388" to be "running"
Sep  3 20:53:20.820: INFO: Pod "execpodtxf2k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.85031ms
Sep  3 20:53:22.822: INFO: Pod "execpodtxf2k": Phase="Running", Reason="", readiness=true. Elapsed: 2.005515079s
Sep  3 20:53:22.822: INFO: Pod "execpodtxf2k" satisfied condition "running"
Sep  3 20:53:23.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Sep  3 20:53:24.036: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep  3 20:53:24.036: INFO: stdout: ""
Sep  3 20:53:25.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Sep  3 20:53:25.200: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep  3 20:53:25.200: INFO: stdout: ""
Sep  3 20:53:26.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Sep  3 20:53:26.181: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep  3 20:53:26.181: INFO: stdout: ""
Sep  3 20:53:27.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Sep  3 20:53:27.198: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep  3 20:53:27.198: INFO: stdout: ""
Sep  3 20:53:28.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Sep  3 20:53:28.182: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Sep  3 20:53:28.182: INFO: stdout: "nodeport-test-wpv8h"
Sep  3 20:53:28.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.145.62 80'
Sep  3 20:53:28.311: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.145.62 80\nConnection to 10.96.145.62 80 port [tcp/http] succeeded!\n"
Sep  3 20:53:28.311: INFO: stdout: "nodeport-test-wpv8h"
Sep  3 20:53:28.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.2 30269'
Sep  3 20:53:28.440: INFO: stderr: "+ nc -v -t -w 2 172.18.0.2 30269\n+ echo hostName\nConnection to 172.18.0.2 30269 port [tcp/*] succeeded!\n"
Sep  3 20:53:28.440: INFO: stdout: "nodeport-test-wpv8h"
Sep  3 20:53:28.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.3 30269'
Sep  3 20:53:28.579: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.3 30269\nConnection to 172.18.0.3 30269 port [tcp/*] succeeded!\n"
Sep  3 20:53:28.579: INFO: stdout: ""
Sep  3 20:53:29.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.3 30269'
Sep  3 20:53:29.709: INFO: stderr: "+ nc -v -t -w 2 172.18.0.3 30269\nConnection to 172.18.0.3 30269 port [tcp/*] succeeded!\n+ echo hostName\n"
Sep  3 20:53:29.710: INFO: stdout: "nodeport-test-26mdv"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 20:53:29.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9388" for this suite. 09/03/22 20:53:29.713
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":53,"skipped":840,"failed":0}
------------------------------
• [SLOW TEST] [12.007 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:53:17.71
    Sep  3 20:53:17.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 20:53:17.711
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:17.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:17.731
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-9388 09/03/22 20:53:17.733
    STEP: creating replication controller nodeport-test in namespace services-9388 09/03/22 20:53:17.749
    I0903 20:53:17.761360      24 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9388, replica count: 2
    I0903 20:53:20.813069      24 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  3 20:53:20.813: INFO: Creating new exec pod
    Sep  3 20:53:20.817: INFO: Waiting up to 5m0s for pod "execpodtxf2k" in namespace "services-9388" to be "running"
    Sep  3 20:53:20.820: INFO: Pod "execpodtxf2k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.85031ms
    Sep  3 20:53:22.822: INFO: Pod "execpodtxf2k": Phase="Running", Reason="", readiness=true. Elapsed: 2.005515079s
    Sep  3 20:53:22.822: INFO: Pod "execpodtxf2k" satisfied condition "running"
    Sep  3 20:53:23.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Sep  3 20:53:24.036: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Sep  3 20:53:24.036: INFO: stdout: ""
    Sep  3 20:53:25.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Sep  3 20:53:25.200: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Sep  3 20:53:25.200: INFO: stdout: ""
    Sep  3 20:53:26.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Sep  3 20:53:26.181: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Sep  3 20:53:26.181: INFO: stdout: ""
    Sep  3 20:53:27.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Sep  3 20:53:27.198: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Sep  3 20:53:27.198: INFO: stdout: ""
    Sep  3 20:53:28.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Sep  3 20:53:28.182: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Sep  3 20:53:28.182: INFO: stdout: "nodeport-test-wpv8h"
    Sep  3 20:53:28.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.145.62 80'
    Sep  3 20:53:28.311: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.145.62 80\nConnection to 10.96.145.62 80 port [tcp/http] succeeded!\n"
    Sep  3 20:53:28.311: INFO: stdout: "nodeport-test-wpv8h"
    Sep  3 20:53:28.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.2 30269'
    Sep  3 20:53:28.440: INFO: stderr: "+ nc -v -t -w 2 172.18.0.2 30269\n+ echo hostName\nConnection to 172.18.0.2 30269 port [tcp/*] succeeded!\n"
    Sep  3 20:53:28.440: INFO: stdout: "nodeport-test-wpv8h"
    Sep  3 20:53:28.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.3 30269'
    Sep  3 20:53:28.579: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.3 30269\nConnection to 172.18.0.3 30269 port [tcp/*] succeeded!\n"
    Sep  3 20:53:28.579: INFO: stdout: ""
    Sep  3 20:53:29.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-9388 exec execpodtxf2k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.3 30269'
    Sep  3 20:53:29.709: INFO: stderr: "+ nc -v -t -w 2 172.18.0.3 30269\nConnection to 172.18.0.3 30269 port [tcp/*] succeeded!\n+ echo hostName\n"
    Sep  3 20:53:29.710: INFO: stdout: "nodeport-test-26mdv"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 20:53:29.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9388" for this suite. 09/03/22 20:53:29.713
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:53:29.717
Sep  3 20:53:29.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 20:53:29.718
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:29.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:29.73
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 20:53:29.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3570" for this suite. 09/03/22 20:53:29.757
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":54,"skipped":848,"failed":0}
------------------------------
• [0.043 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:53:29.717
    Sep  3 20:53:29.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 20:53:29.718
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:29.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:29.73
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 20:53:29.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3570" for this suite. 09/03/22 20:53:29.757
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:53:29.762
Sep  3 20:53:29.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 20:53:29.762
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:29.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:29.776
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Sep  3 20:53:29.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/03/22 20:53:31.93
Sep  3 20:53:31.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-414 --namespace=crd-publish-openapi-414 create -f -'
Sep  3 20:53:32.633: INFO: stderr: ""
Sep  3 20:53:32.633: INFO: stdout: "e2e-test-crd-publish-openapi-9065-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  3 20:53:32.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-414 --namespace=crd-publish-openapi-414 delete e2e-test-crd-publish-openapi-9065-crds test-cr'
Sep  3 20:53:32.708: INFO: stderr: ""
Sep  3 20:53:32.708: INFO: stdout: "e2e-test-crd-publish-openapi-9065-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep  3 20:53:32.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-414 --namespace=crd-publish-openapi-414 apply -f -'
Sep  3 20:53:32.923: INFO: stderr: ""
Sep  3 20:53:32.923: INFO: stdout: "e2e-test-crd-publish-openapi-9065-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  3 20:53:32.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-414 --namespace=crd-publish-openapi-414 delete e2e-test-crd-publish-openapi-9065-crds test-cr'
Sep  3 20:53:32.995: INFO: stderr: ""
Sep  3 20:53:32.995: INFO: stdout: "e2e-test-crd-publish-openapi-9065-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 09/03/22 20:53:32.995
Sep  3 20:53:32.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-414 explain e2e-test-crd-publish-openapi-9065-crds'
Sep  3 20:53:33.249: INFO: stderr: ""
Sep  3 20:53:33.249: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9065-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 20:53:36.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-414" for this suite. 09/03/22 20:53:36.044
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":55,"skipped":856,"failed":0}
------------------------------
• [SLOW TEST] [6.289 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:53:29.762
    Sep  3 20:53:29.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 20:53:29.762
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:29.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:29.776
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Sep  3 20:53:29.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/03/22 20:53:31.93
    Sep  3 20:53:31.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-414 --namespace=crd-publish-openapi-414 create -f -'
    Sep  3 20:53:32.633: INFO: stderr: ""
    Sep  3 20:53:32.633: INFO: stdout: "e2e-test-crd-publish-openapi-9065-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  3 20:53:32.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-414 --namespace=crd-publish-openapi-414 delete e2e-test-crd-publish-openapi-9065-crds test-cr'
    Sep  3 20:53:32.708: INFO: stderr: ""
    Sep  3 20:53:32.708: INFO: stdout: "e2e-test-crd-publish-openapi-9065-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Sep  3 20:53:32.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-414 --namespace=crd-publish-openapi-414 apply -f -'
    Sep  3 20:53:32.923: INFO: stderr: ""
    Sep  3 20:53:32.923: INFO: stdout: "e2e-test-crd-publish-openapi-9065-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  3 20:53:32.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-414 --namespace=crd-publish-openapi-414 delete e2e-test-crd-publish-openapi-9065-crds test-cr'
    Sep  3 20:53:32.995: INFO: stderr: ""
    Sep  3 20:53:32.995: INFO: stdout: "e2e-test-crd-publish-openapi-9065-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 09/03/22 20:53:32.995
    Sep  3 20:53:32.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-414 explain e2e-test-crd-publish-openapi-9065-crds'
    Sep  3 20:53:33.249: INFO: stderr: ""
    Sep  3 20:53:33.249: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9065-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 20:53:36.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-414" for this suite. 09/03/22 20:53:36.044
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:53:36.053
Sep  3 20:53:36.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 20:53:36.054
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:36.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:36.088
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-f09978a9-94aa-4403-8415-c8e42e7e9f88 09/03/22 20:53:36.093
STEP: Creating a pod to test consume secrets 09/03/22 20:53:36.1
Sep  3 20:53:36.141: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba" in namespace "projected-2824" to be "Succeeded or Failed"
Sep  3 20:53:36.174: INFO: Pod "pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba": Phase="Pending", Reason="", readiness=false. Elapsed: 33.294293ms
Sep  3 20:53:38.177: INFO: Pod "pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035837049s
Sep  3 20:53:40.178: INFO: Pod "pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037263003s
STEP: Saw pod success 09/03/22 20:53:40.178
Sep  3 20:53:40.178: INFO: Pod "pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba" satisfied condition "Succeeded or Failed"
Sep  3 20:53:40.181: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba container projected-secret-volume-test: <nil>
STEP: delete the pod 09/03/22 20:53:40.189
Sep  3 20:53:40.199: INFO: Waiting for pod pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba to disappear
Sep  3 20:53:40.201: INFO: Pod pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Sep  3 20:53:40.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2824" for this suite. 09/03/22 20:53:40.215
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":56,"skipped":880,"failed":0}
------------------------------
• [4.169 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:53:36.053
    Sep  3 20:53:36.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 20:53:36.054
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:36.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:36.088
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-f09978a9-94aa-4403-8415-c8e42e7e9f88 09/03/22 20:53:36.093
    STEP: Creating a pod to test consume secrets 09/03/22 20:53:36.1
    Sep  3 20:53:36.141: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba" in namespace "projected-2824" to be "Succeeded or Failed"
    Sep  3 20:53:36.174: INFO: Pod "pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba": Phase="Pending", Reason="", readiness=false. Elapsed: 33.294293ms
    Sep  3 20:53:38.177: INFO: Pod "pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035837049s
    Sep  3 20:53:40.178: INFO: Pod "pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037263003s
    STEP: Saw pod success 09/03/22 20:53:40.178
    Sep  3 20:53:40.178: INFO: Pod "pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba" satisfied condition "Succeeded or Failed"
    Sep  3 20:53:40.181: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 20:53:40.189
    Sep  3 20:53:40.199: INFO: Waiting for pod pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba to disappear
    Sep  3 20:53:40.201: INFO: Pod pod-projected-secrets-d54912a6-7fed-46c6-adc6-f361ae92d4ba no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Sep  3 20:53:40.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2824" for this suite. 09/03/22 20:53:40.215
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:53:40.223
Sep  3 20:53:40.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename statefulset 09/03/22 20:53:40.224
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:40.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:40.24
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2054 09/03/22 20:53:40.244
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 09/03/22 20:53:40.247
STEP: Creating stateful set ss in namespace statefulset-2054 09/03/22 20:53:40.25
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2054 09/03/22 20:53:40.258
Sep  3 20:53:40.265: INFO: Found 0 stateful pods, waiting for 1
Sep  3 20:53:50.268: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/03/22 20:53:50.268
Sep  3 20:53:50.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 20:53:50.435: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 20:53:50.435: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 20:53:50.435: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 20:53:50.438: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  3 20:54:00.440: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 20:54:00.440: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 20:54:00.452: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999998s
Sep  3 20:54:01.457: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994398706s
Sep  3 20:54:02.459: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989755913s
Sep  3 20:54:03.463: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986201521s
Sep  3 20:54:04.466: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983296828s
Sep  3 20:54:05.470: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979651136s
Sep  3 20:54:06.473: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976521644s
Sep  3 20:54:07.476: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.973129412s
Sep  3 20:54:08.479: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.969902117s
Sep  3 20:54:09.483: INFO: Verifying statefulset ss doesn't scale past 1 for another 966.381827ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2054 09/03/22 20:54:10.484
Sep  3 20:54:10.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 20:54:10.624: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 20:54:10.624: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 20:54:10.624: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 20:54:10.626: INFO: Found 1 stateful pods, waiting for 3
Sep  3 20:54:20.630: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 20:54:20.630: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 20:54:20.630: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 09/03/22 20:54:20.63
STEP: Scale down will halt with unhealthy stateful pod 09/03/22 20:54:20.63
Sep  3 20:54:20.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 20:54:20.776: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 20:54:20.776: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 20:54:20.776: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 20:54:20.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 20:54:20.917: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 20:54:20.917: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 20:54:20.917: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 20:54:20.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 20:54:21.068: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 20:54:21.068: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 20:54:21.068: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 20:54:21.068: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 20:54:21.070: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep  3 20:54:31.077: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 20:54:31.077: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 20:54:31.077: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 20:54:31.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999997s
Sep  3 20:54:32.087: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997011707s
Sep  3 20:54:33.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993551856s
Sep  3 20:54:34.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990388607s
Sep  3 20:54:35.099: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986765256s
Sep  3 20:54:36.102: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981734202s
Sep  3 20:54:37.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978700353s
Sep  3 20:54:38.114: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970684992s
Sep  3 20:54:39.118: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96649914s
Sep  3 20:54:40.121: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.403864ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2054 09/03/22 20:54:41.122
Sep  3 20:54:41.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 20:54:41.312: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 20:54:41.312: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 20:54:41.312: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 20:54:41.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 20:54:41.502: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 20:54:41.502: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 20:54:41.502: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 20:54:41.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 20:54:41.666: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 20:54:41.666: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 20:54:41.666: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 20:54:41.666: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 09/03/22 20:54:51.679
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Sep  3 20:54:51.680: INFO: Deleting all statefulset in ns statefulset-2054
Sep  3 20:54:51.682: INFO: Scaling statefulset ss to 0
Sep  3 20:54:51.688: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 20:54:51.689: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Sep  3 20:54:51.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2054" for this suite. 09/03/22 20:54:51.704
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":57,"skipped":881,"failed":0}
------------------------------
• [SLOW TEST] [71.485 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:53:40.223
    Sep  3 20:53:40.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename statefulset 09/03/22 20:53:40.224
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:53:40.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:53:40.24
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2054 09/03/22 20:53:40.244
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 09/03/22 20:53:40.247
    STEP: Creating stateful set ss in namespace statefulset-2054 09/03/22 20:53:40.25
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2054 09/03/22 20:53:40.258
    Sep  3 20:53:40.265: INFO: Found 0 stateful pods, waiting for 1
    Sep  3 20:53:50.268: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/03/22 20:53:50.268
    Sep  3 20:53:50.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  3 20:53:50.435: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  3 20:53:50.435: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  3 20:53:50.435: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  3 20:53:50.438: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  3 20:54:00.440: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  3 20:54:00.440: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  3 20:54:00.452: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999998s
    Sep  3 20:54:01.457: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994398706s
    Sep  3 20:54:02.459: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989755913s
    Sep  3 20:54:03.463: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986201521s
    Sep  3 20:54:04.466: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983296828s
    Sep  3 20:54:05.470: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979651136s
    Sep  3 20:54:06.473: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976521644s
    Sep  3 20:54:07.476: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.973129412s
    Sep  3 20:54:08.479: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.969902117s
    Sep  3 20:54:09.483: INFO: Verifying statefulset ss doesn't scale past 1 for another 966.381827ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2054 09/03/22 20:54:10.484
    Sep  3 20:54:10.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  3 20:54:10.624: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  3 20:54:10.624: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  3 20:54:10.624: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  3 20:54:10.626: INFO: Found 1 stateful pods, waiting for 3
    Sep  3 20:54:20.630: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 20:54:20.630: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 20:54:20.630: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 09/03/22 20:54:20.63
    STEP: Scale down will halt with unhealthy stateful pod 09/03/22 20:54:20.63
    Sep  3 20:54:20.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  3 20:54:20.776: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  3 20:54:20.776: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  3 20:54:20.776: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  3 20:54:20.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  3 20:54:20.917: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  3 20:54:20.917: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  3 20:54:20.917: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  3 20:54:20.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  3 20:54:21.068: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  3 20:54:21.068: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  3 20:54:21.068: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  3 20:54:21.068: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  3 20:54:21.070: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Sep  3 20:54:31.077: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  3 20:54:31.077: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  3 20:54:31.077: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  3 20:54:31.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999997s
    Sep  3 20:54:32.087: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997011707s
    Sep  3 20:54:33.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993551856s
    Sep  3 20:54:34.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990388607s
    Sep  3 20:54:35.099: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986765256s
    Sep  3 20:54:36.102: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981734202s
    Sep  3 20:54:37.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978700353s
    Sep  3 20:54:38.114: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970684992s
    Sep  3 20:54:39.118: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96649914s
    Sep  3 20:54:40.121: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.403864ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2054 09/03/22 20:54:41.122
    Sep  3 20:54:41.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  3 20:54:41.312: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  3 20:54:41.312: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  3 20:54:41.312: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  3 20:54:41.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  3 20:54:41.502: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  3 20:54:41.502: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  3 20:54:41.502: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  3 20:54:41.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-2054 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  3 20:54:41.666: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  3 20:54:41.666: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  3 20:54:41.666: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  3 20:54:41.666: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 09/03/22 20:54:51.679
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Sep  3 20:54:51.680: INFO: Deleting all statefulset in ns statefulset-2054
    Sep  3 20:54:51.682: INFO: Scaling statefulset ss to 0
    Sep  3 20:54:51.688: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  3 20:54:51.689: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Sep  3 20:54:51.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2054" for this suite. 09/03/22 20:54:51.704
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:54:51.71
Sep  3 20:54:51.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename runtimeclass 09/03/22 20:54:51.711
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:54:51.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:54:51.723
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Sep  3 20:54:51.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3689" for this suite. 09/03/22 20:54:51.732
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":58,"skipped":907,"failed":0}
------------------------------
• [0.025 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:54:51.71
    Sep  3 20:54:51.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename runtimeclass 09/03/22 20:54:51.711
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:54:51.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:54:51.723
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Sep  3 20:54:51.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3689" for this suite. 09/03/22 20:54:51.732
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:54:51.739
Sep  3 20:54:51.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 20:54:51.74
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:54:51.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:54:51.75
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 09/03/22 20:54:51.752
Sep  3 20:54:51.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a" in namespace "downward-api-1728" to be "Succeeded or Failed"
Sep  3 20:54:51.759: INFO: Pod "downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.96911ms
Sep  3 20:54:53.762: INFO: Pod "downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a": Phase="Running", Reason="", readiness=false. Elapsed: 2.004429544s
Sep  3 20:54:55.762: INFO: Pod "downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004441252s
STEP: Saw pod success 09/03/22 20:54:55.762
Sep  3 20:54:55.762: INFO: Pod "downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a" satisfied condition "Succeeded or Failed"
Sep  3 20:54:55.763: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a container client-container: <nil>
STEP: delete the pod 09/03/22 20:54:55.768
Sep  3 20:54:55.776: INFO: Waiting for pod downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a to disappear
Sep  3 20:54:55.781: INFO: Pod downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Sep  3 20:54:55.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1728" for this suite. 09/03/22 20:54:55.784
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":59,"skipped":924,"failed":0}
------------------------------
• [4.049 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:54:51.739
    Sep  3 20:54:51.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 20:54:51.74
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:54:51.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:54:51.75
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 09/03/22 20:54:51.752
    Sep  3 20:54:51.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a" in namespace "downward-api-1728" to be "Succeeded or Failed"
    Sep  3 20:54:51.759: INFO: Pod "downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.96911ms
    Sep  3 20:54:53.762: INFO: Pod "downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a": Phase="Running", Reason="", readiness=false. Elapsed: 2.004429544s
    Sep  3 20:54:55.762: INFO: Pod "downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004441252s
    STEP: Saw pod success 09/03/22 20:54:55.762
    Sep  3 20:54:55.762: INFO: Pod "downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a" satisfied condition "Succeeded or Failed"
    Sep  3 20:54:55.763: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a container client-container: <nil>
    STEP: delete the pod 09/03/22 20:54:55.768
    Sep  3 20:54:55.776: INFO: Waiting for pod downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a to disappear
    Sep  3 20:54:55.781: INFO: Pod downwardapi-volume-75040567-0b82-4f2e-abfa-625035b5229a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Sep  3 20:54:55.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1728" for this suite. 09/03/22 20:54:55.784
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:54:55.791
Sep  3 20:54:55.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename init-container 09/03/22 20:54:55.792
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:54:55.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:54:55.802
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 09/03/22 20:54:55.805
Sep  3 20:54:55.805: INFO: PodSpec: initContainers in spec.initContainers
Sep  3 20:55:40.575: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-6fb7afc3-4afd-4dd5-bbde-c4b7f3853968", GenerateName:"", Namespace:"init-container-5857", SelfLink:"", UID:"c3ecad7b-b9e5-48eb-9afc-9be0fecfa300", ResourceVersion:"5178", Generation:0, CreationTimestamp:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"805839606"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000cf2fa8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.September, 3, 20, 55, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000cf2ff0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-djh54", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003700160), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-djh54", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-djh54", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-djh54", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0003127f0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kind-worker2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0005b96c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000312d20)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000312fe0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000312fe8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000312fec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00116de40), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.18.0.3", PodIP:"10.244.1.71", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.71"}}, StartTime:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005b97a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005b9810)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://643f8677d3d9e8ce87dd407c84f09492e094154678eca71458ff8d0c5acdde0e", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0037001e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0037001c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0003133ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Sep  3 20:55:40.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5857" for this suite. 09/03/22 20:55:40.587
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":60,"skipped":935,"failed":0}
------------------------------
• [SLOW TEST] [44.805 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:54:55.791
    Sep  3 20:54:55.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename init-container 09/03/22 20:54:55.792
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:54:55.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:54:55.802
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 09/03/22 20:54:55.805
    Sep  3 20:54:55.805: INFO: PodSpec: initContainers in spec.initContainers
    Sep  3 20:55:40.575: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-6fb7afc3-4afd-4dd5-bbde-c4b7f3853968", GenerateName:"", Namespace:"init-container-5857", SelfLink:"", UID:"c3ecad7b-b9e5-48eb-9afc-9be0fecfa300", ResourceVersion:"5178", Generation:0, CreationTimestamp:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"805839606"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000cf2fa8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.September, 3, 20, 55, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000cf2ff0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-djh54", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003700160), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-djh54", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-djh54", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-djh54", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0003127f0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kind-worker2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0005b96c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000312d20)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000312fe0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000312fe8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000312fec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00116de40), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.18.0.3", PodIP:"10.244.1.71", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.71"}}, StartTime:time.Date(2022, time.September, 3, 20, 54, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005b97a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005b9810)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://643f8677d3d9e8ce87dd407c84f09492e094154678eca71458ff8d0c5acdde0e", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0037001e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0037001c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0003133ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Sep  3 20:55:40.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5857" for this suite. 09/03/22 20:55:40.587
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:55:40.603
Sep  3 20:55:40.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 20:55:40.604
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:55:40.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:55:40.62
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 09/03/22 20:55:40.622
Sep  3 20:55:40.628: INFO: Waiting up to 5m0s for pod "pod-1abe1762-8c24-4506-b53d-1555c22b5820" in namespace "emptydir-4881" to be "Succeeded or Failed"
Sep  3 20:55:40.631: INFO: Pod "pod-1abe1762-8c24-4506-b53d-1555c22b5820": Phase="Pending", Reason="", readiness=false. Elapsed: 3.371608ms
Sep  3 20:55:42.635: INFO: Pod "pod-1abe1762-8c24-4506-b53d-1555c22b5820": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007106396s
Sep  3 20:55:44.634: INFO: Pod "pod-1abe1762-8c24-4506-b53d-1555c22b5820": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006128702s
Sep  3 20:55:46.635: INFO: Pod "pod-1abe1762-8c24-4506-b53d-1555c22b5820": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007282635s
STEP: Saw pod success 09/03/22 20:55:46.635
Sep  3 20:55:46.635: INFO: Pod "pod-1abe1762-8c24-4506-b53d-1555c22b5820" satisfied condition "Succeeded or Failed"
Sep  3 20:55:46.637: INFO: Trying to get logs from node kind-worker2 pod pod-1abe1762-8c24-4506-b53d-1555c22b5820 container test-container: <nil>
STEP: delete the pod 09/03/22 20:55:46.643
Sep  3 20:55:46.648: INFO: Waiting for pod pod-1abe1762-8c24-4506-b53d-1555c22b5820 to disappear
Sep  3 20:55:46.650: INFO: Pod pod-1abe1762-8c24-4506-b53d-1555c22b5820 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 20:55:46.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4881" for this suite. 09/03/22 20:55:46.652
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":61,"skipped":965,"failed":0}
------------------------------
• [SLOW TEST] [6.053 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:55:40.603
    Sep  3 20:55:40.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 20:55:40.604
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:55:40.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:55:40.62
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/03/22 20:55:40.622
    Sep  3 20:55:40.628: INFO: Waiting up to 5m0s for pod "pod-1abe1762-8c24-4506-b53d-1555c22b5820" in namespace "emptydir-4881" to be "Succeeded or Failed"
    Sep  3 20:55:40.631: INFO: Pod "pod-1abe1762-8c24-4506-b53d-1555c22b5820": Phase="Pending", Reason="", readiness=false. Elapsed: 3.371608ms
    Sep  3 20:55:42.635: INFO: Pod "pod-1abe1762-8c24-4506-b53d-1555c22b5820": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007106396s
    Sep  3 20:55:44.634: INFO: Pod "pod-1abe1762-8c24-4506-b53d-1555c22b5820": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006128702s
    Sep  3 20:55:46.635: INFO: Pod "pod-1abe1762-8c24-4506-b53d-1555c22b5820": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007282635s
    STEP: Saw pod success 09/03/22 20:55:46.635
    Sep  3 20:55:46.635: INFO: Pod "pod-1abe1762-8c24-4506-b53d-1555c22b5820" satisfied condition "Succeeded or Failed"
    Sep  3 20:55:46.637: INFO: Trying to get logs from node kind-worker2 pod pod-1abe1762-8c24-4506-b53d-1555c22b5820 container test-container: <nil>
    STEP: delete the pod 09/03/22 20:55:46.643
    Sep  3 20:55:46.648: INFO: Waiting for pod pod-1abe1762-8c24-4506-b53d-1555c22b5820 to disappear
    Sep  3 20:55:46.650: INFO: Pod pod-1abe1762-8c24-4506-b53d-1555c22b5820 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 20:55:46.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4881" for this suite. 09/03/22 20:55:46.652
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:55:46.657
Sep  3 20:55:46.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 20:55:46.658
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:55:46.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:55:46.669
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 09/03/22 20:55:46.672
Sep  3 20:55:46.678: INFO: Waiting up to 5m0s for pod "pod-2ea72716-5340-4531-990c-8d18c43b2c8f" in namespace "emptydir-3430" to be "Succeeded or Failed"
Sep  3 20:55:46.679: INFO: Pod "pod-2ea72716-5340-4531-990c-8d18c43b2c8f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.852004ms
Sep  3 20:55:48.694: INFO: Pod "pod-2ea72716-5340-4531-990c-8d18c43b2c8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016473565s
Sep  3 20:55:50.683: INFO: Pod "pod-2ea72716-5340-4531-990c-8d18c43b2c8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005455473s
STEP: Saw pod success 09/03/22 20:55:50.683
Sep  3 20:55:50.683: INFO: Pod "pod-2ea72716-5340-4531-990c-8d18c43b2c8f" satisfied condition "Succeeded or Failed"
Sep  3 20:55:50.685: INFO: Trying to get logs from node kind-worker2 pod pod-2ea72716-5340-4531-990c-8d18c43b2c8f container test-container: <nil>
STEP: delete the pod 09/03/22 20:55:50.689
Sep  3 20:55:50.695: INFO: Waiting for pod pod-2ea72716-5340-4531-990c-8d18c43b2c8f to disappear
Sep  3 20:55:50.696: INFO: Pod pod-2ea72716-5340-4531-990c-8d18c43b2c8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 20:55:50.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3430" for this suite. 09/03/22 20:55:50.699
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":62,"skipped":966,"failed":0}
------------------------------
• [4.045 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:55:46.657
    Sep  3 20:55:46.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 20:55:46.658
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:55:46.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:55:46.669
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/03/22 20:55:46.672
    Sep  3 20:55:46.678: INFO: Waiting up to 5m0s for pod "pod-2ea72716-5340-4531-990c-8d18c43b2c8f" in namespace "emptydir-3430" to be "Succeeded or Failed"
    Sep  3 20:55:46.679: INFO: Pod "pod-2ea72716-5340-4531-990c-8d18c43b2c8f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.852004ms
    Sep  3 20:55:48.694: INFO: Pod "pod-2ea72716-5340-4531-990c-8d18c43b2c8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016473565s
    Sep  3 20:55:50.683: INFO: Pod "pod-2ea72716-5340-4531-990c-8d18c43b2c8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005455473s
    STEP: Saw pod success 09/03/22 20:55:50.683
    Sep  3 20:55:50.683: INFO: Pod "pod-2ea72716-5340-4531-990c-8d18c43b2c8f" satisfied condition "Succeeded or Failed"
    Sep  3 20:55:50.685: INFO: Trying to get logs from node kind-worker2 pod pod-2ea72716-5340-4531-990c-8d18c43b2c8f container test-container: <nil>
    STEP: delete the pod 09/03/22 20:55:50.689
    Sep  3 20:55:50.695: INFO: Waiting for pod pod-2ea72716-5340-4531-990c-8d18c43b2c8f to disappear
    Sep  3 20:55:50.696: INFO: Pod pod-2ea72716-5340-4531-990c-8d18c43b2c8f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 20:55:50.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3430" for this suite. 09/03/22 20:55:50.699
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:55:50.703
Sep  3 20:55:50.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename endpointslicemirroring 09/03/22 20:55:50.704
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:55:50.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:55:50.716
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 09/03/22 20:55:50.733
Sep  3 20:55:50.753: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 09/03/22 20:55:52.756
Sep  3 20:55:52.760: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 09/03/22 20:55:54.764
Sep  3 20:55:54.769: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Sep  3 20:55:56.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-1988" for this suite. 09/03/22 20:55:56.776
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":63,"skipped":977,"failed":0}
------------------------------
• [SLOW TEST] [6.076 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:55:50.703
    Sep  3 20:55:50.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename endpointslicemirroring 09/03/22 20:55:50.704
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:55:50.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:55:50.716
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 09/03/22 20:55:50.733
    Sep  3 20:55:50.753: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 09/03/22 20:55:52.756
    Sep  3 20:55:52.760: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 09/03/22 20:55:54.764
    Sep  3 20:55:54.769: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Sep  3 20:55:56.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-1988" for this suite. 09/03/22 20:55:56.776
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:55:56.782
Sep  3 20:55:56.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 20:55:56.783
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:55:56.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:55:56.795
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 09/03/22 20:55:56.797
Sep  3 20:55:56.798: INFO: Creating e2e-svc-a-mn7rj
Sep  3 20:55:56.803: INFO: Creating e2e-svc-b-mcf2v
Sep  3 20:55:56.829: INFO: Creating e2e-svc-c-pcpp5
STEP: deleting service collection 09/03/22 20:55:56.845
Sep  3 20:55:56.884: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 20:55:56.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5227" for this suite. 09/03/22 20:55:56.887
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":64,"skipped":985,"failed":0}
------------------------------
• [0.110 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:55:56.782
    Sep  3 20:55:56.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 20:55:56.783
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:55:56.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:55:56.795
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 09/03/22 20:55:56.797
    Sep  3 20:55:56.798: INFO: Creating e2e-svc-a-mn7rj
    Sep  3 20:55:56.803: INFO: Creating e2e-svc-b-mcf2v
    Sep  3 20:55:56.829: INFO: Creating e2e-svc-c-pcpp5
    STEP: deleting service collection 09/03/22 20:55:56.845
    Sep  3 20:55:56.884: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 20:55:56.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5227" for this suite. 09/03/22 20:55:56.887
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:55:56.895
Sep  3 20:55:56.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 20:55:56.896
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:55:56.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:55:56.921
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 20:55:56.945
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 20:55:57.769
STEP: Deploying the webhook pod 09/03/22 20:55:57.774
STEP: Wait for the deployment to be ready 09/03/22 20:55:57.779
Sep  3 20:55:57.790: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 20:55:59.796
STEP: Verifying the service has paired with the endpoint 09/03/22 20:55:59.819
Sep  3 20:56:00.819: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Sep  3 20:56:00.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/03/22 20:56:01.329
STEP: Creating a custom resource that should be denied by the webhook 09/03/22 20:56:01.342
STEP: Creating a custom resource whose deletion would be denied by the webhook 09/03/22 20:56:03.377
STEP: Updating the custom resource with disallowed data should be denied 09/03/22 20:56:03.385
STEP: Deleting the custom resource should be denied 09/03/22 20:56:03.391
STEP: Remove the offending key and value from the custom resource data 09/03/22 20:56:03.396
STEP: Deleting the updated custom resource should be successful 09/03/22 20:56:03.403
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 20:56:03.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6564" for this suite. 09/03/22 20:56:03.92
STEP: Destroying namespace "webhook-6564-markers" for this suite. 09/03/22 20:56:03.924
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":65,"skipped":986,"failed":0}
------------------------------
• [SLOW TEST] [7.109 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:55:56.895
    Sep  3 20:55:56.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 20:55:56.896
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:55:56.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:55:56.921
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 20:55:56.945
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 20:55:57.769
    STEP: Deploying the webhook pod 09/03/22 20:55:57.774
    STEP: Wait for the deployment to be ready 09/03/22 20:55:57.779
    Sep  3 20:55:57.790: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 20:55:59.796
    STEP: Verifying the service has paired with the endpoint 09/03/22 20:55:59.819
    Sep  3 20:56:00.819: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Sep  3 20:56:00.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/03/22 20:56:01.329
    STEP: Creating a custom resource that should be denied by the webhook 09/03/22 20:56:01.342
    STEP: Creating a custom resource whose deletion would be denied by the webhook 09/03/22 20:56:03.377
    STEP: Updating the custom resource with disallowed data should be denied 09/03/22 20:56:03.385
    STEP: Deleting the custom resource should be denied 09/03/22 20:56:03.391
    STEP: Remove the offending key and value from the custom resource data 09/03/22 20:56:03.396
    STEP: Deleting the updated custom resource should be successful 09/03/22 20:56:03.403
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 20:56:03.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6564" for this suite. 09/03/22 20:56:03.92
    STEP: Destroying namespace "webhook-6564-markers" for this suite. 09/03/22 20:56:03.924
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:56:04.007
Sep  3 20:56:04.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename resourcequota 09/03/22 20:56:04.01
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:56:04.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:56:04.078
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 09/03/22 20:56:21.083
STEP: Creating a ResourceQuota 09/03/22 20:56:26.086
STEP: Ensuring resource quota status is calculated 09/03/22 20:56:26.09
STEP: Creating a ConfigMap 09/03/22 20:56:28.093
STEP: Ensuring resource quota status captures configMap creation 09/03/22 20:56:28.1
STEP: Deleting a ConfigMap 09/03/22 20:56:30.103
STEP: Ensuring resource quota status released usage 09/03/22 20:56:30.107
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Sep  3 20:56:32.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6190" for this suite. 09/03/22 20:56:32.113
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":66,"skipped":990,"failed":0}
------------------------------
• [SLOW TEST] [28.109 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:56:04.007
    Sep  3 20:56:04.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename resourcequota 09/03/22 20:56:04.01
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:56:04.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:56:04.078
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 09/03/22 20:56:21.083
    STEP: Creating a ResourceQuota 09/03/22 20:56:26.086
    STEP: Ensuring resource quota status is calculated 09/03/22 20:56:26.09
    STEP: Creating a ConfigMap 09/03/22 20:56:28.093
    STEP: Ensuring resource quota status captures configMap creation 09/03/22 20:56:28.1
    STEP: Deleting a ConfigMap 09/03/22 20:56:30.103
    STEP: Ensuring resource quota status released usage 09/03/22 20:56:30.107
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Sep  3 20:56:32.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6190" for this suite. 09/03/22 20:56:32.113
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:56:32.116
Sep  3 20:56:32.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 20:56:32.117
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:56:32.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:56:32.132
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Sep  3 20:56:32.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/03/22 20:56:34.398
Sep  3 20:56:34.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 create -f -'
Sep  3 20:56:35.028: INFO: stderr: ""
Sep  3 20:56:35.028: INFO: stdout: "e2e-test-crd-publish-openapi-7332-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  3 20:56:35.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 delete e2e-test-crd-publish-openapi-7332-crds test-foo'
Sep  3 20:56:35.104: INFO: stderr: ""
Sep  3 20:56:35.104: INFO: stdout: "e2e-test-crd-publish-openapi-7332-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep  3 20:56:35.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 apply -f -'
Sep  3 20:56:35.315: INFO: stderr: ""
Sep  3 20:56:35.315: INFO: stdout: "e2e-test-crd-publish-openapi-7332-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  3 20:56:35.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 delete e2e-test-crd-publish-openapi-7332-crds test-foo'
Sep  3 20:56:35.395: INFO: stderr: ""
Sep  3 20:56:35.395: INFO: stdout: "e2e-test-crd-publish-openapi-7332-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/03/22 20:56:35.395
Sep  3 20:56:35.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 create -f -'
Sep  3 20:56:35.610: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/03/22 20:56:35.61
Sep  3 20:56:35.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 create -f -'
Sep  3 20:56:35.799: INFO: rc: 1
Sep  3 20:56:35.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 apply -f -'
Sep  3 20:56:36.104: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/03/22 20:56:36.104
Sep  3 20:56:36.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 create -f -'
Sep  3 20:56:36.313: INFO: rc: 1
Sep  3 20:56:36.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 apply -f -'
Sep  3 20:56:36.510: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 09/03/22 20:56:36.51
Sep  3 20:56:36.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 explain e2e-test-crd-publish-openapi-7332-crds'
Sep  3 20:56:36.702: INFO: stderr: ""
Sep  3 20:56:36.702: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7332-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 09/03/22 20:56:36.702
Sep  3 20:56:36.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 explain e2e-test-crd-publish-openapi-7332-crds.metadata'
Sep  3 20:56:36.890: INFO: stderr: ""
Sep  3 20:56:36.890: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7332-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep  3 20:56:36.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 explain e2e-test-crd-publish-openapi-7332-crds.spec'
Sep  3 20:56:37.079: INFO: stderr: ""
Sep  3 20:56:37.079: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7332-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep  3 20:56:37.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 explain e2e-test-crd-publish-openapi-7332-crds.spec.bars'
Sep  3 20:56:37.326: INFO: stderr: ""
Sep  3 20:56:37.327: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7332-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/03/22 20:56:37.327
Sep  3 20:56:37.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 explain e2e-test-crd-publish-openapi-7332-crds.spec.bars2'
Sep  3 20:56:37.570: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 20:56:39.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8717" for this suite. 09/03/22 20:56:39.768
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":67,"skipped":992,"failed":0}
------------------------------
• [SLOW TEST] [7.656 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:56:32.116
    Sep  3 20:56:32.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 20:56:32.117
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:56:32.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:56:32.132
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Sep  3 20:56:32.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/03/22 20:56:34.398
    Sep  3 20:56:34.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 create -f -'
    Sep  3 20:56:35.028: INFO: stderr: ""
    Sep  3 20:56:35.028: INFO: stdout: "e2e-test-crd-publish-openapi-7332-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  3 20:56:35.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 delete e2e-test-crd-publish-openapi-7332-crds test-foo'
    Sep  3 20:56:35.104: INFO: stderr: ""
    Sep  3 20:56:35.104: INFO: stdout: "e2e-test-crd-publish-openapi-7332-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Sep  3 20:56:35.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 apply -f -'
    Sep  3 20:56:35.315: INFO: stderr: ""
    Sep  3 20:56:35.315: INFO: stdout: "e2e-test-crd-publish-openapi-7332-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  3 20:56:35.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 delete e2e-test-crd-publish-openapi-7332-crds test-foo'
    Sep  3 20:56:35.395: INFO: stderr: ""
    Sep  3 20:56:35.395: INFO: stdout: "e2e-test-crd-publish-openapi-7332-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/03/22 20:56:35.395
    Sep  3 20:56:35.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 create -f -'
    Sep  3 20:56:35.610: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/03/22 20:56:35.61
    Sep  3 20:56:35.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 create -f -'
    Sep  3 20:56:35.799: INFO: rc: 1
    Sep  3 20:56:35.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 apply -f -'
    Sep  3 20:56:36.104: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/03/22 20:56:36.104
    Sep  3 20:56:36.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 create -f -'
    Sep  3 20:56:36.313: INFO: rc: 1
    Sep  3 20:56:36.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 --namespace=crd-publish-openapi-8717 apply -f -'
    Sep  3 20:56:36.510: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 09/03/22 20:56:36.51
    Sep  3 20:56:36.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 explain e2e-test-crd-publish-openapi-7332-crds'
    Sep  3 20:56:36.702: INFO: stderr: ""
    Sep  3 20:56:36.702: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7332-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 09/03/22 20:56:36.702
    Sep  3 20:56:36.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 explain e2e-test-crd-publish-openapi-7332-crds.metadata'
    Sep  3 20:56:36.890: INFO: stderr: ""
    Sep  3 20:56:36.890: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7332-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Sep  3 20:56:36.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 explain e2e-test-crd-publish-openapi-7332-crds.spec'
    Sep  3 20:56:37.079: INFO: stderr: ""
    Sep  3 20:56:37.079: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7332-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Sep  3 20:56:37.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 explain e2e-test-crd-publish-openapi-7332-crds.spec.bars'
    Sep  3 20:56:37.326: INFO: stderr: ""
    Sep  3 20:56:37.327: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7332-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/03/22 20:56:37.327
    Sep  3 20:56:37.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8717 explain e2e-test-crd-publish-openapi-7332-crds.spec.bars2'
    Sep  3 20:56:37.570: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 20:56:39.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8717" for this suite. 09/03/22 20:56:39.768
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:56:39.776
Sep  3 20:56:39.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pods 09/03/22 20:56:39.777
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:56:39.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:56:39.791
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Sep  3 20:56:39.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: creating the pod 09/03/22 20:56:39.794
STEP: submitting the pod to kubernetes 09/03/22 20:56:39.794
Sep  3 20:56:39.799: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6" in namespace "pods-8443" to be "running and ready"
Sep  3 20:56:39.811: INFO: Pod "pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.820029ms
Sep  3 20:56:39.812: INFO: The phase of Pod pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:56:41.816: INFO: Pod "pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017164074s
Sep  3 20:56:41.816: INFO: The phase of Pod pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:56:43.815: INFO: Pod "pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6": Phase="Running", Reason="", readiness=true. Elapsed: 4.016634092s
Sep  3 20:56:43.815: INFO: The phase of Pod pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6 is Running (Ready = true)
Sep  3 20:56:43.815: INFO: Pod "pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Sep  3 20:56:43.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8443" for this suite. 09/03/22 20:56:43.911
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":68,"skipped":1086,"failed":0}
------------------------------
• [4.140 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:56:39.776
    Sep  3 20:56:39.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pods 09/03/22 20:56:39.777
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:56:39.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:56:39.791
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Sep  3 20:56:39.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: creating the pod 09/03/22 20:56:39.794
    STEP: submitting the pod to kubernetes 09/03/22 20:56:39.794
    Sep  3 20:56:39.799: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6" in namespace "pods-8443" to be "running and ready"
    Sep  3 20:56:39.811: INFO: Pod "pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.820029ms
    Sep  3 20:56:39.812: INFO: The phase of Pod pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:56:41.816: INFO: Pod "pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017164074s
    Sep  3 20:56:41.816: INFO: The phase of Pod pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:56:43.815: INFO: Pod "pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6": Phase="Running", Reason="", readiness=true. Elapsed: 4.016634092s
    Sep  3 20:56:43.815: INFO: The phase of Pod pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6 is Running (Ready = true)
    Sep  3 20:56:43.815: INFO: Pod "pod-exec-websocket-e2de45ed-208a-4e73-9437-3caf41f3e0e6" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Sep  3 20:56:43.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8443" for this suite. 09/03/22 20:56:43.911
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:56:43.919
Sep  3 20:56:43.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 20:56:43.92
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:56:43.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:56:43.932
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 09/03/22 20:56:43.934
Sep  3 20:56:43.939: INFO: Waiting up to 5m0s for pod "annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc" in namespace "projected-2214" to be "running and ready"
Sep  3 20:56:43.959: INFO: Pod "annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.535951ms
Sep  3 20:56:43.959: INFO: The phase of Pod annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:56:45.963: INFO: Pod "annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.02451798s
Sep  3 20:56:45.963: INFO: The phase of Pod annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc is Running (Ready = true)
Sep  3 20:56:45.963: INFO: Pod "annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc" satisfied condition "running and ready"
Sep  3 20:56:46.478: INFO: Successfully updated pod "annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Sep  3 20:56:50.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2214" for this suite. 09/03/22 20:56:50.514
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":69,"skipped":1092,"failed":0}
------------------------------
• [SLOW TEST] [6.599 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:56:43.919
    Sep  3 20:56:43.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 20:56:43.92
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:56:43.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:56:43.932
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 09/03/22 20:56:43.934
    Sep  3 20:56:43.939: INFO: Waiting up to 5m0s for pod "annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc" in namespace "projected-2214" to be "running and ready"
    Sep  3 20:56:43.959: INFO: Pod "annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.535951ms
    Sep  3 20:56:43.959: INFO: The phase of Pod annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:56:45.963: INFO: Pod "annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.02451798s
    Sep  3 20:56:45.963: INFO: The phase of Pod annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc is Running (Ready = true)
    Sep  3 20:56:45.963: INFO: Pod "annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc" satisfied condition "running and ready"
    Sep  3 20:56:46.478: INFO: Successfully updated pod "annotationupdatee20ab71d-a301-48e7-abd1-daa6451d3fdc"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Sep  3 20:56:50.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2214" for this suite. 09/03/22 20:56:50.514
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:56:50.519
Sep  3 20:56:50.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename resourcequota 09/03/22 20:56:50.52
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:56:50.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:56:50.533
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 09/03/22 20:56:50.535
STEP: Creating a ResourceQuota 09/03/22 20:56:55.538
STEP: Ensuring resource quota status is calculated 09/03/22 20:56:55.541
STEP: Creating a Service 09/03/22 20:56:57.544
STEP: Creating a NodePort Service 09/03/22 20:56:57.562
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/03/22 20:56:57.598
STEP: Ensuring resource quota status captures service creation 09/03/22 20:56:57.617
STEP: Deleting Services 09/03/22 20:56:59.622
STEP: Ensuring resource quota status released usage 09/03/22 20:56:59.671
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Sep  3 20:57:01.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-684" for this suite. 09/03/22 20:57:01.676
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":70,"skipped":1092,"failed":0}
------------------------------
• [SLOW TEST] [11.160 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:56:50.519
    Sep  3 20:56:50.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename resourcequota 09/03/22 20:56:50.52
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:56:50.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:56:50.533
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 09/03/22 20:56:50.535
    STEP: Creating a ResourceQuota 09/03/22 20:56:55.538
    STEP: Ensuring resource quota status is calculated 09/03/22 20:56:55.541
    STEP: Creating a Service 09/03/22 20:56:57.544
    STEP: Creating a NodePort Service 09/03/22 20:56:57.562
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/03/22 20:56:57.598
    STEP: Ensuring resource quota status captures service creation 09/03/22 20:56:57.617
    STEP: Deleting Services 09/03/22 20:56:59.622
    STEP: Ensuring resource quota status released usage 09/03/22 20:56:59.671
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Sep  3 20:57:01.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-684" for this suite. 09/03/22 20:57:01.676
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:01.681
Sep  3 20:57:01.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 20:57:01.682
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:01.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:01.694
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2bddcdd9-e09c-4311-9e50-c4c2f77eeff2 09/03/22 20:57:01.699
STEP: Creating the pod 09/03/22 20:57:01.701
Sep  3 20:57:01.706: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b9a3d385-b442-4e47-8d96-8036e41095ec" in namespace "projected-894" to be "running and ready"
Sep  3 20:57:01.709: INFO: Pod "pod-projected-configmaps-b9a3d385-b442-4e47-8d96-8036e41095ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.522006ms
Sep  3 20:57:01.709: INFO: The phase of Pod pod-projected-configmaps-b9a3d385-b442-4e47-8d96-8036e41095ec is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:57:03.712: INFO: Pod "pod-projected-configmaps-b9a3d385-b442-4e47-8d96-8036e41095ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.005282417s
Sep  3 20:57:03.712: INFO: The phase of Pod pod-projected-configmaps-b9a3d385-b442-4e47-8d96-8036e41095ec is Running (Ready = true)
Sep  3 20:57:03.712: INFO: Pod "pod-projected-configmaps-b9a3d385-b442-4e47-8d96-8036e41095ec" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-2bddcdd9-e09c-4311-9e50-c4c2f77eeff2 09/03/22 20:57:03.717
STEP: waiting to observe update in volume 09/03/22 20:57:03.721
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Sep  3 20:57:05.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-894" for this suite. 09/03/22 20:57:05.732
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":71,"skipped":1132,"failed":0}
------------------------------
• [4.053 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:01.681
    Sep  3 20:57:01.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 20:57:01.682
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:01.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:01.694
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-2bddcdd9-e09c-4311-9e50-c4c2f77eeff2 09/03/22 20:57:01.699
    STEP: Creating the pod 09/03/22 20:57:01.701
    Sep  3 20:57:01.706: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b9a3d385-b442-4e47-8d96-8036e41095ec" in namespace "projected-894" to be "running and ready"
    Sep  3 20:57:01.709: INFO: Pod "pod-projected-configmaps-b9a3d385-b442-4e47-8d96-8036e41095ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.522006ms
    Sep  3 20:57:01.709: INFO: The phase of Pod pod-projected-configmaps-b9a3d385-b442-4e47-8d96-8036e41095ec is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:57:03.712: INFO: Pod "pod-projected-configmaps-b9a3d385-b442-4e47-8d96-8036e41095ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.005282417s
    Sep  3 20:57:03.712: INFO: The phase of Pod pod-projected-configmaps-b9a3d385-b442-4e47-8d96-8036e41095ec is Running (Ready = true)
    Sep  3 20:57:03.712: INFO: Pod "pod-projected-configmaps-b9a3d385-b442-4e47-8d96-8036e41095ec" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-2bddcdd9-e09c-4311-9e50-c4c2f77eeff2 09/03/22 20:57:03.717
    STEP: waiting to observe update in volume 09/03/22 20:57:03.721
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Sep  3 20:57:05.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-894" for this suite. 09/03/22 20:57:05.732
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:05.741
Sep  3 20:57:05.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 20:57:05.742
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:05.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:05.759
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 20:57:05.768
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 20:57:06.612
STEP: Deploying the webhook pod 09/03/22 20:57:06.617
STEP: Wait for the deployment to be ready 09/03/22 20:57:06.624
Sep  3 20:57:06.633: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 20:57:08.639
STEP: Verifying the service has paired with the endpoint 09/03/22 20:57:08.662
Sep  3 20:57:09.663: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Sep  3 20:57:09.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-348-crds.webhook.example.com via the AdmissionRegistration API 09/03/22 20:57:10.172
STEP: Creating a custom resource that should be mutated by the webhook 09/03/22 20:57:10.185
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 20:57:12.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-961" for this suite. 09/03/22 20:57:12.749
STEP: Destroying namespace "webhook-961-markers" for this suite. 09/03/22 20:57:12.752
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":72,"skipped":1161,"failed":0}
------------------------------
• [SLOW TEST] [7.083 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:05.741
    Sep  3 20:57:05.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 20:57:05.742
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:05.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:05.759
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 20:57:05.768
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 20:57:06.612
    STEP: Deploying the webhook pod 09/03/22 20:57:06.617
    STEP: Wait for the deployment to be ready 09/03/22 20:57:06.624
    Sep  3 20:57:06.633: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 20:57:08.639
    STEP: Verifying the service has paired with the endpoint 09/03/22 20:57:08.662
    Sep  3 20:57:09.663: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Sep  3 20:57:09.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-348-crds.webhook.example.com via the AdmissionRegistration API 09/03/22 20:57:10.172
    STEP: Creating a custom resource that should be mutated by the webhook 09/03/22 20:57:10.185
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 20:57:12.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-961" for this suite. 09/03/22 20:57:12.749
    STEP: Destroying namespace "webhook-961-markers" for this suite. 09/03/22 20:57:12.752
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:12.842
Sep  3 20:57:12.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename crd-webhook 09/03/22 20:57:12.843
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:12.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:12.879
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/03/22 20:57:12.885
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/03/22 20:57:13.337
STEP: Deploying the custom resource conversion webhook pod 09/03/22 20:57:13.34
STEP: Wait for the deployment to be ready 09/03/22 20:57:13.347
Sep  3 20:57:13.357: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 20:57:15.364
STEP: Verifying the service has paired with the endpoint 09/03/22 20:57:15.374
Sep  3 20:57:16.374: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Sep  3 20:57:16.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Creating a v1 custom resource 09/03/22 20:57:18.949
STEP: v2 custom resource should be converted 09/03/22 20:57:18.952
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 20:57:19.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5015" for this suite. 09/03/22 20:57:19.466
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":73,"skipped":1190,"failed":0}
------------------------------
• [SLOW TEST] [6.743 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:12.842
    Sep  3 20:57:12.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename crd-webhook 09/03/22 20:57:12.843
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:12.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:12.879
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/03/22 20:57:12.885
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/03/22 20:57:13.337
    STEP: Deploying the custom resource conversion webhook pod 09/03/22 20:57:13.34
    STEP: Wait for the deployment to be ready 09/03/22 20:57:13.347
    Sep  3 20:57:13.357: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 20:57:15.364
    STEP: Verifying the service has paired with the endpoint 09/03/22 20:57:15.374
    Sep  3 20:57:16.374: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Sep  3 20:57:16.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Creating a v1 custom resource 09/03/22 20:57:18.949
    STEP: v2 custom resource should be converted 09/03/22 20:57:18.952
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 20:57:19.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-5015" for this suite. 09/03/22 20:57:19.466
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:19.593
Sep  3 20:57:19.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 20:57:19.596
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:19.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:19.631
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-6313 09/03/22 20:57:19.635
STEP: creating service affinity-nodeport in namespace services-6313 09/03/22 20:57:19.635
STEP: creating replication controller affinity-nodeport in namespace services-6313 09/03/22 20:57:19.654
I0903 20:57:19.673152      24 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6313, replica count: 3
I0903 20:57:22.725105      24 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 20:57:22.731: INFO: Creating new exec pod
Sep  3 20:57:22.736: INFO: Waiting up to 5m0s for pod "execpod-affinityq6crk" in namespace "services-6313" to be "running"
Sep  3 20:57:22.739: INFO: Pod "execpod-affinityq6crk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191705ms
Sep  3 20:57:24.743: INFO: Pod "execpod-affinityq6crk": Phase="Running", Reason="", readiness=true. Elapsed: 2.006169296s
Sep  3 20:57:24.743: INFO: Pod "execpod-affinityq6crk" satisfied condition "running"
Sep  3 20:57:25.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6313 exec execpod-affinityq6crk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Sep  3 20:57:25.889: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep  3 20:57:25.890: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 20:57:25.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6313 exec execpod-affinityq6crk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.111.128 80'
Sep  3 20:57:26.070: INFO: stderr: "+ nc -v -t -w 2 10.96.111.128 80\n+ echo hostName\nConnection to 10.96.111.128 80 port [tcp/http] succeeded!\n"
Sep  3 20:57:26.070: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 20:57:26.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6313 exec execpod-affinityq6crk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.2 31878'
Sep  3 20:57:26.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.2 31878\nConnection to 172.18.0.2 31878 port [tcp/*] succeeded!\n"
Sep  3 20:57:26.217: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 20:57:26.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6313 exec execpod-affinityq6crk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.3 31878'
Sep  3 20:57:26.384: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.3 31878\nConnection to 172.18.0.3 31878 port [tcp/*] succeeded!\n"
Sep  3 20:57:26.384: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 20:57:26.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6313 exec execpod-affinityq6crk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.18.0.2:31878/ ; done'
Sep  3 20:57:26.631: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n"
Sep  3 20:57:26.631: INFO: stdout: "\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm"
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.632: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.632: INFO: Received response from host: affinity-nodeport-dqzlm
Sep  3 20:57:26.632: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6313, will wait for the garbage collector to delete the pods 09/03/22 20:57:26.639
Sep  3 20:57:26.707: INFO: Deleting ReplicationController affinity-nodeport took: 6.527215ms
Sep  3 20:57:26.808: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.144637ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 20:57:29.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6313" for this suite. 09/03/22 20:57:29.028
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":74,"skipped":1190,"failed":0}
------------------------------
• [SLOW TEST] [9.439 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:19.593
    Sep  3 20:57:19.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 20:57:19.596
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:19.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:19.631
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-6313 09/03/22 20:57:19.635
    STEP: creating service affinity-nodeport in namespace services-6313 09/03/22 20:57:19.635
    STEP: creating replication controller affinity-nodeport in namespace services-6313 09/03/22 20:57:19.654
    I0903 20:57:19.673152      24 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6313, replica count: 3
    I0903 20:57:22.725105      24 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  3 20:57:22.731: INFO: Creating new exec pod
    Sep  3 20:57:22.736: INFO: Waiting up to 5m0s for pod "execpod-affinityq6crk" in namespace "services-6313" to be "running"
    Sep  3 20:57:22.739: INFO: Pod "execpod-affinityq6crk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191705ms
    Sep  3 20:57:24.743: INFO: Pod "execpod-affinityq6crk": Phase="Running", Reason="", readiness=true. Elapsed: 2.006169296s
    Sep  3 20:57:24.743: INFO: Pod "execpod-affinityq6crk" satisfied condition "running"
    Sep  3 20:57:25.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6313 exec execpod-affinityq6crk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Sep  3 20:57:25.889: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Sep  3 20:57:25.890: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 20:57:25.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6313 exec execpod-affinityq6crk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.111.128 80'
    Sep  3 20:57:26.070: INFO: stderr: "+ nc -v -t -w 2 10.96.111.128 80\n+ echo hostName\nConnection to 10.96.111.128 80 port [tcp/http] succeeded!\n"
    Sep  3 20:57:26.070: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 20:57:26.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6313 exec execpod-affinityq6crk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.2 31878'
    Sep  3 20:57:26.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.2 31878\nConnection to 172.18.0.2 31878 port [tcp/*] succeeded!\n"
    Sep  3 20:57:26.217: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 20:57:26.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6313 exec execpod-affinityq6crk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.3 31878'
    Sep  3 20:57:26.384: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.3 31878\nConnection to 172.18.0.3 31878 port [tcp/*] succeeded!\n"
    Sep  3 20:57:26.384: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 20:57:26.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6313 exec execpod-affinityq6crk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.18.0.2:31878/ ; done'
    Sep  3 20:57:26.631: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31878/\n"
    Sep  3 20:57:26.631: INFO: stdout: "\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm\naffinity-nodeport-dqzlm"
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.631: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.632: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.632: INFO: Received response from host: affinity-nodeport-dqzlm
    Sep  3 20:57:26.632: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-6313, will wait for the garbage collector to delete the pods 09/03/22 20:57:26.639
    Sep  3 20:57:26.707: INFO: Deleting ReplicationController affinity-nodeport took: 6.527215ms
    Sep  3 20:57:26.808: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.144637ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 20:57:29.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6313" for this suite. 09/03/22 20:57:29.028
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:29.032
Sep  3 20:57:29.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pods 09/03/22 20:57:29.033
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:29.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:29.044
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 09/03/22 20:57:29.046
Sep  3 20:57:29.052: INFO: created test-pod-1
Sep  3 20:57:29.055: INFO: created test-pod-2
Sep  3 20:57:29.079: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 09/03/22 20:57:29.08
Sep  3 20:57:29.080: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2073' to be running and ready
Sep  3 20:57:29.091: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  3 20:57:29.091: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  3 20:57:29.091: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  3 20:57:29.091: INFO: 0 / 3 pods in namespace 'pods-2073' are running and ready (0 seconds elapsed)
Sep  3 20:57:29.091: INFO: expected 0 pod replicas in namespace 'pods-2073', 0 are Running and Ready.
Sep  3 20:57:29.091: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
Sep  3 20:57:29.091: INFO: test-pod-1  kind-worker2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  }]
Sep  3 20:57:29.091: INFO: test-pod-2  kind-worker2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  }]
Sep  3 20:57:29.091: INFO: test-pod-3  kind-worker2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  }]
Sep  3 20:57:29.091: INFO: 
Sep  3 20:57:31.099: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  3 20:57:31.099: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  3 20:57:31.099: INFO: 1 / 3 pods in namespace 'pods-2073' are running and ready (2 seconds elapsed)
Sep  3 20:57:31.099: INFO: expected 0 pod replicas in namespace 'pods-2073', 0 are Running and Ready.
Sep  3 20:57:31.099: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
Sep  3 20:57:31.099: INFO: test-pod-1  kind-worker2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  }]
Sep  3 20:57:31.099: INFO: test-pod-2  kind-worker2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  }]
Sep  3 20:57:31.099: INFO: 
Sep  3 20:57:33.099: INFO: 3 / 3 pods in namespace 'pods-2073' are running and ready (4 seconds elapsed)
Sep  3 20:57:33.099: INFO: expected 0 pod replicas in namespace 'pods-2073', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 09/03/22 20:57:33.13
Sep  3 20:57:33.140: INFO: Pod quantity 3 is different from expected quantity 0
Sep  3 20:57:34.143: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Sep  3 20:57:35.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2073" for this suite. 09/03/22 20:57:35.145
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":75,"skipped":1202,"failed":0}
------------------------------
• [SLOW TEST] [6.116 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:29.032
    Sep  3 20:57:29.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pods 09/03/22 20:57:29.033
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:29.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:29.044
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 09/03/22 20:57:29.046
    Sep  3 20:57:29.052: INFO: created test-pod-1
    Sep  3 20:57:29.055: INFO: created test-pod-2
    Sep  3 20:57:29.079: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 09/03/22 20:57:29.08
    Sep  3 20:57:29.080: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2073' to be running and ready
    Sep  3 20:57:29.091: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  3 20:57:29.091: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  3 20:57:29.091: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  3 20:57:29.091: INFO: 0 / 3 pods in namespace 'pods-2073' are running and ready (0 seconds elapsed)
    Sep  3 20:57:29.091: INFO: expected 0 pod replicas in namespace 'pods-2073', 0 are Running and Ready.
    Sep  3 20:57:29.091: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
    Sep  3 20:57:29.091: INFO: test-pod-1  kind-worker2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  }]
    Sep  3 20:57:29.091: INFO: test-pod-2  kind-worker2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  }]
    Sep  3 20:57:29.091: INFO: test-pod-3  kind-worker2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  }]
    Sep  3 20:57:29.091: INFO: 
    Sep  3 20:57:31.099: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  3 20:57:31.099: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  3 20:57:31.099: INFO: 1 / 3 pods in namespace 'pods-2073' are running and ready (2 seconds elapsed)
    Sep  3 20:57:31.099: INFO: expected 0 pod replicas in namespace 'pods-2073', 0 are Running and Ready.
    Sep  3 20:57:31.099: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
    Sep  3 20:57:31.099: INFO: test-pod-1  kind-worker2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  }]
    Sep  3 20:57:31.099: INFO: test-pod-2  kind-worker2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 20:57:29 +0000 UTC  }]
    Sep  3 20:57:31.099: INFO: 
    Sep  3 20:57:33.099: INFO: 3 / 3 pods in namespace 'pods-2073' are running and ready (4 seconds elapsed)
    Sep  3 20:57:33.099: INFO: expected 0 pod replicas in namespace 'pods-2073', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 09/03/22 20:57:33.13
    Sep  3 20:57:33.140: INFO: Pod quantity 3 is different from expected quantity 0
    Sep  3 20:57:34.143: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Sep  3 20:57:35.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2073" for this suite. 09/03/22 20:57:35.145
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:35.15
Sep  3 20:57:35.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename namespaces 09/03/22 20:57:35.151
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:35.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:35.162
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 09/03/22 20:57:35.17
Sep  3 20:57:35.181: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 09/03/22 20:57:35.181
Sep  3 20:57:35.186: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 09/03/22 20:57:35.186
Sep  3 20:57:35.194: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Sep  3 20:57:35.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8323" for this suite. 09/03/22 20:57:35.199
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":76,"skipped":1210,"failed":0}
------------------------------
• [0.057 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:35.15
    Sep  3 20:57:35.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename namespaces 09/03/22 20:57:35.151
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:35.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:35.162
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 09/03/22 20:57:35.17
    Sep  3 20:57:35.181: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 09/03/22 20:57:35.181
    Sep  3 20:57:35.186: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 09/03/22 20:57:35.186
    Sep  3 20:57:35.194: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 20:57:35.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8323" for this suite. 09/03/22 20:57:35.199
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:35.211
Sep  3 20:57:35.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename init-container 09/03/22 20:57:35.212
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:35.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:35.274
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 09/03/22 20:57:35.276
Sep  3 20:57:35.278: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Sep  3 20:57:39.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1301" for this suite. 09/03/22 20:57:39.062
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":77,"skipped":1218,"failed":0}
------------------------------
• [3.859 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:35.211
    Sep  3 20:57:35.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename init-container 09/03/22 20:57:35.212
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:35.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:35.274
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 09/03/22 20:57:35.276
    Sep  3 20:57:35.278: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Sep  3 20:57:39.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1301" for this suite. 09/03/22 20:57:39.062
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:39.075
Sep  3 20:57:39.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 20:57:39.076
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:39.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:39.087
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 09/03/22 20:57:39.09
Sep  3 20:57:39.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2814 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  3 20:57:39.159: INFO: stderr: ""
Sep  3 20:57:39.159: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 09/03/22 20:57:39.159
Sep  3 20:57:39.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2814 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Sep  3 20:57:40.178: INFO: stderr: ""
Sep  3 20:57:40.178: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 09/03/22 20:57:40.178
Sep  3 20:57:40.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2814 delete pods e2e-test-httpd-pod'
Sep  3 20:57:43.109: INFO: stderr: ""
Sep  3 20:57:43.109: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 20:57:43.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2814" for this suite. 09/03/22 20:57:43.113
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":78,"skipped":1228,"failed":0}
------------------------------
• [4.042 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:39.075
    Sep  3 20:57:39.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 20:57:39.076
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:39.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:39.087
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 09/03/22 20:57:39.09
    Sep  3 20:57:39.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2814 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  3 20:57:39.159: INFO: stderr: ""
    Sep  3 20:57:39.159: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 09/03/22 20:57:39.159
    Sep  3 20:57:39.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2814 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Sep  3 20:57:40.178: INFO: stderr: ""
    Sep  3 20:57:40.178: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 09/03/22 20:57:40.178
    Sep  3 20:57:40.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2814 delete pods e2e-test-httpd-pod'
    Sep  3 20:57:43.109: INFO: stderr: ""
    Sep  3 20:57:43.109: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 20:57:43.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2814" for this suite. 09/03/22 20:57:43.113
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:43.119
Sep  3 20:57:43.119: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename containers 09/03/22 20:57:43.119
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:43.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:43.132
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 09/03/22 20:57:43.134
Sep  3 20:57:43.138: INFO: Waiting up to 5m0s for pod "client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43" in namespace "containers-2287" to be "Succeeded or Failed"
Sep  3 20:57:43.143: INFO: Pod "client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171809ms
Sep  3 20:57:45.147: INFO: Pod "client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008168753s
Sep  3 20:57:47.146: INFO: Pod "client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007149043s
STEP: Saw pod success 09/03/22 20:57:47.146
Sep  3 20:57:47.146: INFO: Pod "client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43" satisfied condition "Succeeded or Failed"
Sep  3 20:57:47.148: INFO: Trying to get logs from node kind-worker2 pod client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43 container agnhost-container: <nil>
STEP: delete the pod 09/03/22 20:57:47.151
Sep  3 20:57:47.158: INFO: Waiting for pod client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43 to disappear
Sep  3 20:57:47.159: INFO: Pod client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Sep  3 20:57:47.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2287" for this suite. 09/03/22 20:57:47.162
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":79,"skipped":1266,"failed":0}
------------------------------
• [4.046 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:43.119
    Sep  3 20:57:43.119: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename containers 09/03/22 20:57:43.119
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:43.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:43.132
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 09/03/22 20:57:43.134
    Sep  3 20:57:43.138: INFO: Waiting up to 5m0s for pod "client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43" in namespace "containers-2287" to be "Succeeded or Failed"
    Sep  3 20:57:43.143: INFO: Pod "client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171809ms
    Sep  3 20:57:45.147: INFO: Pod "client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008168753s
    Sep  3 20:57:47.146: INFO: Pod "client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007149043s
    STEP: Saw pod success 09/03/22 20:57:47.146
    Sep  3 20:57:47.146: INFO: Pod "client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43" satisfied condition "Succeeded or Failed"
    Sep  3 20:57:47.148: INFO: Trying to get logs from node kind-worker2 pod client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43 container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 20:57:47.151
    Sep  3 20:57:47.158: INFO: Waiting for pod client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43 to disappear
    Sep  3 20:57:47.159: INFO: Pod client-containers-da635692-e294-4c2c-9d1f-0fb20eddbe43 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Sep  3 20:57:47.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2287" for this suite. 09/03/22 20:57:47.162
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:47.167
Sep  3 20:57:47.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 20:57:47.169
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:47.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:47.186
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-1fb375b2-c5d6-4337-968c-dce4e9f1c8f8 09/03/22 20:57:47.2
STEP: Creating configMap with name cm-test-opt-upd-181932c1-33d3-41d6-96cd-27ea1cf511cb 09/03/22 20:57:47.206
STEP: Creating the pod 09/03/22 20:57:47.213
Sep  3 20:57:47.223: INFO: Waiting up to 5m0s for pod "pod-configmaps-f586dda1-15f3-45f5-8a74-b0f13de8b16b" in namespace "configmap-5623" to be "running and ready"
Sep  3 20:57:47.228: INFO: Pod "pod-configmaps-f586dda1-15f3-45f5-8a74-b0f13de8b16b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.83491ms
Sep  3 20:57:47.228: INFO: The phase of Pod pod-configmaps-f586dda1-15f3-45f5-8a74-b0f13de8b16b is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:57:49.231: INFO: Pod "pod-configmaps-f586dda1-15f3-45f5-8a74-b0f13de8b16b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008394008s
Sep  3 20:57:49.232: INFO: The phase of Pod pod-configmaps-f586dda1-15f3-45f5-8a74-b0f13de8b16b is Running (Ready = true)
Sep  3 20:57:49.232: INFO: Pod "pod-configmaps-f586dda1-15f3-45f5-8a74-b0f13de8b16b" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-1fb375b2-c5d6-4337-968c-dce4e9f1c8f8 09/03/22 20:57:49.247
STEP: Updating configmap cm-test-opt-upd-181932c1-33d3-41d6-96cd-27ea1cf511cb 09/03/22 20:57:49.251
STEP: Creating configMap with name cm-test-opt-create-a140137f-aebf-4215-b6c0-fe3775d3c1cc 09/03/22 20:57:49.253
STEP: waiting to observe update in volume 09/03/22 20:57:49.256
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 20:57:53.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5623" for this suite. 09/03/22 20:57:53.279
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":80,"skipped":1282,"failed":0}
------------------------------
• [SLOW TEST] [6.116 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:47.167
    Sep  3 20:57:47.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 20:57:47.169
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:47.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:47.186
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-1fb375b2-c5d6-4337-968c-dce4e9f1c8f8 09/03/22 20:57:47.2
    STEP: Creating configMap with name cm-test-opt-upd-181932c1-33d3-41d6-96cd-27ea1cf511cb 09/03/22 20:57:47.206
    STEP: Creating the pod 09/03/22 20:57:47.213
    Sep  3 20:57:47.223: INFO: Waiting up to 5m0s for pod "pod-configmaps-f586dda1-15f3-45f5-8a74-b0f13de8b16b" in namespace "configmap-5623" to be "running and ready"
    Sep  3 20:57:47.228: INFO: Pod "pod-configmaps-f586dda1-15f3-45f5-8a74-b0f13de8b16b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.83491ms
    Sep  3 20:57:47.228: INFO: The phase of Pod pod-configmaps-f586dda1-15f3-45f5-8a74-b0f13de8b16b is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:57:49.231: INFO: Pod "pod-configmaps-f586dda1-15f3-45f5-8a74-b0f13de8b16b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008394008s
    Sep  3 20:57:49.232: INFO: The phase of Pod pod-configmaps-f586dda1-15f3-45f5-8a74-b0f13de8b16b is Running (Ready = true)
    Sep  3 20:57:49.232: INFO: Pod "pod-configmaps-f586dda1-15f3-45f5-8a74-b0f13de8b16b" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-1fb375b2-c5d6-4337-968c-dce4e9f1c8f8 09/03/22 20:57:49.247
    STEP: Updating configmap cm-test-opt-upd-181932c1-33d3-41d6-96cd-27ea1cf511cb 09/03/22 20:57:49.251
    STEP: Creating configMap with name cm-test-opt-create-a140137f-aebf-4215-b6c0-fe3775d3c1cc 09/03/22 20:57:49.253
    STEP: waiting to observe update in volume 09/03/22 20:57:49.256
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 20:57:53.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5623" for this suite. 09/03/22 20:57:53.279
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:53.294
Sep  3 20:57:53.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename lease-test 09/03/22 20:57:53.295
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:53.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:53.309
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Sep  3 20:57:53.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6652" for this suite. 09/03/22 20:57:53.346
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":81,"skipped":1320,"failed":0}
------------------------------
• [0.055 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:53.294
    Sep  3 20:57:53.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename lease-test 09/03/22 20:57:53.295
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:53.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:53.309
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Sep  3 20:57:53.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-6652" for this suite. 09/03/22 20:57:53.346
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:53.358
Sep  3 20:57:53.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-lifecycle-hook 09/03/22 20:57:53.359
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:53.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:53.37
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 09/03/22 20:57:53.382
Sep  3 20:57:53.391: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1716" to be "running and ready"
Sep  3 20:57:53.393: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.966904ms
Sep  3 20:57:53.393: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:57:55.396: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004301913s
Sep  3 20:57:55.396: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  3 20:57:55.396: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 09/03/22 20:57:55.398
Sep  3 20:57:55.401: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1716" to be "running and ready"
Sep  3 20:57:55.404: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.802407ms
Sep  3 20:57:55.404: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:57:57.406: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005168915s
Sep  3 20:57:57.406: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Sep  3 20:57:57.406: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/03/22 20:57:57.408
STEP: delete the pod with lifecycle hook 09/03/22 20:57:57.421
Sep  3 20:57:57.426: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  3 20:57:57.428: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  3 20:57:59.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  3 20:57:59.431: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Sep  3 20:57:59.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1716" for this suite. 09/03/22 20:57:59.434
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":82,"skipped":1344,"failed":0}
------------------------------
• [SLOW TEST] [6.080 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:53.358
    Sep  3 20:57:53.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/03/22 20:57:53.359
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:53.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:53.37
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 09/03/22 20:57:53.382
    Sep  3 20:57:53.391: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1716" to be "running and ready"
    Sep  3 20:57:53.393: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.966904ms
    Sep  3 20:57:53.393: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:57:55.396: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004301913s
    Sep  3 20:57:55.396: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  3 20:57:55.396: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 09/03/22 20:57:55.398
    Sep  3 20:57:55.401: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1716" to be "running and ready"
    Sep  3 20:57:55.404: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.802407ms
    Sep  3 20:57:55.404: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:57:57.406: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005168915s
    Sep  3 20:57:57.406: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Sep  3 20:57:57.406: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/03/22 20:57:57.408
    STEP: delete the pod with lifecycle hook 09/03/22 20:57:57.421
    Sep  3 20:57:57.426: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  3 20:57:57.428: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  3 20:57:59.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  3 20:57:59.431: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Sep  3 20:57:59.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1716" for this suite. 09/03/22 20:57:59.434
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:57:59.466
Sep  3 20:57:59.466: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 20:57:59.468
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:59.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:59.484
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-520ebd1a-86f1-4660-8d43-7f387dce6711 09/03/22 20:57:59.487
STEP: Creating a pod to test consume secrets 09/03/22 20:57:59.49
Sep  3 20:57:59.497: INFO: Waiting up to 5m0s for pod "pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361" in namespace "secrets-2084" to be "Succeeded or Failed"
Sep  3 20:57:59.503: INFO: Pod "pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361": Phase="Pending", Reason="", readiness=false. Elapsed: 5.875313ms
Sep  3 20:58:01.506: INFO: Pod "pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00936633s
Sep  3 20:58:03.506: INFO: Pod "pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008824542s
STEP: Saw pod success 09/03/22 20:58:03.506
Sep  3 20:58:03.506: INFO: Pod "pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361" satisfied condition "Succeeded or Failed"
Sep  3 20:58:03.509: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361 container secret-volume-test: <nil>
STEP: delete the pod 09/03/22 20:58:03.518
Sep  3 20:58:03.526: INFO: Waiting for pod pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361 to disappear
Sep  3 20:58:03.528: INFO: Pod pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Sep  3 20:58:03.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2084" for this suite. 09/03/22 20:58:03.532
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":83,"skipped":1389,"failed":0}
------------------------------
• [4.070 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:57:59.466
    Sep  3 20:57:59.466: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 20:57:59.468
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:57:59.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:57:59.484
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-520ebd1a-86f1-4660-8d43-7f387dce6711 09/03/22 20:57:59.487
    STEP: Creating a pod to test consume secrets 09/03/22 20:57:59.49
    Sep  3 20:57:59.497: INFO: Waiting up to 5m0s for pod "pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361" in namespace "secrets-2084" to be "Succeeded or Failed"
    Sep  3 20:57:59.503: INFO: Pod "pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361": Phase="Pending", Reason="", readiness=false. Elapsed: 5.875313ms
    Sep  3 20:58:01.506: INFO: Pod "pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00936633s
    Sep  3 20:58:03.506: INFO: Pod "pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008824542s
    STEP: Saw pod success 09/03/22 20:58:03.506
    Sep  3 20:58:03.506: INFO: Pod "pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361" satisfied condition "Succeeded or Failed"
    Sep  3 20:58:03.509: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361 container secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 20:58:03.518
    Sep  3 20:58:03.526: INFO: Waiting for pod pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361 to disappear
    Sep  3 20:58:03.528: INFO: Pod pod-secrets-13958c95-fa8e-4d45-89fe-68c777dcf361 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 20:58:03.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2084" for this suite. 09/03/22 20:58:03.532
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:58:03.537
Sep  3 20:58:03.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 20:58:03.538
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:58:03.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:58:03.552
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-ba138d5b-5923-4d0e-8a80-e1793d0fb451 09/03/22 20:58:03.555
STEP: Creating a pod to test consume secrets 09/03/22 20:58:03.559
Sep  3 20:58:03.565: INFO: Waiting up to 5m0s for pod "pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e" in namespace "secrets-6132" to be "Succeeded or Failed"
Sep  3 20:58:03.568: INFO: Pod "pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.008107ms
Sep  3 20:58:05.571: INFO: Pod "pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005745526s
Sep  3 20:58:07.571: INFO: Pod "pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005436439s
STEP: Saw pod success 09/03/22 20:58:07.571
Sep  3 20:58:07.571: INFO: Pod "pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e" satisfied condition "Succeeded or Failed"
Sep  3 20:58:07.573: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e container secret-env-test: <nil>
STEP: delete the pod 09/03/22 20:58:07.577
Sep  3 20:58:07.583: INFO: Waiting for pod pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e to disappear
Sep  3 20:58:07.585: INFO: Pod pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Sep  3 20:58:07.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6132" for this suite. 09/03/22 20:58:07.587
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":84,"skipped":1421,"failed":0}
------------------------------
• [4.053 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:58:03.537
    Sep  3 20:58:03.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 20:58:03.538
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:58:03.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:58:03.552
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-ba138d5b-5923-4d0e-8a80-e1793d0fb451 09/03/22 20:58:03.555
    STEP: Creating a pod to test consume secrets 09/03/22 20:58:03.559
    Sep  3 20:58:03.565: INFO: Waiting up to 5m0s for pod "pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e" in namespace "secrets-6132" to be "Succeeded or Failed"
    Sep  3 20:58:03.568: INFO: Pod "pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.008107ms
    Sep  3 20:58:05.571: INFO: Pod "pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005745526s
    Sep  3 20:58:07.571: INFO: Pod "pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005436439s
    STEP: Saw pod success 09/03/22 20:58:07.571
    Sep  3 20:58:07.571: INFO: Pod "pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e" satisfied condition "Succeeded or Failed"
    Sep  3 20:58:07.573: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e container secret-env-test: <nil>
    STEP: delete the pod 09/03/22 20:58:07.577
    Sep  3 20:58:07.583: INFO: Waiting for pod pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e to disappear
    Sep  3 20:58:07.585: INFO: Pod pod-secrets-b917a4e5-873e-44e2-9a7c-784a3f53ef7e no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 20:58:07.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6132" for this suite. 09/03/22 20:58:07.587
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:58:07.595
Sep  3 20:58:07.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 20:58:07.597
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:58:07.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:58:07.608
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/03/22 20:58:07.61
Sep  3 20:58:07.614: INFO: Waiting up to 5m0s for pod "pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d" in namespace "emptydir-9037" to be "Succeeded or Failed"
Sep  3 20:58:07.618: INFO: Pod "pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.627508ms
Sep  3 20:58:09.621: INFO: Pod "pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006680233s
Sep  3 20:58:11.622: INFO: Pod "pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008170057s
STEP: Saw pod success 09/03/22 20:58:11.622
Sep  3 20:58:11.623: INFO: Pod "pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d" satisfied condition "Succeeded or Failed"
Sep  3 20:58:11.624: INFO: Trying to get logs from node kind-worker2 pod pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d container test-container: <nil>
STEP: delete the pod 09/03/22 20:58:11.628
Sep  3 20:58:11.636: INFO: Waiting for pod pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d to disappear
Sep  3 20:58:11.638: INFO: Pod pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 20:58:11.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9037" for this suite. 09/03/22 20:58:11.64
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":85,"skipped":1424,"failed":0}
------------------------------
• [4.048 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:58:07.595
    Sep  3 20:58:07.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 20:58:07.597
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:58:07.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:58:07.608
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/03/22 20:58:07.61
    Sep  3 20:58:07.614: INFO: Waiting up to 5m0s for pod "pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d" in namespace "emptydir-9037" to be "Succeeded or Failed"
    Sep  3 20:58:07.618: INFO: Pod "pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.627508ms
    Sep  3 20:58:09.621: INFO: Pod "pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006680233s
    Sep  3 20:58:11.622: INFO: Pod "pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008170057s
    STEP: Saw pod success 09/03/22 20:58:11.622
    Sep  3 20:58:11.623: INFO: Pod "pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d" satisfied condition "Succeeded or Failed"
    Sep  3 20:58:11.624: INFO: Trying to get logs from node kind-worker2 pod pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d container test-container: <nil>
    STEP: delete the pod 09/03/22 20:58:11.628
    Sep  3 20:58:11.636: INFO: Waiting for pod pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d to disappear
    Sep  3 20:58:11.638: INFO: Pod pod-e0daf71d-9a65-4ff4-a052-4bb911637c0d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 20:58:11.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9037" for this suite. 09/03/22 20:58:11.64
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:58:11.649
Sep  3 20:58:11.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 20:58:11.65
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:58:11.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:58:11.66
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 20:58:11.668
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 20:58:12.131
STEP: Deploying the webhook pod 09/03/22 20:58:12.136
STEP: Wait for the deployment to be ready 09/03/22 20:58:12.143
Sep  3 20:58:12.153: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 20:58:14.158
STEP: Verifying the service has paired with the endpoint 09/03/22 20:58:14.162
Sep  3 20:58:15.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/03/22 20:58:15.165
STEP: Registering slow webhook via the AdmissionRegistration API 09/03/22 20:58:15.165
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/03/22 20:58:15.188
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/03/22 20:58:16.196
STEP: Registering slow webhook via the AdmissionRegistration API 09/03/22 20:58:16.196
STEP: Having no error when timeout is longer than webhook latency 09/03/22 20:58:17.212
STEP: Registering slow webhook via the AdmissionRegistration API 09/03/22 20:58:17.212
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/03/22 20:58:22.234
STEP: Registering slow webhook via the AdmissionRegistration API 09/03/22 20:58:22.235
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 20:58:27.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7257" for this suite. 09/03/22 20:58:27.254
STEP: Destroying namespace "webhook-7257-markers" for this suite. 09/03/22 20:58:27.258
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":86,"skipped":1446,"failed":0}
------------------------------
• [SLOW TEST] [15.678 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:58:11.649
    Sep  3 20:58:11.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 20:58:11.65
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:58:11.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:58:11.66
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 20:58:11.668
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 20:58:12.131
    STEP: Deploying the webhook pod 09/03/22 20:58:12.136
    STEP: Wait for the deployment to be ready 09/03/22 20:58:12.143
    Sep  3 20:58:12.153: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 20:58:14.158
    STEP: Verifying the service has paired with the endpoint 09/03/22 20:58:14.162
    Sep  3 20:58:15.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/03/22 20:58:15.165
    STEP: Registering slow webhook via the AdmissionRegistration API 09/03/22 20:58:15.165
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/03/22 20:58:15.188
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/03/22 20:58:16.196
    STEP: Registering slow webhook via the AdmissionRegistration API 09/03/22 20:58:16.196
    STEP: Having no error when timeout is longer than webhook latency 09/03/22 20:58:17.212
    STEP: Registering slow webhook via the AdmissionRegistration API 09/03/22 20:58:17.212
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/03/22 20:58:22.234
    STEP: Registering slow webhook via the AdmissionRegistration API 09/03/22 20:58:22.235
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 20:58:27.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7257" for this suite. 09/03/22 20:58:27.254
    STEP: Destroying namespace "webhook-7257-markers" for this suite. 09/03/22 20:58:27.258
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:58:27.33
Sep  3 20:58:27.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-lifecycle-hook 09/03/22 20:58:27.331
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:58:27.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:58:27.414
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 09/03/22 20:58:27.426
Sep  3 20:58:27.431: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-158" to be "running and ready"
Sep  3 20:58:27.450: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.378342ms
Sep  3 20:58:27.450: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:58:29.459: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.02734881s
Sep  3 20:58:29.459: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  3 20:58:29.459: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 09/03/22 20:58:29.46
Sep  3 20:58:29.468: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-158" to be "running and ready"
Sep  3 20:58:29.473: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.325512ms
Sep  3 20:58:29.473: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  3 20:58:31.477: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009545769s
Sep  3 20:58:31.480: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Sep  3 20:58:31.480: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/03/22 20:58:31.483
Sep  3 20:58:31.495: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  3 20:58:31.512: INFO: Pod pod-with-prestop-http-hook still exists
Sep  3 20:58:33.512: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  3 20:58:33.516: INFO: Pod pod-with-prestop-http-hook still exists
Sep  3 20:58:35.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  3 20:58:35.515: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 09/03/22 20:58:35.515
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Sep  3 20:58:35.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-158" for this suite. 09/03/22 20:58:35.522
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":87,"skipped":1455,"failed":0}
------------------------------
• [SLOW TEST] [8.196 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:58:27.33
    Sep  3 20:58:27.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/03/22 20:58:27.331
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:58:27.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:58:27.414
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 09/03/22 20:58:27.426
    Sep  3 20:58:27.431: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-158" to be "running and ready"
    Sep  3 20:58:27.450: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.378342ms
    Sep  3 20:58:27.450: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:58:29.459: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.02734881s
    Sep  3 20:58:29.459: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  3 20:58:29.459: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 09/03/22 20:58:29.46
    Sep  3 20:58:29.468: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-158" to be "running and ready"
    Sep  3 20:58:29.473: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.325512ms
    Sep  3 20:58:29.473: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 20:58:31.477: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009545769s
    Sep  3 20:58:31.480: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Sep  3 20:58:31.480: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/03/22 20:58:31.483
    Sep  3 20:58:31.495: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  3 20:58:31.512: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  3 20:58:33.512: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  3 20:58:33.516: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  3 20:58:35.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  3 20:58:35.515: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 09/03/22 20:58:35.515
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Sep  3 20:58:35.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-158" for this suite. 09/03/22 20:58:35.522
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:58:35.527
Sep  3 20:58:35.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename csistoragecapacity 09/03/22 20:58:35.528
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:58:35.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:58:35.539
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 09/03/22 20:58:35.541
STEP: getting /apis/storage.k8s.io 09/03/22 20:58:35.543
STEP: getting /apis/storage.k8s.io/v1 09/03/22 20:58:35.544
STEP: creating 09/03/22 20:58:35.545
STEP: watching 09/03/22 20:58:35.555
Sep  3 20:58:35.556: INFO: starting watch
STEP: getting 09/03/22 20:58:35.568
STEP: listing in namespace 09/03/22 20:58:35.571
STEP: listing across namespaces 09/03/22 20:58:35.573
STEP: patching 09/03/22 20:58:35.575
STEP: updating 09/03/22 20:58:35.579
Sep  3 20:58:35.582: INFO: waiting for watch events with expected annotations in namespace
Sep  3 20:58:35.582: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 09/03/22 20:58:35.582
STEP: deleting a collection 09/03/22 20:58:35.59
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Sep  3 20:58:35.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-7534" for this suite. 09/03/22 20:58:35.604
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":88,"skipped":1459,"failed":0}
------------------------------
• [0.080 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:58:35.527
    Sep  3 20:58:35.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename csistoragecapacity 09/03/22 20:58:35.528
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:58:35.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:58:35.539
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 09/03/22 20:58:35.541
    STEP: getting /apis/storage.k8s.io 09/03/22 20:58:35.543
    STEP: getting /apis/storage.k8s.io/v1 09/03/22 20:58:35.544
    STEP: creating 09/03/22 20:58:35.545
    STEP: watching 09/03/22 20:58:35.555
    Sep  3 20:58:35.556: INFO: starting watch
    STEP: getting 09/03/22 20:58:35.568
    STEP: listing in namespace 09/03/22 20:58:35.571
    STEP: listing across namespaces 09/03/22 20:58:35.573
    STEP: patching 09/03/22 20:58:35.575
    STEP: updating 09/03/22 20:58:35.579
    Sep  3 20:58:35.582: INFO: waiting for watch events with expected annotations in namespace
    Sep  3 20:58:35.582: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 09/03/22 20:58:35.582
    STEP: deleting a collection 09/03/22 20:58:35.59
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Sep  3 20:58:35.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-7534" for this suite. 09/03/22 20:58:35.604
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:58:35.612
Sep  3 20:58:35.612: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename sched-preemption 09/03/22 20:58:35.613
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:58:35.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:58:35.642
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Sep  3 20:58:35.686: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  3 20:59:35.704: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:59:35.706
Sep  3 20:59:35.706: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename sched-preemption-path 09/03/22 20:59:35.707
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:59:35.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:59:35.718
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Sep  3 20:59:35.728: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Sep  3 20:59:35.730: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Sep  3 20:59:35.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2914" for this suite. 09/03/22 20:59:35.742
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Sep  3 20:59:35.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9383" for this suite. 09/03/22 20:59:35.754
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":89,"skipped":1464,"failed":0}
------------------------------
• [SLOW TEST] [60.163 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:58:35.612
    Sep  3 20:58:35.612: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename sched-preemption 09/03/22 20:58:35.613
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:58:35.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:58:35.642
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Sep  3 20:58:35.686: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  3 20:59:35.704: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:59:35.706
    Sep  3 20:59:35.706: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename sched-preemption-path 09/03/22 20:59:35.707
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:59:35.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:59:35.718
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Sep  3 20:59:35.728: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Sep  3 20:59:35.730: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Sep  3 20:59:35.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-2914" for this suite. 09/03/22 20:59:35.742
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 20:59:35.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9383" for this suite. 09/03/22 20:59:35.754
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:59:35.778
Sep  3 20:59:35.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename daemonsets 09/03/22 20:59:35.779
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:59:35.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:59:35.79
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 09/03/22 20:59:35.803
STEP: Check that daemon pods launch on every node of the cluster. 09/03/22 20:59:35.806
Sep  3 20:59:35.809: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:59:35.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 20:59:35.812: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 20:59:36.815: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:59:36.818: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 20:59:36.818: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 20:59:37.815: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:59:37.818: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  3 20:59:37.818: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/03/22 20:59:37.82
Sep  3 20:59:37.829: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 20:59:37.832: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  3 20:59:37.833: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 09/03/22 20:59:37.833
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 09/03/22 20:59:38.841
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7417, will wait for the garbage collector to delete the pods 09/03/22 20:59:38.841
Sep  3 20:59:38.896: INFO: Deleting DaemonSet.extensions daemon-set took: 3.507806ms
Sep  3 20:59:38.997: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.437468ms
Sep  3 20:59:40.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 20:59:40.500: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  3 20:59:40.501: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6632"},"items":null}

Sep  3 20:59:40.503: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6632"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Sep  3 20:59:40.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7417" for this suite. 09/03/22 20:59:40.512
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":90,"skipped":1470,"failed":0}
------------------------------
• [4.737 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:59:35.778
    Sep  3 20:59:35.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename daemonsets 09/03/22 20:59:35.779
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:59:35.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:59:35.79
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 09/03/22 20:59:35.803
    STEP: Check that daemon pods launch on every node of the cluster. 09/03/22 20:59:35.806
    Sep  3 20:59:35.809: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:59:35.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 20:59:35.812: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 20:59:36.815: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:59:36.818: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 20:59:36.818: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 20:59:37.815: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:59:37.818: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  3 20:59:37.818: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/03/22 20:59:37.82
    Sep  3 20:59:37.829: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 20:59:37.832: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  3 20:59:37.833: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 09/03/22 20:59:37.833
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 09/03/22 20:59:38.841
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7417, will wait for the garbage collector to delete the pods 09/03/22 20:59:38.841
    Sep  3 20:59:38.896: INFO: Deleting DaemonSet.extensions daemon-set took: 3.507806ms
    Sep  3 20:59:38.997: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.437468ms
    Sep  3 20:59:40.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 20:59:40.500: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  3 20:59:40.501: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6632"},"items":null}

    Sep  3 20:59:40.503: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6632"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 20:59:40.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7417" for this suite. 09/03/22 20:59:40.512
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:59:40.523
Sep  3 20:59:40.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename sysctl 09/03/22 20:59:40.524
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:59:40.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:59:40.536
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 09/03/22 20:59:40.538
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Sep  3 20:59:40.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5492" for this suite. 09/03/22 20:59:40.543
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":91,"skipped":1489,"failed":0}
------------------------------
• [0.023 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:59:40.523
    Sep  3 20:59:40.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename sysctl 09/03/22 20:59:40.524
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:59:40.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:59:40.536
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 09/03/22 20:59:40.538
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Sep  3 20:59:40.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-5492" for this suite. 09/03/22 20:59:40.543
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:59:40.554
Sep  3 20:59:40.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 20:59:40.555
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:59:40.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:59:40.568
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3066 09/03/22 20:59:40.571
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/03/22 20:59:40.582
STEP: creating service externalsvc in namespace services-3066 09/03/22 20:59:40.582
STEP: creating replication controller externalsvc in namespace services-3066 09/03/22 20:59:40.604
I0903 20:59:40.616936      24 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3066, replica count: 2
I0903 20:59:43.672567      24 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 09/03/22 20:59:43.674
Sep  3 20:59:43.683: INFO: Creating new exec pod
Sep  3 20:59:43.695: INFO: Waiting up to 5m0s for pod "execpodxhrhh" in namespace "services-3066" to be "running"
Sep  3 20:59:43.700: INFO: Pod "execpodxhrhh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.638608ms
Sep  3 20:59:45.705: INFO: Pod "execpodxhrhh": Phase="Running", Reason="", readiness=true. Elapsed: 2.009379892s
Sep  3 20:59:45.705: INFO: Pod "execpodxhrhh" satisfied condition "running"
Sep  3 20:59:45.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-3066 exec execpodxhrhh -- /bin/sh -x -c nslookup nodeport-service.services-3066.svc.cluster.local'
Sep  3 20:59:45.904: INFO: stderr: "+ nslookup nodeport-service.services-3066.svc.cluster.local\n"
Sep  3 20:59:45.904: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3066.svc.cluster.local\tcanonical name = externalsvc.services-3066.svc.cluster.local.\nName:\texternalsvc.services-3066.svc.cluster.local\nAddress: 10.96.34.3\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3066, will wait for the garbage collector to delete the pods 09/03/22 20:59:45.904
Sep  3 20:59:45.964: INFO: Deleting ReplicationController externalsvc took: 6.504907ms
Sep  3 20:59:46.064: INFO: Terminating ReplicationController externalsvc pods took: 100.839209ms
Sep  3 20:59:48.190: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 20:59:48.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3066" for this suite. 09/03/22 20:59:48.204
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":92,"skipped":1518,"failed":0}
------------------------------
• [SLOW TEST] [7.662 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:59:40.554
    Sep  3 20:59:40.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 20:59:40.555
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:59:40.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:59:40.568
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3066 09/03/22 20:59:40.571
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/03/22 20:59:40.582
    STEP: creating service externalsvc in namespace services-3066 09/03/22 20:59:40.582
    STEP: creating replication controller externalsvc in namespace services-3066 09/03/22 20:59:40.604
    I0903 20:59:40.616936      24 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3066, replica count: 2
    I0903 20:59:43.672567      24 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 09/03/22 20:59:43.674
    Sep  3 20:59:43.683: INFO: Creating new exec pod
    Sep  3 20:59:43.695: INFO: Waiting up to 5m0s for pod "execpodxhrhh" in namespace "services-3066" to be "running"
    Sep  3 20:59:43.700: INFO: Pod "execpodxhrhh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.638608ms
    Sep  3 20:59:45.705: INFO: Pod "execpodxhrhh": Phase="Running", Reason="", readiness=true. Elapsed: 2.009379892s
    Sep  3 20:59:45.705: INFO: Pod "execpodxhrhh" satisfied condition "running"
    Sep  3 20:59:45.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-3066 exec execpodxhrhh -- /bin/sh -x -c nslookup nodeport-service.services-3066.svc.cluster.local'
    Sep  3 20:59:45.904: INFO: stderr: "+ nslookup nodeport-service.services-3066.svc.cluster.local\n"
    Sep  3 20:59:45.904: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3066.svc.cluster.local\tcanonical name = externalsvc.services-3066.svc.cluster.local.\nName:\texternalsvc.services-3066.svc.cluster.local\nAddress: 10.96.34.3\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3066, will wait for the garbage collector to delete the pods 09/03/22 20:59:45.904
    Sep  3 20:59:45.964: INFO: Deleting ReplicationController externalsvc took: 6.504907ms
    Sep  3 20:59:46.064: INFO: Terminating ReplicationController externalsvc pods took: 100.839209ms
    Sep  3 20:59:48.190: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 20:59:48.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3066" for this suite. 09/03/22 20:59:48.204
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 20:59:48.219
Sep  3 20:59:48.219: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-probe 09/03/22 20:59:48.22
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:59:48.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:59:48.234
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d in namespace container-probe-7789 09/03/22 20:59:48.238
Sep  3 20:59:48.243: INFO: Waiting up to 5m0s for pod "liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d" in namespace "container-probe-7789" to be "not pending"
Sep  3 20:59:48.245: INFO: Pod "liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020202ms
Sep  3 20:59:50.249: INFO: Pod "liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005638363s
Sep  3 20:59:52.248: INFO: Pod "liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d": Phase="Running", Reason="", readiness=true. Elapsed: 4.004902319s
Sep  3 20:59:52.248: INFO: Pod "liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d" satisfied condition "not pending"
Sep  3 20:59:52.248: INFO: Started pod liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d in namespace container-probe-7789
STEP: checking the pod's current state and verifying that restartCount is present 09/03/22 20:59:52.248
Sep  3 20:59:52.250: INFO: Initial restart count of pod liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d is 0
STEP: deleting the pod 09/03/22 21:03:52.646
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Sep  3 21:03:52.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7789" for this suite. 09/03/22 21:03:52.672
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":93,"skipped":1531,"failed":0}
------------------------------
• [SLOW TEST] [244.458 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 20:59:48.219
    Sep  3 20:59:48.219: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-probe 09/03/22 20:59:48.22
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 20:59:48.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 20:59:48.234
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d in namespace container-probe-7789 09/03/22 20:59:48.238
    Sep  3 20:59:48.243: INFO: Waiting up to 5m0s for pod "liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d" in namespace "container-probe-7789" to be "not pending"
    Sep  3 20:59:48.245: INFO: Pod "liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020202ms
    Sep  3 20:59:50.249: INFO: Pod "liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005638363s
    Sep  3 20:59:52.248: INFO: Pod "liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d": Phase="Running", Reason="", readiness=true. Elapsed: 4.004902319s
    Sep  3 20:59:52.248: INFO: Pod "liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d" satisfied condition "not pending"
    Sep  3 20:59:52.248: INFO: Started pod liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d in namespace container-probe-7789
    STEP: checking the pod's current state and verifying that restartCount is present 09/03/22 20:59:52.248
    Sep  3 20:59:52.250: INFO: Initial restart count of pod liveness-e374357d-5f7a-4e37-bf82-50ce59ddbe5d is 0
    STEP: deleting the pod 09/03/22 21:03:52.646
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Sep  3 21:03:52.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7789" for this suite. 09/03/22 21:03:52.672
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:03:52.686
Sep  3 21:03:52.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-probe 09/03/22 21:03:52.687
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:03:52.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:03:52.708
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267 in namespace container-probe-3858 09/03/22 21:03:52.712
Sep  3 21:03:52.718: INFO: Waiting up to 5m0s for pod "test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267" in namespace "container-probe-3858" to be "not pending"
Sep  3 21:03:52.722: INFO: Pod "test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267": Phase="Pending", Reason="", readiness=false. Elapsed: 4.206408ms
Sep  3 21:03:54.726: INFO: Pod "test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267": Phase="Running", Reason="", readiness=true. Elapsed: 2.007850787s
Sep  3 21:03:54.726: INFO: Pod "test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267" satisfied condition "not pending"
Sep  3 21:03:54.726: INFO: Started pod test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267 in namespace container-probe-3858
STEP: checking the pod's current state and verifying that restartCount is present 09/03/22 21:03:54.726
Sep  3 21:03:54.728: INFO: Initial restart count of pod test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267 is 0
STEP: deleting the pod 09/03/22 21:07:55.11
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Sep  3 21:07:55.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3858" for this suite. 09/03/22 21:07:55.138
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":94,"skipped":1590,"failed":0}
------------------------------
• [SLOW TEST] [242.459 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:03:52.686
    Sep  3 21:03:52.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-probe 09/03/22 21:03:52.687
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:03:52.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:03:52.708
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267 in namespace container-probe-3858 09/03/22 21:03:52.712
    Sep  3 21:03:52.718: INFO: Waiting up to 5m0s for pod "test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267" in namespace "container-probe-3858" to be "not pending"
    Sep  3 21:03:52.722: INFO: Pod "test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267": Phase="Pending", Reason="", readiness=false. Elapsed: 4.206408ms
    Sep  3 21:03:54.726: INFO: Pod "test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267": Phase="Running", Reason="", readiness=true. Elapsed: 2.007850787s
    Sep  3 21:03:54.726: INFO: Pod "test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267" satisfied condition "not pending"
    Sep  3 21:03:54.726: INFO: Started pod test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267 in namespace container-probe-3858
    STEP: checking the pod's current state and verifying that restartCount is present 09/03/22 21:03:54.726
    Sep  3 21:03:54.728: INFO: Initial restart count of pod test-webserver-4a4697f8-c8cf-4ace-a4c8-5424b0acd267 is 0
    STEP: deleting the pod 09/03/22 21:07:55.11
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Sep  3 21:07:55.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3858" for this suite. 09/03/22 21:07:55.138
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:07:55.148
Sep  3 21:07:55.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 21:07:55.149
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:07:55.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:07:55.193
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-7598/secret-test-108cf660-ac4d-4862-827b-870dd0295392 09/03/22 21:07:55.197
STEP: Creating a pod to test consume secrets 09/03/22 21:07:55.202
Sep  3 21:07:55.216: INFO: Waiting up to 5m0s for pod "pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7" in namespace "secrets-7598" to be "Succeeded or Failed"
Sep  3 21:07:55.229: INFO: Pod "pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.44466ms
Sep  3 21:07:57.232: INFO: Pod "pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015966163s
Sep  3 21:07:59.232: INFO: Pod "pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015422847s
STEP: Saw pod success 09/03/22 21:07:59.232
Sep  3 21:07:59.232: INFO: Pod "pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7" satisfied condition "Succeeded or Failed"
Sep  3 21:07:59.233: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7 container env-test: <nil>
STEP: delete the pod 09/03/22 21:07:59.248
Sep  3 21:07:59.255: INFO: Waiting for pod pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7 to disappear
Sep  3 21:07:59.261: INFO: Pod pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Sep  3 21:07:59.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7598" for this suite. 09/03/22 21:07:59.264
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":95,"skipped":1603,"failed":0}
------------------------------
• [4.119 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:07:55.148
    Sep  3 21:07:55.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 21:07:55.149
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:07:55.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:07:55.193
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-7598/secret-test-108cf660-ac4d-4862-827b-870dd0295392 09/03/22 21:07:55.197
    STEP: Creating a pod to test consume secrets 09/03/22 21:07:55.202
    Sep  3 21:07:55.216: INFO: Waiting up to 5m0s for pod "pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7" in namespace "secrets-7598" to be "Succeeded or Failed"
    Sep  3 21:07:55.229: INFO: Pod "pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.44466ms
    Sep  3 21:07:57.232: INFO: Pod "pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015966163s
    Sep  3 21:07:59.232: INFO: Pod "pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015422847s
    STEP: Saw pod success 09/03/22 21:07:59.232
    Sep  3 21:07:59.232: INFO: Pod "pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7" satisfied condition "Succeeded or Failed"
    Sep  3 21:07:59.233: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7 container env-test: <nil>
    STEP: delete the pod 09/03/22 21:07:59.248
    Sep  3 21:07:59.255: INFO: Waiting for pod pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7 to disappear
    Sep  3 21:07:59.261: INFO: Pod pod-configmaps-5865f25d-5498-4211-acb5-3f67e4a030c7 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 21:07:59.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7598" for this suite. 09/03/22 21:07:59.264
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:07:59.268
Sep  3 21:07:59.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 21:07:59.269
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:07:59.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:07:59.28
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-5020/configmap-test-6f34ada3-8ebc-4f85-af7d-1e1f62ec1cf0 09/03/22 21:07:59.282
STEP: Creating a pod to test consume configMaps 09/03/22 21:07:59.284
Sep  3 21:07:59.288: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d" in namespace "configmap-5020" to be "Succeeded or Failed"
Sep  3 21:07:59.290: INFO: Pod "pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.691908ms
Sep  3 21:08:01.293: INFO: Pod "pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004476368s
Sep  3 21:08:03.293: INFO: Pod "pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004674062s
STEP: Saw pod success 09/03/22 21:08:03.293
Sep  3 21:08:03.293: INFO: Pod "pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d" satisfied condition "Succeeded or Failed"
Sep  3 21:08:03.296: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d container env-test: <nil>
STEP: delete the pod 09/03/22 21:08:03.304
Sep  3 21:08:03.309: INFO: Waiting for pod pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d to disappear
Sep  3 21:08:03.311: INFO: Pod pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 21:08:03.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5020" for this suite. 09/03/22 21:08:03.313
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":96,"skipped":1605,"failed":0}
------------------------------
• [4.050 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:07:59.268
    Sep  3 21:07:59.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 21:07:59.269
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:07:59.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:07:59.28
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-5020/configmap-test-6f34ada3-8ebc-4f85-af7d-1e1f62ec1cf0 09/03/22 21:07:59.282
    STEP: Creating a pod to test consume configMaps 09/03/22 21:07:59.284
    Sep  3 21:07:59.288: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d" in namespace "configmap-5020" to be "Succeeded or Failed"
    Sep  3 21:07:59.290: INFO: Pod "pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.691908ms
    Sep  3 21:08:01.293: INFO: Pod "pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004476368s
    Sep  3 21:08:03.293: INFO: Pod "pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004674062s
    STEP: Saw pod success 09/03/22 21:08:03.293
    Sep  3 21:08:03.293: INFO: Pod "pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d" satisfied condition "Succeeded or Failed"
    Sep  3 21:08:03.296: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d container env-test: <nil>
    STEP: delete the pod 09/03/22 21:08:03.304
    Sep  3 21:08:03.309: INFO: Waiting for pod pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d to disappear
    Sep  3 21:08:03.311: INFO: Pod pod-configmaps-e0f7f95a-f114-4dbc-925a-6ff3c526f78d no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 21:08:03.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5020" for this suite. 09/03/22 21:08:03.313
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:08:03.321
Sep  3 21:08:03.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 21:08:03.322
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:03.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:03.333
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-3f202eb2-1a90-4a4a-86d4-dfcea6b976e5 09/03/22 21:08:03.335
STEP: Creating a pod to test consume secrets 09/03/22 21:08:03.338
Sep  3 21:08:03.343: INFO: Waiting up to 5m0s for pod "pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f" in namespace "secrets-1596" to be "Succeeded or Failed"
Sep  3 21:08:03.347: INFO: Pod "pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.827418ms
Sep  3 21:08:05.350: INFO: Pod "pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006848225s
Sep  3 21:08:07.351: INFO: Pod "pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007552322s
STEP: Saw pod success 09/03/22 21:08:07.351
Sep  3 21:08:07.352: INFO: Pod "pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f" satisfied condition "Succeeded or Failed"
Sep  3 21:08:07.353: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f container secret-volume-test: <nil>
STEP: delete the pod 09/03/22 21:08:07.357
Sep  3 21:08:07.363: INFO: Waiting for pod pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f to disappear
Sep  3 21:08:07.365: INFO: Pod pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Sep  3 21:08:07.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1596" for this suite. 09/03/22 21:08:07.37
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":97,"skipped":1614,"failed":0}
------------------------------
• [4.052 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:08:03.321
    Sep  3 21:08:03.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 21:08:03.322
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:03.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:03.333
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-3f202eb2-1a90-4a4a-86d4-dfcea6b976e5 09/03/22 21:08:03.335
    STEP: Creating a pod to test consume secrets 09/03/22 21:08:03.338
    Sep  3 21:08:03.343: INFO: Waiting up to 5m0s for pod "pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f" in namespace "secrets-1596" to be "Succeeded or Failed"
    Sep  3 21:08:03.347: INFO: Pod "pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.827418ms
    Sep  3 21:08:05.350: INFO: Pod "pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006848225s
    Sep  3 21:08:07.351: INFO: Pod "pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007552322s
    STEP: Saw pod success 09/03/22 21:08:07.351
    Sep  3 21:08:07.352: INFO: Pod "pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f" satisfied condition "Succeeded or Failed"
    Sep  3 21:08:07.353: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f container secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 21:08:07.357
    Sep  3 21:08:07.363: INFO: Waiting for pod pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f to disappear
    Sep  3 21:08:07.365: INFO: Pod pod-secrets-8e5c4b1e-a7bb-435c-80a1-96f630a8a47f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 21:08:07.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1596" for this suite. 09/03/22 21:08:07.37
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:08:07.378
Sep  3 21:08:07.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename endpointslice 09/03/22 21:08:07.379
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:07.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:07.393
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 09/03/22 21:08:12.526
STEP: referencing matching pods with named port 09/03/22 21:08:17.531
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/03/22 21:08:22.537
STEP: recreating EndpointSlices after they've been deleted 09/03/22 21:08:27.543
Sep  3 21:08:27.561: INFO: EndpointSlice for Service endpointslice-4319/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Sep  3 21:08:37.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4319" for this suite. 09/03/22 21:08:37.571
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":98,"skipped":1703,"failed":0}
------------------------------
• [SLOW TEST] [30.197 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:08:07.378
    Sep  3 21:08:07.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename endpointslice 09/03/22 21:08:07.379
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:07.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:07.393
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 09/03/22 21:08:12.526
    STEP: referencing matching pods with named port 09/03/22 21:08:17.531
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/03/22 21:08:22.537
    STEP: recreating EndpointSlices after they've been deleted 09/03/22 21:08:27.543
    Sep  3 21:08:27.561: INFO: EndpointSlice for Service endpointslice-4319/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Sep  3 21:08:37.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4319" for this suite. 09/03/22 21:08:37.571
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:08:37.576
Sep  3 21:08:37.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-runtime 09/03/22 21:08:37.577
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:37.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:37.597
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 09/03/22 21:08:37.599
STEP: wait for the container to reach Succeeded 09/03/22 21:08:37.605
STEP: get the container status 09/03/22 21:08:40.617
STEP: the container should be terminated 09/03/22 21:08:40.619
STEP: the termination message should be set 09/03/22 21:08:40.619
Sep  3 21:08:40.619: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/03/22 21:08:40.619
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Sep  3 21:08:40.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1610" for this suite. 09/03/22 21:08:40.629
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":99,"skipped":1713,"failed":0}
------------------------------
• [3.057 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:08:37.576
    Sep  3 21:08:37.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-runtime 09/03/22 21:08:37.577
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:37.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:37.597
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 09/03/22 21:08:37.599
    STEP: wait for the container to reach Succeeded 09/03/22 21:08:37.605
    STEP: get the container status 09/03/22 21:08:40.617
    STEP: the container should be terminated 09/03/22 21:08:40.619
    STEP: the termination message should be set 09/03/22 21:08:40.619
    Sep  3 21:08:40.619: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/03/22 21:08:40.619
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Sep  3 21:08:40.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1610" for this suite. 09/03/22 21:08:40.629
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:08:40.639
Sep  3 21:08:40.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:08:40.64
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:40.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:40.65
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 09/03/22 21:08:40.652
Sep  3 21:08:40.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-6599 create -f -'
Sep  3 21:08:40.844: INFO: stderr: ""
Sep  3 21:08:40.844: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 09/03/22 21:08:40.844
Sep  3 21:08:40.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-6599 diff -f -'
Sep  3 21:08:41.115: INFO: rc: 1
Sep  3 21:08:41.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-6599 delete -f -'
Sep  3 21:08:41.193: INFO: stderr: ""
Sep  3 21:08:41.193: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:08:41.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6599" for this suite. 09/03/22 21:08:41.205
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":100,"skipped":1721,"failed":0}
------------------------------
• [0.576 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:08:40.639
    Sep  3 21:08:40.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:08:40.64
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:40.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:40.65
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 09/03/22 21:08:40.652
    Sep  3 21:08:40.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-6599 create -f -'
    Sep  3 21:08:40.844: INFO: stderr: ""
    Sep  3 21:08:40.844: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 09/03/22 21:08:40.844
    Sep  3 21:08:40.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-6599 diff -f -'
    Sep  3 21:08:41.115: INFO: rc: 1
    Sep  3 21:08:41.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-6599 delete -f -'
    Sep  3 21:08:41.193: INFO: stderr: ""
    Sep  3 21:08:41.193: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:08:41.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6599" for this suite. 09/03/22 21:08:41.205
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:08:41.216
Sep  3 21:08:41.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename disruption 09/03/22 21:08:41.218
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:41.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:41.256
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 09/03/22 21:08:41.267
STEP: Updating PodDisruptionBudget status 09/03/22 21:08:43.274
STEP: Waiting for all pods to be running 09/03/22 21:08:43.279
Sep  3 21:08:43.286: INFO: running pods: 0 < 1
STEP: locating a running pod 09/03/22 21:08:45.29
STEP: Waiting for the pdb to be processed 09/03/22 21:08:45.298
STEP: Patching PodDisruptionBudget status 09/03/22 21:08:45.309
STEP: Waiting for the pdb to be processed 09/03/22 21:08:45.316
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Sep  3 21:08:45.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6841" for this suite. 09/03/22 21:08:45.322
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":101,"skipped":1760,"failed":0}
------------------------------
• [4.114 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:08:41.216
    Sep  3 21:08:41.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename disruption 09/03/22 21:08:41.218
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:41.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:41.256
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 09/03/22 21:08:41.267
    STEP: Updating PodDisruptionBudget status 09/03/22 21:08:43.274
    STEP: Waiting for all pods to be running 09/03/22 21:08:43.279
    Sep  3 21:08:43.286: INFO: running pods: 0 < 1
    STEP: locating a running pod 09/03/22 21:08:45.29
    STEP: Waiting for the pdb to be processed 09/03/22 21:08:45.298
    STEP: Patching PodDisruptionBudget status 09/03/22 21:08:45.309
    STEP: Waiting for the pdb to be processed 09/03/22 21:08:45.316
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Sep  3 21:08:45.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6841" for this suite. 09/03/22 21:08:45.322
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:08:45.333
Sep  3 21:08:45.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename var-expansion 09/03/22 21:08:45.334
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:45.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:45.351
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Sep  3 21:08:45.366: INFO: Waiting up to 2m0s for pod "var-expansion-ecaa391b-2f94-4060-aaa0-e5b56127c730" in namespace "var-expansion-3656" to be "container 0 failed with reason CreateContainerConfigError"
Sep  3 21:08:45.368: INFO: Pod "var-expansion-ecaa391b-2f94-4060-aaa0-e5b56127c730": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054902ms
Sep  3 21:08:47.371: INFO: Pod "var-expansion-ecaa391b-2f94-4060-aaa0-e5b56127c730": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005346423s
Sep  3 21:08:47.371: INFO: Pod "var-expansion-ecaa391b-2f94-4060-aaa0-e5b56127c730" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  3 21:08:47.371: INFO: Deleting pod "var-expansion-ecaa391b-2f94-4060-aaa0-e5b56127c730" in namespace "var-expansion-3656"
Sep  3 21:08:47.375: INFO: Wait up to 5m0s for pod "var-expansion-ecaa391b-2f94-4060-aaa0-e5b56127c730" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Sep  3 21:08:49.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3656" for this suite. 09/03/22 21:08:49.385
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":102,"skipped":1761,"failed":0}
------------------------------
• [4.057 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:08:45.333
    Sep  3 21:08:45.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename var-expansion 09/03/22 21:08:45.334
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:45.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:45.351
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Sep  3 21:08:45.366: INFO: Waiting up to 2m0s for pod "var-expansion-ecaa391b-2f94-4060-aaa0-e5b56127c730" in namespace "var-expansion-3656" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  3 21:08:45.368: INFO: Pod "var-expansion-ecaa391b-2f94-4060-aaa0-e5b56127c730": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054902ms
    Sep  3 21:08:47.371: INFO: Pod "var-expansion-ecaa391b-2f94-4060-aaa0-e5b56127c730": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005346423s
    Sep  3 21:08:47.371: INFO: Pod "var-expansion-ecaa391b-2f94-4060-aaa0-e5b56127c730" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  3 21:08:47.371: INFO: Deleting pod "var-expansion-ecaa391b-2f94-4060-aaa0-e5b56127c730" in namespace "var-expansion-3656"
    Sep  3 21:08:47.375: INFO: Wait up to 5m0s for pod "var-expansion-ecaa391b-2f94-4060-aaa0-e5b56127c730" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Sep  3 21:08:49.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3656" for this suite. 09/03/22 21:08:49.385
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:08:49.391
Sep  3 21:08:49.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:08:49.392
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:49.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:49.411
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 09/03/22 21:08:49.415
Sep  3 21:08:49.422: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46" in namespace "projected-7870" to be "Succeeded or Failed"
Sep  3 21:08:49.429: INFO: Pod "downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46": Phase="Pending", Reason="", readiness=false. Elapsed: 6.217011ms
Sep  3 21:08:51.432: INFO: Pod "downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009881671s
Sep  3 21:08:53.436: INFO: Pod "downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013284731s
STEP: Saw pod success 09/03/22 21:08:53.436
Sep  3 21:08:53.436: INFO: Pod "downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46" satisfied condition "Succeeded or Failed"
Sep  3 21:08:53.441: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46 container client-container: <nil>
STEP: delete the pod 09/03/22 21:08:53.445
Sep  3 21:08:53.459: INFO: Waiting for pod downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46 to disappear
Sep  3 21:08:53.465: INFO: Pod downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Sep  3 21:08:53.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7870" for this suite. 09/03/22 21:08:53.467
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":103,"skipped":1802,"failed":0}
------------------------------
• [4.080 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:08:49.391
    Sep  3 21:08:49.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:08:49.392
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:49.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:49.411
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 09/03/22 21:08:49.415
    Sep  3 21:08:49.422: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46" in namespace "projected-7870" to be "Succeeded or Failed"
    Sep  3 21:08:49.429: INFO: Pod "downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46": Phase="Pending", Reason="", readiness=false. Elapsed: 6.217011ms
    Sep  3 21:08:51.432: INFO: Pod "downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009881671s
    Sep  3 21:08:53.436: INFO: Pod "downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013284731s
    STEP: Saw pod success 09/03/22 21:08:53.436
    Sep  3 21:08:53.436: INFO: Pod "downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46" satisfied condition "Succeeded or Failed"
    Sep  3 21:08:53.441: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46 container client-container: <nil>
    STEP: delete the pod 09/03/22 21:08:53.445
    Sep  3 21:08:53.459: INFO: Waiting for pod downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46 to disappear
    Sep  3 21:08:53.465: INFO: Pod downwardapi-volume-d2eb96cf-ab29-40ff-8043-325b51699c46 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Sep  3 21:08:53.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7870" for this suite. 09/03/22 21:08:53.467
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:08:53.481
Sep  3 21:08:53.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pods 09/03/22 21:08:53.482
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:53.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:53.518
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 09/03/22 21:08:53.521
STEP: setting up watch 09/03/22 21:08:53.521
STEP: submitting the pod to kubernetes 09/03/22 21:08:53.623
STEP: verifying the pod is in kubernetes 09/03/22 21:08:53.629
STEP: verifying pod creation was observed 09/03/22 21:08:53.633
Sep  3 21:08:53.633: INFO: Waiting up to 5m0s for pod "pod-submit-remove-e56aafa6-aa60-46e9-a32a-3d56fb2fb9fa" in namespace "pods-8699" to be "running"
Sep  3 21:08:53.649: INFO: Pod "pod-submit-remove-e56aafa6-aa60-46e9-a32a-3d56fb2fb9fa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.036029ms
Sep  3 21:08:55.653: INFO: Pod "pod-submit-remove-e56aafa6-aa60-46e9-a32a-3d56fb2fb9fa": Phase="Running", Reason="", readiness=true. Elapsed: 2.019572989s
Sep  3 21:08:55.653: INFO: Pod "pod-submit-remove-e56aafa6-aa60-46e9-a32a-3d56fb2fb9fa" satisfied condition "running"
STEP: deleting the pod gracefully 09/03/22 21:08:55.654
STEP: verifying pod deletion was observed 09/03/22 21:08:55.658
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Sep  3 21:08:57.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8699" for this suite. 09/03/22 21:08:57.633
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":104,"skipped":1841,"failed":0}
------------------------------
• [4.159 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:08:53.481
    Sep  3 21:08:53.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pods 09/03/22 21:08:53.482
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:53.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:53.518
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 09/03/22 21:08:53.521
    STEP: setting up watch 09/03/22 21:08:53.521
    STEP: submitting the pod to kubernetes 09/03/22 21:08:53.623
    STEP: verifying the pod is in kubernetes 09/03/22 21:08:53.629
    STEP: verifying pod creation was observed 09/03/22 21:08:53.633
    Sep  3 21:08:53.633: INFO: Waiting up to 5m0s for pod "pod-submit-remove-e56aafa6-aa60-46e9-a32a-3d56fb2fb9fa" in namespace "pods-8699" to be "running"
    Sep  3 21:08:53.649: INFO: Pod "pod-submit-remove-e56aafa6-aa60-46e9-a32a-3d56fb2fb9fa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.036029ms
    Sep  3 21:08:55.653: INFO: Pod "pod-submit-remove-e56aafa6-aa60-46e9-a32a-3d56fb2fb9fa": Phase="Running", Reason="", readiness=true. Elapsed: 2.019572989s
    Sep  3 21:08:55.653: INFO: Pod "pod-submit-remove-e56aafa6-aa60-46e9-a32a-3d56fb2fb9fa" satisfied condition "running"
    STEP: deleting the pod gracefully 09/03/22 21:08:55.654
    STEP: verifying pod deletion was observed 09/03/22 21:08:55.658
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Sep  3 21:08:57.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8699" for this suite. 09/03/22 21:08:57.633
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:08:57.643
Sep  3 21:08:57.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename daemonsets 09/03/22 21:08:57.644
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:57.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:57.654
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 09/03/22 21:08:57.668
STEP: Check that daemon pods launch on every node of the cluster. 09/03/22 21:08:57.67
Sep  3 21:08:57.673: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:08:57.676: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:08:57.676: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 21:08:58.680: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:08:58.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  3 21:08:58.682: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 21:08:59.681: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:08:59.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  3 21:08:59.683: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 09/03/22 21:08:59.684
Sep  3 21:08:59.687: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 09/03/22 21:08:59.687
Sep  3 21:08:59.693: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 09/03/22 21:08:59.693
Sep  3 21:08:59.695: INFO: Observed &DaemonSet event: ADDED
Sep  3 21:08:59.695: INFO: Observed &DaemonSet event: MODIFIED
Sep  3 21:08:59.696: INFO: Observed &DaemonSet event: MODIFIED
Sep  3 21:08:59.696: INFO: Observed &DaemonSet event: MODIFIED
Sep  3 21:08:59.696: INFO: Found daemon set daemon-set in namespace daemonsets-7011 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  3 21:08:59.696: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 09/03/22 21:08:59.696
STEP: watching for the daemon set status to be patched 09/03/22 21:08:59.701
Sep  3 21:08:59.704: INFO: Observed &DaemonSet event: ADDED
Sep  3 21:08:59.705: INFO: Observed &DaemonSet event: MODIFIED
Sep  3 21:08:59.705: INFO: Observed &DaemonSet event: MODIFIED
Sep  3 21:08:59.705: INFO: Observed &DaemonSet event: MODIFIED
Sep  3 21:08:59.705: INFO: Observed daemon set daemon-set in namespace daemonsets-7011 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  3 21:08:59.706: INFO: Observed &DaemonSet event: MODIFIED
Sep  3 21:08:59.706: INFO: Found daemon set daemon-set in namespace daemonsets-7011 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Sep  3 21:08:59.706: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 09/03/22 21:08:59.708
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7011, will wait for the garbage collector to delete the pods 09/03/22 21:08:59.708
Sep  3 21:08:59.764: INFO: Deleting DaemonSet.extensions daemon-set took: 3.790023ms
Sep  3 21:08:59.864: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.302307ms
Sep  3 21:09:02.167: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:09:02.167: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  3 21:09:02.169: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7971"},"items":null}

Sep  3 21:09:02.171: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7971"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:09:02.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7011" for this suite. 09/03/22 21:09:02.18
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":105,"skipped":1844,"failed":0}
------------------------------
• [4.540 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:08:57.643
    Sep  3 21:08:57.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename daemonsets 09/03/22 21:08:57.644
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:08:57.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:08:57.654
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 09/03/22 21:08:57.668
    STEP: Check that daemon pods launch on every node of the cluster. 09/03/22 21:08:57.67
    Sep  3 21:08:57.673: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:08:57.676: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:08:57.676: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 21:08:58.680: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:08:58.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  3 21:08:58.682: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 21:08:59.681: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:08:59.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  3 21:08:59.683: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 09/03/22 21:08:59.684
    Sep  3 21:08:59.687: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 09/03/22 21:08:59.687
    Sep  3 21:08:59.693: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 09/03/22 21:08:59.693
    Sep  3 21:08:59.695: INFO: Observed &DaemonSet event: ADDED
    Sep  3 21:08:59.695: INFO: Observed &DaemonSet event: MODIFIED
    Sep  3 21:08:59.696: INFO: Observed &DaemonSet event: MODIFIED
    Sep  3 21:08:59.696: INFO: Observed &DaemonSet event: MODIFIED
    Sep  3 21:08:59.696: INFO: Found daemon set daemon-set in namespace daemonsets-7011 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  3 21:08:59.696: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 09/03/22 21:08:59.696
    STEP: watching for the daemon set status to be patched 09/03/22 21:08:59.701
    Sep  3 21:08:59.704: INFO: Observed &DaemonSet event: ADDED
    Sep  3 21:08:59.705: INFO: Observed &DaemonSet event: MODIFIED
    Sep  3 21:08:59.705: INFO: Observed &DaemonSet event: MODIFIED
    Sep  3 21:08:59.705: INFO: Observed &DaemonSet event: MODIFIED
    Sep  3 21:08:59.705: INFO: Observed daemon set daemon-set in namespace daemonsets-7011 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  3 21:08:59.706: INFO: Observed &DaemonSet event: MODIFIED
    Sep  3 21:08:59.706: INFO: Found daemon set daemon-set in namespace daemonsets-7011 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Sep  3 21:08:59.706: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 09/03/22 21:08:59.708
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7011, will wait for the garbage collector to delete the pods 09/03/22 21:08:59.708
    Sep  3 21:08:59.764: INFO: Deleting DaemonSet.extensions daemon-set took: 3.790023ms
    Sep  3 21:08:59.864: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.302307ms
    Sep  3 21:09:02.167: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:09:02.167: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  3 21:09:02.169: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7971"},"items":null}

    Sep  3 21:09:02.171: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7971"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:09:02.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7011" for this suite. 09/03/22 21:09:02.18
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:09:02.183
Sep  3 21:09:02.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename custom-resource-definition 09/03/22 21:09:02.184
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:09:02.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:09:02.198
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Sep  3 21:09:02.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:09:03.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1658" for this suite. 09/03/22 21:09:03.22
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":106,"skipped":1844,"failed":0}
------------------------------
• [1.041 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:09:02.183
    Sep  3 21:09:02.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename custom-resource-definition 09/03/22 21:09:02.184
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:09:02.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:09:02.198
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Sep  3 21:09:02.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:09:03.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1658" for this suite. 09/03/22 21:09:03.22
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:09:03.233
Sep  3 21:09:03.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename resourcequota 09/03/22 21:09:03.234
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:09:03.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:09:03.244
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 09/03/22 21:09:03.247
STEP: Counting existing ResourceQuota 09/03/22 21:09:08.25
STEP: Creating a ResourceQuota 09/03/22 21:09:13.252
STEP: Ensuring resource quota status is calculated 09/03/22 21:09:13.256
STEP: Creating a Secret 09/03/22 21:09:15.261
STEP: Ensuring resource quota status captures secret creation 09/03/22 21:09:15.275
STEP: Deleting a secret 09/03/22 21:09:17.278
STEP: Ensuring resource quota status released usage 09/03/22 21:09:17.282
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Sep  3 21:09:19.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3424" for this suite. 09/03/22 21:09:19.287
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":107,"skipped":1867,"failed":0}
------------------------------
• [SLOW TEST] [16.058 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:09:03.233
    Sep  3 21:09:03.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename resourcequota 09/03/22 21:09:03.234
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:09:03.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:09:03.244
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 09/03/22 21:09:03.247
    STEP: Counting existing ResourceQuota 09/03/22 21:09:08.25
    STEP: Creating a ResourceQuota 09/03/22 21:09:13.252
    STEP: Ensuring resource quota status is calculated 09/03/22 21:09:13.256
    STEP: Creating a Secret 09/03/22 21:09:15.261
    STEP: Ensuring resource quota status captures secret creation 09/03/22 21:09:15.275
    STEP: Deleting a secret 09/03/22 21:09:17.278
    STEP: Ensuring resource quota status released usage 09/03/22 21:09:17.282
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Sep  3 21:09:19.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3424" for this suite. 09/03/22 21:09:19.287
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:09:19.292
Sep  3 21:09:19.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:09:19.294
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:09:19.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:09:19.316
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 09/03/22 21:09:19.321
Sep  3 21:09:19.331: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3" in namespace "projected-8076" to be "Succeeded or Failed"
Sep  3 21:09:19.335: INFO: Pod "downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.687415ms
Sep  3 21:09:21.342: INFO: Pod "downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010955222s
Sep  3 21:09:23.337: INFO: Pod "downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006196337s
STEP: Saw pod success 09/03/22 21:09:23.337
Sep  3 21:09:23.337: INFO: Pod "downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3" satisfied condition "Succeeded or Failed"
Sep  3 21:09:23.340: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3 container client-container: <nil>
STEP: delete the pod 09/03/22 21:09:23.344
Sep  3 21:09:23.350: INFO: Waiting for pod downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3 to disappear
Sep  3 21:09:23.352: INFO: Pod downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Sep  3 21:09:23.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8076" for this suite. 09/03/22 21:09:23.355
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":108,"skipped":1868,"failed":0}
------------------------------
• [4.066 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:09:19.292
    Sep  3 21:09:19.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:09:19.294
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:09:19.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:09:19.316
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 09/03/22 21:09:19.321
    Sep  3 21:09:19.331: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3" in namespace "projected-8076" to be "Succeeded or Failed"
    Sep  3 21:09:19.335: INFO: Pod "downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.687415ms
    Sep  3 21:09:21.342: INFO: Pod "downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010955222s
    Sep  3 21:09:23.337: INFO: Pod "downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006196337s
    STEP: Saw pod success 09/03/22 21:09:23.337
    Sep  3 21:09:23.337: INFO: Pod "downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3" satisfied condition "Succeeded or Failed"
    Sep  3 21:09:23.340: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3 container client-container: <nil>
    STEP: delete the pod 09/03/22 21:09:23.344
    Sep  3 21:09:23.350: INFO: Waiting for pod downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3 to disappear
    Sep  3 21:09:23.352: INFO: Pod downwardapi-volume-4d5985d5-c96e-417b-ac73-c4d9b0d591e3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Sep  3 21:09:23.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8076" for this suite. 09/03/22 21:09:23.355
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:09:23.365
Sep  3 21:09:23.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename dns 09/03/22 21:09:23.366
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:09:23.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:09:23.381
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 09/03/22 21:09:23.384
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5079.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local; sleep 1; done
 09/03/22 21:09:23.388
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5079.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local; sleep 1; done
 09/03/22 21:09:23.388
STEP: creating a pod to probe DNS 09/03/22 21:09:23.388
STEP: submitting the pod to kubernetes 09/03/22 21:09:23.388
Sep  3 21:09:23.394: INFO: Waiting up to 15m0s for pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd" in namespace "dns-5079" to be "running"
Sep  3 21:09:23.396: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.337807ms
Sep  3 21:09:25.406: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011985866s
Sep  3 21:09:27.401: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007045181s
Sep  3 21:09:29.401: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007151205s
Sep  3 21:09:31.399: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005824188s
Sep  3 21:09:33.399: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd": Phase="Running", Reason="", readiness=true. Elapsed: 10.005465073s
Sep  3 21:09:33.399: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd" satisfied condition "running"
STEP: retrieving the pod 09/03/22 21:09:33.399
STEP: looking for the results for each expected name from probers 09/03/22 21:09:33.403
Sep  3 21:09:33.408: INFO: DNS probes using dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd succeeded

STEP: deleting the pod 09/03/22 21:09:33.408
STEP: changing the externalName to bar.example.com 09/03/22 21:09:33.421
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5079.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local; sleep 1; done
 09/03/22 21:09:33.434
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5079.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local; sleep 1; done
 09/03/22 21:09:33.434
STEP: creating a second pod to probe DNS 09/03/22 21:09:33.434
STEP: submitting the pod to kubernetes 09/03/22 21:09:33.434
Sep  3 21:09:33.440: INFO: Waiting up to 15m0s for pod "dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9" in namespace "dns-5079" to be "running"
Sep  3 21:09:33.448: INFO: Pod "dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.776823ms
Sep  3 21:09:35.451: INFO: Pod "dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011324921s
Sep  3 21:09:37.458: INFO: Pod "dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9": Phase="Running", Reason="", readiness=true. Elapsed: 4.017724625s
Sep  3 21:09:37.458: INFO: Pod "dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9" satisfied condition "running"
STEP: retrieving the pod 09/03/22 21:09:37.458
STEP: looking for the results for each expected name from probers 09/03/22 21:09:37.463
Sep  3 21:09:37.466: INFO: File wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  3 21:09:37.468: INFO: File jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  3 21:09:37.468: INFO: Lookups using dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 failed for: [wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local]

Sep  3 21:09:42.472: INFO: File wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  3 21:09:42.475: INFO: File jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  3 21:09:42.475: INFO: Lookups using dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 failed for: [wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local]

Sep  3 21:09:47.474: INFO: File wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  3 21:09:47.476: INFO: File jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  3 21:09:47.476: INFO: Lookups using dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 failed for: [wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local]

Sep  3 21:09:52.471: INFO: File wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  3 21:09:52.474: INFO: File jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  3 21:09:52.474: INFO: Lookups using dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 failed for: [wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local]

Sep  3 21:09:57.476: INFO: DNS probes using dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 succeeded

STEP: deleting the pod 09/03/22 21:09:57.476
STEP: changing the service to type=ClusterIP 09/03/22 21:09:57.484
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5079.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local; sleep 1; done
 09/03/22 21:09:57.509
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5079.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local; sleep 1; done
 09/03/22 21:09:57.509
STEP: creating a third pod to probe DNS 09/03/22 21:09:57.51
STEP: submitting the pod to kubernetes 09/03/22 21:09:57.532
Sep  3 21:09:57.547: INFO: Waiting up to 15m0s for pod "dns-test-178b4324-1d52-434b-807c-0da727a9f9b9" in namespace "dns-5079" to be "running"
Sep  3 21:09:57.562: INFO: Pod "dns-test-178b4324-1d52-434b-807c-0da727a9f9b9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.124942ms
Sep  3 21:09:59.566: INFO: Pod "dns-test-178b4324-1d52-434b-807c-0da727a9f9b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.018442793s
Sep  3 21:09:59.566: INFO: Pod "dns-test-178b4324-1d52-434b-807c-0da727a9f9b9" satisfied condition "running"
STEP: retrieving the pod 09/03/22 21:09:59.566
STEP: looking for the results for each expected name from probers 09/03/22 21:09:59.568
Sep  3 21:09:59.578: INFO: DNS probes using dns-test-178b4324-1d52-434b-807c-0da727a9f9b9 succeeded

STEP: deleting the pod 09/03/22 21:09:59.578
STEP: deleting the test externalName service 09/03/22 21:09:59.587
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Sep  3 21:09:59.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5079" for this suite. 09/03/22 21:09:59.608
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":109,"skipped":1912,"failed":0}
------------------------------
• [SLOW TEST] [36.258 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:09:23.365
    Sep  3 21:09:23.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename dns 09/03/22 21:09:23.366
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:09:23.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:09:23.381
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 09/03/22 21:09:23.384
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5079.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local; sleep 1; done
     09/03/22 21:09:23.388
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5079.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local; sleep 1; done
     09/03/22 21:09:23.388
    STEP: creating a pod to probe DNS 09/03/22 21:09:23.388
    STEP: submitting the pod to kubernetes 09/03/22 21:09:23.388
    Sep  3 21:09:23.394: INFO: Waiting up to 15m0s for pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd" in namespace "dns-5079" to be "running"
    Sep  3 21:09:23.396: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.337807ms
    Sep  3 21:09:25.406: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011985866s
    Sep  3 21:09:27.401: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007045181s
    Sep  3 21:09:29.401: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007151205s
    Sep  3 21:09:31.399: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005824188s
    Sep  3 21:09:33.399: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd": Phase="Running", Reason="", readiness=true. Elapsed: 10.005465073s
    Sep  3 21:09:33.399: INFO: Pod "dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd" satisfied condition "running"
    STEP: retrieving the pod 09/03/22 21:09:33.399
    STEP: looking for the results for each expected name from probers 09/03/22 21:09:33.403
    Sep  3 21:09:33.408: INFO: DNS probes using dns-test-a11c9e4c-3fdc-448a-9b62-c6ca210a80bd succeeded

    STEP: deleting the pod 09/03/22 21:09:33.408
    STEP: changing the externalName to bar.example.com 09/03/22 21:09:33.421
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5079.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local; sleep 1; done
     09/03/22 21:09:33.434
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5079.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local; sleep 1; done
     09/03/22 21:09:33.434
    STEP: creating a second pod to probe DNS 09/03/22 21:09:33.434
    STEP: submitting the pod to kubernetes 09/03/22 21:09:33.434
    Sep  3 21:09:33.440: INFO: Waiting up to 15m0s for pod "dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9" in namespace "dns-5079" to be "running"
    Sep  3 21:09:33.448: INFO: Pod "dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.776823ms
    Sep  3 21:09:35.451: INFO: Pod "dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011324921s
    Sep  3 21:09:37.458: INFO: Pod "dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9": Phase="Running", Reason="", readiness=true. Elapsed: 4.017724625s
    Sep  3 21:09:37.458: INFO: Pod "dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9" satisfied condition "running"
    STEP: retrieving the pod 09/03/22 21:09:37.458
    STEP: looking for the results for each expected name from probers 09/03/22 21:09:37.463
    Sep  3 21:09:37.466: INFO: File wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  3 21:09:37.468: INFO: File jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  3 21:09:37.468: INFO: Lookups using dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 failed for: [wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local]

    Sep  3 21:09:42.472: INFO: File wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  3 21:09:42.475: INFO: File jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  3 21:09:42.475: INFO: Lookups using dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 failed for: [wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local]

    Sep  3 21:09:47.474: INFO: File wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  3 21:09:47.476: INFO: File jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  3 21:09:47.476: INFO: Lookups using dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 failed for: [wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local]

    Sep  3 21:09:52.471: INFO: File wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  3 21:09:52.474: INFO: File jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local from pod  dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  3 21:09:52.474: INFO: Lookups using dns-5079/dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 failed for: [wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local]

    Sep  3 21:09:57.476: INFO: DNS probes using dns-test-fbb47bc7-44ac-47c1-8681-004197ad7eb9 succeeded

    STEP: deleting the pod 09/03/22 21:09:57.476
    STEP: changing the service to type=ClusterIP 09/03/22 21:09:57.484
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5079.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5079.svc.cluster.local; sleep 1; done
     09/03/22 21:09:57.509
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5079.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5079.svc.cluster.local; sleep 1; done
     09/03/22 21:09:57.509
    STEP: creating a third pod to probe DNS 09/03/22 21:09:57.51
    STEP: submitting the pod to kubernetes 09/03/22 21:09:57.532
    Sep  3 21:09:57.547: INFO: Waiting up to 15m0s for pod "dns-test-178b4324-1d52-434b-807c-0da727a9f9b9" in namespace "dns-5079" to be "running"
    Sep  3 21:09:57.562: INFO: Pod "dns-test-178b4324-1d52-434b-807c-0da727a9f9b9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.124942ms
    Sep  3 21:09:59.566: INFO: Pod "dns-test-178b4324-1d52-434b-807c-0da727a9f9b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.018442793s
    Sep  3 21:09:59.566: INFO: Pod "dns-test-178b4324-1d52-434b-807c-0da727a9f9b9" satisfied condition "running"
    STEP: retrieving the pod 09/03/22 21:09:59.566
    STEP: looking for the results for each expected name from probers 09/03/22 21:09:59.568
    Sep  3 21:09:59.578: INFO: DNS probes using dns-test-178b4324-1d52-434b-807c-0da727a9f9b9 succeeded

    STEP: deleting the pod 09/03/22 21:09:59.578
    STEP: deleting the test externalName service 09/03/22 21:09:59.587
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Sep  3 21:09:59.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5079" for this suite. 09/03/22 21:09:59.608
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:09:59.633
Sep  3 21:09:59.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:09:59.638
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:09:59.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:09:59.661
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-51123b32-1425-4f36-b317-3d66176fe3e9 09/03/22 21:09:59.667
STEP: Creating a pod to test consume configMaps 09/03/22 21:09:59.675
Sep  3 21:09:59.687: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b" in namespace "projected-847" to be "Succeeded or Failed"
Sep  3 21:09:59.700: INFO: Pod "pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.085638ms
Sep  3 21:10:01.704: INFO: Pod "pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016495984s
Sep  3 21:10:03.705: INFO: Pod "pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017522118s
STEP: Saw pod success 09/03/22 21:10:03.705
Sep  3 21:10:03.705: INFO: Pod "pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b" satisfied condition "Succeeded or Failed"
Sep  3 21:10:03.708: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b container agnhost-container: <nil>
STEP: delete the pod 09/03/22 21:10:03.711
Sep  3 21:10:03.718: INFO: Waiting for pod pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b to disappear
Sep  3 21:10:03.721: INFO: Pod pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Sep  3 21:10:03.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-847" for this suite. 09/03/22 21:10:03.723
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":110,"skipped":1915,"failed":0}
------------------------------
• [4.097 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:09:59.633
    Sep  3 21:09:59.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:09:59.638
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:09:59.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:09:59.661
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-51123b32-1425-4f36-b317-3d66176fe3e9 09/03/22 21:09:59.667
    STEP: Creating a pod to test consume configMaps 09/03/22 21:09:59.675
    Sep  3 21:09:59.687: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b" in namespace "projected-847" to be "Succeeded or Failed"
    Sep  3 21:09:59.700: INFO: Pod "pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.085638ms
    Sep  3 21:10:01.704: INFO: Pod "pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016495984s
    Sep  3 21:10:03.705: INFO: Pod "pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017522118s
    STEP: Saw pod success 09/03/22 21:10:03.705
    Sep  3 21:10:03.705: INFO: Pod "pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b" satisfied condition "Succeeded or Failed"
    Sep  3 21:10:03.708: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 21:10:03.711
    Sep  3 21:10:03.718: INFO: Waiting for pod pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b to disappear
    Sep  3 21:10:03.721: INFO: Pod pod-projected-configmaps-41afc565-e8f2-4825-b63f-adc6a132ed2b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Sep  3 21:10:03.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-847" for this suite. 09/03/22 21:10:03.723
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:10:03.735
Sep  3 21:10:03.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 21:10:03.736
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:10:03.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:10:03.748
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-68a1fc27-502a-4b66-8d41-4811c0ff2779 09/03/22 21:10:03.75
STEP: Creating a pod to test consume configMaps 09/03/22 21:10:03.752
Sep  3 21:10:03.757: INFO: Waiting up to 5m0s for pod "pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda" in namespace "configmap-1646" to be "Succeeded or Failed"
Sep  3 21:10:03.760: INFO: Pod "pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.534708ms
Sep  3 21:10:05.763: INFO: Pod "pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005564747s
Sep  3 21:10:07.763: INFO: Pod "pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005448377s
STEP: Saw pod success 09/03/22 21:10:07.763
Sep  3 21:10:07.763: INFO: Pod "pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda" satisfied condition "Succeeded or Failed"
Sep  3 21:10:07.765: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda container agnhost-container: <nil>
STEP: delete the pod 09/03/22 21:10:07.769
Sep  3 21:10:07.775: INFO: Waiting for pod pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda to disappear
Sep  3 21:10:07.777: INFO: Pod pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 21:10:07.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1646" for this suite. 09/03/22 21:10:07.78
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":111,"skipped":1925,"failed":0}
------------------------------
• [4.048 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:10:03.735
    Sep  3 21:10:03.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 21:10:03.736
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:10:03.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:10:03.748
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-68a1fc27-502a-4b66-8d41-4811c0ff2779 09/03/22 21:10:03.75
    STEP: Creating a pod to test consume configMaps 09/03/22 21:10:03.752
    Sep  3 21:10:03.757: INFO: Waiting up to 5m0s for pod "pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda" in namespace "configmap-1646" to be "Succeeded or Failed"
    Sep  3 21:10:03.760: INFO: Pod "pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.534708ms
    Sep  3 21:10:05.763: INFO: Pod "pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005564747s
    Sep  3 21:10:07.763: INFO: Pod "pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005448377s
    STEP: Saw pod success 09/03/22 21:10:07.763
    Sep  3 21:10:07.763: INFO: Pod "pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda" satisfied condition "Succeeded or Failed"
    Sep  3 21:10:07.765: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 21:10:07.769
    Sep  3 21:10:07.775: INFO: Waiting for pod pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda to disappear
    Sep  3 21:10:07.777: INFO: Pod pod-configmaps-05b0ac5f-1722-4804-a6d2-592b07292bda no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 21:10:07.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1646" for this suite. 09/03/22 21:10:07.78
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:10:07.789
Sep  3 21:10:07.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 21:10:07.79
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:10:07.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:10:07.801
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 09/03/22 21:10:07.803
Sep  3 21:10:07.808: INFO: Waiting up to 5m0s for pod "pod-0a7287e6-b168-42f1-ad12-812301672fa5" in namespace "emptydir-175" to be "Succeeded or Failed"
Sep  3 21:10:07.810: INFO: Pod "pod-0a7287e6-b168-42f1-ad12-812301672fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.318707ms
Sep  3 21:10:09.815: INFO: Pod "pod-0a7287e6-b168-42f1-ad12-812301672fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007246955s
Sep  3 21:10:11.813: INFO: Pod "pod-0a7287e6-b168-42f1-ad12-812301672fa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005175288s
STEP: Saw pod success 09/03/22 21:10:11.813
Sep  3 21:10:11.814: INFO: Pod "pod-0a7287e6-b168-42f1-ad12-812301672fa5" satisfied condition "Succeeded or Failed"
Sep  3 21:10:11.816: INFO: Trying to get logs from node kind-worker2 pod pod-0a7287e6-b168-42f1-ad12-812301672fa5 container test-container: <nil>
STEP: delete the pod 09/03/22 21:10:11.829
Sep  3 21:10:11.837: INFO: Waiting for pod pod-0a7287e6-b168-42f1-ad12-812301672fa5 to disappear
Sep  3 21:10:11.838: INFO: Pod pod-0a7287e6-b168-42f1-ad12-812301672fa5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 21:10:11.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-175" for this suite. 09/03/22 21:10:11.849
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":112,"skipped":1945,"failed":0}
------------------------------
• [4.065 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:10:07.789
    Sep  3 21:10:07.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 21:10:07.79
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:10:07.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:10:07.801
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/03/22 21:10:07.803
    Sep  3 21:10:07.808: INFO: Waiting up to 5m0s for pod "pod-0a7287e6-b168-42f1-ad12-812301672fa5" in namespace "emptydir-175" to be "Succeeded or Failed"
    Sep  3 21:10:07.810: INFO: Pod "pod-0a7287e6-b168-42f1-ad12-812301672fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.318707ms
    Sep  3 21:10:09.815: INFO: Pod "pod-0a7287e6-b168-42f1-ad12-812301672fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007246955s
    Sep  3 21:10:11.813: INFO: Pod "pod-0a7287e6-b168-42f1-ad12-812301672fa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005175288s
    STEP: Saw pod success 09/03/22 21:10:11.813
    Sep  3 21:10:11.814: INFO: Pod "pod-0a7287e6-b168-42f1-ad12-812301672fa5" satisfied condition "Succeeded or Failed"
    Sep  3 21:10:11.816: INFO: Trying to get logs from node kind-worker2 pod pod-0a7287e6-b168-42f1-ad12-812301672fa5 container test-container: <nil>
    STEP: delete the pod 09/03/22 21:10:11.829
    Sep  3 21:10:11.837: INFO: Waiting for pod pod-0a7287e6-b168-42f1-ad12-812301672fa5 to disappear
    Sep  3 21:10:11.838: INFO: Pod pod-0a7287e6-b168-42f1-ad12-812301672fa5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 21:10:11.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-175" for this suite. 09/03/22 21:10:11.849
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:10:11.861
Sep  3 21:10:11.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename taint-single-pod 09/03/22 21:10:11.862
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:10:11.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:10:11.872
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Sep  3 21:10:11.876: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  3 21:11:11.889: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Sep  3 21:11:11.891: INFO: Starting informer...
STEP: Starting pod... 09/03/22 21:11:11.891
Sep  3 21:11:12.103: INFO: Pod is running on kind-worker2. Tainting Node
STEP: Trying to apply a taint on the Node 09/03/22 21:11:12.103
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/03/22 21:11:12.111
STEP: Waiting short time to make sure Pod is queued for deletion 09/03/22 21:11:12.114
Sep  3 21:11:12.114: INFO: Pod wasn't evicted. Proceeding
Sep  3 21:11:12.114: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/03/22 21:11:12.121
STEP: Waiting some time to make sure that toleration time passed. 09/03/22 21:11:12.124
Sep  3 21:12:27.126: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:12:27.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7924" for this suite. 09/03/22 21:12:27.129
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":113,"skipped":1984,"failed":0}
------------------------------
• [SLOW TEST] [135.272 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:10:11.861
    Sep  3 21:10:11.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename taint-single-pod 09/03/22 21:10:11.862
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:10:11.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:10:11.872
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Sep  3 21:10:11.876: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  3 21:11:11.889: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Sep  3 21:11:11.891: INFO: Starting informer...
    STEP: Starting pod... 09/03/22 21:11:11.891
    Sep  3 21:11:12.103: INFO: Pod is running on kind-worker2. Tainting Node
    STEP: Trying to apply a taint on the Node 09/03/22 21:11:12.103
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/03/22 21:11:12.111
    STEP: Waiting short time to make sure Pod is queued for deletion 09/03/22 21:11:12.114
    Sep  3 21:11:12.114: INFO: Pod wasn't evicted. Proceeding
    Sep  3 21:11:12.114: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/03/22 21:11:12.121
    STEP: Waiting some time to make sure that toleration time passed. 09/03/22 21:11:12.124
    Sep  3 21:12:27.126: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:12:27.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-7924" for this suite. 09/03/22 21:12:27.129
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:12:27.133
Sep  3 21:12:27.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename watch 09/03/22 21:12:27.134
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:12:27.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:12:27.144
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 09/03/22 21:12:27.146
STEP: creating a new configmap 09/03/22 21:12:27.147
STEP: modifying the configmap once 09/03/22 21:12:27.15
STEP: changing the label value of the configmap 09/03/22 21:12:27.154
STEP: Expecting to observe a delete notification for the watched object 09/03/22 21:12:27.158
Sep  3 21:12:27.158: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4631  5ae64946-efd1-42f1-b2d5-d9afe27be253 8544 0 2022-09-03 21:12:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-09-03 21:12:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:12:27.158: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4631  5ae64946-efd1-42f1-b2d5-d9afe27be253 8545 0 2022-09-03 21:12:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-09-03 21:12:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:12:27.158: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4631  5ae64946-efd1-42f1-b2d5-d9afe27be253 8546 0 2022-09-03 21:12:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-09-03 21:12:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 09/03/22 21:12:27.159
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/03/22 21:12:27.164
STEP: changing the label value of the configmap back 09/03/22 21:12:37.164
STEP: modifying the configmap a third time 09/03/22 21:12:37.173
STEP: deleting the configmap 09/03/22 21:12:37.177
STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/03/22 21:12:37.181
Sep  3 21:12:37.181: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4631  5ae64946-efd1-42f1-b2d5-d9afe27be253 8575 0 2022-09-03 21:12:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-09-03 21:12:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:12:37.181: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4631  5ae64946-efd1-42f1-b2d5-d9afe27be253 8576 0 2022-09-03 21:12:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-09-03 21:12:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:12:37.181: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4631  5ae64946-efd1-42f1-b2d5-d9afe27be253 8577 0 2022-09-03 21:12:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-09-03 21:12:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Sep  3 21:12:37.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4631" for this suite. 09/03/22 21:12:37.184
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":114,"skipped":1987,"failed":0}
------------------------------
• [SLOW TEST] [10.055 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:12:27.133
    Sep  3 21:12:27.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename watch 09/03/22 21:12:27.134
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:12:27.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:12:27.144
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 09/03/22 21:12:27.146
    STEP: creating a new configmap 09/03/22 21:12:27.147
    STEP: modifying the configmap once 09/03/22 21:12:27.15
    STEP: changing the label value of the configmap 09/03/22 21:12:27.154
    STEP: Expecting to observe a delete notification for the watched object 09/03/22 21:12:27.158
    Sep  3 21:12:27.158: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4631  5ae64946-efd1-42f1-b2d5-d9afe27be253 8544 0 2022-09-03 21:12:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-09-03 21:12:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:12:27.158: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4631  5ae64946-efd1-42f1-b2d5-d9afe27be253 8545 0 2022-09-03 21:12:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-09-03 21:12:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:12:27.158: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4631  5ae64946-efd1-42f1-b2d5-d9afe27be253 8546 0 2022-09-03 21:12:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-09-03 21:12:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 09/03/22 21:12:27.159
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/03/22 21:12:27.164
    STEP: changing the label value of the configmap back 09/03/22 21:12:37.164
    STEP: modifying the configmap a third time 09/03/22 21:12:37.173
    STEP: deleting the configmap 09/03/22 21:12:37.177
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/03/22 21:12:37.181
    Sep  3 21:12:37.181: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4631  5ae64946-efd1-42f1-b2d5-d9afe27be253 8575 0 2022-09-03 21:12:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-09-03 21:12:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:12:37.181: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4631  5ae64946-efd1-42f1-b2d5-d9afe27be253 8576 0 2022-09-03 21:12:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-09-03 21:12:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:12:37.181: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4631  5ae64946-efd1-42f1-b2d5-d9afe27be253 8577 0 2022-09-03 21:12:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-09-03 21:12:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Sep  3 21:12:37.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4631" for this suite. 09/03/22 21:12:37.184
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:12:37.2
Sep  3 21:12:37.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 21:12:37.201
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:12:37.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:12:37.214
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 21:12:37.228
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:12:37.989
STEP: Deploying the webhook pod 09/03/22 21:12:37.994
STEP: Wait for the deployment to be ready 09/03/22 21:12:37.999
Sep  3 21:12:38.015: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 21:12:40.022
STEP: Verifying the service has paired with the endpoint 09/03/22 21:12:40.037
Sep  3 21:12:41.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 09/03/22 21:12:41.041
STEP: create a pod that should be denied by the webhook 09/03/22 21:12:41.055
STEP: create a pod that causes the webhook to hang 09/03/22 21:12:41.065
STEP: create a configmap that should be denied by the webhook 09/03/22 21:12:51.07
STEP: create a configmap that should be admitted by the webhook 09/03/22 21:12:51.098
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/03/22 21:12:51.108
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/03/22 21:12:51.113
STEP: create a namespace that bypass the webhook 09/03/22 21:12:51.117
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/03/22 21:12:51.121
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:12:51.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6936" for this suite. 09/03/22 21:12:51.145
STEP: Destroying namespace "webhook-6936-markers" for this suite. 09/03/22 21:12:51.148
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":115,"skipped":1995,"failed":0}
------------------------------
• [SLOW TEST] [14.071 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:12:37.2
    Sep  3 21:12:37.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 21:12:37.201
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:12:37.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:12:37.214
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 21:12:37.228
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:12:37.989
    STEP: Deploying the webhook pod 09/03/22 21:12:37.994
    STEP: Wait for the deployment to be ready 09/03/22 21:12:37.999
    Sep  3 21:12:38.015: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 21:12:40.022
    STEP: Verifying the service has paired with the endpoint 09/03/22 21:12:40.037
    Sep  3 21:12:41.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 09/03/22 21:12:41.041
    STEP: create a pod that should be denied by the webhook 09/03/22 21:12:41.055
    STEP: create a pod that causes the webhook to hang 09/03/22 21:12:41.065
    STEP: create a configmap that should be denied by the webhook 09/03/22 21:12:51.07
    STEP: create a configmap that should be admitted by the webhook 09/03/22 21:12:51.098
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/03/22 21:12:51.108
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/03/22 21:12:51.113
    STEP: create a namespace that bypass the webhook 09/03/22 21:12:51.117
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/03/22 21:12:51.121
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:12:51.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6936" for this suite. 09/03/22 21:12:51.145
    STEP: Destroying namespace "webhook-6936-markers" for this suite. 09/03/22 21:12:51.148
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:12:51.272
Sep  3 21:12:51.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename deployment 09/03/22 21:12:51.273
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:12:51.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:12:51.317
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Sep  3 21:12:51.356: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  3 21:12:56.360: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/03/22 21:12:56.36
Sep  3 21:12:56.360: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  3 21:12:58.363: INFO: Creating deployment "test-rollover-deployment"
Sep  3 21:12:58.370: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  3 21:13:00.375: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  3 21:13:00.379: INFO: Ensure that both replica sets have 1 created replica
Sep  3 21:13:00.383: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  3 21:13:00.391: INFO: Updating deployment test-rollover-deployment
Sep  3 21:13:00.391: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  3 21:13:02.397: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  3 21:13:02.401: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  3 21:13:02.405: INFO: all replica sets need to contain the pod-template-hash label
Sep  3 21:13:02.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:13:04.410: INFO: all replica sets need to contain the pod-template-hash label
Sep  3 21:13:04.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:13:06.412: INFO: all replica sets need to contain the pod-template-hash label
Sep  3 21:13:06.412: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:13:08.410: INFO: all replica sets need to contain the pod-template-hash label
Sep  3 21:13:08.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:13:10.409: INFO: all replica sets need to contain the pod-template-hash label
Sep  3 21:13:10.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:13:12.410: INFO: 
Sep  3 21:13:12.410: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  3 21:13:12.416: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6951  62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf 8784 2 2022-09-03 21:12:58 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-09-03 21:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:13:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036ffec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-09-03 21:12:58 +0000 UTC,LastTransitionTime:2022-09-03 21:12:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2022-09-03 21:13:11 +0000 UTC,LastTransitionTime:2022-09-03 21:12:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  3 21:13:12.419: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-6951  b87d485b-6985-41ec-9e2d-924dce5dfae1 8774 2 2022-09-03 21:13:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf 0xc003a8c497 0xc003a8c498}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:13:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a8c548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:13:12.419: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  3 21:13:12.419: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6951  1feb8b21-6529-4b9e-b4a9-c722b983865c 8783 2 2022-09-03 21:12:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf 0xc003a8c25f 0xc003a8c270}] [] [{e2e.test Update apps/v1 2022-09-03 21:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:13:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:13:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a8c328 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:13:12.419: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-6951  809f4931-9e30-46fc-886c-92df509ba43b 8746 2 2022-09-03 21:12:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf 0xc003a8c387 0xc003a8c388}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:13:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a8c438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:13:12.423: INFO: Pod "test-rollover-deployment-6d45fd857b-nlb8n" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-nlb8n test-rollover-deployment-6d45fd857b- deployment-6951  436b120e-5b3a-41f5-9e6f-82d70ec25412 8757 0 2022-09-03 21:13:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b b87d485b-6985-41ec-9e2d-924dce5dfae1 0xc002e80057 0xc002e80058}] [] [{kube-controller-manager Update v1 2022-09-03 21:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b87d485b-6985-41ec-9e2d-924dce5dfae1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:13:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlvxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlvxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:13:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:13:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:13:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:13:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.123,StartTime:2022-09-03 21:13:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:13:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://6cde3c887dcfee55b2ce1d84da2aa8e027665ee250dd1568dcaf9a61ebb734d9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Sep  3 21:13:12.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6951" for this suite. 09/03/22 21:13:12.425
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":116,"skipped":1998,"failed":0}
------------------------------
• [SLOW TEST] [21.157 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:12:51.272
    Sep  3 21:12:51.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename deployment 09/03/22 21:12:51.273
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:12:51.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:12:51.317
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Sep  3 21:12:51.356: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Sep  3 21:12:56.360: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/03/22 21:12:56.36
    Sep  3 21:12:56.360: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Sep  3 21:12:58.363: INFO: Creating deployment "test-rollover-deployment"
    Sep  3 21:12:58.370: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Sep  3 21:13:00.375: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Sep  3 21:13:00.379: INFO: Ensure that both replica sets have 1 created replica
    Sep  3 21:13:00.383: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Sep  3 21:13:00.391: INFO: Updating deployment test-rollover-deployment
    Sep  3 21:13:00.391: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Sep  3 21:13:02.397: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Sep  3 21:13:02.401: INFO: Make sure deployment "test-rollover-deployment" is complete
    Sep  3 21:13:02.405: INFO: all replica sets need to contain the pod-template-hash label
    Sep  3 21:13:02.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:13:04.410: INFO: all replica sets need to contain the pod-template-hash label
    Sep  3 21:13:04.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:13:06.412: INFO: all replica sets need to contain the pod-template-hash label
    Sep  3 21:13:06.412: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:13:08.410: INFO: all replica sets need to contain the pod-template-hash label
    Sep  3 21:13:08.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:13:10.409: INFO: all replica sets need to contain the pod-template-hash label
    Sep  3 21:13:10.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 12, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:13:12.410: INFO: 
    Sep  3 21:13:12.410: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  3 21:13:12.416: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6951  62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf 8784 2 2022-09-03 21:12:58 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-09-03 21:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:13:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036ffec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-09-03 21:12:58 +0000 UTC,LastTransitionTime:2022-09-03 21:12:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2022-09-03 21:13:11 +0000 UTC,LastTransitionTime:2022-09-03 21:12:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  3 21:13:12.419: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-6951  b87d485b-6985-41ec-9e2d-924dce5dfae1 8774 2 2022-09-03 21:13:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf 0xc003a8c497 0xc003a8c498}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:13:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a8c548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:13:12.419: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Sep  3 21:13:12.419: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6951  1feb8b21-6529-4b9e-b4a9-c722b983865c 8783 2 2022-09-03 21:12:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf 0xc003a8c25f 0xc003a8c270}] [] [{e2e.test Update apps/v1 2022-09-03 21:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:13:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:13:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a8c328 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:13:12.419: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-6951  809f4931-9e30-46fc-886c-92df509ba43b 8746 2 2022-09-03 21:12:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf 0xc003a8c387 0xc003a8c388}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62c4cd29-2d13-41d4-b6d6-b8d4d663c8bf\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:13:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a8c438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:13:12.423: INFO: Pod "test-rollover-deployment-6d45fd857b-nlb8n" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-nlb8n test-rollover-deployment-6d45fd857b- deployment-6951  436b120e-5b3a-41f5-9e6f-82d70ec25412 8757 0 2022-09-03 21:13:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b b87d485b-6985-41ec-9e2d-924dce5dfae1 0xc002e80057 0xc002e80058}] [] [{kube-controller-manager Update v1 2022-09-03 21:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b87d485b-6985-41ec-9e2d-924dce5dfae1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:13:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlvxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlvxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:13:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:13:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:13:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:13:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.123,StartTime:2022-09-03 21:13:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:13:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://6cde3c887dcfee55b2ce1d84da2aa8e027665ee250dd1568dcaf9a61ebb734d9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Sep  3 21:13:12.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6951" for this suite. 09/03/22 21:13:12.425
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:13:12.435
Sep  3 21:13:12.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 21:13:12.436
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:12.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:12.448
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/03/22 21:13:12.45
Sep  3 21:13:12.454: INFO: Waiting up to 5m0s for pod "pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f" in namespace "emptydir-9496" to be "Succeeded or Failed"
Sep  3 21:13:12.457: INFO: Pod "pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.549906ms
Sep  3 21:13:14.460: INFO: Pod "pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005888462s
Sep  3 21:13:16.460: INFO: Pod "pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005958434s
STEP: Saw pod success 09/03/22 21:13:16.46
Sep  3 21:13:16.461: INFO: Pod "pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f" satisfied condition "Succeeded or Failed"
Sep  3 21:13:16.462: INFO: Trying to get logs from node kind-worker2 pod pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f container test-container: <nil>
STEP: delete the pod 09/03/22 21:13:16.477
Sep  3 21:13:16.486: INFO: Waiting for pod pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f to disappear
Sep  3 21:13:16.487: INFO: Pod pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 21:13:16.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9496" for this suite. 09/03/22 21:13:16.49
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":117,"skipped":2010,"failed":0}
------------------------------
• [4.058 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:13:12.435
    Sep  3 21:13:12.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 21:13:12.436
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:12.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:12.448
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/03/22 21:13:12.45
    Sep  3 21:13:12.454: INFO: Waiting up to 5m0s for pod "pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f" in namespace "emptydir-9496" to be "Succeeded or Failed"
    Sep  3 21:13:12.457: INFO: Pod "pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.549906ms
    Sep  3 21:13:14.460: INFO: Pod "pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005888462s
    Sep  3 21:13:16.460: INFO: Pod "pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005958434s
    STEP: Saw pod success 09/03/22 21:13:16.46
    Sep  3 21:13:16.461: INFO: Pod "pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f" satisfied condition "Succeeded or Failed"
    Sep  3 21:13:16.462: INFO: Trying to get logs from node kind-worker2 pod pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f container test-container: <nil>
    STEP: delete the pod 09/03/22 21:13:16.477
    Sep  3 21:13:16.486: INFO: Waiting for pod pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f to disappear
    Sep  3 21:13:16.487: INFO: Pod pod-ea7598ec-e827-45e4-ad58-b77e0fd3873f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 21:13:16.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9496" for this suite. 09/03/22 21:13:16.49
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:13:16.507
Sep  3 21:13:16.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubelet-test 09/03/22 21:13:16.508
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:16.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:16.521
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Sep  3 21:13:16.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5341" for this suite. 09/03/22 21:13:16.546
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":118,"skipped":2078,"failed":0}
------------------------------
• [0.044 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:13:16.507
    Sep  3 21:13:16.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubelet-test 09/03/22 21:13:16.508
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:16.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:16.521
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Sep  3 21:13:16.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5341" for this suite. 09/03/22 21:13:16.546
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:13:16.551
Sep  3 21:13:16.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 21:13:16.552
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:16.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:16.586
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 09/03/22 21:13:16.589
Sep  3 21:13:16.597: INFO: Waiting up to 5m0s for pod "pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f" in namespace "emptydir-917" to be "Succeeded or Failed"
Sep  3 21:13:16.604: INFO: Pod "pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.431093ms
Sep  3 21:13:18.607: INFO: Pod "pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009637962s
Sep  3 21:13:20.607: INFO: Pod "pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009813662s
Sep  3 21:13:22.607: INFO: Pod "pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009492588s
STEP: Saw pod success 09/03/22 21:13:22.607
Sep  3 21:13:22.607: INFO: Pod "pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f" satisfied condition "Succeeded or Failed"
Sep  3 21:13:22.609: INFO: Trying to get logs from node kind-worker2 pod pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f container test-container: <nil>
STEP: delete the pod 09/03/22 21:13:22.612
Sep  3 21:13:22.618: INFO: Waiting for pod pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f to disappear
Sep  3 21:13:22.619: INFO: Pod pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 21:13:22.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-917" for this suite. 09/03/22 21:13:22.622
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":119,"skipped":2078,"failed":0}
------------------------------
• [SLOW TEST] [6.073 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:13:16.551
    Sep  3 21:13:16.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 21:13:16.552
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:16.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:16.586
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/03/22 21:13:16.589
    Sep  3 21:13:16.597: INFO: Waiting up to 5m0s for pod "pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f" in namespace "emptydir-917" to be "Succeeded or Failed"
    Sep  3 21:13:16.604: INFO: Pod "pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.431093ms
    Sep  3 21:13:18.607: INFO: Pod "pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009637962s
    Sep  3 21:13:20.607: INFO: Pod "pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009813662s
    Sep  3 21:13:22.607: INFO: Pod "pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009492588s
    STEP: Saw pod success 09/03/22 21:13:22.607
    Sep  3 21:13:22.607: INFO: Pod "pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f" satisfied condition "Succeeded or Failed"
    Sep  3 21:13:22.609: INFO: Trying to get logs from node kind-worker2 pod pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f container test-container: <nil>
    STEP: delete the pod 09/03/22 21:13:22.612
    Sep  3 21:13:22.618: INFO: Waiting for pod pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f to disappear
    Sep  3 21:13:22.619: INFO: Pod pod-de5b44f3-b624-4bbe-98a2-ecc94be47a4f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 21:13:22.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-917" for this suite. 09/03/22 21:13:22.622
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:13:22.631
Sep  3 21:13:22.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 21:13:22.633
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:22.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:22.645
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 09/03/22 21:13:22.648
Sep  3 21:13:22.653: INFO: Waiting up to 5m0s for pod "downward-api-913f3190-46a3-48a0-9b02-f456cff2deea" in namespace "downward-api-3966" to be "Succeeded or Failed"
Sep  3 21:13:22.654: INFO: Pod "downward-api-913f3190-46a3-48a0-9b02-f456cff2deea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.396804ms
Sep  3 21:13:24.657: INFO: Pod "downward-api-913f3190-46a3-48a0-9b02-f456cff2deea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004182073s
Sep  3 21:13:26.656: INFO: Pod "downward-api-913f3190-46a3-48a0-9b02-f456cff2deea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003710635s
STEP: Saw pod success 09/03/22 21:13:26.657
Sep  3 21:13:26.657: INFO: Pod "downward-api-913f3190-46a3-48a0-9b02-f456cff2deea" satisfied condition "Succeeded or Failed"
Sep  3 21:13:26.659: INFO: Trying to get logs from node kind-worker2 pod downward-api-913f3190-46a3-48a0-9b02-f456cff2deea container dapi-container: <nil>
STEP: delete the pod 09/03/22 21:13:26.663
Sep  3 21:13:26.669: INFO: Waiting for pod downward-api-913f3190-46a3-48a0-9b02-f456cff2deea to disappear
Sep  3 21:13:26.671: INFO: Pod downward-api-913f3190-46a3-48a0-9b02-f456cff2deea no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Sep  3 21:13:26.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3966" for this suite. 09/03/22 21:13:26.673
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":120,"skipped":2090,"failed":0}
------------------------------
• [4.045 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:13:22.631
    Sep  3 21:13:22.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 21:13:22.633
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:22.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:22.645
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 09/03/22 21:13:22.648
    Sep  3 21:13:22.653: INFO: Waiting up to 5m0s for pod "downward-api-913f3190-46a3-48a0-9b02-f456cff2deea" in namespace "downward-api-3966" to be "Succeeded or Failed"
    Sep  3 21:13:22.654: INFO: Pod "downward-api-913f3190-46a3-48a0-9b02-f456cff2deea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.396804ms
    Sep  3 21:13:24.657: INFO: Pod "downward-api-913f3190-46a3-48a0-9b02-f456cff2deea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004182073s
    Sep  3 21:13:26.656: INFO: Pod "downward-api-913f3190-46a3-48a0-9b02-f456cff2deea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003710635s
    STEP: Saw pod success 09/03/22 21:13:26.657
    Sep  3 21:13:26.657: INFO: Pod "downward-api-913f3190-46a3-48a0-9b02-f456cff2deea" satisfied condition "Succeeded or Failed"
    Sep  3 21:13:26.659: INFO: Trying to get logs from node kind-worker2 pod downward-api-913f3190-46a3-48a0-9b02-f456cff2deea container dapi-container: <nil>
    STEP: delete the pod 09/03/22 21:13:26.663
    Sep  3 21:13:26.669: INFO: Waiting for pod downward-api-913f3190-46a3-48a0-9b02-f456cff2deea to disappear
    Sep  3 21:13:26.671: INFO: Pod downward-api-913f3190-46a3-48a0-9b02-f456cff2deea no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Sep  3 21:13:26.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3966" for this suite. 09/03/22 21:13:26.673
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:13:26.677
Sep  3 21:13:26.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename replication-controller 09/03/22 21:13:26.678
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:26.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:26.689
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 09/03/22 21:13:26.691
STEP: When the matched label of one of its pods change 09/03/22 21:13:26.694
Sep  3 21:13:26.699: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  3 21:13:31.703: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 09/03/22 21:13:31.712
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Sep  3 21:13:31.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8978" for this suite. 09/03/22 21:13:31.727
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":121,"skipped":2090,"failed":0}
------------------------------
• [SLOW TEST] [5.070 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:13:26.677
    Sep  3 21:13:26.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename replication-controller 09/03/22 21:13:26.678
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:26.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:26.689
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 09/03/22 21:13:26.691
    STEP: When the matched label of one of its pods change 09/03/22 21:13:26.694
    Sep  3 21:13:26.699: INFO: Pod name pod-release: Found 0 pods out of 1
    Sep  3 21:13:31.703: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/03/22 21:13:31.712
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Sep  3 21:13:31.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8978" for this suite. 09/03/22 21:13:31.727
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:13:31.75
Sep  3 21:13:31.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename ingressclass 09/03/22 21:13:31.751
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:31.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:31.77
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 09/03/22 21:13:31.772
STEP: getting /apis/networking.k8s.io 09/03/22 21:13:31.774
STEP: getting /apis/networking.k8s.iov1 09/03/22 21:13:31.775
STEP: creating 09/03/22 21:13:31.776
STEP: getting 09/03/22 21:13:31.785
STEP: listing 09/03/22 21:13:31.787
STEP: watching 09/03/22 21:13:31.789
Sep  3 21:13:31.789: INFO: starting watch
STEP: patching 09/03/22 21:13:31.79
STEP: updating 09/03/22 21:13:31.795
Sep  3 21:13:31.799: INFO: waiting for watch events with expected annotations
Sep  3 21:13:31.799: INFO: saw patched and updated annotations
STEP: deleting 09/03/22 21:13:31.799
STEP: deleting a collection 09/03/22 21:13:31.806
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Sep  3 21:13:31.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7963" for this suite. 09/03/22 21:13:31.82
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":122,"skipped":2099,"failed":0}
------------------------------
• [0.075 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:13:31.75
    Sep  3 21:13:31.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename ingressclass 09/03/22 21:13:31.751
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:31.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:31.77
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 09/03/22 21:13:31.772
    STEP: getting /apis/networking.k8s.io 09/03/22 21:13:31.774
    STEP: getting /apis/networking.k8s.iov1 09/03/22 21:13:31.775
    STEP: creating 09/03/22 21:13:31.776
    STEP: getting 09/03/22 21:13:31.785
    STEP: listing 09/03/22 21:13:31.787
    STEP: watching 09/03/22 21:13:31.789
    Sep  3 21:13:31.789: INFO: starting watch
    STEP: patching 09/03/22 21:13:31.79
    STEP: updating 09/03/22 21:13:31.795
    Sep  3 21:13:31.799: INFO: waiting for watch events with expected annotations
    Sep  3 21:13:31.799: INFO: saw patched and updated annotations
    STEP: deleting 09/03/22 21:13:31.799
    STEP: deleting a collection 09/03/22 21:13:31.806
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Sep  3 21:13:31.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-7963" for this suite. 09/03/22 21:13:31.82
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:13:31.827
Sep  3 21:13:31.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:13:31.828
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:31.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:31.842
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 09/03/22 21:13:31.845
Sep  3 21:13:31.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5127 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Sep  3 21:13:31.945: INFO: stderr: ""
Sep  3 21:13:31.945: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 09/03/22 21:13:31.945
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Sep  3 21:13:31.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5127 delete pods e2e-test-httpd-pod'
Sep  3 21:13:34.431: INFO: stderr: ""
Sep  3 21:13:34.431: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:13:34.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5127" for this suite. 09/03/22 21:13:34.433
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":123,"skipped":2103,"failed":0}
------------------------------
• [2.610 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:13:31.827
    Sep  3 21:13:31.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:13:31.828
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:31.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:31.842
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 09/03/22 21:13:31.845
    Sep  3 21:13:31.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5127 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Sep  3 21:13:31.945: INFO: stderr: ""
    Sep  3 21:13:31.945: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 09/03/22 21:13:31.945
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Sep  3 21:13:31.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5127 delete pods e2e-test-httpd-pod'
    Sep  3 21:13:34.431: INFO: stderr: ""
    Sep  3 21:13:34.431: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:13:34.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5127" for this suite. 09/03/22 21:13:34.433
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:13:34.437
Sep  3 21:13:34.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:13:34.438
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:34.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:34.45
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-972d7b39-e9c4-44fd-8d9e-36bef57a554c 09/03/22 21:13:34.452
STEP: Creating a pod to test consume configMaps 09/03/22 21:13:34.454
Sep  3 21:13:34.458: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb" in namespace "projected-4781" to be "Succeeded or Failed"
Sep  3 21:13:34.461: INFO: Pod "pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.39711ms
Sep  3 21:13:36.484: INFO: Pod "pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025626493s
Sep  3 21:13:38.464: INFO: Pod "pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005190576s
STEP: Saw pod success 09/03/22 21:13:38.464
Sep  3 21:13:38.464: INFO: Pod "pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb" satisfied condition "Succeeded or Failed"
Sep  3 21:13:38.466: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb container agnhost-container: <nil>
STEP: delete the pod 09/03/22 21:13:38.47
Sep  3 21:13:38.478: INFO: Waiting for pod pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb to disappear
Sep  3 21:13:38.480: INFO: Pod pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Sep  3 21:13:38.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4781" for this suite. 09/03/22 21:13:38.484
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":124,"skipped":2109,"failed":0}
------------------------------
• [4.050 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:13:34.437
    Sep  3 21:13:34.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:13:34.438
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:34.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:34.45
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-972d7b39-e9c4-44fd-8d9e-36bef57a554c 09/03/22 21:13:34.452
    STEP: Creating a pod to test consume configMaps 09/03/22 21:13:34.454
    Sep  3 21:13:34.458: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb" in namespace "projected-4781" to be "Succeeded or Failed"
    Sep  3 21:13:34.461: INFO: Pod "pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.39711ms
    Sep  3 21:13:36.484: INFO: Pod "pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025626493s
    Sep  3 21:13:38.464: INFO: Pod "pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005190576s
    STEP: Saw pod success 09/03/22 21:13:38.464
    Sep  3 21:13:38.464: INFO: Pod "pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb" satisfied condition "Succeeded or Failed"
    Sep  3 21:13:38.466: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 21:13:38.47
    Sep  3 21:13:38.478: INFO: Waiting for pod pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb to disappear
    Sep  3 21:13:38.480: INFO: Pod pod-projected-configmaps-117a5907-9684-4dd4-a96c-b1d853db94bb no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Sep  3 21:13:38.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4781" for this suite. 09/03/22 21:13:38.484
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:13:38.488
Sep  3 21:13:38.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-probe 09/03/22 21:13:38.488
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:38.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:38.503
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Sep  3 21:14:38.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5819" for this suite. 09/03/22 21:14:38.515
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":125,"skipped":2120,"failed":0}
------------------------------
• [SLOW TEST] [60.031 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:13:38.488
    Sep  3 21:13:38.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-probe 09/03/22 21:13:38.488
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:13:38.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:13:38.503
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Sep  3 21:14:38.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5819" for this suite. 09/03/22 21:14:38.515
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:14:38.52
Sep  3 21:14:38.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename svcaccounts 09/03/22 21:14:38.521
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:38.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:38.534
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Sep  3 21:14:38.537: INFO: Got root ca configmap in namespace "svcaccounts-5063"
Sep  3 21:14:38.539: INFO: Deleted root ca configmap in namespace "svcaccounts-5063"
STEP: waiting for a new root ca configmap created 09/03/22 21:14:39.04
Sep  3 21:14:39.043: INFO: Recreated root ca configmap in namespace "svcaccounts-5063"
Sep  3 21:14:39.046: INFO: Updated root ca configmap in namespace "svcaccounts-5063"
STEP: waiting for the root ca configmap reconciled 09/03/22 21:14:39.547
Sep  3 21:14:39.549: INFO: Reconciled root ca configmap in namespace "svcaccounts-5063"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Sep  3 21:14:39.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5063" for this suite. 09/03/22 21:14:39.551
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":126,"skipped":2121,"failed":0}
------------------------------
• [1.035 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:14:38.52
    Sep  3 21:14:38.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename svcaccounts 09/03/22 21:14:38.521
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:38.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:38.534
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Sep  3 21:14:38.537: INFO: Got root ca configmap in namespace "svcaccounts-5063"
    Sep  3 21:14:38.539: INFO: Deleted root ca configmap in namespace "svcaccounts-5063"
    STEP: waiting for a new root ca configmap created 09/03/22 21:14:39.04
    Sep  3 21:14:39.043: INFO: Recreated root ca configmap in namespace "svcaccounts-5063"
    Sep  3 21:14:39.046: INFO: Updated root ca configmap in namespace "svcaccounts-5063"
    STEP: waiting for the root ca configmap reconciled 09/03/22 21:14:39.547
    Sep  3 21:14:39.549: INFO: Reconciled root ca configmap in namespace "svcaccounts-5063"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Sep  3 21:14:39.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5063" for this suite. 09/03/22 21:14:39.551
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:14:39.556
Sep  3 21:14:39.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename deployment 09/03/22 21:14:39.557
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:39.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:39.57
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 09/03/22 21:14:39.575
Sep  3 21:14:39.575: INFO: Creating simple deployment test-deployment-mlt48
Sep  3 21:14:39.590: INFO: deployment "test-deployment-mlt48" doesn't have the required revision set
Sep  3 21:14:41.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 14, 39, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 14, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 14, 39, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 14, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-mlt48-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 09/03/22 21:14:43.601
Sep  3 21:14:43.604: INFO: Deployment test-deployment-mlt48 has Conditions: [{Available True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlt48-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 09/03/22 21:14:43.604
Sep  3 21:14:43.610: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 14, 42, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 14, 39, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-mlt48-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 09/03/22 21:14:43.61
Sep  3 21:14:43.611: INFO: Observed &Deployment event: ADDED
Sep  3 21:14:43.611: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlt48-777898ffcc"}
Sep  3 21:14:43.612: INFO: Observed &Deployment event: MODIFIED
Sep  3 21:14:43.612: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlt48-777898ffcc"}
Sep  3 21:14:43.612: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  3 21:14:43.612: INFO: Observed &Deployment event: MODIFIED
Sep  3 21:14:43.613: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  3 21:14:43.613: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mlt48-777898ffcc" is progressing.}
Sep  3 21:14:43.613: INFO: Observed &Deployment event: MODIFIED
Sep  3 21:14:43.614: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  3 21:14:43.614: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlt48-777898ffcc" has successfully progressed.}
Sep  3 21:14:43.614: INFO: Observed &Deployment event: MODIFIED
Sep  3 21:14:43.614: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  3 21:14:43.614: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlt48-777898ffcc" has successfully progressed.}
Sep  3 21:14:43.614: INFO: Found Deployment test-deployment-mlt48 in namespace deployment-9679 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  3 21:14:43.614: INFO: Deployment test-deployment-mlt48 has an updated status
STEP: patching the Statefulset Status 09/03/22 21:14:43.614
Sep  3 21:14:43.614: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  3 21:14:43.620: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 09/03/22 21:14:43.62
Sep  3 21:14:43.621: INFO: Observed &Deployment event: ADDED
Sep  3 21:14:43.621: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlt48-777898ffcc"}
Sep  3 21:14:43.622: INFO: Observed &Deployment event: MODIFIED
Sep  3 21:14:43.622: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlt48-777898ffcc"}
Sep  3 21:14:43.622: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  3 21:14:43.622: INFO: Observed &Deployment event: MODIFIED
Sep  3 21:14:43.623: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  3 21:14:43.623: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mlt48-777898ffcc" is progressing.}
Sep  3 21:14:43.623: INFO: Observed &Deployment event: MODIFIED
Sep  3 21:14:43.623: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  3 21:14:43.624: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlt48-777898ffcc" has successfully progressed.}
Sep  3 21:14:43.624: INFO: Observed &Deployment event: MODIFIED
Sep  3 21:14:43.624: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  3 21:14:43.624: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlt48-777898ffcc" has successfully progressed.}
Sep  3 21:14:43.624: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  3 21:14:43.625: INFO: Observed &Deployment event: MODIFIED
Sep  3 21:14:43.625: INFO: Found deployment test-deployment-mlt48 in namespace deployment-9679 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Sep  3 21:14:43.625: INFO: Deployment test-deployment-mlt48 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  3 21:14:43.627: INFO: Deployment "test-deployment-mlt48":
&Deployment{ObjectMeta:{test-deployment-mlt48  deployment-9679  bbc87bfe-3c4a-4f1b-8262-6354a683cf66 9207 1 2022-09-03 21:14:39 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-09-03 21:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:14:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-09-03 21:14:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034ebd88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  3 21:14:43.639: INFO: New ReplicaSet "test-deployment-mlt48-777898ffcc" of Deployment "test-deployment-mlt48":
&ReplicaSet{ObjectMeta:{test-deployment-mlt48-777898ffcc  deployment-9679  6906b0b6-f5a6-4877-b58d-21f85b88b4d8 9195 1 2022-09-03 21:14:39 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-mlt48 bbc87bfe-3c4a-4f1b-8262-6354a683cf66 0xc00390c517 0xc00390c518}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc87bfe-3c4a-4f1b-8262-6354a683cf66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:14:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00390c5c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:14:43.643: INFO: Pod "test-deployment-mlt48-777898ffcc-9jzbn" is available:
&Pod{ObjectMeta:{test-deployment-mlt48-777898ffcc-9jzbn test-deployment-mlt48-777898ffcc- deployment-9679  f6cd5917-107a-4c6c-a30b-9045651d227f 9194 0 2022-09-03 21:14:39 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-mlt48-777898ffcc 6906b0b6-f5a6-4877-b58d-21f85b88b4d8 0xc001d3e167 0xc001d3e168}] [] [{kube-controller-manager Update v1 2022-09-03 21:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6906b0b6-f5a6-4877-b58d-21f85b88b4d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:14:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x2lfc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x2lfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:14:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:14:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:14:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:14:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.132,StartTime:2022-09-03 21:14:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:14:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://94db0ad822d774edfe302a0a43f707239d4355ac64da25848017cdf0a896d4e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Sep  3 21:14:43.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9679" for this suite. 09/03/22 21:14:43.647
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":127,"skipped":2138,"failed":0}
------------------------------
• [4.096 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:14:39.556
    Sep  3 21:14:39.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename deployment 09/03/22 21:14:39.557
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:39.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:39.57
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 09/03/22 21:14:39.575
    Sep  3 21:14:39.575: INFO: Creating simple deployment test-deployment-mlt48
    Sep  3 21:14:39.590: INFO: deployment "test-deployment-mlt48" doesn't have the required revision set
    Sep  3 21:14:41.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 14, 39, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 14, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 14, 39, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 14, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-mlt48-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 09/03/22 21:14:43.601
    Sep  3 21:14:43.604: INFO: Deployment test-deployment-mlt48 has Conditions: [{Available True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlt48-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 09/03/22 21:14:43.604
    Sep  3 21:14:43.610: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 14, 42, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 14, 39, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-mlt48-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 09/03/22 21:14:43.61
    Sep  3 21:14:43.611: INFO: Observed &Deployment event: ADDED
    Sep  3 21:14:43.611: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlt48-777898ffcc"}
    Sep  3 21:14:43.612: INFO: Observed &Deployment event: MODIFIED
    Sep  3 21:14:43.612: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlt48-777898ffcc"}
    Sep  3 21:14:43.612: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  3 21:14:43.612: INFO: Observed &Deployment event: MODIFIED
    Sep  3 21:14:43.613: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  3 21:14:43.613: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mlt48-777898ffcc" is progressing.}
    Sep  3 21:14:43.613: INFO: Observed &Deployment event: MODIFIED
    Sep  3 21:14:43.614: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  3 21:14:43.614: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlt48-777898ffcc" has successfully progressed.}
    Sep  3 21:14:43.614: INFO: Observed &Deployment event: MODIFIED
    Sep  3 21:14:43.614: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  3 21:14:43.614: INFO: Observed Deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlt48-777898ffcc" has successfully progressed.}
    Sep  3 21:14:43.614: INFO: Found Deployment test-deployment-mlt48 in namespace deployment-9679 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  3 21:14:43.614: INFO: Deployment test-deployment-mlt48 has an updated status
    STEP: patching the Statefulset Status 09/03/22 21:14:43.614
    Sep  3 21:14:43.614: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  3 21:14:43.620: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 09/03/22 21:14:43.62
    Sep  3 21:14:43.621: INFO: Observed &Deployment event: ADDED
    Sep  3 21:14:43.621: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlt48-777898ffcc"}
    Sep  3 21:14:43.622: INFO: Observed &Deployment event: MODIFIED
    Sep  3 21:14:43.622: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlt48-777898ffcc"}
    Sep  3 21:14:43.622: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  3 21:14:43.622: INFO: Observed &Deployment event: MODIFIED
    Sep  3 21:14:43.623: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  3 21:14:43.623: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:39 +0000 UTC 2022-09-03 21:14:39 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mlt48-777898ffcc" is progressing.}
    Sep  3 21:14:43.623: INFO: Observed &Deployment event: MODIFIED
    Sep  3 21:14:43.623: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  3 21:14:43.624: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlt48-777898ffcc" has successfully progressed.}
    Sep  3 21:14:43.624: INFO: Observed &Deployment event: MODIFIED
    Sep  3 21:14:43.624: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  3 21:14:43.624: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-03 21:14:42 +0000 UTC 2022-09-03 21:14:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlt48-777898ffcc" has successfully progressed.}
    Sep  3 21:14:43.624: INFO: Observed deployment test-deployment-mlt48 in namespace deployment-9679 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  3 21:14:43.625: INFO: Observed &Deployment event: MODIFIED
    Sep  3 21:14:43.625: INFO: Found deployment test-deployment-mlt48 in namespace deployment-9679 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Sep  3 21:14:43.625: INFO: Deployment test-deployment-mlt48 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  3 21:14:43.627: INFO: Deployment "test-deployment-mlt48":
    &Deployment{ObjectMeta:{test-deployment-mlt48  deployment-9679  bbc87bfe-3c4a-4f1b-8262-6354a683cf66 9207 1 2022-09-03 21:14:39 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-09-03 21:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:14:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-09-03 21:14:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034ebd88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  3 21:14:43.639: INFO: New ReplicaSet "test-deployment-mlt48-777898ffcc" of Deployment "test-deployment-mlt48":
    &ReplicaSet{ObjectMeta:{test-deployment-mlt48-777898ffcc  deployment-9679  6906b0b6-f5a6-4877-b58d-21f85b88b4d8 9195 1 2022-09-03 21:14:39 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-mlt48 bbc87bfe-3c4a-4f1b-8262-6354a683cf66 0xc00390c517 0xc00390c518}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc87bfe-3c4a-4f1b-8262-6354a683cf66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:14:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00390c5c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:14:43.643: INFO: Pod "test-deployment-mlt48-777898ffcc-9jzbn" is available:
    &Pod{ObjectMeta:{test-deployment-mlt48-777898ffcc-9jzbn test-deployment-mlt48-777898ffcc- deployment-9679  f6cd5917-107a-4c6c-a30b-9045651d227f 9194 0 2022-09-03 21:14:39 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-mlt48-777898ffcc 6906b0b6-f5a6-4877-b58d-21f85b88b4d8 0xc001d3e167 0xc001d3e168}] [] [{kube-controller-manager Update v1 2022-09-03 21:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6906b0b6-f5a6-4877-b58d-21f85b88b4d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:14:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x2lfc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x2lfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:14:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:14:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:14:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:14:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.132,StartTime:2022-09-03 21:14:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:14:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://94db0ad822d774edfe302a0a43f707239d4355ac64da25848017cdf0a896d4e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Sep  3 21:14:43.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9679" for this suite. 09/03/22 21:14:43.647
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:14:43.659
Sep  3 21:14:43.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename gc 09/03/22 21:14:43.66
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:43.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:43.673
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 09/03/22 21:14:43.676
STEP: Wait for the Deployment to create new ReplicaSet 09/03/22 21:14:43.679
STEP: delete the deployment 09/03/22 21:14:44.187
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/03/22 21:14:44.19
STEP: Gathering metrics 09/03/22 21:14:44.72
Sep  3 21:14:44.745: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
Sep  3 21:14:44.751: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 6.150814ms
Sep  3 21:14:44.751: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
Sep  3 21:14:44.751: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
Sep  3 21:14:44.822: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Sep  3 21:14:44.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6737" for this suite. 09/03/22 21:14:44.825
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":128,"skipped":2145,"failed":0}
------------------------------
• [1.171 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:14:43.659
    Sep  3 21:14:43.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename gc 09/03/22 21:14:43.66
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:43.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:43.673
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 09/03/22 21:14:43.676
    STEP: Wait for the Deployment to create new ReplicaSet 09/03/22 21:14:43.679
    STEP: delete the deployment 09/03/22 21:14:44.187
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/03/22 21:14:44.19
    STEP: Gathering metrics 09/03/22 21:14:44.72
    Sep  3 21:14:44.745: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
    Sep  3 21:14:44.751: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 6.150814ms
    Sep  3 21:14:44.751: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
    Sep  3 21:14:44.751: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
    Sep  3 21:14:44.822: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Sep  3 21:14:44.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6737" for this suite. 09/03/22 21:14:44.825
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:14:44.835
Sep  3 21:14:44.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 21:14:44.836
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:44.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:44.847
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 09/03/22 21:14:44.852
STEP: watching for the Service to be added 09/03/22 21:14:44.86
Sep  3 21:14:44.863: INFO: Found Service test-service-lqt98 in namespace services-4675 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Sep  3 21:14:44.863: INFO: Service test-service-lqt98 created
STEP: Getting /status 09/03/22 21:14:44.863
Sep  3 21:14:44.868: INFO: Service test-service-lqt98 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 09/03/22 21:14:44.868
STEP: watching for the Service to be patched 09/03/22 21:14:44.874
Sep  3 21:14:44.876: INFO: observed Service test-service-lqt98 in namespace services-4675 with annotations: map[] & LoadBalancer: {[]}
Sep  3 21:14:44.877: INFO: Found Service test-service-lqt98 in namespace services-4675 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Sep  3 21:14:44.877: INFO: Service test-service-lqt98 has service status patched
STEP: updating the ServiceStatus 09/03/22 21:14:44.877
Sep  3 21:14:44.899: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 09/03/22 21:14:44.899
Sep  3 21:14:44.904: INFO: Observed Service test-service-lqt98 in namespace services-4675 with annotations: map[] & Conditions: {[]}
Sep  3 21:14:44.904: INFO: Observed event: &Service{ObjectMeta:{test-service-lqt98  services-4675  5084f845-ac86-483e-83b9-9f2747a149b9 9259 0 2022-09-03 21:14:44 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-09-03 21:14:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-09-03 21:14:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.180.142,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.180.142],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Sep  3 21:14:44.905: INFO: Found Service test-service-lqt98 in namespace services-4675 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  3 21:14:44.905: INFO: Service test-service-lqt98 has service status updated
STEP: patching the service 09/03/22 21:14:44.905
STEP: watching for the Service to be patched 09/03/22 21:14:44.935
Sep  3 21:14:44.943: INFO: observed Service test-service-lqt98 in namespace services-4675 with labels: map[test-service-static:true]
Sep  3 21:14:44.944: INFO: observed Service test-service-lqt98 in namespace services-4675 with labels: map[test-service-static:true]
Sep  3 21:14:44.944: INFO: observed Service test-service-lqt98 in namespace services-4675 with labels: map[test-service-static:true]
Sep  3 21:14:44.944: INFO: Found Service test-service-lqt98 in namespace services-4675 with labels: map[test-service:patched test-service-static:true]
Sep  3 21:14:44.944: INFO: Service test-service-lqt98 patched
STEP: deleting the service 09/03/22 21:14:44.944
STEP: watching for the Service to be deleted 09/03/22 21:14:44.96
Sep  3 21:14:44.974: INFO: Observed event: ADDED
Sep  3 21:14:44.974: INFO: Observed event: MODIFIED
Sep  3 21:14:44.974: INFO: Observed event: MODIFIED
Sep  3 21:14:44.974: INFO: Observed event: MODIFIED
Sep  3 21:14:44.974: INFO: Found Service test-service-lqt98 in namespace services-4675 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Sep  3 21:14:44.974: INFO: Service test-service-lqt98 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 21:14:44.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4675" for this suite. 09/03/22 21:14:44.979
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":129,"skipped":2200,"failed":0}
------------------------------
• [0.150 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:14:44.835
    Sep  3 21:14:44.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 21:14:44.836
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:44.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:44.847
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 09/03/22 21:14:44.852
    STEP: watching for the Service to be added 09/03/22 21:14:44.86
    Sep  3 21:14:44.863: INFO: Found Service test-service-lqt98 in namespace services-4675 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Sep  3 21:14:44.863: INFO: Service test-service-lqt98 created
    STEP: Getting /status 09/03/22 21:14:44.863
    Sep  3 21:14:44.868: INFO: Service test-service-lqt98 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 09/03/22 21:14:44.868
    STEP: watching for the Service to be patched 09/03/22 21:14:44.874
    Sep  3 21:14:44.876: INFO: observed Service test-service-lqt98 in namespace services-4675 with annotations: map[] & LoadBalancer: {[]}
    Sep  3 21:14:44.877: INFO: Found Service test-service-lqt98 in namespace services-4675 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Sep  3 21:14:44.877: INFO: Service test-service-lqt98 has service status patched
    STEP: updating the ServiceStatus 09/03/22 21:14:44.877
    Sep  3 21:14:44.899: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 09/03/22 21:14:44.899
    Sep  3 21:14:44.904: INFO: Observed Service test-service-lqt98 in namespace services-4675 with annotations: map[] & Conditions: {[]}
    Sep  3 21:14:44.904: INFO: Observed event: &Service{ObjectMeta:{test-service-lqt98  services-4675  5084f845-ac86-483e-83b9-9f2747a149b9 9259 0 2022-09-03 21:14:44 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-09-03 21:14:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-09-03 21:14:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.180.142,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.180.142],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Sep  3 21:14:44.905: INFO: Found Service test-service-lqt98 in namespace services-4675 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  3 21:14:44.905: INFO: Service test-service-lqt98 has service status updated
    STEP: patching the service 09/03/22 21:14:44.905
    STEP: watching for the Service to be patched 09/03/22 21:14:44.935
    Sep  3 21:14:44.943: INFO: observed Service test-service-lqt98 in namespace services-4675 with labels: map[test-service-static:true]
    Sep  3 21:14:44.944: INFO: observed Service test-service-lqt98 in namespace services-4675 with labels: map[test-service-static:true]
    Sep  3 21:14:44.944: INFO: observed Service test-service-lqt98 in namespace services-4675 with labels: map[test-service-static:true]
    Sep  3 21:14:44.944: INFO: Found Service test-service-lqt98 in namespace services-4675 with labels: map[test-service:patched test-service-static:true]
    Sep  3 21:14:44.944: INFO: Service test-service-lqt98 patched
    STEP: deleting the service 09/03/22 21:14:44.944
    STEP: watching for the Service to be deleted 09/03/22 21:14:44.96
    Sep  3 21:14:44.974: INFO: Observed event: ADDED
    Sep  3 21:14:44.974: INFO: Observed event: MODIFIED
    Sep  3 21:14:44.974: INFO: Observed event: MODIFIED
    Sep  3 21:14:44.974: INFO: Observed event: MODIFIED
    Sep  3 21:14:44.974: INFO: Found Service test-service-lqt98 in namespace services-4675 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Sep  3 21:14:44.974: INFO: Service test-service-lqt98 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 21:14:44.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4675" for this suite. 09/03/22 21:14:44.979
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:14:44.986
Sep  3 21:14:44.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 21:14:44.987
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:45.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:45.018
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 09/03/22 21:14:45.03
Sep  3 21:14:45.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef" in namespace "downward-api-8341" to be "Succeeded or Failed"
Sep  3 21:14:45.051: INFO: Pod "downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef": Phase="Pending", Reason="", readiness=false. Elapsed: 11.586726ms
Sep  3 21:14:47.053: INFO: Pod "downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014039935s
Sep  3 21:14:49.054: INFO: Pod "downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014875341s
STEP: Saw pod success 09/03/22 21:14:49.054
Sep  3 21:14:49.055: INFO: Pod "downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef" satisfied condition "Succeeded or Failed"
Sep  3 21:14:49.058: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef container client-container: <nil>
STEP: delete the pod 09/03/22 21:14:49.062
Sep  3 21:14:49.068: INFO: Waiting for pod downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef to disappear
Sep  3 21:14:49.070: INFO: Pod downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Sep  3 21:14:49.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8341" for this suite. 09/03/22 21:14:49.072
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":130,"skipped":2205,"failed":0}
------------------------------
• [4.089 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:14:44.986
    Sep  3 21:14:44.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 21:14:44.987
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:45.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:45.018
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 09/03/22 21:14:45.03
    Sep  3 21:14:45.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef" in namespace "downward-api-8341" to be "Succeeded or Failed"
    Sep  3 21:14:45.051: INFO: Pod "downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef": Phase="Pending", Reason="", readiness=false. Elapsed: 11.586726ms
    Sep  3 21:14:47.053: INFO: Pod "downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014039935s
    Sep  3 21:14:49.054: INFO: Pod "downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014875341s
    STEP: Saw pod success 09/03/22 21:14:49.054
    Sep  3 21:14:49.055: INFO: Pod "downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef" satisfied condition "Succeeded or Failed"
    Sep  3 21:14:49.058: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef container client-container: <nil>
    STEP: delete the pod 09/03/22 21:14:49.062
    Sep  3 21:14:49.068: INFO: Waiting for pod downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef to disappear
    Sep  3 21:14:49.070: INFO: Pod downwardapi-volume-6f8154de-4ecf-45bf-b946-b304f12a6aef no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Sep  3 21:14:49.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8341" for this suite. 09/03/22 21:14:49.072
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:14:49.076
Sep  3 21:14:49.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 21:14:49.078
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:49.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:49.091
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-c0568f6b-4b76-4964-826f-d6e6384c8bee 09/03/22 21:14:49.095
STEP: Creating secret with name s-test-opt-upd-d6fdae3c-a315-4cad-9191-c20f6b2ec232 09/03/22 21:14:49.099
STEP: Creating the pod 09/03/22 21:14:49.102
Sep  3 21:14:49.108: INFO: Waiting up to 5m0s for pod "pod-secrets-78d2e098-acb4-4aa0-9299-6e3ad96747b0" in namespace "secrets-6190" to be "running and ready"
Sep  3 21:14:49.132: INFO: Pod "pod-secrets-78d2e098-acb4-4aa0-9299-6e3ad96747b0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.083154ms
Sep  3 21:14:49.132: INFO: The phase of Pod pod-secrets-78d2e098-acb4-4aa0-9299-6e3ad96747b0 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:14:51.135: INFO: Pod "pod-secrets-78d2e098-acb4-4aa0-9299-6e3ad96747b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.027164598s
Sep  3 21:14:51.135: INFO: The phase of Pod pod-secrets-78d2e098-acb4-4aa0-9299-6e3ad96747b0 is Running (Ready = true)
Sep  3 21:14:51.135: INFO: Pod "pod-secrets-78d2e098-acb4-4aa0-9299-6e3ad96747b0" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-c0568f6b-4b76-4964-826f-d6e6384c8bee 09/03/22 21:14:51.148
STEP: Updating secret s-test-opt-upd-d6fdae3c-a315-4cad-9191-c20f6b2ec232 09/03/22 21:14:51.151
STEP: Creating secret with name s-test-opt-create-2b61e9fa-b6dd-4489-98c9-3a7fd0b3994f 09/03/22 21:14:51.155
STEP: waiting to observe update in volume 09/03/22 21:14:51.158
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Sep  3 21:14:53.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6190" for this suite. 09/03/22 21:14:53.18
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":131,"skipped":2217,"failed":0}
------------------------------
• [4.107 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:14:49.076
    Sep  3 21:14:49.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 21:14:49.078
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:49.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:49.091
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-c0568f6b-4b76-4964-826f-d6e6384c8bee 09/03/22 21:14:49.095
    STEP: Creating secret with name s-test-opt-upd-d6fdae3c-a315-4cad-9191-c20f6b2ec232 09/03/22 21:14:49.099
    STEP: Creating the pod 09/03/22 21:14:49.102
    Sep  3 21:14:49.108: INFO: Waiting up to 5m0s for pod "pod-secrets-78d2e098-acb4-4aa0-9299-6e3ad96747b0" in namespace "secrets-6190" to be "running and ready"
    Sep  3 21:14:49.132: INFO: Pod "pod-secrets-78d2e098-acb4-4aa0-9299-6e3ad96747b0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.083154ms
    Sep  3 21:14:49.132: INFO: The phase of Pod pod-secrets-78d2e098-acb4-4aa0-9299-6e3ad96747b0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:14:51.135: INFO: Pod "pod-secrets-78d2e098-acb4-4aa0-9299-6e3ad96747b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.027164598s
    Sep  3 21:14:51.135: INFO: The phase of Pod pod-secrets-78d2e098-acb4-4aa0-9299-6e3ad96747b0 is Running (Ready = true)
    Sep  3 21:14:51.135: INFO: Pod "pod-secrets-78d2e098-acb4-4aa0-9299-6e3ad96747b0" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-c0568f6b-4b76-4964-826f-d6e6384c8bee 09/03/22 21:14:51.148
    STEP: Updating secret s-test-opt-upd-d6fdae3c-a315-4cad-9191-c20f6b2ec232 09/03/22 21:14:51.151
    STEP: Creating secret with name s-test-opt-create-2b61e9fa-b6dd-4489-98c9-3a7fd0b3994f 09/03/22 21:14:51.155
    STEP: waiting to observe update in volume 09/03/22 21:14:51.158
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 21:14:53.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6190" for this suite. 09/03/22 21:14:53.18
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:14:53.188
Sep  3 21:14:53.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename cronjob 09/03/22 21:14:53.189
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:53.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:53.209
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 09/03/22 21:14:53.217
STEP: Ensuring a job is scheduled 09/03/22 21:14:53.226
STEP: Ensuring exactly one is scheduled 09/03/22 21:15:01.229
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/03/22 21:15:01.231
STEP: Ensuring no more jobs are scheduled 09/03/22 21:15:01.234
STEP: Removing cronjob 09/03/22 21:20:01.24
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Sep  3 21:20:01.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7871" for this suite. 09/03/22 21:20:01.247
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":132,"skipped":2221,"failed":0}
------------------------------
• [SLOW TEST] [308.070 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:14:53.188
    Sep  3 21:14:53.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename cronjob 09/03/22 21:14:53.189
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:14:53.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:14:53.209
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 09/03/22 21:14:53.217
    STEP: Ensuring a job is scheduled 09/03/22 21:14:53.226
    STEP: Ensuring exactly one is scheduled 09/03/22 21:15:01.229
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/03/22 21:15:01.231
    STEP: Ensuring no more jobs are scheduled 09/03/22 21:15:01.234
    STEP: Removing cronjob 09/03/22 21:20:01.24
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Sep  3 21:20:01.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7871" for this suite. 09/03/22 21:20:01.247
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:20:01.259
Sep  3 21:20:01.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename custom-resource-definition 09/03/22 21:20:01.26
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:01.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:01.276
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Sep  3 21:20:01.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:20:01.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5545" for this suite. 09/03/22 21:20:01.809
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":133,"skipped":2228,"failed":0}
------------------------------
• [0.555 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:20:01.259
    Sep  3 21:20:01.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename custom-resource-definition 09/03/22 21:20:01.26
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:01.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:01.276
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Sep  3 21:20:01.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:20:01.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-5545" for this suite. 09/03/22 21:20:01.809
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:20:01.819
Sep  3 21:20:01.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename discovery 09/03/22 21:20:01.82
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:01.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:01.832
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 09/03/22 21:20:01.835
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Sep  3 21:20:02.289: INFO: Checking APIGroup: apiregistration.k8s.io
Sep  3 21:20:02.290: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep  3 21:20:02.290: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Sep  3 21:20:02.291: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep  3 21:20:02.291: INFO: Checking APIGroup: apps
Sep  3 21:20:02.292: INFO: PreferredVersion.GroupVersion: apps/v1
Sep  3 21:20:02.293: INFO: Versions found [{apps/v1 v1}]
Sep  3 21:20:02.293: INFO: apps/v1 matches apps/v1
Sep  3 21:20:02.293: INFO: Checking APIGroup: events.k8s.io
Sep  3 21:20:02.294: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep  3 21:20:02.298: INFO: Versions found [{events.k8s.io/v1 v1}]
Sep  3 21:20:02.298: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep  3 21:20:02.298: INFO: Checking APIGroup: authentication.k8s.io
Sep  3 21:20:02.300: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep  3 21:20:02.300: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Sep  3 21:20:02.300: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep  3 21:20:02.301: INFO: Checking APIGroup: authorization.k8s.io
Sep  3 21:20:02.302: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep  3 21:20:02.302: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Sep  3 21:20:02.302: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep  3 21:20:02.302: INFO: Checking APIGroup: autoscaling
Sep  3 21:20:02.303: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Sep  3 21:20:02.303: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Sep  3 21:20:02.303: INFO: autoscaling/v2 matches autoscaling/v2
Sep  3 21:20:02.303: INFO: Checking APIGroup: batch
Sep  3 21:20:02.304: INFO: PreferredVersion.GroupVersion: batch/v1
Sep  3 21:20:02.304: INFO: Versions found [{batch/v1 v1}]
Sep  3 21:20:02.304: INFO: batch/v1 matches batch/v1
Sep  3 21:20:02.304: INFO: Checking APIGroup: certificates.k8s.io
Sep  3 21:20:02.305: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep  3 21:20:02.305: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Sep  3 21:20:02.305: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep  3 21:20:02.305: INFO: Checking APIGroup: networking.k8s.io
Sep  3 21:20:02.306: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep  3 21:20:02.306: INFO: Versions found [{networking.k8s.io/v1 v1}]
Sep  3 21:20:02.306: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep  3 21:20:02.306: INFO: Checking APIGroup: policy
Sep  3 21:20:02.307: INFO: PreferredVersion.GroupVersion: policy/v1
Sep  3 21:20:02.307: INFO: Versions found [{policy/v1 v1}]
Sep  3 21:20:02.307: INFO: policy/v1 matches policy/v1
Sep  3 21:20:02.307: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep  3 21:20:02.308: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep  3 21:20:02.308: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Sep  3 21:20:02.308: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep  3 21:20:02.308: INFO: Checking APIGroup: storage.k8s.io
Sep  3 21:20:02.309: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep  3 21:20:02.309: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep  3 21:20:02.309: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep  3 21:20:02.309: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep  3 21:20:02.310: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep  3 21:20:02.310: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Sep  3 21:20:02.310: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep  3 21:20:02.310: INFO: Checking APIGroup: apiextensions.k8s.io
Sep  3 21:20:02.311: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep  3 21:20:02.311: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Sep  3 21:20:02.311: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep  3 21:20:02.311: INFO: Checking APIGroup: scheduling.k8s.io
Sep  3 21:20:02.313: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep  3 21:20:02.313: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Sep  3 21:20:02.313: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep  3 21:20:02.313: INFO: Checking APIGroup: coordination.k8s.io
Sep  3 21:20:02.316: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep  3 21:20:02.316: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Sep  3 21:20:02.316: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep  3 21:20:02.316: INFO: Checking APIGroup: node.k8s.io
Sep  3 21:20:02.318: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep  3 21:20:02.318: INFO: Versions found [{node.k8s.io/v1 v1}]
Sep  3 21:20:02.319: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep  3 21:20:02.319: INFO: Checking APIGroup: discovery.k8s.io
Sep  3 21:20:02.319: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Sep  3 21:20:02.320: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Sep  3 21:20:02.320: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Sep  3 21:20:02.320: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep  3 21:20:02.321: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Sep  3 21:20:02.321: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Sep  3 21:20:02.321: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Sep  3 21:20:02.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9529" for this suite. 09/03/22 21:20:02.326
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":134,"skipped":2230,"failed":0}
------------------------------
• [0.512 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:20:01.819
    Sep  3 21:20:01.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename discovery 09/03/22 21:20:01.82
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:01.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:01.832
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 09/03/22 21:20:01.835
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Sep  3 21:20:02.289: INFO: Checking APIGroup: apiregistration.k8s.io
    Sep  3 21:20:02.290: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Sep  3 21:20:02.290: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Sep  3 21:20:02.291: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Sep  3 21:20:02.291: INFO: Checking APIGroup: apps
    Sep  3 21:20:02.292: INFO: PreferredVersion.GroupVersion: apps/v1
    Sep  3 21:20:02.293: INFO: Versions found [{apps/v1 v1}]
    Sep  3 21:20:02.293: INFO: apps/v1 matches apps/v1
    Sep  3 21:20:02.293: INFO: Checking APIGroup: events.k8s.io
    Sep  3 21:20:02.294: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Sep  3 21:20:02.298: INFO: Versions found [{events.k8s.io/v1 v1}]
    Sep  3 21:20:02.298: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Sep  3 21:20:02.298: INFO: Checking APIGroup: authentication.k8s.io
    Sep  3 21:20:02.300: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Sep  3 21:20:02.300: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Sep  3 21:20:02.300: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Sep  3 21:20:02.301: INFO: Checking APIGroup: authorization.k8s.io
    Sep  3 21:20:02.302: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Sep  3 21:20:02.302: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Sep  3 21:20:02.302: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Sep  3 21:20:02.302: INFO: Checking APIGroup: autoscaling
    Sep  3 21:20:02.303: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Sep  3 21:20:02.303: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Sep  3 21:20:02.303: INFO: autoscaling/v2 matches autoscaling/v2
    Sep  3 21:20:02.303: INFO: Checking APIGroup: batch
    Sep  3 21:20:02.304: INFO: PreferredVersion.GroupVersion: batch/v1
    Sep  3 21:20:02.304: INFO: Versions found [{batch/v1 v1}]
    Sep  3 21:20:02.304: INFO: batch/v1 matches batch/v1
    Sep  3 21:20:02.304: INFO: Checking APIGroup: certificates.k8s.io
    Sep  3 21:20:02.305: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Sep  3 21:20:02.305: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Sep  3 21:20:02.305: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Sep  3 21:20:02.305: INFO: Checking APIGroup: networking.k8s.io
    Sep  3 21:20:02.306: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Sep  3 21:20:02.306: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Sep  3 21:20:02.306: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Sep  3 21:20:02.306: INFO: Checking APIGroup: policy
    Sep  3 21:20:02.307: INFO: PreferredVersion.GroupVersion: policy/v1
    Sep  3 21:20:02.307: INFO: Versions found [{policy/v1 v1}]
    Sep  3 21:20:02.307: INFO: policy/v1 matches policy/v1
    Sep  3 21:20:02.307: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Sep  3 21:20:02.308: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Sep  3 21:20:02.308: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Sep  3 21:20:02.308: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Sep  3 21:20:02.308: INFO: Checking APIGroup: storage.k8s.io
    Sep  3 21:20:02.309: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Sep  3 21:20:02.309: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Sep  3 21:20:02.309: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Sep  3 21:20:02.309: INFO: Checking APIGroup: admissionregistration.k8s.io
    Sep  3 21:20:02.310: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Sep  3 21:20:02.310: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Sep  3 21:20:02.310: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Sep  3 21:20:02.310: INFO: Checking APIGroup: apiextensions.k8s.io
    Sep  3 21:20:02.311: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Sep  3 21:20:02.311: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Sep  3 21:20:02.311: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Sep  3 21:20:02.311: INFO: Checking APIGroup: scheduling.k8s.io
    Sep  3 21:20:02.313: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Sep  3 21:20:02.313: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Sep  3 21:20:02.313: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Sep  3 21:20:02.313: INFO: Checking APIGroup: coordination.k8s.io
    Sep  3 21:20:02.316: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Sep  3 21:20:02.316: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Sep  3 21:20:02.316: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Sep  3 21:20:02.316: INFO: Checking APIGroup: node.k8s.io
    Sep  3 21:20:02.318: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Sep  3 21:20:02.318: INFO: Versions found [{node.k8s.io/v1 v1}]
    Sep  3 21:20:02.319: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Sep  3 21:20:02.319: INFO: Checking APIGroup: discovery.k8s.io
    Sep  3 21:20:02.319: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Sep  3 21:20:02.320: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Sep  3 21:20:02.320: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Sep  3 21:20:02.320: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Sep  3 21:20:02.321: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Sep  3 21:20:02.321: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Sep  3 21:20:02.321: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Sep  3 21:20:02.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-9529" for this suite. 09/03/22 21:20:02.326
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:20:02.345
Sep  3 21:20:02.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:20:02.346
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:02.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:02.361
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 09/03/22 21:20:02.365
Sep  3 21:20:02.382: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e" in namespace "projected-8663" to be "Succeeded or Failed"
Sep  3 21:20:02.385: INFO: Pod "downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.22501ms
Sep  3 21:20:04.402: INFO: Pod "downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02025172s
Sep  3 21:20:06.390: INFO: Pod "downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008007031s
STEP: Saw pod success 09/03/22 21:20:06.39
Sep  3 21:20:06.391: INFO: Pod "downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e" satisfied condition "Succeeded or Failed"
Sep  3 21:20:06.393: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e container client-container: <nil>
STEP: delete the pod 09/03/22 21:20:06.407
Sep  3 21:20:06.416: INFO: Waiting for pod downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e to disappear
Sep  3 21:20:06.417: INFO: Pod downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Sep  3 21:20:06.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8663" for this suite. 09/03/22 21:20:06.419
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":135,"skipped":2238,"failed":0}
------------------------------
• [4.077 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:20:02.345
    Sep  3 21:20:02.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:20:02.346
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:02.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:02.361
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 09/03/22 21:20:02.365
    Sep  3 21:20:02.382: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e" in namespace "projected-8663" to be "Succeeded or Failed"
    Sep  3 21:20:02.385: INFO: Pod "downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.22501ms
    Sep  3 21:20:04.402: INFO: Pod "downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02025172s
    Sep  3 21:20:06.390: INFO: Pod "downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008007031s
    STEP: Saw pod success 09/03/22 21:20:06.39
    Sep  3 21:20:06.391: INFO: Pod "downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e" satisfied condition "Succeeded or Failed"
    Sep  3 21:20:06.393: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e container client-container: <nil>
    STEP: delete the pod 09/03/22 21:20:06.407
    Sep  3 21:20:06.416: INFO: Waiting for pod downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e to disappear
    Sep  3 21:20:06.417: INFO: Pod downwardapi-volume-f7e44b61-7287-48d0-8cdb-920b627b402e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Sep  3 21:20:06.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8663" for this suite. 09/03/22 21:20:06.419
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:20:06.434
Sep  3 21:20:06.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:20:06.435
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:06.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:06.445
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-7111f78c-0627-4f8b-a91c-3a766204c5f5 09/03/22 21:20:06.447
STEP: Creating a pod to test consume secrets 09/03/22 21:20:06.449
Sep  3 21:20:06.453: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c" in namespace "projected-806" to be "Succeeded or Failed"
Sep  3 21:20:06.455: INFO: Pod "pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103107ms
Sep  3 21:20:08.458: INFO: Pod "pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00530857s
Sep  3 21:20:10.458: INFO: Pod "pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004592058s
STEP: Saw pod success 09/03/22 21:20:10.458
Sep  3 21:20:10.458: INFO: Pod "pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c" satisfied condition "Succeeded or Failed"
Sep  3 21:20:10.459: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c container projected-secret-volume-test: <nil>
STEP: delete the pod 09/03/22 21:20:10.463
Sep  3 21:20:10.469: INFO: Waiting for pod pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c to disappear
Sep  3 21:20:10.471: INFO: Pod pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Sep  3 21:20:10.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-806" for this suite. 09/03/22 21:20:10.474
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":136,"skipped":2289,"failed":0}
------------------------------
• [4.042 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:20:06.434
    Sep  3 21:20:06.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:20:06.435
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:06.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:06.445
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-7111f78c-0627-4f8b-a91c-3a766204c5f5 09/03/22 21:20:06.447
    STEP: Creating a pod to test consume secrets 09/03/22 21:20:06.449
    Sep  3 21:20:06.453: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c" in namespace "projected-806" to be "Succeeded or Failed"
    Sep  3 21:20:06.455: INFO: Pod "pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103107ms
    Sep  3 21:20:08.458: INFO: Pod "pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00530857s
    Sep  3 21:20:10.458: INFO: Pod "pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004592058s
    STEP: Saw pod success 09/03/22 21:20:10.458
    Sep  3 21:20:10.458: INFO: Pod "pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c" satisfied condition "Succeeded or Failed"
    Sep  3 21:20:10.459: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 21:20:10.463
    Sep  3 21:20:10.469: INFO: Waiting for pod pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c to disappear
    Sep  3 21:20:10.471: INFO: Pod pod-projected-secrets-01a4024e-02b9-4fd3-89a1-8acc78cf7d8c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Sep  3 21:20:10.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-806" for this suite. 09/03/22 21:20:10.474
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:20:10.484
Sep  3 21:20:10.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 21:20:10.486
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:10.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:10.495
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-e38817b5-6a2e-4662-aac5-f76d910f28b4 09/03/22 21:20:10.497
STEP: Creating a pod to test consume configMaps 09/03/22 21:20:10.5
Sep  3 21:20:10.504: INFO: Waiting up to 5m0s for pod "pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5" in namespace "configmap-8135" to be "Succeeded or Failed"
Sep  3 21:20:10.508: INFO: Pod "pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.267206ms
Sep  3 21:20:12.511: INFO: Pod "pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006021749s
Sep  3 21:20:14.512: INFO: Pod "pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007474889s
STEP: Saw pod success 09/03/22 21:20:14.512
Sep  3 21:20:14.512: INFO: Pod "pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5" satisfied condition "Succeeded or Failed"
Sep  3 21:20:14.514: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5 container configmap-volume-test: <nil>
STEP: delete the pod 09/03/22 21:20:14.518
Sep  3 21:20:14.524: INFO: Waiting for pod pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5 to disappear
Sep  3 21:20:14.527: INFO: Pod pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 21:20:14.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8135" for this suite. 09/03/22 21:20:14.529
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":137,"skipped":2337,"failed":0}
------------------------------
• [4.049 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:20:10.484
    Sep  3 21:20:10.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 21:20:10.486
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:10.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:10.495
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-e38817b5-6a2e-4662-aac5-f76d910f28b4 09/03/22 21:20:10.497
    STEP: Creating a pod to test consume configMaps 09/03/22 21:20:10.5
    Sep  3 21:20:10.504: INFO: Waiting up to 5m0s for pod "pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5" in namespace "configmap-8135" to be "Succeeded or Failed"
    Sep  3 21:20:10.508: INFO: Pod "pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.267206ms
    Sep  3 21:20:12.511: INFO: Pod "pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006021749s
    Sep  3 21:20:14.512: INFO: Pod "pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007474889s
    STEP: Saw pod success 09/03/22 21:20:14.512
    Sep  3 21:20:14.512: INFO: Pod "pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5" satisfied condition "Succeeded or Failed"
    Sep  3 21:20:14.514: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5 container configmap-volume-test: <nil>
    STEP: delete the pod 09/03/22 21:20:14.518
    Sep  3 21:20:14.524: INFO: Waiting for pod pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5 to disappear
    Sep  3 21:20:14.527: INFO: Pod pod-configmaps-31053e27-2ea3-4965-b939-6c96850d9ee5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 21:20:14.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8135" for this suite. 09/03/22 21:20:14.529
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:20:14.541
Sep  3 21:20:14.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename watch 09/03/22 21:20:14.542
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:14.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:14.553
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 09/03/22 21:20:14.554
STEP: creating a new configmap 09/03/22 21:20:14.555
STEP: modifying the configmap once 09/03/22 21:20:14.558
STEP: closing the watch once it receives two notifications 09/03/22 21:20:14.562
Sep  3 21:20:14.562: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4972  b0bf7add-edd8-4fe1-9b2e-091bc52c427b 9972 0 2022-09-03 21:20:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-09-03 21:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:20:14.562: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4972  b0bf7add-edd8-4fe1-9b2e-091bc52c427b 9973 0 2022-09-03 21:20:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-09-03 21:20:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 09/03/22 21:20:14.562
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/03/22 21:20:14.566
STEP: deleting the configmap 09/03/22 21:20:14.567
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/03/22 21:20:14.57
Sep  3 21:20:14.570: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4972  b0bf7add-edd8-4fe1-9b2e-091bc52c427b 9974 0 2022-09-03 21:20:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-09-03 21:20:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:20:14.570: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4972  b0bf7add-edd8-4fe1-9b2e-091bc52c427b 9975 0 2022-09-03 21:20:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-09-03 21:20:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Sep  3 21:20:14.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4972" for this suite. 09/03/22 21:20:14.572
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":138,"skipped":2345,"failed":0}
------------------------------
• [0.034 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:20:14.541
    Sep  3 21:20:14.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename watch 09/03/22 21:20:14.542
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:14.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:14.553
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 09/03/22 21:20:14.554
    STEP: creating a new configmap 09/03/22 21:20:14.555
    STEP: modifying the configmap once 09/03/22 21:20:14.558
    STEP: closing the watch once it receives two notifications 09/03/22 21:20:14.562
    Sep  3 21:20:14.562: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4972  b0bf7add-edd8-4fe1-9b2e-091bc52c427b 9972 0 2022-09-03 21:20:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-09-03 21:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:20:14.562: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4972  b0bf7add-edd8-4fe1-9b2e-091bc52c427b 9973 0 2022-09-03 21:20:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-09-03 21:20:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 09/03/22 21:20:14.562
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/03/22 21:20:14.566
    STEP: deleting the configmap 09/03/22 21:20:14.567
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/03/22 21:20:14.57
    Sep  3 21:20:14.570: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4972  b0bf7add-edd8-4fe1-9b2e-091bc52c427b 9974 0 2022-09-03 21:20:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-09-03 21:20:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:20:14.570: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4972  b0bf7add-edd8-4fe1-9b2e-091bc52c427b 9975 0 2022-09-03 21:20:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-09-03 21:20:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Sep  3 21:20:14.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4972" for this suite. 09/03/22 21:20:14.572
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:20:14.576
Sep  3 21:20:14.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename dns 09/03/22 21:20:14.577
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:14.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:14.597
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 09/03/22 21:20:14.6
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 42.43.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.43.42_udp@PTR;check="$$(dig +tcp +noall +answer +search 42.43.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.43.42_tcp@PTR;sleep 1; done
 09/03/22 21:20:14.619
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 42.43.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.43.42_udp@PTR;check="$$(dig +tcp +noall +answer +search 42.43.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.43.42_tcp@PTR;sleep 1; done
 09/03/22 21:20:14.619
STEP: creating a pod to probe DNS 09/03/22 21:20:14.619
STEP: submitting the pod to kubernetes 09/03/22 21:20:14.62
Sep  3 21:20:14.647: INFO: Waiting up to 15m0s for pod "dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054" in namespace "dns-1498" to be "running"
Sep  3 21:20:14.656: INFO: Pod "dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054": Phase="Pending", Reason="", readiness=false. Elapsed: 9.347418ms
Sep  3 21:20:16.660: INFO: Pod "dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054": Phase="Running", Reason="", readiness=true. Elapsed: 2.012701661s
Sep  3 21:20:16.660: INFO: Pod "dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054" satisfied condition "running"
STEP: retrieving the pod 09/03/22 21:20:16.66
STEP: looking for the results for each expected name from probers 09/03/22 21:20:16.662
Sep  3 21:20:16.665: INFO: Unable to read wheezy_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:16.667: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:16.669: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:16.671: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:16.680: INFO: Unable to read jessie_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:16.683: INFO: Unable to read jessie_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:16.684: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:16.686: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:16.696: INFO: Lookups using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 failed for: [wheezy_udp@dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_udp@dns-test-service.dns-1498.svc.cluster.local jessie_tcp@dns-test-service.dns-1498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local]

Sep  3 21:20:21.700: INFO: Unable to read wheezy_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:21.702: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:21.706: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:21.709: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:21.721: INFO: Unable to read jessie_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:21.723: INFO: Unable to read jessie_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:21.726: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:21.728: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:21.737: INFO: Lookups using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 failed for: [wheezy_udp@dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_udp@dns-test-service.dns-1498.svc.cluster.local jessie_tcp@dns-test-service.dns-1498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local]

Sep  3 21:20:26.700: INFO: Unable to read wheezy_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:26.702: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:26.704: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:26.707: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:26.718: INFO: Unable to read jessie_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:26.720: INFO: Unable to read jessie_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:26.723: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:26.726: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:26.734: INFO: Lookups using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 failed for: [wheezy_udp@dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_udp@dns-test-service.dns-1498.svc.cluster.local jessie_tcp@dns-test-service.dns-1498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local]

Sep  3 21:20:31.699: INFO: Unable to read wheezy_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:31.701: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:31.703: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:31.705: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:31.716: INFO: Unable to read jessie_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:31.717: INFO: Unable to read jessie_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:31.720: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:31.722: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:31.729: INFO: Lookups using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 failed for: [wheezy_udp@dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_udp@dns-test-service.dns-1498.svc.cluster.local jessie_tcp@dns-test-service.dns-1498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local]

Sep  3 21:20:36.701: INFO: Unable to read wheezy_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:36.703: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:36.705: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:36.707: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:36.717: INFO: Unable to read jessie_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:36.720: INFO: Unable to read jessie_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:36.722: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:36.724: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:36.731: INFO: Lookups using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 failed for: [wheezy_udp@dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_udp@dns-test-service.dns-1498.svc.cluster.local jessie_tcp@dns-test-service.dns-1498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local]

Sep  3 21:20:41.700: INFO: Unable to read wheezy_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:41.702: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:41.704: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:41.706: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:41.716: INFO: Unable to read jessie_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:41.718: INFO: Unable to read jessie_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:41.720: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:41.723: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
Sep  3 21:20:41.762: INFO: Lookups using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 failed for: [wheezy_udp@dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_udp@dns-test-service.dns-1498.svc.cluster.local jessie_tcp@dns-test-service.dns-1498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local]

Sep  3 21:20:46.731: INFO: DNS probes using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 succeeded

STEP: deleting the pod 09/03/22 21:20:46.732
STEP: deleting the test service 09/03/22 21:20:46.761
STEP: deleting the test headless service 09/03/22 21:20:46.839
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Sep  3 21:20:46.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1498" for this suite. 09/03/22 21:20:46.88
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":139,"skipped":2359,"failed":0}
------------------------------
• [SLOW TEST] [32.315 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:20:14.576
    Sep  3 21:20:14.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename dns 09/03/22 21:20:14.577
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:14.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:14.597
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 09/03/22 21:20:14.6
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 42.43.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.43.42_udp@PTR;check="$$(dig +tcp +noall +answer +search 42.43.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.43.42_tcp@PTR;sleep 1; done
     09/03/22 21:20:14.619
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 42.43.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.43.42_udp@PTR;check="$$(dig +tcp +noall +answer +search 42.43.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.43.42_tcp@PTR;sleep 1; done
     09/03/22 21:20:14.619
    STEP: creating a pod to probe DNS 09/03/22 21:20:14.619
    STEP: submitting the pod to kubernetes 09/03/22 21:20:14.62
    Sep  3 21:20:14.647: INFO: Waiting up to 15m0s for pod "dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054" in namespace "dns-1498" to be "running"
    Sep  3 21:20:14.656: INFO: Pod "dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054": Phase="Pending", Reason="", readiness=false. Elapsed: 9.347418ms
    Sep  3 21:20:16.660: INFO: Pod "dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054": Phase="Running", Reason="", readiness=true. Elapsed: 2.012701661s
    Sep  3 21:20:16.660: INFO: Pod "dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054" satisfied condition "running"
    STEP: retrieving the pod 09/03/22 21:20:16.66
    STEP: looking for the results for each expected name from probers 09/03/22 21:20:16.662
    Sep  3 21:20:16.665: INFO: Unable to read wheezy_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:16.667: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:16.669: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:16.671: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:16.680: INFO: Unable to read jessie_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:16.683: INFO: Unable to read jessie_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:16.684: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:16.686: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:16.696: INFO: Lookups using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 failed for: [wheezy_udp@dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_udp@dns-test-service.dns-1498.svc.cluster.local jessie_tcp@dns-test-service.dns-1498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local]

    Sep  3 21:20:21.700: INFO: Unable to read wheezy_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:21.702: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:21.706: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:21.709: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:21.721: INFO: Unable to read jessie_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:21.723: INFO: Unable to read jessie_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:21.726: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:21.728: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:21.737: INFO: Lookups using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 failed for: [wheezy_udp@dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_udp@dns-test-service.dns-1498.svc.cluster.local jessie_tcp@dns-test-service.dns-1498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local]

    Sep  3 21:20:26.700: INFO: Unable to read wheezy_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:26.702: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:26.704: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:26.707: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:26.718: INFO: Unable to read jessie_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:26.720: INFO: Unable to read jessie_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:26.723: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:26.726: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:26.734: INFO: Lookups using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 failed for: [wheezy_udp@dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_udp@dns-test-service.dns-1498.svc.cluster.local jessie_tcp@dns-test-service.dns-1498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local]

    Sep  3 21:20:31.699: INFO: Unable to read wheezy_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:31.701: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:31.703: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:31.705: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:31.716: INFO: Unable to read jessie_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:31.717: INFO: Unable to read jessie_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:31.720: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:31.722: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:31.729: INFO: Lookups using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 failed for: [wheezy_udp@dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_udp@dns-test-service.dns-1498.svc.cluster.local jessie_tcp@dns-test-service.dns-1498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local]

    Sep  3 21:20:36.701: INFO: Unable to read wheezy_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:36.703: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:36.705: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:36.707: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:36.717: INFO: Unable to read jessie_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:36.720: INFO: Unable to read jessie_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:36.722: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:36.724: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:36.731: INFO: Lookups using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 failed for: [wheezy_udp@dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_udp@dns-test-service.dns-1498.svc.cluster.local jessie_tcp@dns-test-service.dns-1498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local]

    Sep  3 21:20:41.700: INFO: Unable to read wheezy_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:41.702: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:41.704: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:41.706: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:41.716: INFO: Unable to read jessie_udp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:41.718: INFO: Unable to read jessie_tcp@dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:41.720: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:41.723: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local from pod dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054: the server could not find the requested resource (get pods dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054)
    Sep  3 21:20:41.762: INFO: Lookups using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 failed for: [wheezy_udp@dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@dns-test-service.dns-1498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_udp@dns-test-service.dns-1498.svc.cluster.local jessie_tcp@dns-test-service.dns-1498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1498.svc.cluster.local]

    Sep  3 21:20:46.731: INFO: DNS probes using dns-1498/dns-test-2a01bfce-0ac1-4e90-bf31-67ada2523054 succeeded

    STEP: deleting the pod 09/03/22 21:20:46.732
    STEP: deleting the test service 09/03/22 21:20:46.761
    STEP: deleting the test headless service 09/03/22 21:20:46.839
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Sep  3 21:20:46.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1498" for this suite. 09/03/22 21:20:46.88
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:20:46.916
Sep  3 21:20:46.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:20:46.922
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:46.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:46.958
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 09/03/22 21:20:46.964
Sep  3 21:20:46.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 create -f -'
Sep  3 21:20:48.014: INFO: stderr: ""
Sep  3 21:20:48.014: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/03/22 21:20:48.014
Sep  3 21:20:48.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  3 21:20:48.206: INFO: stderr: ""
Sep  3 21:20:48.206: INFO: stdout: "update-demo-nautilus-5ds75 update-demo-nautilus-m2ql5 "
Sep  3 21:20:48.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods update-demo-nautilus-5ds75 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  3 21:20:48.282: INFO: stderr: ""
Sep  3 21:20:48.282: INFO: stdout: ""
Sep  3 21:20:48.282: INFO: update-demo-nautilus-5ds75 is created but not running
Sep  3 21:20:53.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  3 21:20:53.376: INFO: stderr: ""
Sep  3 21:20:53.376: INFO: stdout: "update-demo-nautilus-5ds75 update-demo-nautilus-m2ql5 "
Sep  3 21:20:53.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods update-demo-nautilus-5ds75 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  3 21:20:53.478: INFO: stderr: ""
Sep  3 21:20:53.478: INFO: stdout: ""
Sep  3 21:20:53.478: INFO: update-demo-nautilus-5ds75 is created but not running
Sep  3 21:20:58.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  3 21:20:58.551: INFO: stderr: ""
Sep  3 21:20:58.551: INFO: stdout: "update-demo-nautilus-5ds75 update-demo-nautilus-m2ql5 "
Sep  3 21:20:58.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods update-demo-nautilus-5ds75 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  3 21:20:58.621: INFO: stderr: ""
Sep  3 21:20:58.621: INFO: stdout: "true"
Sep  3 21:20:58.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods update-demo-nautilus-5ds75 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  3 21:20:58.696: INFO: stderr: ""
Sep  3 21:20:58.696: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Sep  3 21:20:58.696: INFO: validating pod update-demo-nautilus-5ds75
Sep  3 21:20:58.699: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 21:20:58.699: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 21:20:58.699: INFO: update-demo-nautilus-5ds75 is verified up and running
Sep  3 21:20:58.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods update-demo-nautilus-m2ql5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  3 21:20:58.778: INFO: stderr: ""
Sep  3 21:20:58.778: INFO: stdout: "true"
Sep  3 21:20:58.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods update-demo-nautilus-m2ql5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  3 21:20:58.849: INFO: stderr: ""
Sep  3 21:20:58.849: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Sep  3 21:20:58.849: INFO: validating pod update-demo-nautilus-m2ql5
Sep  3 21:20:58.853: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 21:20:58.853: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 21:20:58.853: INFO: update-demo-nautilus-m2ql5 is verified up and running
STEP: using delete to clean up resources 09/03/22 21:20:58.853
Sep  3 21:20:58.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 delete --grace-period=0 --force -f -'
Sep  3 21:20:58.928: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 21:20:58.928: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  3 21:20:58.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get rc,svc -l name=update-demo --no-headers'
Sep  3 21:20:59.534: INFO: stderr: "No resources found in kubectl-212 namespace.\n"
Sep  3 21:20:59.534: INFO: stdout: ""
Sep  3 21:20:59.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  3 21:20:59.609: INFO: stderr: ""
Sep  3 21:20:59.609: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:20:59.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-212" for this suite. 09/03/22 21:20:59.611
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":140,"skipped":2377,"failed":0}
------------------------------
• [SLOW TEST] [12.700 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:20:46.916
    Sep  3 21:20:46.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:20:46.922
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:46.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:46.958
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 09/03/22 21:20:46.964
    Sep  3 21:20:46.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 create -f -'
    Sep  3 21:20:48.014: INFO: stderr: ""
    Sep  3 21:20:48.014: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/03/22 21:20:48.014
    Sep  3 21:20:48.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  3 21:20:48.206: INFO: stderr: ""
    Sep  3 21:20:48.206: INFO: stdout: "update-demo-nautilus-5ds75 update-demo-nautilus-m2ql5 "
    Sep  3 21:20:48.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods update-demo-nautilus-5ds75 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  3 21:20:48.282: INFO: stderr: ""
    Sep  3 21:20:48.282: INFO: stdout: ""
    Sep  3 21:20:48.282: INFO: update-demo-nautilus-5ds75 is created but not running
    Sep  3 21:20:53.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  3 21:20:53.376: INFO: stderr: ""
    Sep  3 21:20:53.376: INFO: stdout: "update-demo-nautilus-5ds75 update-demo-nautilus-m2ql5 "
    Sep  3 21:20:53.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods update-demo-nautilus-5ds75 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  3 21:20:53.478: INFO: stderr: ""
    Sep  3 21:20:53.478: INFO: stdout: ""
    Sep  3 21:20:53.478: INFO: update-demo-nautilus-5ds75 is created but not running
    Sep  3 21:20:58.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  3 21:20:58.551: INFO: stderr: ""
    Sep  3 21:20:58.551: INFO: stdout: "update-demo-nautilus-5ds75 update-demo-nautilus-m2ql5 "
    Sep  3 21:20:58.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods update-demo-nautilus-5ds75 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  3 21:20:58.621: INFO: stderr: ""
    Sep  3 21:20:58.621: INFO: stdout: "true"
    Sep  3 21:20:58.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods update-demo-nautilus-5ds75 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  3 21:20:58.696: INFO: stderr: ""
    Sep  3 21:20:58.696: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Sep  3 21:20:58.696: INFO: validating pod update-demo-nautilus-5ds75
    Sep  3 21:20:58.699: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  3 21:20:58.699: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  3 21:20:58.699: INFO: update-demo-nautilus-5ds75 is verified up and running
    Sep  3 21:20:58.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods update-demo-nautilus-m2ql5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  3 21:20:58.778: INFO: stderr: ""
    Sep  3 21:20:58.778: INFO: stdout: "true"
    Sep  3 21:20:58.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods update-demo-nautilus-m2ql5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  3 21:20:58.849: INFO: stderr: ""
    Sep  3 21:20:58.849: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Sep  3 21:20:58.849: INFO: validating pod update-demo-nautilus-m2ql5
    Sep  3 21:20:58.853: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  3 21:20:58.853: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  3 21:20:58.853: INFO: update-demo-nautilus-m2ql5 is verified up and running
    STEP: using delete to clean up resources 09/03/22 21:20:58.853
    Sep  3 21:20:58.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 delete --grace-period=0 --force -f -'
    Sep  3 21:20:58.928: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  3 21:20:58.928: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  3 21:20:58.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get rc,svc -l name=update-demo --no-headers'
    Sep  3 21:20:59.534: INFO: stderr: "No resources found in kubectl-212 namespace.\n"
    Sep  3 21:20:59.534: INFO: stdout: ""
    Sep  3 21:20:59.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-212 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  3 21:20:59.609: INFO: stderr: ""
    Sep  3 21:20:59.609: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:20:59.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-212" for this suite. 09/03/22 21:20:59.611
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:20:59.616
Sep  3 21:20:59.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename replication-controller 09/03/22 21:20:59.618
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:59.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:59.644
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Sep  3 21:20:59.724: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/03/22 21:21:00.782
STEP: Checking rc "condition-test" has the desired failure condition set 09/03/22 21:21:00.787
STEP: Scaling down rc "condition-test" to satisfy pod quota 09/03/22 21:21:01.793
Sep  3 21:21:01.798: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 09/03/22 21:21:01.799
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Sep  3 21:21:02.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9393" for this suite. 09/03/22 21:21:02.809
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":141,"skipped":2400,"failed":0}
------------------------------
• [3.197 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:20:59.616
    Sep  3 21:20:59.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename replication-controller 09/03/22 21:20:59.618
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:20:59.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:20:59.644
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Sep  3 21:20:59.724: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/03/22 21:21:00.782
    STEP: Checking rc "condition-test" has the desired failure condition set 09/03/22 21:21:00.787
    STEP: Scaling down rc "condition-test" to satisfy pod quota 09/03/22 21:21:01.793
    Sep  3 21:21:01.798: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 09/03/22 21:21:01.799
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Sep  3 21:21:02.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9393" for this suite. 09/03/22 21:21:02.809
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:21:02.815
Sep  3 21:21:02.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 21:21:02.816
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:02.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:02.83
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-9c4b87fe-2a73-4680-9c38-c5087f491822 09/03/22 21:21:02.836
STEP: Creating the pod 09/03/22 21:21:02.839
Sep  3 21:21:02.844: INFO: Waiting up to 5m0s for pod "pod-configmaps-25802ef1-349a-4a0b-9845-14ee14e5b747" in namespace "configmap-7487" to be "running"
Sep  3 21:21:02.847: INFO: Pod "pod-configmaps-25802ef1-349a-4a0b-9845-14ee14e5b747": Phase="Pending", Reason="", readiness=false. Elapsed: 2.655705ms
Sep  3 21:21:04.849: INFO: Pod "pod-configmaps-25802ef1-349a-4a0b-9845-14ee14e5b747": Phase="Running", Reason="", readiness=false. Elapsed: 2.00522608s
Sep  3 21:21:04.849: INFO: Pod "pod-configmaps-25802ef1-349a-4a0b-9845-14ee14e5b747" satisfied condition "running"
STEP: Waiting for pod with text data 09/03/22 21:21:04.849
STEP: Waiting for pod with binary data 09/03/22 21:21:04.854
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 21:21:04.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7487" for this suite. 09/03/22 21:21:04.865
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":142,"skipped":2405,"failed":0}
------------------------------
• [2.055 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:21:02.815
    Sep  3 21:21:02.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 21:21:02.816
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:02.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:02.83
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-9c4b87fe-2a73-4680-9c38-c5087f491822 09/03/22 21:21:02.836
    STEP: Creating the pod 09/03/22 21:21:02.839
    Sep  3 21:21:02.844: INFO: Waiting up to 5m0s for pod "pod-configmaps-25802ef1-349a-4a0b-9845-14ee14e5b747" in namespace "configmap-7487" to be "running"
    Sep  3 21:21:02.847: INFO: Pod "pod-configmaps-25802ef1-349a-4a0b-9845-14ee14e5b747": Phase="Pending", Reason="", readiness=false. Elapsed: 2.655705ms
    Sep  3 21:21:04.849: INFO: Pod "pod-configmaps-25802ef1-349a-4a0b-9845-14ee14e5b747": Phase="Running", Reason="", readiness=false. Elapsed: 2.00522608s
    Sep  3 21:21:04.849: INFO: Pod "pod-configmaps-25802ef1-349a-4a0b-9845-14ee14e5b747" satisfied condition "running"
    STEP: Waiting for pod with text data 09/03/22 21:21:04.849
    STEP: Waiting for pod with binary data 09/03/22 21:21:04.854
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 21:21:04.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7487" for this suite. 09/03/22 21:21:04.865
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:21:04.876
Sep  3 21:21:04.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename dns 09/03/22 21:21:04.877
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:04.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:04.891
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 09/03/22 21:21:04.894
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8803.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8803.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 09/03/22 21:21:04.898
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8803.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8803.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 09/03/22 21:21:04.898
STEP: creating a pod to probe DNS 09/03/22 21:21:04.898
STEP: submitting the pod to kubernetes 09/03/22 21:21:04.898
Sep  3 21:21:04.908: INFO: Waiting up to 15m0s for pod "dns-test-7eea3f27-5b7c-4e63-84e8-39c4aee4e41a" in namespace "dns-8803" to be "running"
Sep  3 21:21:04.923: INFO: Pod "dns-test-7eea3f27-5b7c-4e63-84e8-39c4aee4e41a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.724629ms
Sep  3 21:21:06.926: INFO: Pod "dns-test-7eea3f27-5b7c-4e63-84e8-39c4aee4e41a": Phase="Running", Reason="", readiness=true. Elapsed: 2.017974387s
Sep  3 21:21:06.926: INFO: Pod "dns-test-7eea3f27-5b7c-4e63-84e8-39c4aee4e41a" satisfied condition "running"
STEP: retrieving the pod 09/03/22 21:21:06.926
STEP: looking for the results for each expected name from probers 09/03/22 21:21:06.928
Sep  3 21:21:06.938: INFO: DNS probes using dns-8803/dns-test-7eea3f27-5b7c-4e63-84e8-39c4aee4e41a succeeded

STEP: deleting the pod 09/03/22 21:21:06.938
STEP: deleting the test headless service 09/03/22 21:21:06.949
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Sep  3 21:21:06.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8803" for this suite. 09/03/22 21:21:06.968
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":143,"skipped":2454,"failed":0}
------------------------------
• [2.109 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:21:04.876
    Sep  3 21:21:04.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename dns 09/03/22 21:21:04.877
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:04.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:04.891
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 09/03/22 21:21:04.894
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8803.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8803.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     09/03/22 21:21:04.898
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8803.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8803.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     09/03/22 21:21:04.898
    STEP: creating a pod to probe DNS 09/03/22 21:21:04.898
    STEP: submitting the pod to kubernetes 09/03/22 21:21:04.898
    Sep  3 21:21:04.908: INFO: Waiting up to 15m0s for pod "dns-test-7eea3f27-5b7c-4e63-84e8-39c4aee4e41a" in namespace "dns-8803" to be "running"
    Sep  3 21:21:04.923: INFO: Pod "dns-test-7eea3f27-5b7c-4e63-84e8-39c4aee4e41a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.724629ms
    Sep  3 21:21:06.926: INFO: Pod "dns-test-7eea3f27-5b7c-4e63-84e8-39c4aee4e41a": Phase="Running", Reason="", readiness=true. Elapsed: 2.017974387s
    Sep  3 21:21:06.926: INFO: Pod "dns-test-7eea3f27-5b7c-4e63-84e8-39c4aee4e41a" satisfied condition "running"
    STEP: retrieving the pod 09/03/22 21:21:06.926
    STEP: looking for the results for each expected name from probers 09/03/22 21:21:06.928
    Sep  3 21:21:06.938: INFO: DNS probes using dns-8803/dns-test-7eea3f27-5b7c-4e63-84e8-39c4aee4e41a succeeded

    STEP: deleting the pod 09/03/22 21:21:06.938
    STEP: deleting the test headless service 09/03/22 21:21:06.949
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Sep  3 21:21:06.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8803" for this suite. 09/03/22 21:21:06.968
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:21:06.986
Sep  3 21:21:06.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename init-container 09/03/22 21:21:06.987
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:06.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:07
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 09/03/22 21:21:07.002
Sep  3 21:21:07.002: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Sep  3 21:21:10.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8416" for this suite. 09/03/22 21:21:10.547
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":144,"skipped":2455,"failed":0}
------------------------------
• [3.566 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:21:06.986
    Sep  3 21:21:06.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename init-container 09/03/22 21:21:06.987
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:06.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:07
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 09/03/22 21:21:07.002
    Sep  3 21:21:07.002: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Sep  3 21:21:10.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-8416" for this suite. 09/03/22 21:21:10.547
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:21:10.552
Sep  3 21:21:10.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename dns 09/03/22 21:21:10.553
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:10.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:10.575
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/03/22 21:21:10.578
Sep  3 21:21:10.586: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6066  16ebf7f3-4d39-48ff-9266-23c15c37eab6 10367 0 2022-09-03 21:21:10 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-09-03 21:21:10 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8jhp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8jhp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:21:10.587: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6066" to be "running and ready"
Sep  3 21:21:10.591: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394709ms
Sep  3 21:21:10.591: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:21:12.594: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.007203445s
Sep  3 21:21:12.594: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Sep  3 21:21:12.594: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 09/03/22 21:21:12.594
Sep  3 21:21:12.594: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6066 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 21:21:12.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 21:21:12.595: INFO: ExecWithOptions: Clientset creation
Sep  3 21:21:12.595: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-6066/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 09/03/22 21:21:12.704
Sep  3 21:21:12.705: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6066 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 21:21:12.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 21:21:12.706: INFO: ExecWithOptions: Clientset creation
Sep  3 21:21:12.706: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-6066/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  3 21:21:12.784: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Sep  3 21:21:12.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6066" for this suite. 09/03/22 21:21:12.797
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":145,"skipped":2468,"failed":0}
------------------------------
• [2.248 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:21:10.552
    Sep  3 21:21:10.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename dns 09/03/22 21:21:10.553
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:10.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:10.575
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/03/22 21:21:10.578
    Sep  3 21:21:10.586: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6066  16ebf7f3-4d39-48ff-9266-23c15c37eab6 10367 0 2022-09-03 21:21:10 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-09-03 21:21:10 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8jhp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8jhp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:21:10.587: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6066" to be "running and ready"
    Sep  3 21:21:10.591: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394709ms
    Sep  3 21:21:10.591: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:21:12.594: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.007203445s
    Sep  3 21:21:12.594: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Sep  3 21:21:12.594: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 09/03/22 21:21:12.594
    Sep  3 21:21:12.594: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6066 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 21:21:12.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 21:21:12.595: INFO: ExecWithOptions: Clientset creation
    Sep  3 21:21:12.595: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-6066/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 09/03/22 21:21:12.704
    Sep  3 21:21:12.705: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6066 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 21:21:12.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 21:21:12.706: INFO: ExecWithOptions: Clientset creation
    Sep  3 21:21:12.706: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-6066/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  3 21:21:12.784: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Sep  3 21:21:12.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6066" for this suite. 09/03/22 21:21:12.797
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:21:12.801
Sep  3 21:21:12.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:21:12.802
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:12.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:12.813
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 09/03/22 21:21:12.815
Sep  3 21:21:12.815: INFO: namespace kubectl-7256
Sep  3 21:21:12.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-7256 create -f -'
Sep  3 21:21:13.061: INFO: stderr: ""
Sep  3 21:21:13.061: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/03/22 21:21:13.061
Sep  3 21:21:14.065: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 21:21:14.065: INFO: Found 0 / 1
Sep  3 21:21:15.066: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 21:21:15.066: INFO: Found 1 / 1
Sep  3 21:21:15.066: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  3 21:21:15.068: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 21:21:15.068: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  3 21:21:15.068: INFO: wait on agnhost-primary startup in kubectl-7256 
Sep  3 21:21:15.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-7256 logs agnhost-primary-s5ffc agnhost-primary'
Sep  3 21:21:15.142: INFO: stderr: ""
Sep  3 21:21:15.142: INFO: stdout: "Paused\n"
STEP: exposing RC 09/03/22 21:21:15.142
Sep  3 21:21:15.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-7256 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep  3 21:21:15.252: INFO: stderr: ""
Sep  3 21:21:15.252: INFO: stdout: "service/rm2 exposed\n"
Sep  3 21:21:15.263: INFO: Service rm2 in namespace kubectl-7256 found.
STEP: exposing service 09/03/22 21:21:17.267
Sep  3 21:21:17.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-7256 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep  3 21:21:17.354: INFO: stderr: ""
Sep  3 21:21:17.354: INFO: stdout: "service/rm3 exposed\n"
Sep  3 21:21:17.360: INFO: Service rm3 in namespace kubectl-7256 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:21:19.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7256" for this suite. 09/03/22 21:21:19.367
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":146,"skipped":2471,"failed":0}
------------------------------
• [SLOW TEST] [6.570 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:21:12.801
    Sep  3 21:21:12.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:21:12.802
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:12.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:12.813
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 09/03/22 21:21:12.815
    Sep  3 21:21:12.815: INFO: namespace kubectl-7256
    Sep  3 21:21:12.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-7256 create -f -'
    Sep  3 21:21:13.061: INFO: stderr: ""
    Sep  3 21:21:13.061: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/03/22 21:21:13.061
    Sep  3 21:21:14.065: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  3 21:21:14.065: INFO: Found 0 / 1
    Sep  3 21:21:15.066: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  3 21:21:15.066: INFO: Found 1 / 1
    Sep  3 21:21:15.066: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  3 21:21:15.068: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  3 21:21:15.068: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  3 21:21:15.068: INFO: wait on agnhost-primary startup in kubectl-7256 
    Sep  3 21:21:15.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-7256 logs agnhost-primary-s5ffc agnhost-primary'
    Sep  3 21:21:15.142: INFO: stderr: ""
    Sep  3 21:21:15.142: INFO: stdout: "Paused\n"
    STEP: exposing RC 09/03/22 21:21:15.142
    Sep  3 21:21:15.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-7256 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Sep  3 21:21:15.252: INFO: stderr: ""
    Sep  3 21:21:15.252: INFO: stdout: "service/rm2 exposed\n"
    Sep  3 21:21:15.263: INFO: Service rm2 in namespace kubectl-7256 found.
    STEP: exposing service 09/03/22 21:21:17.267
    Sep  3 21:21:17.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-7256 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Sep  3 21:21:17.354: INFO: stderr: ""
    Sep  3 21:21:17.354: INFO: stdout: "service/rm3 exposed\n"
    Sep  3 21:21:17.360: INFO: Service rm3 in namespace kubectl-7256 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:21:19.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7256" for this suite. 09/03/22 21:21:19.367
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:21:19.372
Sep  3 21:21:19.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pods 09/03/22 21:21:19.372
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:19.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:19.384
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 09/03/22 21:21:19.386
STEP: submitting the pod to kubernetes 09/03/22 21:21:19.386
Sep  3 21:21:19.391: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a" in namespace "pods-6995" to be "running and ready"
Sep  3 21:21:19.394: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.548207ms
Sep  3 21:21:19.394: INFO: The phase of Pod pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:21:21.399: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008221955s
Sep  3 21:21:21.399: INFO: The phase of Pod pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a is Running (Ready = true)
Sep  3 21:21:21.399: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/03/22 21:21:21.4
STEP: updating the pod 09/03/22 21:21:21.402
Sep  3 21:21:21.911: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a"
Sep  3 21:21:21.911: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a" in namespace "pods-6995" to be "terminated with reason DeadlineExceeded"
Sep  3 21:21:21.913: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a": Phase="Running", Reason="", readiness=true. Elapsed: 1.508003ms
Sep  3 21:21:23.916: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004967085s
Sep  3 21:21:25.916: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.00431816s
Sep  3 21:21:25.916: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Sep  3 21:21:25.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6995" for this suite. 09/03/22 21:21:25.919
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":147,"skipped":2491,"failed":0}
------------------------------
• [SLOW TEST] [6.550 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:21:19.372
    Sep  3 21:21:19.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pods 09/03/22 21:21:19.372
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:19.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:19.384
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 09/03/22 21:21:19.386
    STEP: submitting the pod to kubernetes 09/03/22 21:21:19.386
    Sep  3 21:21:19.391: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a" in namespace "pods-6995" to be "running and ready"
    Sep  3 21:21:19.394: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.548207ms
    Sep  3 21:21:19.394: INFO: The phase of Pod pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:21:21.399: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008221955s
    Sep  3 21:21:21.399: INFO: The phase of Pod pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a is Running (Ready = true)
    Sep  3 21:21:21.399: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/03/22 21:21:21.4
    STEP: updating the pod 09/03/22 21:21:21.402
    Sep  3 21:21:21.911: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a"
    Sep  3 21:21:21.911: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a" in namespace "pods-6995" to be "terminated with reason DeadlineExceeded"
    Sep  3 21:21:21.913: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a": Phase="Running", Reason="", readiness=true. Elapsed: 1.508003ms
    Sep  3 21:21:23.916: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004967085s
    Sep  3 21:21:25.916: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.00431816s
    Sep  3 21:21:25.916: INFO: Pod "pod-update-activedeadlineseconds-e4d248a9-d940-434a-a937-e728ab8cd88a" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Sep  3 21:21:25.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6995" for this suite. 09/03/22 21:21:25.919
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:21:25.927
Sep  3 21:21:25.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:21:25.928
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:25.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:25.946
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 09/03/22 21:21:25.948
Sep  3 21:21:25.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-284 cluster-info'
Sep  3 21:21:26.046: INFO: stderr: ""
Sep  3 21:21:26.046: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:21:26.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-284" for this suite. 09/03/22 21:21:26.049
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":148,"skipped":2518,"failed":0}
------------------------------
• [0.126 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:21:25.927
    Sep  3 21:21:25.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:21:25.928
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:25.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:25.946
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 09/03/22 21:21:25.948
    Sep  3 21:21:25.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-284 cluster-info'
    Sep  3 21:21:26.046: INFO: stderr: ""
    Sep  3 21:21:26.046: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:21:26.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-284" for this suite. 09/03/22 21:21:26.049
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:21:26.057
Sep  3 21:21:26.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:21:26.059
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:26.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:26.074
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-c7bb9716-a0eb-4645-aaa6-172c67cfb5e2 09/03/22 21:21:26.077
STEP: Creating secret with name secret-projected-all-test-volume-107f5da1-69f1-4c7f-8b16-487d9fee57cf 09/03/22 21:21:26.079
STEP: Creating a pod to test Check all projections for projected volume plugin 09/03/22 21:21:26.082
Sep  3 21:21:26.086: INFO: Waiting up to 5m0s for pod "projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167" in namespace "projected-9424" to be "Succeeded or Failed"
Sep  3 21:21:26.093: INFO: Pod "projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167": Phase="Pending", Reason="", readiness=false. Elapsed: 6.421313ms
Sep  3 21:21:28.095: INFO: Pod "projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009198395s
Sep  3 21:21:30.097: INFO: Pod "projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010435076s
STEP: Saw pod success 09/03/22 21:21:30.097
Sep  3 21:21:30.097: INFO: Pod "projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167" satisfied condition "Succeeded or Failed"
Sep  3 21:21:30.099: INFO: Trying to get logs from node kind-worker2 pod projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167 container projected-all-volume-test: <nil>
STEP: delete the pod 09/03/22 21:21:30.102
Sep  3 21:21:30.109: INFO: Waiting for pod projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167 to disappear
Sep  3 21:21:30.111: INFO: Pod projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Sep  3 21:21:30.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9424" for this suite. 09/03/22 21:21:30.113
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":149,"skipped":2542,"failed":0}
------------------------------
• [4.060 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:21:26.057
    Sep  3 21:21:26.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:21:26.059
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:26.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:26.074
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-c7bb9716-a0eb-4645-aaa6-172c67cfb5e2 09/03/22 21:21:26.077
    STEP: Creating secret with name secret-projected-all-test-volume-107f5da1-69f1-4c7f-8b16-487d9fee57cf 09/03/22 21:21:26.079
    STEP: Creating a pod to test Check all projections for projected volume plugin 09/03/22 21:21:26.082
    Sep  3 21:21:26.086: INFO: Waiting up to 5m0s for pod "projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167" in namespace "projected-9424" to be "Succeeded or Failed"
    Sep  3 21:21:26.093: INFO: Pod "projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167": Phase="Pending", Reason="", readiness=false. Elapsed: 6.421313ms
    Sep  3 21:21:28.095: INFO: Pod "projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009198395s
    Sep  3 21:21:30.097: INFO: Pod "projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010435076s
    STEP: Saw pod success 09/03/22 21:21:30.097
    Sep  3 21:21:30.097: INFO: Pod "projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167" satisfied condition "Succeeded or Failed"
    Sep  3 21:21:30.099: INFO: Trying to get logs from node kind-worker2 pod projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167 container projected-all-volume-test: <nil>
    STEP: delete the pod 09/03/22 21:21:30.102
    Sep  3 21:21:30.109: INFO: Waiting for pod projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167 to disappear
    Sep  3 21:21:30.111: INFO: Pod projected-volume-9d604baa-48df-4e0d-b6fc-f0bafc728167 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Sep  3 21:21:30.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9424" for this suite. 09/03/22 21:21:30.113
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:21:30.125
Sep  3 21:21:30.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 21:21:30.127
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:30.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:30.14
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/03/22 21:21:30.142
Sep  3 21:21:30.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 21:21:32.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:21:42.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6738" for this suite. 09/03/22 21:21:43.002
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":150,"skipped":2595,"failed":0}
------------------------------
• [SLOW TEST] [12.880 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:21:30.125
    Sep  3 21:21:30.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 21:21:30.127
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:30.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:30.14
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/03/22 21:21:30.142
    Sep  3 21:21:30.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 21:21:32.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:21:42.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6738" for this suite. 09/03/22 21:21:43.002
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:21:43.011
Sep  3 21:21:43.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename init-container 09/03/22 21:21:43.012
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:43.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:43.023
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 09/03/22 21:21:43.026
Sep  3 21:21:43.026: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Sep  3 21:21:48.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4348" for this suite. 09/03/22 21:21:48.741
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":151,"skipped":2645,"failed":0}
------------------------------
• [SLOW TEST] [5.734 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:21:43.011
    Sep  3 21:21:43.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename init-container 09/03/22 21:21:43.012
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:43.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:43.023
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 09/03/22 21:21:43.026
    Sep  3 21:21:43.026: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Sep  3 21:21:48.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4348" for this suite. 09/03/22 21:21:48.741
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:21:48.745
Sep  3 21:21:48.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-probe 09/03/22 21:21:48.746
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:48.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:48.757
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 in namespace container-probe-3722 09/03/22 21:21:48.759
Sep  3 21:21:48.764: INFO: Waiting up to 5m0s for pod "liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290" in namespace "container-probe-3722" to be "not pending"
Sep  3 21:21:48.767: INFO: Pod "liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290": Phase="Pending", Reason="", readiness=false. Elapsed: 2.225005ms
Sep  3 21:21:50.770: INFO: Pod "liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290": Phase="Running", Reason="", readiness=true. Elapsed: 2.004879449s
Sep  3 21:21:50.770: INFO: Pod "liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290" satisfied condition "not pending"
Sep  3 21:21:50.770: INFO: Started pod liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 in namespace container-probe-3722
STEP: checking the pod's current state and verifying that restartCount is present 09/03/22 21:21:50.77
Sep  3 21:21:50.771: INFO: Initial restart count of pod liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 is 0
Sep  3 21:22:10.804: INFO: Restart count of pod container-probe-3722/liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 is now 1 (20.032600104s elapsed)
Sep  3 21:22:30.835: INFO: Restart count of pod container-probe-3722/liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 is now 2 (40.064008935s elapsed)
Sep  3 21:22:50.870: INFO: Restart count of pod container-probe-3722/liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 is now 3 (1m0.098271163s elapsed)
Sep  3 21:23:10.902: INFO: Restart count of pod container-probe-3722/liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 is now 4 (1m20.13074359s elapsed)
Sep  3 21:24:13.002: INFO: Restart count of pod container-probe-3722/liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 is now 5 (2m22.230380018s elapsed)
STEP: deleting the pod 09/03/22 21:24:13.002
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Sep  3 21:24:13.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3722" for this suite. 09/03/22 21:24:13.025
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":152,"skipped":2646,"failed":0}
------------------------------
• [SLOW TEST] [144.285 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:21:48.745
    Sep  3 21:21:48.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-probe 09/03/22 21:21:48.746
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:21:48.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:21:48.757
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 in namespace container-probe-3722 09/03/22 21:21:48.759
    Sep  3 21:21:48.764: INFO: Waiting up to 5m0s for pod "liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290" in namespace "container-probe-3722" to be "not pending"
    Sep  3 21:21:48.767: INFO: Pod "liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290": Phase="Pending", Reason="", readiness=false. Elapsed: 2.225005ms
    Sep  3 21:21:50.770: INFO: Pod "liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290": Phase="Running", Reason="", readiness=true. Elapsed: 2.004879449s
    Sep  3 21:21:50.770: INFO: Pod "liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290" satisfied condition "not pending"
    Sep  3 21:21:50.770: INFO: Started pod liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 in namespace container-probe-3722
    STEP: checking the pod's current state and verifying that restartCount is present 09/03/22 21:21:50.77
    Sep  3 21:21:50.771: INFO: Initial restart count of pod liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 is 0
    Sep  3 21:22:10.804: INFO: Restart count of pod container-probe-3722/liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 is now 1 (20.032600104s elapsed)
    Sep  3 21:22:30.835: INFO: Restart count of pod container-probe-3722/liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 is now 2 (40.064008935s elapsed)
    Sep  3 21:22:50.870: INFO: Restart count of pod container-probe-3722/liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 is now 3 (1m0.098271163s elapsed)
    Sep  3 21:23:10.902: INFO: Restart count of pod container-probe-3722/liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 is now 4 (1m20.13074359s elapsed)
    Sep  3 21:24:13.002: INFO: Restart count of pod container-probe-3722/liveness-1f087e7c-aaf2-4a0a-9aee-44efc88cd290 is now 5 (2m22.230380018s elapsed)
    STEP: deleting the pod 09/03/22 21:24:13.002
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Sep  3 21:24:13.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3722" for this suite. 09/03/22 21:24:13.025
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:24:13.034
Sep  3 21:24:13.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename daemonsets 09/03/22 21:24:13.035
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:24:13.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:24:13.051
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 09/03/22 21:24:13.072
STEP: Check that daemon pods launch on every node of the cluster. 09/03/22 21:24:13.078
Sep  3 21:24:13.083: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:24:13.088: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:24:13.089: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 21:24:14.091: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:24:14.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  3 21:24:14.094: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  3 21:24:15.092: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:24:15.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  3 21:24:15.094: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 09/03/22 21:24:15.096
STEP: DeleteCollection of the DaemonSets 09/03/22 21:24:15.099
STEP: Verify that ReplicaSets have been deleted 09/03/22 21:24:15.103
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Sep  3 21:24:15.123: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10932"},"items":null}

Sep  3 21:24:15.127: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10932"},"items":[{"metadata":{"name":"daemon-set-6svwb","generateName":"daemon-set-","namespace":"daemonsets-6176","uid":"5f9d9e0b-1d0f-439f-b273-59d91c18680b","resourceVersion":"10922","creationTimestamp":"2022-09-03T21:24:13Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e060cb05-fe63-42da-bd05-3d22271f6832","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-09-03T21:24:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e060cb05-fe63-42da-bd05-3d22271f6832\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-09-03T21:24:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-647n2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-647n2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kind-worker","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kind-worker"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:13Z"}],"hostIP":"172.18.0.2","podIP":"10.244.2.28","podIPs":[{"ip":"10.244.2.28"}],"startTime":"2022-09-03T21:24:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-09-03T21:24:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://d7ecc051643da18a0e6038eb12c9fa418d888eb3d86ab1de8c64c344fc8fe062","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fpk59","generateName":"daemon-set-","namespace":"daemonsets-6176","uid":"0e385195-0712-4ea6-989e-8a11386421ed","resourceVersion":"10930","creationTimestamp":"2022-09-03T21:24:13Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e060cb05-fe63-42da-bd05-3d22271f6832","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-09-03T21:24:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e060cb05-fe63-42da-bd05-3d22271f6832\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-09-03T21:24:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gzgvk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gzgvk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kind-worker2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kind-worker2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:13Z"}],"hostIP":"172.18.0.3","podIP":"10.244.1.152","podIPs":[{"ip":"10.244.1.152"}],"startTime":"2022-09-03T21:24:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-09-03T21:24:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://2d4db4611c5914b30a5f9977f539d0b7f3d608d990fd949a2f874bbc3e3f1123","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:24:15.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6176" for this suite. 09/03/22 21:24:15.143
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":153,"skipped":2667,"failed":0}
------------------------------
• [2.114 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:24:13.034
    Sep  3 21:24:13.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename daemonsets 09/03/22 21:24:13.035
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:24:13.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:24:13.051
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 09/03/22 21:24:13.072
    STEP: Check that daemon pods launch on every node of the cluster. 09/03/22 21:24:13.078
    Sep  3 21:24:13.083: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:24:13.088: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:24:13.089: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 21:24:14.091: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:24:14.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  3 21:24:14.094: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  3 21:24:15.092: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:24:15.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  3 21:24:15.094: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 09/03/22 21:24:15.096
    STEP: DeleteCollection of the DaemonSets 09/03/22 21:24:15.099
    STEP: Verify that ReplicaSets have been deleted 09/03/22 21:24:15.103
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Sep  3 21:24:15.123: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10932"},"items":null}

    Sep  3 21:24:15.127: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10932"},"items":[{"metadata":{"name":"daemon-set-6svwb","generateName":"daemon-set-","namespace":"daemonsets-6176","uid":"5f9d9e0b-1d0f-439f-b273-59d91c18680b","resourceVersion":"10922","creationTimestamp":"2022-09-03T21:24:13Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e060cb05-fe63-42da-bd05-3d22271f6832","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-09-03T21:24:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e060cb05-fe63-42da-bd05-3d22271f6832\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-09-03T21:24:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-647n2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-647n2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kind-worker","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kind-worker"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:13Z"}],"hostIP":"172.18.0.2","podIP":"10.244.2.28","podIPs":[{"ip":"10.244.2.28"}],"startTime":"2022-09-03T21:24:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-09-03T21:24:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://d7ecc051643da18a0e6038eb12c9fa418d888eb3d86ab1de8c64c344fc8fe062","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fpk59","generateName":"daemon-set-","namespace":"daemonsets-6176","uid":"0e385195-0712-4ea6-989e-8a11386421ed","resourceVersion":"10930","creationTimestamp":"2022-09-03T21:24:13Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e060cb05-fe63-42da-bd05-3d22271f6832","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-09-03T21:24:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e060cb05-fe63-42da-bd05-3d22271f6832\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-09-03T21:24:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gzgvk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gzgvk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kind-worker2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kind-worker2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-03T21:24:13Z"}],"hostIP":"172.18.0.3","podIP":"10.244.1.152","podIPs":[{"ip":"10.244.1.152"}],"startTime":"2022-09-03T21:24:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-09-03T21:24:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://2d4db4611c5914b30a5f9977f539d0b7f3d608d990fd949a2f874bbc3e3f1123","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:24:15.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6176" for this suite. 09/03/22 21:24:15.143
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:24:15.166
Sep  3 21:24:15.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename sched-preemption 09/03/22 21:24:15.167
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:24:15.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:24:15.191
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Sep  3 21:24:15.209: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  3 21:25:15.228: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 09/03/22 21:25:15.229
Sep  3 21:25:15.242: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  3 21:25:15.249: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  3 21:25:15.265: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  3 21:25:15.270: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/03/22 21:25:15.271
Sep  3 21:25:15.271: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6652" to be "running"
Sep  3 21:25:15.274: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.889968ms
Sep  3 21:25:17.277: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00638635s
Sep  3 21:25:19.278: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006928863s
Sep  3 21:25:21.278: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007216362s
Sep  3 21:25:23.277: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005983521s
Sep  3 21:25:25.276: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.005423497s
Sep  3 21:25:25.277: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  3 21:25:25.277: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6652" to be "running"
Sep  3 21:25:25.278: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.769107ms
Sep  3 21:25:25.278: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  3 21:25:25.279: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6652" to be "running"
Sep  3 21:25:25.280: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.642306ms
Sep  3 21:25:27.283: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.004613796s
Sep  3 21:25:27.283: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  3 21:25:27.283: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6652" to be "running"
Sep  3 21:25:27.286: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.348009ms
Sep  3 21:25:27.286: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/03/22 21:25:27.286
Sep  3 21:25:27.289: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6652" to be "running"
Sep  3 21:25:27.291: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031707ms
Sep  3 21:25:29.297: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007480107s
Sep  3 21:25:31.298: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009187633s
Sep  3 21:25:31.299: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:25:31.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6652" for this suite. 09/03/22 21:25:31.31
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":154,"skipped":2683,"failed":0}
------------------------------
• [SLOW TEST] [76.189 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:24:15.166
    Sep  3 21:24:15.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename sched-preemption 09/03/22 21:24:15.167
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:24:15.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:24:15.191
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Sep  3 21:24:15.209: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  3 21:25:15.228: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 09/03/22 21:25:15.229
    Sep  3 21:25:15.242: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  3 21:25:15.249: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  3 21:25:15.265: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  3 21:25:15.270: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/03/22 21:25:15.271
    Sep  3 21:25:15.271: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6652" to be "running"
    Sep  3 21:25:15.274: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.889968ms
    Sep  3 21:25:17.277: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00638635s
    Sep  3 21:25:19.278: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006928863s
    Sep  3 21:25:21.278: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007216362s
    Sep  3 21:25:23.277: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005983521s
    Sep  3 21:25:25.276: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.005423497s
    Sep  3 21:25:25.277: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  3 21:25:25.277: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6652" to be "running"
    Sep  3 21:25:25.278: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.769107ms
    Sep  3 21:25:25.278: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  3 21:25:25.279: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6652" to be "running"
    Sep  3 21:25:25.280: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.642306ms
    Sep  3 21:25:27.283: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.004613796s
    Sep  3 21:25:27.283: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  3 21:25:27.283: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6652" to be "running"
    Sep  3 21:25:27.286: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.348009ms
    Sep  3 21:25:27.286: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/03/22 21:25:27.286
    Sep  3 21:25:27.289: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6652" to be "running"
    Sep  3 21:25:27.291: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031707ms
    Sep  3 21:25:29.297: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007480107s
    Sep  3 21:25:31.298: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009187633s
    Sep  3 21:25:31.299: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:25:31.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6652" for this suite. 09/03/22 21:25:31.31
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:25:31.356
Sep  3 21:25:31.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir-wrapper 09/03/22 21:25:31.357
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:25:31.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:25:31.41
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 09/03/22 21:25:31.412
STEP: Creating RC which spawns configmap-volume pods 09/03/22 21:25:31.611
Sep  3 21:25:31.723: INFO: Pod name wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545: Found 3 pods out of 5
Sep  3 21:25:36.753: INFO: Pod name wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/03/22 21:25:36.753
Sep  3 21:25:36.753: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:25:36.758: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.939619ms
Sep  3 21:25:38.762: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008908514s
Sep  3 21:25:40.763: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009803095s
Sep  3 21:25:42.765: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012073482s
Sep  3 21:25:44.762: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008233246s
Sep  3 21:25:46.761: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf": Phase="Running", Reason="", readiness=true. Elapsed: 10.00793833s
Sep  3 21:25:46.761: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf" satisfied condition "running"
Sep  3 21:25:46.761: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-5c2g2" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:25:46.764: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-5c2g2": Phase="Running", Reason="", readiness=true. Elapsed: 2.219508ms
Sep  3 21:25:46.764: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-5c2g2" satisfied condition "running"
Sep  3 21:25:46.764: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-9lzlw" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:25:46.766: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-9lzlw": Phase="Running", Reason="", readiness=true. Elapsed: 2.406209ms
Sep  3 21:25:46.766: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-9lzlw" satisfied condition "running"
Sep  3 21:25:46.766: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-ckl6n" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:25:46.768: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-ckl6n": Phase="Running", Reason="", readiness=true. Elapsed: 1.874506ms
Sep  3 21:25:46.768: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-ckl6n" satisfied condition "running"
Sep  3 21:25:46.768: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-t4f42" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:25:46.770: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-t4f42": Phase="Running", Reason="", readiness=true. Elapsed: 2.410309ms
Sep  3 21:25:46.770: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-t4f42" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545 in namespace emptydir-wrapper-8277, will wait for the garbage collector to delete the pods 09/03/22 21:25:46.77
Sep  3 21:25:46.828: INFO: Deleting ReplicationController wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545 took: 4.404215ms
Sep  3 21:25:46.929: INFO: Terminating ReplicationController wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545 pods took: 101.03305ms
STEP: Creating RC which spawns configmap-volume pods 09/03/22 21:25:51.334
Sep  3 21:25:51.346: INFO: Pod name wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88: Found 0 pods out of 5
Sep  3 21:25:56.368: INFO: Pod name wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/03/22 21:25:56.368
Sep  3 21:25:56.368: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:25:56.406: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd": Phase="Pending", Reason="", readiness=false. Elapsed: 38.28444ms
Sep  3 21:25:58.410: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041388936s
Sep  3 21:26:00.411: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042544426s
Sep  3 21:26:02.411: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04273713s
Sep  3 21:26:04.410: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042136816s
Sep  3 21:26:06.410: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd": Phase="Running", Reason="", readiness=true. Elapsed: 10.041420001s
Sep  3 21:26:06.410: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd" satisfied condition "running"
Sep  3 21:26:06.410: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-8fh2d" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:26:06.412: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-8fh2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.047208ms
Sep  3 21:26:06.412: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-8fh2d" satisfied condition "running"
Sep  3 21:26:06.412: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-9kzgk" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:26:06.414: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-9kzgk": Phase="Running", Reason="", readiness=true. Elapsed: 2.059309ms
Sep  3 21:26:06.414: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-9kzgk" satisfied condition "running"
Sep  3 21:26:06.414: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-v5ffh" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:26:06.416: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-v5ffh": Phase="Running", Reason="", readiness=true. Elapsed: 1.936507ms
Sep  3 21:26:06.416: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-v5ffh" satisfied condition "running"
Sep  3 21:26:06.416: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-zc45z" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:26:06.424: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-zc45z": Phase="Running", Reason="", readiness=true. Elapsed: 8.000732ms
Sep  3 21:26:06.424: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-zc45z" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88 in namespace emptydir-wrapper-8277, will wait for the garbage collector to delete the pods 09/03/22 21:26:06.424
Sep  3 21:26:06.492: INFO: Deleting ReplicationController wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88 took: 10.320641ms
Sep  3 21:26:06.593: INFO: Terminating ReplicationController wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88 pods took: 100.949903ms
STEP: Creating RC which spawns configmap-volume pods 09/03/22 21:26:10.999
Sep  3 21:26:11.017: INFO: Pod name wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a: Found 0 pods out of 5
Sep  3 21:26:16.030: INFO: Pod name wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/03/22 21:26:16.03
Sep  3 21:26:16.030: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:26:16.033: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.648409ms
Sep  3 21:26:18.036: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005618123s
Sep  3 21:26:20.040: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009758366s
Sep  3 21:26:22.036: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006017708s
Sep  3 21:26:24.043: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012866185s
Sep  3 21:26:26.037: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42": Phase="Running", Reason="", readiness=true. Elapsed: 10.006640719s
Sep  3 21:26:26.037: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42" satisfied condition "running"
Sep  3 21:26:26.038: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-f6wts" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:26:26.041: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-f6wts": Phase="Running", Reason="", readiness=true. Elapsed: 3.266011ms
Sep  3 21:26:26.041: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-f6wts" satisfied condition "running"
Sep  3 21:26:26.041: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-h7hh9" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:26:26.045: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-h7hh9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.323712ms
Sep  3 21:26:28.053: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-h7hh9": Phase="Running", Reason="", readiness=true. Elapsed: 2.01149084s
Sep  3 21:26:28.053: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-h7hh9" satisfied condition "running"
Sep  3 21:26:28.053: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-rqgf9" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:26:28.064: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-rqgf9": Phase="Running", Reason="", readiness=true. Elapsed: 10.718836ms
Sep  3 21:26:28.065: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-rqgf9" satisfied condition "running"
Sep  3 21:26:28.065: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-zq4mr" in namespace "emptydir-wrapper-8277" to be "running"
Sep  3 21:26:28.069: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-zq4mr": Phase="Running", Reason="", readiness=true. Elapsed: 4.153914ms
Sep  3 21:26:28.073: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-zq4mr" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a in namespace emptydir-wrapper-8277, will wait for the garbage collector to delete the pods 09/03/22 21:26:28.073
Sep  3 21:26:28.149: INFO: Deleting ReplicationController wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a took: 5.139117ms
Sep  3 21:26:28.249: INFO: Terminating ReplicationController wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a pods took: 100.611237ms
STEP: Cleaning up the configMaps 09/03/22 21:26:30.85
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Sep  3 21:26:30.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8277" for this suite. 09/03/22 21:26:30.993
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":155,"skipped":2701,"failed":0}
------------------------------
• [SLOW TEST] [59.643 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:25:31.356
    Sep  3 21:25:31.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir-wrapper 09/03/22 21:25:31.357
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:25:31.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:25:31.41
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 09/03/22 21:25:31.412
    STEP: Creating RC which spawns configmap-volume pods 09/03/22 21:25:31.611
    Sep  3 21:25:31.723: INFO: Pod name wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545: Found 3 pods out of 5
    Sep  3 21:25:36.753: INFO: Pod name wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/03/22 21:25:36.753
    Sep  3 21:25:36.753: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:25:36.758: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.939619ms
    Sep  3 21:25:38.762: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008908514s
    Sep  3 21:25:40.763: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009803095s
    Sep  3 21:25:42.765: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012073482s
    Sep  3 21:25:44.762: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008233246s
    Sep  3 21:25:46.761: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf": Phase="Running", Reason="", readiness=true. Elapsed: 10.00793833s
    Sep  3 21:25:46.761: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-48kzf" satisfied condition "running"
    Sep  3 21:25:46.761: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-5c2g2" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:25:46.764: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-5c2g2": Phase="Running", Reason="", readiness=true. Elapsed: 2.219508ms
    Sep  3 21:25:46.764: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-5c2g2" satisfied condition "running"
    Sep  3 21:25:46.764: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-9lzlw" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:25:46.766: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-9lzlw": Phase="Running", Reason="", readiness=true. Elapsed: 2.406209ms
    Sep  3 21:25:46.766: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-9lzlw" satisfied condition "running"
    Sep  3 21:25:46.766: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-ckl6n" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:25:46.768: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-ckl6n": Phase="Running", Reason="", readiness=true. Elapsed: 1.874506ms
    Sep  3 21:25:46.768: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-ckl6n" satisfied condition "running"
    Sep  3 21:25:46.768: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-t4f42" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:25:46.770: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-t4f42": Phase="Running", Reason="", readiness=true. Elapsed: 2.410309ms
    Sep  3 21:25:46.770: INFO: Pod "wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545-t4f42" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545 in namespace emptydir-wrapper-8277, will wait for the garbage collector to delete the pods 09/03/22 21:25:46.77
    Sep  3 21:25:46.828: INFO: Deleting ReplicationController wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545 took: 4.404215ms
    Sep  3 21:25:46.929: INFO: Terminating ReplicationController wrapped-volume-race-23186ce6-8364-448f-9aa1-fab2dc9b0545 pods took: 101.03305ms
    STEP: Creating RC which spawns configmap-volume pods 09/03/22 21:25:51.334
    Sep  3 21:25:51.346: INFO: Pod name wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88: Found 0 pods out of 5
    Sep  3 21:25:56.368: INFO: Pod name wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/03/22 21:25:56.368
    Sep  3 21:25:56.368: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:25:56.406: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd": Phase="Pending", Reason="", readiness=false. Elapsed: 38.28444ms
    Sep  3 21:25:58.410: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041388936s
    Sep  3 21:26:00.411: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042544426s
    Sep  3 21:26:02.411: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04273713s
    Sep  3 21:26:04.410: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042136816s
    Sep  3 21:26:06.410: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd": Phase="Running", Reason="", readiness=true. Elapsed: 10.041420001s
    Sep  3 21:26:06.410: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-5pzhd" satisfied condition "running"
    Sep  3 21:26:06.410: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-8fh2d" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:26:06.412: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-8fh2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.047208ms
    Sep  3 21:26:06.412: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-8fh2d" satisfied condition "running"
    Sep  3 21:26:06.412: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-9kzgk" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:26:06.414: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-9kzgk": Phase="Running", Reason="", readiness=true. Elapsed: 2.059309ms
    Sep  3 21:26:06.414: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-9kzgk" satisfied condition "running"
    Sep  3 21:26:06.414: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-v5ffh" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:26:06.416: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-v5ffh": Phase="Running", Reason="", readiness=true. Elapsed: 1.936507ms
    Sep  3 21:26:06.416: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-v5ffh" satisfied condition "running"
    Sep  3 21:26:06.416: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-zc45z" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:26:06.424: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-zc45z": Phase="Running", Reason="", readiness=true. Elapsed: 8.000732ms
    Sep  3 21:26:06.424: INFO: Pod "wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88-zc45z" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88 in namespace emptydir-wrapper-8277, will wait for the garbage collector to delete the pods 09/03/22 21:26:06.424
    Sep  3 21:26:06.492: INFO: Deleting ReplicationController wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88 took: 10.320641ms
    Sep  3 21:26:06.593: INFO: Terminating ReplicationController wrapped-volume-race-20d579d5-820b-4274-b2d7-1d6308561e88 pods took: 100.949903ms
    STEP: Creating RC which spawns configmap-volume pods 09/03/22 21:26:10.999
    Sep  3 21:26:11.017: INFO: Pod name wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a: Found 0 pods out of 5
    Sep  3 21:26:16.030: INFO: Pod name wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/03/22 21:26:16.03
    Sep  3 21:26:16.030: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:26:16.033: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.648409ms
    Sep  3 21:26:18.036: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005618123s
    Sep  3 21:26:20.040: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009758366s
    Sep  3 21:26:22.036: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006017708s
    Sep  3 21:26:24.043: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012866185s
    Sep  3 21:26:26.037: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42": Phase="Running", Reason="", readiness=true. Elapsed: 10.006640719s
    Sep  3 21:26:26.037: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-9wv42" satisfied condition "running"
    Sep  3 21:26:26.038: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-f6wts" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:26:26.041: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-f6wts": Phase="Running", Reason="", readiness=true. Elapsed: 3.266011ms
    Sep  3 21:26:26.041: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-f6wts" satisfied condition "running"
    Sep  3 21:26:26.041: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-h7hh9" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:26:26.045: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-h7hh9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.323712ms
    Sep  3 21:26:28.053: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-h7hh9": Phase="Running", Reason="", readiness=true. Elapsed: 2.01149084s
    Sep  3 21:26:28.053: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-h7hh9" satisfied condition "running"
    Sep  3 21:26:28.053: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-rqgf9" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:26:28.064: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-rqgf9": Phase="Running", Reason="", readiness=true. Elapsed: 10.718836ms
    Sep  3 21:26:28.065: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-rqgf9" satisfied condition "running"
    Sep  3 21:26:28.065: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-zq4mr" in namespace "emptydir-wrapper-8277" to be "running"
    Sep  3 21:26:28.069: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-zq4mr": Phase="Running", Reason="", readiness=true. Elapsed: 4.153914ms
    Sep  3 21:26:28.073: INFO: Pod "wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a-zq4mr" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a in namespace emptydir-wrapper-8277, will wait for the garbage collector to delete the pods 09/03/22 21:26:28.073
    Sep  3 21:26:28.149: INFO: Deleting ReplicationController wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a took: 5.139117ms
    Sep  3 21:26:28.249: INFO: Terminating ReplicationController wrapped-volume-race-ff481cc1-cb34-4a3e-9c11-6a13dd42ef8a pods took: 100.611237ms
    STEP: Cleaning up the configMaps 09/03/22 21:26:30.85
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Sep  3 21:26:30.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-8277" for this suite. 09/03/22 21:26:30.993
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:26:31.003
Sep  3 21:26:31.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 21:26:31.004
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:26:31.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:26:31.014
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6516 09/03/22 21:26:31.017
STEP: changing the ExternalName service to type=ClusterIP 09/03/22 21:26:31.021
STEP: creating replication controller externalname-service in namespace services-6516 09/03/22 21:26:31.032
I0903 21:26:31.068914      24 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6516, replica count: 2
I0903 21:26:34.120197      24 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 21:26:34.120: INFO: Creating new exec pod
Sep  3 21:26:34.129: INFO: Waiting up to 5m0s for pod "execpodkdg2x" in namespace "services-6516" to be "running"
Sep  3 21:26:34.139: INFO: Pod "execpodkdg2x": Phase="Pending", Reason="", readiness=false. Elapsed: 9.458631ms
Sep  3 21:26:36.142: INFO: Pod "execpodkdg2x": Phase="Running", Reason="", readiness=true. Elapsed: 2.012828297s
Sep  3 21:26:36.142: INFO: Pod "execpodkdg2x" satisfied condition "running"
Sep  3 21:26:37.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6516 exec execpodkdg2x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Sep  3 21:26:37.317: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  3 21:26:37.317: INFO: stdout: "externalname-service-2q6tz"
Sep  3 21:26:37.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6516 exec execpodkdg2x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.134.17 80'
Sep  3 21:26:37.458: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.134.17 80\nConnection to 10.96.134.17 80 port [tcp/http] succeeded!\n"
Sep  3 21:26:37.458: INFO: stdout: "externalname-service-2q6tz"
Sep  3 21:26:37.458: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 21:26:37.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6516" for this suite. 09/03/22 21:26:37.527
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":156,"skipped":2705,"failed":0}
------------------------------
• [SLOW TEST] [6.530 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:26:31.003
    Sep  3 21:26:31.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 21:26:31.004
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:26:31.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:26:31.014
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6516 09/03/22 21:26:31.017
    STEP: changing the ExternalName service to type=ClusterIP 09/03/22 21:26:31.021
    STEP: creating replication controller externalname-service in namespace services-6516 09/03/22 21:26:31.032
    I0903 21:26:31.068914      24 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6516, replica count: 2
    I0903 21:26:34.120197      24 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  3 21:26:34.120: INFO: Creating new exec pod
    Sep  3 21:26:34.129: INFO: Waiting up to 5m0s for pod "execpodkdg2x" in namespace "services-6516" to be "running"
    Sep  3 21:26:34.139: INFO: Pod "execpodkdg2x": Phase="Pending", Reason="", readiness=false. Elapsed: 9.458631ms
    Sep  3 21:26:36.142: INFO: Pod "execpodkdg2x": Phase="Running", Reason="", readiness=true. Elapsed: 2.012828297s
    Sep  3 21:26:36.142: INFO: Pod "execpodkdg2x" satisfied condition "running"
    Sep  3 21:26:37.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6516 exec execpodkdg2x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Sep  3 21:26:37.317: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  3 21:26:37.317: INFO: stdout: "externalname-service-2q6tz"
    Sep  3 21:26:37.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6516 exec execpodkdg2x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.134.17 80'
    Sep  3 21:26:37.458: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.134.17 80\nConnection to 10.96.134.17 80 port [tcp/http] succeeded!\n"
    Sep  3 21:26:37.458: INFO: stdout: "externalname-service-2q6tz"
    Sep  3 21:26:37.458: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 21:26:37.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6516" for this suite. 09/03/22 21:26:37.527
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:26:37.534
Sep  3 21:26:37.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 21:26:37.534
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:26:37.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:26:37.559
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 09/03/22 21:26:37.567
Sep  3 21:26:37.573: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2" in namespace "downward-api-7043" to be "Succeeded or Failed"
Sep  3 21:26:37.594: INFO: Pod "downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.703267ms
Sep  3 21:26:39.600: INFO: Pod "downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025912317s
Sep  3 21:26:41.600: INFO: Pod "downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02602325s
STEP: Saw pod success 09/03/22 21:26:41.6
Sep  3 21:26:41.600: INFO: Pod "downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2" satisfied condition "Succeeded or Failed"
Sep  3 21:26:41.602: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2 container client-container: <nil>
STEP: delete the pod 09/03/22 21:26:41.615
Sep  3 21:26:41.622: INFO: Waiting for pod downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2 to disappear
Sep  3 21:26:41.624: INFO: Pod downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Sep  3 21:26:41.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7043" for this suite. 09/03/22 21:26:41.626
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":157,"skipped":2719,"failed":0}
------------------------------
• [4.095 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:26:37.534
    Sep  3 21:26:37.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 21:26:37.534
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:26:37.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:26:37.559
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 09/03/22 21:26:37.567
    Sep  3 21:26:37.573: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2" in namespace "downward-api-7043" to be "Succeeded or Failed"
    Sep  3 21:26:37.594: INFO: Pod "downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.703267ms
    Sep  3 21:26:39.600: INFO: Pod "downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025912317s
    Sep  3 21:26:41.600: INFO: Pod "downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02602325s
    STEP: Saw pod success 09/03/22 21:26:41.6
    Sep  3 21:26:41.600: INFO: Pod "downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2" satisfied condition "Succeeded or Failed"
    Sep  3 21:26:41.602: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2 container client-container: <nil>
    STEP: delete the pod 09/03/22 21:26:41.615
    Sep  3 21:26:41.622: INFO: Waiting for pod downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2 to disappear
    Sep  3 21:26:41.624: INFO: Pod downwardapi-volume-1493e55e-ab2e-48f7-8281-df96798b0de2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Sep  3 21:26:41.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7043" for this suite. 09/03/22 21:26:41.626
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:26:41.63
Sep  3 21:26:41.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename resourcequota 09/03/22 21:26:41.631
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:26:41.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:26:41.647
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 09/03/22 21:26:41.649
STEP: Ensuring ResourceQuota status is calculated 09/03/22 21:26:41.652
STEP: Creating a ResourceQuota with not best effort scope 09/03/22 21:26:43.661
STEP: Ensuring ResourceQuota status is calculated 09/03/22 21:26:43.678
STEP: Creating a best-effort pod 09/03/22 21:26:45.682
STEP: Ensuring resource quota with best effort scope captures the pod usage 09/03/22 21:26:45.691
STEP: Ensuring resource quota with not best effort ignored the pod usage 09/03/22 21:26:47.695
STEP: Deleting the pod 09/03/22 21:26:49.699
STEP: Ensuring resource quota status released the pod usage 09/03/22 21:26:49.716
STEP: Creating a not best-effort pod 09/03/22 21:26:51.72
STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/03/22 21:26:51.727
STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/03/22 21:26:53.73
STEP: Deleting the pod 09/03/22 21:26:55.734
STEP: Ensuring resource quota status released the pod usage 09/03/22 21:26:55.742
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Sep  3 21:26:57.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2015" for this suite. 09/03/22 21:26:57.747
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":158,"skipped":2727,"failed":0}
------------------------------
• [SLOW TEST] [16.120 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:26:41.63
    Sep  3 21:26:41.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename resourcequota 09/03/22 21:26:41.631
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:26:41.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:26:41.647
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 09/03/22 21:26:41.649
    STEP: Ensuring ResourceQuota status is calculated 09/03/22 21:26:41.652
    STEP: Creating a ResourceQuota with not best effort scope 09/03/22 21:26:43.661
    STEP: Ensuring ResourceQuota status is calculated 09/03/22 21:26:43.678
    STEP: Creating a best-effort pod 09/03/22 21:26:45.682
    STEP: Ensuring resource quota with best effort scope captures the pod usage 09/03/22 21:26:45.691
    STEP: Ensuring resource quota with not best effort ignored the pod usage 09/03/22 21:26:47.695
    STEP: Deleting the pod 09/03/22 21:26:49.699
    STEP: Ensuring resource quota status released the pod usage 09/03/22 21:26:49.716
    STEP: Creating a not best-effort pod 09/03/22 21:26:51.72
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/03/22 21:26:51.727
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/03/22 21:26:53.73
    STEP: Deleting the pod 09/03/22 21:26:55.734
    STEP: Ensuring resource quota status released the pod usage 09/03/22 21:26:55.742
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Sep  3 21:26:57.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2015" for this suite. 09/03/22 21:26:57.747
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:26:57.756
Sep  3 21:26:57.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:26:57.757
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:26:57.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:26:57.771
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 09/03/22 21:26:57.783
Sep  3 21:26:57.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe" in namespace "projected-4021" to be "Succeeded or Failed"
Sep  3 21:26:57.793: INFO: Pod "downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.815205ms
Sep  3 21:26:59.796: INFO: Pod "downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005112117s
Sep  3 21:27:01.796: INFO: Pod "downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004786376s
STEP: Saw pod success 09/03/22 21:27:01.796
Sep  3 21:27:01.796: INFO: Pod "downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe" satisfied condition "Succeeded or Failed"
Sep  3 21:27:01.798: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe container client-container: <nil>
STEP: delete the pod 09/03/22 21:27:01.802
Sep  3 21:27:01.807: INFO: Waiting for pod downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe to disappear
Sep  3 21:27:01.809: INFO: Pod downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Sep  3 21:27:01.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4021" for this suite. 09/03/22 21:27:01.812
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":159,"skipped":2731,"failed":0}
------------------------------
• [4.060 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:26:57.756
    Sep  3 21:26:57.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:26:57.757
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:26:57.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:26:57.771
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 09/03/22 21:26:57.783
    Sep  3 21:26:57.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe" in namespace "projected-4021" to be "Succeeded or Failed"
    Sep  3 21:26:57.793: INFO: Pod "downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.815205ms
    Sep  3 21:26:59.796: INFO: Pod "downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005112117s
    Sep  3 21:27:01.796: INFO: Pod "downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004786376s
    STEP: Saw pod success 09/03/22 21:27:01.796
    Sep  3 21:27:01.796: INFO: Pod "downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe" satisfied condition "Succeeded or Failed"
    Sep  3 21:27:01.798: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe container client-container: <nil>
    STEP: delete the pod 09/03/22 21:27:01.802
    Sep  3 21:27:01.807: INFO: Waiting for pod downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe to disappear
    Sep  3 21:27:01.809: INFO: Pod downwardapi-volume-76a61e43-ec22-48af-88dc-f3d07d1a51fe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Sep  3 21:27:01.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4021" for this suite. 09/03/22 21:27:01.812
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:27:01.817
Sep  3 21:27:01.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename replicaset 09/03/22 21:27:01.818
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:27:01.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:27:01.83
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Sep  3 21:27:01.839: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  3 21:27:06.843: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/03/22 21:27:06.843
STEP: Scaling up "test-rs" replicaset  09/03/22 21:27:06.844
Sep  3 21:27:06.849: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 09/03/22 21:27:06.85
W0903 21:27:06.854925      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  3 21:27:06.856: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
Sep  3 21:27:06.878: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
Sep  3 21:27:06.908: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
Sep  3 21:27:06.919: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
Sep  3 21:27:08.581: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 2, AvailableReplicas 2
Sep  3 21:27:08.599: INFO: observed Replicaset test-rs in namespace replicaset-9272 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Sep  3 21:27:08.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9272" for this suite. 09/03/22 21:27:08.603
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":160,"skipped":2752,"failed":0}
------------------------------
• [SLOW TEST] [6.790 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:27:01.817
    Sep  3 21:27:01.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename replicaset 09/03/22 21:27:01.818
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:27:01.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:27:01.83
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Sep  3 21:27:01.839: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  3 21:27:06.843: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/03/22 21:27:06.843
    STEP: Scaling up "test-rs" replicaset  09/03/22 21:27:06.844
    Sep  3 21:27:06.849: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 09/03/22 21:27:06.85
    W0903 21:27:06.854925      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  3 21:27:06.856: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
    Sep  3 21:27:06.878: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
    Sep  3 21:27:06.908: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
    Sep  3 21:27:06.919: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
    Sep  3 21:27:08.581: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 2, AvailableReplicas 2
    Sep  3 21:27:08.599: INFO: observed Replicaset test-rs in namespace replicaset-9272 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Sep  3 21:27:08.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9272" for this suite. 09/03/22 21:27:08.603
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:27:08.612
Sep  3 21:27:08.612: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename watch 09/03/22 21:27:08.613
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:27:08.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:27:08.625
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 09/03/22 21:27:08.627
STEP: modifying the configmap once 09/03/22 21:27:08.63
STEP: modifying the configmap a second time 09/03/22 21:27:08.635
STEP: deleting the configmap 09/03/22 21:27:08.639
STEP: creating a watch on configmaps from the resource version returned by the first update 09/03/22 21:27:08.642
STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/03/22 21:27:08.643
Sep  3 21:27:08.643: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5439  f8d6044f-3df5-4bf8-8c7d-f5651d1a3f7c 12317 0 2022-09-03 21:27:08 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-09-03 21:27:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:27:08.644: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5439  f8d6044f-3df5-4bf8-8c7d-f5651d1a3f7c 12318 0 2022-09-03 21:27:08 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-09-03 21:27:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Sep  3 21:27:08.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5439" for this suite. 09/03/22 21:27:08.647
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":161,"skipped":2795,"failed":0}
------------------------------
• [0.038 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:27:08.612
    Sep  3 21:27:08.612: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename watch 09/03/22 21:27:08.613
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:27:08.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:27:08.625
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 09/03/22 21:27:08.627
    STEP: modifying the configmap once 09/03/22 21:27:08.63
    STEP: modifying the configmap a second time 09/03/22 21:27:08.635
    STEP: deleting the configmap 09/03/22 21:27:08.639
    STEP: creating a watch on configmaps from the resource version returned by the first update 09/03/22 21:27:08.642
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/03/22 21:27:08.643
    Sep  3 21:27:08.643: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5439  f8d6044f-3df5-4bf8-8c7d-f5651d1a3f7c 12317 0 2022-09-03 21:27:08 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-09-03 21:27:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:27:08.644: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5439  f8d6044f-3df5-4bf8-8c7d-f5651d1a3f7c 12318 0 2022-09-03 21:27:08 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-09-03 21:27:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Sep  3 21:27:08.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5439" for this suite. 09/03/22 21:27:08.647
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:27:08.654
Sep  3 21:27:08.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 21:27:08.655
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:27:08.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:27:08.67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 09/03/22 21:27:08.673
Sep  3 21:27:08.678: INFO: Waiting up to 5m0s for pod "annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4" in namespace "downward-api-2219" to be "running and ready"
Sep  3 21:27:08.682: INFO: Pod "annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.55691ms
Sep  3 21:27:08.682: INFO: The phase of Pod annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:27:10.685: INFO: Pod "annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006491715s
Sep  3 21:27:10.685: INFO: The phase of Pod annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4 is Running (Ready = true)
Sep  3 21:27:10.685: INFO: Pod "annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4" satisfied condition "running and ready"
Sep  3 21:27:11.209: INFO: Successfully updated pod "annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Sep  3 21:27:15.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2219" for this suite. 09/03/22 21:27:15.228
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":162,"skipped":2815,"failed":0}
------------------------------
• [SLOW TEST] [6.577 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:27:08.654
    Sep  3 21:27:08.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 21:27:08.655
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:27:08.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:27:08.67
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 09/03/22 21:27:08.673
    Sep  3 21:27:08.678: INFO: Waiting up to 5m0s for pod "annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4" in namespace "downward-api-2219" to be "running and ready"
    Sep  3 21:27:08.682: INFO: Pod "annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.55691ms
    Sep  3 21:27:08.682: INFO: The phase of Pod annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:27:10.685: INFO: Pod "annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006491715s
    Sep  3 21:27:10.685: INFO: The phase of Pod annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4 is Running (Ready = true)
    Sep  3 21:27:10.685: INFO: Pod "annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4" satisfied condition "running and ready"
    Sep  3 21:27:11.209: INFO: Successfully updated pod "annotationupdated09502c6-a29e-402f-bdb5-e1c5fe2fd4a4"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Sep  3 21:27:15.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2219" for this suite. 09/03/22 21:27:15.228
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:27:15.235
Sep  3 21:27:15.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename subpath 09/03/22 21:27:15.236
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:27:15.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:27:15.248
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/03/22 21:27:15.25
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-9c2w 09/03/22 21:27:15.255
STEP: Creating a pod to test atomic-volume-subpath 09/03/22 21:27:15.255
Sep  3 21:27:15.262: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-9c2w" in namespace "subpath-7902" to be "Succeeded or Failed"
Sep  3 21:27:15.269: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Pending", Reason="", readiness=false. Elapsed: 7.006016ms
Sep  3 21:27:17.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 2.010377419s
Sep  3 21:27:19.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 4.010319414s
Sep  3 21:27:21.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 6.009999909s
Sep  3 21:27:23.271: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 8.009773563s
Sep  3 21:27:25.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 10.010163294s
Sep  3 21:27:27.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 12.010558926s
Sep  3 21:27:29.274: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 14.012270771s
Sep  3 21:27:31.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 16.010527416s
Sep  3 21:27:33.271: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 18.009707502s
Sep  3 21:27:35.273: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 20.011016594s
Sep  3 21:27:37.273: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=false. Elapsed: 22.011272583s
Sep  3 21:27:39.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010424448s
STEP: Saw pod success 09/03/22 21:27:39.272
Sep  3 21:27:39.272: INFO: Pod "pod-subpath-test-projected-9c2w" satisfied condition "Succeeded or Failed"
Sep  3 21:27:39.274: INFO: Trying to get logs from node kind-worker2 pod pod-subpath-test-projected-9c2w container test-container-subpath-projected-9c2w: <nil>
STEP: delete the pod 09/03/22 21:27:39.279
Sep  3 21:27:39.288: INFO: Waiting for pod pod-subpath-test-projected-9c2w to disappear
Sep  3 21:27:39.289: INFO: Pod pod-subpath-test-projected-9c2w no longer exists
STEP: Deleting pod pod-subpath-test-projected-9c2w 09/03/22 21:27:39.29
Sep  3 21:27:39.290: INFO: Deleting pod "pod-subpath-test-projected-9c2w" in namespace "subpath-7902"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Sep  3 21:27:39.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7902" for this suite. 09/03/22 21:27:39.296
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":163,"skipped":2853,"failed":0}
------------------------------
• [SLOW TEST] [24.066 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:27:15.235
    Sep  3 21:27:15.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename subpath 09/03/22 21:27:15.236
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:27:15.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:27:15.248
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/03/22 21:27:15.25
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-9c2w 09/03/22 21:27:15.255
    STEP: Creating a pod to test atomic-volume-subpath 09/03/22 21:27:15.255
    Sep  3 21:27:15.262: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-9c2w" in namespace "subpath-7902" to be "Succeeded or Failed"
    Sep  3 21:27:15.269: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Pending", Reason="", readiness=false. Elapsed: 7.006016ms
    Sep  3 21:27:17.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 2.010377419s
    Sep  3 21:27:19.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 4.010319414s
    Sep  3 21:27:21.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 6.009999909s
    Sep  3 21:27:23.271: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 8.009773563s
    Sep  3 21:27:25.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 10.010163294s
    Sep  3 21:27:27.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 12.010558926s
    Sep  3 21:27:29.274: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 14.012270771s
    Sep  3 21:27:31.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 16.010527416s
    Sep  3 21:27:33.271: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 18.009707502s
    Sep  3 21:27:35.273: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=true. Elapsed: 20.011016594s
    Sep  3 21:27:37.273: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Running", Reason="", readiness=false. Elapsed: 22.011272583s
    Sep  3 21:27:39.272: INFO: Pod "pod-subpath-test-projected-9c2w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010424448s
    STEP: Saw pod success 09/03/22 21:27:39.272
    Sep  3 21:27:39.272: INFO: Pod "pod-subpath-test-projected-9c2w" satisfied condition "Succeeded or Failed"
    Sep  3 21:27:39.274: INFO: Trying to get logs from node kind-worker2 pod pod-subpath-test-projected-9c2w container test-container-subpath-projected-9c2w: <nil>
    STEP: delete the pod 09/03/22 21:27:39.279
    Sep  3 21:27:39.288: INFO: Waiting for pod pod-subpath-test-projected-9c2w to disappear
    Sep  3 21:27:39.289: INFO: Pod pod-subpath-test-projected-9c2w no longer exists
    STEP: Deleting pod pod-subpath-test-projected-9c2w 09/03/22 21:27:39.29
    Sep  3 21:27:39.290: INFO: Deleting pod "pod-subpath-test-projected-9c2w" in namespace "subpath-7902"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Sep  3 21:27:39.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7902" for this suite. 09/03/22 21:27:39.296
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:27:39.327
Sep  3 21:27:39.327: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename dns 09/03/22 21:27:39.328
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:27:39.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:27:39.343
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 09/03/22 21:27:39.346
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local;sleep 1; done
 09/03/22 21:27:39.35
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local;sleep 1; done
 09/03/22 21:27:39.35
STEP: creating a pod to probe DNS 09/03/22 21:27:39.35
STEP: submitting the pod to kubernetes 09/03/22 21:27:39.35
Sep  3 21:27:39.360: INFO: Waiting up to 15m0s for pod "dns-test-36e079ff-f696-4a79-b763-6bd9e877b573" in namespace "dns-7734" to be "running"
Sep  3 21:27:39.363: INFO: Pod "dns-test-36e079ff-f696-4a79-b763-6bd9e877b573": Phase="Pending", Reason="", readiness=false. Elapsed: 3.273182ms
Sep  3 21:27:41.366: INFO: Pod "dns-test-36e079ff-f696-4a79-b763-6bd9e877b573": Phase="Running", Reason="", readiness=true. Elapsed: 2.006151164s
Sep  3 21:27:41.366: INFO: Pod "dns-test-36e079ff-f696-4a79-b763-6bd9e877b573" satisfied condition "running"
STEP: retrieving the pod 09/03/22 21:27:41.366
STEP: looking for the results for each expected name from probers 09/03/22 21:27:41.368
Sep  3 21:27:41.371: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:41.374: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:41.378: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:41.380: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:41.383: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:41.387: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:41.390: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:41.393: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:41.393: INFO: Lookups using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local]

Sep  3 21:27:46.397: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:46.399: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:46.401: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:46.403: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:46.404: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:46.406: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:46.407: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:46.409: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:46.409: INFO: Lookups using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local]

Sep  3 21:27:51.402: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:51.404: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:51.406: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:51.408: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:51.410: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:51.412: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:51.414: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:51.416: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:51.416: INFO: Lookups using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local]

Sep  3 21:27:56.398: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:56.400: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:56.402: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:56.404: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:56.406: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:56.408: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:56.410: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:56.412: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:27:56.412: INFO: Lookups using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local]

Sep  3 21:28:01.396: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:01.398: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:01.400: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:01.402: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:01.404: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:01.407: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:01.409: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:01.411: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:01.411: INFO: Lookups using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local]

Sep  3 21:28:06.397: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:06.400: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:06.403: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:06.405: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:06.406: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:06.408: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:06.410: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:06.412: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
Sep  3 21:28:06.412: INFO: Lookups using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local]

Sep  3 21:28:11.418: INFO: DNS probes using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 succeeded

STEP: deleting the pod 09/03/22 21:28:11.418
STEP: deleting the test headless service 09/03/22 21:28:11.441
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Sep  3 21:28:11.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7734" for this suite. 09/03/22 21:28:11.5
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":164,"skipped":2897,"failed":0}
------------------------------
• [SLOW TEST] [32.183 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:27:39.327
    Sep  3 21:27:39.327: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename dns 09/03/22 21:27:39.328
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:27:39.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:27:39.343
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 09/03/22 21:27:39.346
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local;sleep 1; done
     09/03/22 21:27:39.35
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7734.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local;sleep 1; done
     09/03/22 21:27:39.35
    STEP: creating a pod to probe DNS 09/03/22 21:27:39.35
    STEP: submitting the pod to kubernetes 09/03/22 21:27:39.35
    Sep  3 21:27:39.360: INFO: Waiting up to 15m0s for pod "dns-test-36e079ff-f696-4a79-b763-6bd9e877b573" in namespace "dns-7734" to be "running"
    Sep  3 21:27:39.363: INFO: Pod "dns-test-36e079ff-f696-4a79-b763-6bd9e877b573": Phase="Pending", Reason="", readiness=false. Elapsed: 3.273182ms
    Sep  3 21:27:41.366: INFO: Pod "dns-test-36e079ff-f696-4a79-b763-6bd9e877b573": Phase="Running", Reason="", readiness=true. Elapsed: 2.006151164s
    Sep  3 21:27:41.366: INFO: Pod "dns-test-36e079ff-f696-4a79-b763-6bd9e877b573" satisfied condition "running"
    STEP: retrieving the pod 09/03/22 21:27:41.366
    STEP: looking for the results for each expected name from probers 09/03/22 21:27:41.368
    Sep  3 21:27:41.371: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:41.374: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:41.378: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:41.380: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:41.383: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:41.387: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:41.390: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:41.393: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:41.393: INFO: Lookups using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local]

    Sep  3 21:27:46.397: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:46.399: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:46.401: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:46.403: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:46.404: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:46.406: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:46.407: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:46.409: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:46.409: INFO: Lookups using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local]

    Sep  3 21:27:51.402: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:51.404: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:51.406: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:51.408: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:51.410: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:51.412: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:51.414: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:51.416: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:51.416: INFO: Lookups using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local]

    Sep  3 21:27:56.398: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:56.400: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:56.402: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:56.404: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:56.406: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:56.408: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:56.410: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:56.412: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:27:56.412: INFO: Lookups using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local]

    Sep  3 21:28:01.396: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:01.398: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:01.400: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:01.402: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:01.404: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:01.407: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:01.409: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:01.411: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:01.411: INFO: Lookups using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local]

    Sep  3 21:28:06.397: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:06.400: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:06.403: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:06.405: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:06.406: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:06.408: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:06.410: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:06.412: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local from pod dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573: the server could not find the requested resource (get pods dns-test-36e079ff-f696-4a79-b763-6bd9e877b573)
    Sep  3 21:28:06.412: INFO: Lookups using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7734.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7734.svc.cluster.local jessie_udp@dns-test-service-2.dns-7734.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7734.svc.cluster.local]

    Sep  3 21:28:11.418: INFO: DNS probes using dns-7734/dns-test-36e079ff-f696-4a79-b763-6bd9e877b573 succeeded

    STEP: deleting the pod 09/03/22 21:28:11.418
    STEP: deleting the test headless service 09/03/22 21:28:11.441
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Sep  3 21:28:11.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7734" for this suite. 09/03/22 21:28:11.5
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:28:11.516
Sep  3 21:28:11.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 21:28:11.518
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:11.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:11.538
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Sep  3 21:28:11.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/03/22 21:28:13.816
Sep  3 21:28:13.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8593 --namespace=crd-publish-openapi-8593 create -f -'
Sep  3 21:28:14.527: INFO: stderr: ""
Sep  3 21:28:14.527: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  3 21:28:14.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8593 --namespace=crd-publish-openapi-8593 delete e2e-test-crd-publish-openapi-1931-crds test-cr'
Sep  3 21:28:14.598: INFO: stderr: ""
Sep  3 21:28:14.598: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep  3 21:28:14.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8593 --namespace=crd-publish-openapi-8593 apply -f -'
Sep  3 21:28:14.811: INFO: stderr: ""
Sep  3 21:28:14.811: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  3 21:28:14.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8593 --namespace=crd-publish-openapi-8593 delete e2e-test-crd-publish-openapi-1931-crds test-cr'
Sep  3 21:28:14.898: INFO: stderr: ""
Sep  3 21:28:14.898: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/03/22 21:28:14.898
Sep  3 21:28:14.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8593 explain e2e-test-crd-publish-openapi-1931-crds'
Sep  3 21:28:15.090: INFO: stderr: ""
Sep  3 21:28:15.090: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1931-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:28:17.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8593" for this suite. 09/03/22 21:28:17.29
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":165,"skipped":2900,"failed":0}
------------------------------
• [SLOW TEST] [5.778 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:28:11.516
    Sep  3 21:28:11.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 21:28:11.518
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:11.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:11.538
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Sep  3 21:28:11.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/03/22 21:28:13.816
    Sep  3 21:28:13.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8593 --namespace=crd-publish-openapi-8593 create -f -'
    Sep  3 21:28:14.527: INFO: stderr: ""
    Sep  3 21:28:14.527: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  3 21:28:14.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8593 --namespace=crd-publish-openapi-8593 delete e2e-test-crd-publish-openapi-1931-crds test-cr'
    Sep  3 21:28:14.598: INFO: stderr: ""
    Sep  3 21:28:14.598: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Sep  3 21:28:14.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8593 --namespace=crd-publish-openapi-8593 apply -f -'
    Sep  3 21:28:14.811: INFO: stderr: ""
    Sep  3 21:28:14.811: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  3 21:28:14.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8593 --namespace=crd-publish-openapi-8593 delete e2e-test-crd-publish-openapi-1931-crds test-cr'
    Sep  3 21:28:14.898: INFO: stderr: ""
    Sep  3 21:28:14.898: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/03/22 21:28:14.898
    Sep  3 21:28:14.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=crd-publish-openapi-8593 explain e2e-test-crd-publish-openapi-1931-crds'
    Sep  3 21:28:15.090: INFO: stderr: ""
    Sep  3 21:28:15.090: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1931-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:28:17.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8593" for this suite. 09/03/22 21:28:17.29
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:28:17.295
Sep  3 21:28:17.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:28:17.296
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:17.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:17.312
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Sep  3 21:28:17.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 create -f -'
Sep  3 21:28:17.995: INFO: stderr: ""
Sep  3 21:28:17.996: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep  3 21:28:17.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 create -f -'
Sep  3 21:28:18.271: INFO: stderr: ""
Sep  3 21:28:18.271: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/03/22 21:28:18.271
Sep  3 21:28:19.275: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 21:28:19.275: INFO: Found 1 / 1
Sep  3 21:28:19.275: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  3 21:28:19.277: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 21:28:19.277: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  3 21:28:19.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 describe pod agnhost-primary-wlz98'
Sep  3 21:28:19.366: INFO: stderr: ""
Sep  3 21:28:19.366: INFO: stdout: "Name:             agnhost-primary-wlz98\nNamespace:        kubectl-2912\nPriority:         0\nService Account:  default\nNode:             kind-worker2/172.18.0.3\nStart Time:       Sat, 03 Sep 2022 21:28:18 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.1.169\nIPs:\n  IP:           10.244.1.169\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://c1db9268da2d5961025ac3a0fdd07635f327566a78b79ca476cf9f00064c8454\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 03 Sep 2022 21:28:18 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-24rjq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-24rjq:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2912/agnhost-primary-wlz98 to kind-worker2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Sep  3 21:28:19.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 describe rc agnhost-primary'
Sep  3 21:28:19.494: INFO: stderr: ""
Sep  3 21:28:19.494: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2912\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-wlz98\n"
Sep  3 21:28:19.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 describe service agnhost-primary'
Sep  3 21:28:19.627: INFO: stderr: ""
Sep  3 21:28:19.627: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2912\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.82.183\nIPs:               10.96.82.183\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.169:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  3 21:28:19.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 describe node kind-control-plane'
Sep  3 21:28:19.732: INFO: stderr: ""
Sep  3 21:28:19.732: INFO: stdout: "Name:               kind-control-plane\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kind-control-plane\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 03 Sep 2022 20:37:13 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  kind-control-plane\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 03 Sep 2022 21:28:18 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 03 Sep 2022 21:24:43 +0000   Sat, 03 Sep 2022 20:37:05 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 03 Sep 2022 21:24:43 +0000   Sat, 03 Sep 2022 20:37:05 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 03 Sep 2022 21:24:43 +0000   Sat, 03 Sep 2022 20:37:05 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 03 Sep 2022 21:24:43 +0000   Sat, 03 Sep 2022 20:37:37 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.18.0.4\n  Hostname:    kind-control-plane\nCapacity:\n  cpu:                2\n  ephemeral-storage:  87204404Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7110648Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  87204404Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7110648Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 aabe04f104e14629adec1c26ce2a0cc7\n  System UUID:                1b3b7647-843a-4f6d-bf76-499acf3d02dc\n  Boot ID:                    ea612797-79e6-45e1-b0ed-1b921eb2261e\n  Kernel Version:             5.15.0-1017-azure\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.7\n  Kubelet Version:            v1.25.0\n  Kube-Proxy Version:         v1.25.0\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   kind://docker/kind/kind-control-plane\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-565d847f94-tl84c                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (2%)     50m\n  kube-system                 coredns-565d847f94-xcrw2                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (2%)     50m\n  kube-system                 etcd-kind-control-plane                                    100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         51m\n  kube-system                 kindnet-x98jt                                              100m (5%)     100m (5%)   50Mi (0%)        50Mi (0%)      50m\n  kube-system                 kube-apiserver-kind-control-plane                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                 kube-controller-manager-kind-control-plane                 200m (10%)    0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                 kube-proxy-9htcw                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 kube-scheduler-kind-control-plane                          100m (5%)     0 (0%)      0 (0%)           0 (0%)         51m\n  local-path-storage          local-path-provisioner-684f458cdd-fzpmh                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-mkf8x    0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                950m (47%)  100m (5%)\n  memory             290Mi (4%)  390Mi (5%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 50m                kube-proxy       \n  Normal  NodeHasSufficientMemory  51m (x5 over 51m)  kubelet          Node kind-control-plane status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    51m (x5 over 51m)  kubelet          Node kind-control-plane status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     51m (x4 over 51m)  kubelet          Node kind-control-plane status is now: NodeHasSufficientPID\n  Normal  Starting                 51m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  51m                kubelet          Node kind-control-plane status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    51m                kubelet          Node kind-control-plane status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     51m                kubelet          Node kind-control-plane status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  51m                kubelet          Updated Node Allocatable limit across pods\n  Normal  RegisteredNode           50m                node-controller  Node kind-control-plane event: Registered Node kind-control-plane in Controller\n  Normal  NodeReady                50m                kubelet          Node kind-control-plane status is now: NodeReady\n"
Sep  3 21:28:19.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 describe namespace kubectl-2912'
Sep  3 21:28:19.817: INFO: stderr: ""
Sep  3 21:28:19.817: INFO: stdout: "Name:         kubectl-2912\nLabels:       e2e-framework=kubectl\n              e2e-run=1f4750ba-5505-4208-8172-9446f89360af\n              kubernetes.io/metadata.name=kubectl-2912\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:28:19.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2912" for this suite. 09/03/22 21:28:19.82
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":166,"skipped":2904,"failed":0}
------------------------------
• [2.528 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:28:17.295
    Sep  3 21:28:17.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:28:17.296
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:17.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:17.312
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Sep  3 21:28:17.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 create -f -'
    Sep  3 21:28:17.995: INFO: stderr: ""
    Sep  3 21:28:17.996: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Sep  3 21:28:17.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 create -f -'
    Sep  3 21:28:18.271: INFO: stderr: ""
    Sep  3 21:28:18.271: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/03/22 21:28:18.271
    Sep  3 21:28:19.275: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  3 21:28:19.275: INFO: Found 1 / 1
    Sep  3 21:28:19.275: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  3 21:28:19.277: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  3 21:28:19.277: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  3 21:28:19.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 describe pod agnhost-primary-wlz98'
    Sep  3 21:28:19.366: INFO: stderr: ""
    Sep  3 21:28:19.366: INFO: stdout: "Name:             agnhost-primary-wlz98\nNamespace:        kubectl-2912\nPriority:         0\nService Account:  default\nNode:             kind-worker2/172.18.0.3\nStart Time:       Sat, 03 Sep 2022 21:28:18 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.1.169\nIPs:\n  IP:           10.244.1.169\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://c1db9268da2d5961025ac3a0fdd07635f327566a78b79ca476cf9f00064c8454\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 03 Sep 2022 21:28:18 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-24rjq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-24rjq:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2912/agnhost-primary-wlz98 to kind-worker2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Sep  3 21:28:19.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 describe rc agnhost-primary'
    Sep  3 21:28:19.494: INFO: stderr: ""
    Sep  3 21:28:19.494: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2912\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-wlz98\n"
    Sep  3 21:28:19.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 describe service agnhost-primary'
    Sep  3 21:28:19.627: INFO: stderr: ""
    Sep  3 21:28:19.627: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2912\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.82.183\nIPs:               10.96.82.183\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.169:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Sep  3 21:28:19.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 describe node kind-control-plane'
    Sep  3 21:28:19.732: INFO: stderr: ""
    Sep  3 21:28:19.732: INFO: stdout: "Name:               kind-control-plane\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kind-control-plane\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 03 Sep 2022 20:37:13 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  kind-control-plane\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 03 Sep 2022 21:28:18 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 03 Sep 2022 21:24:43 +0000   Sat, 03 Sep 2022 20:37:05 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 03 Sep 2022 21:24:43 +0000   Sat, 03 Sep 2022 20:37:05 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 03 Sep 2022 21:24:43 +0000   Sat, 03 Sep 2022 20:37:05 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 03 Sep 2022 21:24:43 +0000   Sat, 03 Sep 2022 20:37:37 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.18.0.4\n  Hostname:    kind-control-plane\nCapacity:\n  cpu:                2\n  ephemeral-storage:  87204404Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7110648Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  87204404Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7110648Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 aabe04f104e14629adec1c26ce2a0cc7\n  System UUID:                1b3b7647-843a-4f6d-bf76-499acf3d02dc\n  Boot ID:                    ea612797-79e6-45e1-b0ed-1b921eb2261e\n  Kernel Version:             5.15.0-1017-azure\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.7\n  Kubelet Version:            v1.25.0\n  Kube-Proxy Version:         v1.25.0\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   kind://docker/kind/kind-control-plane\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-565d847f94-tl84c                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (2%)     50m\n  kube-system                 coredns-565d847f94-xcrw2                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (2%)     50m\n  kube-system                 etcd-kind-control-plane                                    100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         51m\n  kube-system                 kindnet-x98jt                                              100m (5%)     100m (5%)   50Mi (0%)        50Mi (0%)      50m\n  kube-system                 kube-apiserver-kind-control-plane                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                 kube-controller-manager-kind-control-plane                 200m (10%)    0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                 kube-proxy-9htcw                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 kube-scheduler-kind-control-plane                          100m (5%)     0 (0%)      0 (0%)           0 (0%)         51m\n  local-path-storage          local-path-provisioner-684f458cdd-fzpmh                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-mkf8x    0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                950m (47%)  100m (5%)\n  memory             290Mi (4%)  390Mi (5%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 50m                kube-proxy       \n  Normal  NodeHasSufficientMemory  51m (x5 over 51m)  kubelet          Node kind-control-plane status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    51m (x5 over 51m)  kubelet          Node kind-control-plane status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     51m (x4 over 51m)  kubelet          Node kind-control-plane status is now: NodeHasSufficientPID\n  Normal  Starting                 51m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  51m                kubelet          Node kind-control-plane status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    51m                kubelet          Node kind-control-plane status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     51m                kubelet          Node kind-control-plane status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  51m                kubelet          Updated Node Allocatable limit across pods\n  Normal  RegisteredNode           50m                node-controller  Node kind-control-plane event: Registered Node kind-control-plane in Controller\n  Normal  NodeReady                50m                kubelet          Node kind-control-plane status is now: NodeReady\n"
    Sep  3 21:28:19.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2912 describe namespace kubectl-2912'
    Sep  3 21:28:19.817: INFO: stderr: ""
    Sep  3 21:28:19.817: INFO: stdout: "Name:         kubectl-2912\nLabels:       e2e-framework=kubectl\n              e2e-run=1f4750ba-5505-4208-8172-9446f89360af\n              kubernetes.io/metadata.name=kubectl-2912\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:28:19.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2912" for this suite. 09/03/22 21:28:19.82
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:28:19.825
Sep  3 21:28:19.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename resourcequota 09/03/22 21:28:19.826
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:19.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:19.837
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 09/03/22 21:28:19.839
STEP: Creating a ResourceQuota 09/03/22 21:28:24.842
STEP: Ensuring resource quota status is calculated 09/03/22 21:28:24.847
STEP: Creating a ReplicaSet 09/03/22 21:28:26.849
STEP: Ensuring resource quota status captures replicaset creation 09/03/22 21:28:26.858
STEP: Deleting a ReplicaSet 09/03/22 21:28:28.861
STEP: Ensuring resource quota status released usage 09/03/22 21:28:28.864
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Sep  3 21:28:30.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3072" for this suite. 09/03/22 21:28:30.87
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":167,"skipped":2926,"failed":0}
------------------------------
• [SLOW TEST] [11.048 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:28:19.825
    Sep  3 21:28:19.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename resourcequota 09/03/22 21:28:19.826
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:19.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:19.837
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 09/03/22 21:28:19.839
    STEP: Creating a ResourceQuota 09/03/22 21:28:24.842
    STEP: Ensuring resource quota status is calculated 09/03/22 21:28:24.847
    STEP: Creating a ReplicaSet 09/03/22 21:28:26.849
    STEP: Ensuring resource quota status captures replicaset creation 09/03/22 21:28:26.858
    STEP: Deleting a ReplicaSet 09/03/22 21:28:28.861
    STEP: Ensuring resource quota status released usage 09/03/22 21:28:28.864
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Sep  3 21:28:30.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3072" for this suite. 09/03/22 21:28:30.87
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:28:30.874
Sep  3 21:28:30.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename certificates 09/03/22 21:28:30.875
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:30.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:30.887
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 09/03/22 21:28:32.39
STEP: getting /apis/certificates.k8s.io 09/03/22 21:28:32.393
STEP: getting /apis/certificates.k8s.io/v1 09/03/22 21:28:32.394
STEP: creating 09/03/22 21:28:32.396
STEP: getting 09/03/22 21:28:32.414
STEP: listing 09/03/22 21:28:32.42
STEP: watching 09/03/22 21:28:32.422
Sep  3 21:28:32.423: INFO: starting watch
STEP: patching 09/03/22 21:28:32.424
STEP: updating 09/03/22 21:28:32.428
Sep  3 21:28:32.431: INFO: waiting for watch events with expected annotations
Sep  3 21:28:32.431: INFO: saw patched and updated annotations
STEP: getting /approval 09/03/22 21:28:32.432
STEP: patching /approval 09/03/22 21:28:32.442
STEP: updating /approval 09/03/22 21:28:32.446
STEP: getting /status 09/03/22 21:28:32.452
STEP: patching /status 09/03/22 21:28:32.454
STEP: updating /status 09/03/22 21:28:32.459
STEP: deleting 09/03/22 21:28:32.464
STEP: deleting a collection 09/03/22 21:28:32.473
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:28:32.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2427" for this suite. 09/03/22 21:28:32.483
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":168,"skipped":2939,"failed":0}
------------------------------
• [1.612 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:28:30.874
    Sep  3 21:28:30.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename certificates 09/03/22 21:28:30.875
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:30.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:30.887
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 09/03/22 21:28:32.39
    STEP: getting /apis/certificates.k8s.io 09/03/22 21:28:32.393
    STEP: getting /apis/certificates.k8s.io/v1 09/03/22 21:28:32.394
    STEP: creating 09/03/22 21:28:32.396
    STEP: getting 09/03/22 21:28:32.414
    STEP: listing 09/03/22 21:28:32.42
    STEP: watching 09/03/22 21:28:32.422
    Sep  3 21:28:32.423: INFO: starting watch
    STEP: patching 09/03/22 21:28:32.424
    STEP: updating 09/03/22 21:28:32.428
    Sep  3 21:28:32.431: INFO: waiting for watch events with expected annotations
    Sep  3 21:28:32.431: INFO: saw patched and updated annotations
    STEP: getting /approval 09/03/22 21:28:32.432
    STEP: patching /approval 09/03/22 21:28:32.442
    STEP: updating /approval 09/03/22 21:28:32.446
    STEP: getting /status 09/03/22 21:28:32.452
    STEP: patching /status 09/03/22 21:28:32.454
    STEP: updating /status 09/03/22 21:28:32.459
    STEP: deleting 09/03/22 21:28:32.464
    STEP: deleting a collection 09/03/22 21:28:32.473
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:28:32.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-2427" for this suite. 09/03/22 21:28:32.483
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:28:32.487
Sep  3 21:28:32.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:28:32.488
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:32.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:32.501
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-bc985e2f-8555-45b9-8020-5fc83834392c 09/03/22 21:28:32.503
STEP: Creating a pod to test consume configMaps 09/03/22 21:28:32.506
Sep  3 21:28:32.514: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11" in namespace "projected-4242" to be "Succeeded or Failed"
Sep  3 21:28:32.517: INFO: Pod "pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.874906ms
Sep  3 21:28:34.521: INFO: Pod "pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007059388s
Sep  3 21:28:36.522: INFO: Pod "pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008024806s
STEP: Saw pod success 09/03/22 21:28:36.522
Sep  3 21:28:36.522: INFO: Pod "pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11" satisfied condition "Succeeded or Failed"
Sep  3 21:28:36.524: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11 container agnhost-container: <nil>
STEP: delete the pod 09/03/22 21:28:36.528
Sep  3 21:28:36.533: INFO: Waiting for pod pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11 to disappear
Sep  3 21:28:36.535: INFO: Pod pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Sep  3 21:28:36.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4242" for this suite. 09/03/22 21:28:36.537
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":169,"skipped":2953,"failed":0}
------------------------------
• [4.054 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:28:32.487
    Sep  3 21:28:32.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:28:32.488
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:32.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:32.501
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-bc985e2f-8555-45b9-8020-5fc83834392c 09/03/22 21:28:32.503
    STEP: Creating a pod to test consume configMaps 09/03/22 21:28:32.506
    Sep  3 21:28:32.514: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11" in namespace "projected-4242" to be "Succeeded or Failed"
    Sep  3 21:28:32.517: INFO: Pod "pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.874906ms
    Sep  3 21:28:34.521: INFO: Pod "pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007059388s
    Sep  3 21:28:36.522: INFO: Pod "pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008024806s
    STEP: Saw pod success 09/03/22 21:28:36.522
    Sep  3 21:28:36.522: INFO: Pod "pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11" satisfied condition "Succeeded or Failed"
    Sep  3 21:28:36.524: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11 container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 21:28:36.528
    Sep  3 21:28:36.533: INFO: Waiting for pod pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11 to disappear
    Sep  3 21:28:36.535: INFO: Pod pod-projected-configmaps-fbd01023-9142-4785-994a-58fb67dbef11 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Sep  3 21:28:36.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4242" for this suite. 09/03/22 21:28:36.537
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:28:36.555
Sep  3 21:28:36.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename containers 09/03/22 21:28:36.556
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:36.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:36.569
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 09/03/22 21:28:36.571
Sep  3 21:28:36.575: INFO: Waiting up to 5m0s for pod "client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4" in namespace "containers-5885" to be "Succeeded or Failed"
Sep  3 21:28:36.577: INFO: Pod "client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083104ms
Sep  3 21:28:38.584: INFO: Pod "client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009221525s
Sep  3 21:28:40.581: INFO: Pod "client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005319423s
STEP: Saw pod success 09/03/22 21:28:40.581
Sep  3 21:28:40.581: INFO: Pod "client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4" satisfied condition "Succeeded or Failed"
Sep  3 21:28:40.583: INFO: Trying to get logs from node kind-worker2 pod client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4 container agnhost-container: <nil>
STEP: delete the pod 09/03/22 21:28:40.587
Sep  3 21:28:40.594: INFO: Waiting for pod client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4 to disappear
Sep  3 21:28:40.596: INFO: Pod client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Sep  3 21:28:40.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5885" for this suite. 09/03/22 21:28:40.598
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":170,"skipped":3005,"failed":0}
------------------------------
• [4.046 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:28:36.555
    Sep  3 21:28:36.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename containers 09/03/22 21:28:36.556
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:36.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:36.569
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 09/03/22 21:28:36.571
    Sep  3 21:28:36.575: INFO: Waiting up to 5m0s for pod "client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4" in namespace "containers-5885" to be "Succeeded or Failed"
    Sep  3 21:28:36.577: INFO: Pod "client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083104ms
    Sep  3 21:28:38.584: INFO: Pod "client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009221525s
    Sep  3 21:28:40.581: INFO: Pod "client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005319423s
    STEP: Saw pod success 09/03/22 21:28:40.581
    Sep  3 21:28:40.581: INFO: Pod "client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4" satisfied condition "Succeeded or Failed"
    Sep  3 21:28:40.583: INFO: Trying to get logs from node kind-worker2 pod client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4 container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 21:28:40.587
    Sep  3 21:28:40.594: INFO: Waiting for pod client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4 to disappear
    Sep  3 21:28:40.596: INFO: Pod client-containers-dae9e36e-80b0-44cf-b259-1ee034eb0ad4 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Sep  3 21:28:40.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5885" for this suite. 09/03/22 21:28:40.598
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:28:40.601
Sep  3 21:28:40.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 21:28:40.602
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:40.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:40.621
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 21:28:40.629
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:28:41.03
STEP: Deploying the webhook pod 09/03/22 21:28:41.037
STEP: Wait for the deployment to be ready 09/03/22 21:28:41.044
Sep  3 21:28:41.049: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 09/03/22 21:28:43.056
STEP: Verifying the service has paired with the endpoint 09/03/22 21:28:43.062
Sep  3 21:28:44.062: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 09/03/22 21:28:44.068
STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/03/22 21:28:44.089
STEP: Creating a configMap that should not be mutated 09/03/22 21:28:44.093
STEP: Patching a mutating webhook configuration's rules to include the create operation 09/03/22 21:28:44.098
STEP: Creating a configMap that should be mutated 09/03/22 21:28:44.103
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:28:44.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2399" for this suite. 09/03/22 21:28:44.141
STEP: Destroying namespace "webhook-2399-markers" for this suite. 09/03/22 21:28:44.145
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":171,"skipped":3009,"failed":0}
------------------------------
• [3.619 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:28:40.601
    Sep  3 21:28:40.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 21:28:40.602
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:40.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:40.621
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 21:28:40.629
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:28:41.03
    STEP: Deploying the webhook pod 09/03/22 21:28:41.037
    STEP: Wait for the deployment to be ready 09/03/22 21:28:41.044
    Sep  3 21:28:41.049: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 09/03/22 21:28:43.056
    STEP: Verifying the service has paired with the endpoint 09/03/22 21:28:43.062
    Sep  3 21:28:44.062: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 09/03/22 21:28:44.068
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/03/22 21:28:44.089
    STEP: Creating a configMap that should not be mutated 09/03/22 21:28:44.093
    STEP: Patching a mutating webhook configuration's rules to include the create operation 09/03/22 21:28:44.098
    STEP: Creating a configMap that should be mutated 09/03/22 21:28:44.103
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:28:44.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2399" for this suite. 09/03/22 21:28:44.141
    STEP: Destroying namespace "webhook-2399-markers" for this suite. 09/03/22 21:28:44.145
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:28:44.247
Sep  3 21:28:44.247: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename custom-resource-definition 09/03/22 21:28:44.248
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:44.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:44.265
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Sep  3 21:28:44.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:28:50.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2455" for this suite. 09/03/22 21:28:50.424
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":172,"skipped":3061,"failed":0}
------------------------------
• [SLOW TEST] [6.180 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:28:44.247
    Sep  3 21:28:44.247: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename custom-resource-definition 09/03/22 21:28:44.248
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:44.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:44.265
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Sep  3 21:28:44.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:28:50.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2455" for this suite. 09/03/22 21:28:50.424
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:28:50.431
Sep  3 21:28:50.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename disruption 09/03/22 21:28:50.432
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:50.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:50.445
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:28:50.447
Sep  3 21:28:50.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename disruption-2 09/03/22 21:28:50.448
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:50.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:50.459
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 09/03/22 21:28:50.463
STEP: Waiting for the pdb to be processed 09/03/22 21:28:52.477
STEP: Waiting for the pdb to be processed 09/03/22 21:28:52.484
STEP: listing a collection of PDBs across all namespaces 09/03/22 21:28:54.492
STEP: listing a collection of PDBs in namespace disruption-4735 09/03/22 21:28:54.495
STEP: deleting a collection of PDBs 09/03/22 21:28:54.497
STEP: Waiting for the PDB collection to be deleted 09/03/22 21:28:54.504
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Sep  3 21:28:54.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-8770" for this suite. 09/03/22 21:28:54.509
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Sep  3 21:28:54.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4735" for this suite. 09/03/22 21:28:54.515
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":173,"skipped":3070,"failed":0}
------------------------------
• [4.088 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:28:50.431
    Sep  3 21:28:50.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename disruption 09/03/22 21:28:50.432
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:50.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:50.445
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:28:50.447
    Sep  3 21:28:50.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename disruption-2 09/03/22 21:28:50.448
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:50.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:50.459
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 09/03/22 21:28:50.463
    STEP: Waiting for the pdb to be processed 09/03/22 21:28:52.477
    STEP: Waiting for the pdb to be processed 09/03/22 21:28:52.484
    STEP: listing a collection of PDBs across all namespaces 09/03/22 21:28:54.492
    STEP: listing a collection of PDBs in namespace disruption-4735 09/03/22 21:28:54.495
    STEP: deleting a collection of PDBs 09/03/22 21:28:54.497
    STEP: Waiting for the PDB collection to be deleted 09/03/22 21:28:54.504
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Sep  3 21:28:54.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-8770" for this suite. 09/03/22 21:28:54.509
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Sep  3 21:28:54.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4735" for this suite. 09/03/22 21:28:54.515
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:28:54.521
Sep  3 21:28:54.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename namespaces 09/03/22 21:28:54.522
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:54.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:54.534
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 09/03/22 21:28:54.537
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:54.546
STEP: Creating a service in the namespace 09/03/22 21:28:54.549
STEP: Deleting the namespace 09/03/22 21:28:54.564
STEP: Waiting for the namespace to be removed. 09/03/22 21:28:54.58
STEP: Recreating the namespace 09/03/22 21:29:00.583
STEP: Verifying there is no service in the namespace 09/03/22 21:29:00.592
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:29:00.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2275" for this suite. 09/03/22 21:29:00.601
STEP: Destroying namespace "nsdeletetest-4406" for this suite. 09/03/22 21:29:00.603
Sep  3 21:29:00.605: INFO: Namespace nsdeletetest-4406 was already deleted
STEP: Destroying namespace "nsdeletetest-4960" for this suite. 09/03/22 21:29:00.605
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":174,"skipped":3074,"failed":0}
------------------------------
• [SLOW TEST] [6.088 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:28:54.521
    Sep  3 21:28:54.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename namespaces 09/03/22 21:28:54.522
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:54.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:28:54.534
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 09/03/22 21:28:54.537
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:28:54.546
    STEP: Creating a service in the namespace 09/03/22 21:28:54.549
    STEP: Deleting the namespace 09/03/22 21:28:54.564
    STEP: Waiting for the namespace to be removed. 09/03/22 21:28:54.58
    STEP: Recreating the namespace 09/03/22 21:29:00.583
    STEP: Verifying there is no service in the namespace 09/03/22 21:29:00.592
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:29:00.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2275" for this suite. 09/03/22 21:29:00.601
    STEP: Destroying namespace "nsdeletetest-4406" for this suite. 09/03/22 21:29:00.603
    Sep  3 21:29:00.605: INFO: Namespace nsdeletetest-4406 was already deleted
    STEP: Destroying namespace "nsdeletetest-4960" for this suite. 09/03/22 21:29:00.605
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:29:00.611
Sep  3 21:29:00.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename svcaccounts 09/03/22 21:29:00.612
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:29:00.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:29:00.629
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 09/03/22 21:29:00.632
STEP: watching for the ServiceAccount to be added 09/03/22 21:29:00.636
STEP: patching the ServiceAccount 09/03/22 21:29:00.637
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/03/22 21:29:00.641
STEP: deleting the ServiceAccount 09/03/22 21:29:00.643
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Sep  3 21:29:00.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9222" for this suite. 09/03/22 21:29:00.653
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":175,"skipped":3108,"failed":0}
------------------------------
• [0.046 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:29:00.611
    Sep  3 21:29:00.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename svcaccounts 09/03/22 21:29:00.612
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:29:00.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:29:00.629
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 09/03/22 21:29:00.632
    STEP: watching for the ServiceAccount to be added 09/03/22 21:29:00.636
    STEP: patching the ServiceAccount 09/03/22 21:29:00.637
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/03/22 21:29:00.641
    STEP: deleting the ServiceAccount 09/03/22 21:29:00.643
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Sep  3 21:29:00.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9222" for this suite. 09/03/22 21:29:00.653
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:29:00.663
Sep  3 21:29:00.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename hostport 09/03/22 21:29:00.664
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:29:00.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:29:00.672
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/03/22 21:29:00.678
Sep  3 21:29:00.685: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1821" to be "running and ready"
Sep  3 21:29:00.690: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.924613ms
Sep  3 21:29:00.690: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:29:02.692: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007657988s
Sep  3 21:29:02.693: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  3 21:29:02.693: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.18.0.2 on the node which pod1 resides and expect scheduled 09/03/22 21:29:02.693
Sep  3 21:29:02.697: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1821" to be "running and ready"
Sep  3 21:29:02.718: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.702757ms
Sep  3 21:29:02.718: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:29:04.722: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.024802736s
Sep  3 21:29:04.722: INFO: The phase of Pod pod2 is Running (Ready = false)
Sep  3 21:29:06.721: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.024263258s
Sep  3 21:29:06.721: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  3 21:29:06.721: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.18.0.2 but use UDP protocol on the node which pod2 resides 09/03/22 21:29:06.722
Sep  3 21:29:06.726: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1821" to be "running and ready"
Sep  3 21:29:06.727: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.714607ms
Sep  3 21:29:06.728: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:29:08.731: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005613423s
Sep  3 21:29:08.731: INFO: The phase of Pod pod3 is Running (Ready = true)
Sep  3 21:29:08.731: INFO: Pod "pod3" satisfied condition "running and ready"
Sep  3 21:29:08.735: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1821" to be "running and ready"
Sep  3 21:29:08.741: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.313221ms
Sep  3 21:29:08.741: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:29:10.745: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.00981684s
Sep  3 21:29:10.746: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Sep  3 21:29:10.746: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/03/22 21:29:10.748
Sep  3 21:29:10.748: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.18.0.2 http://127.0.0.1:54323/hostname] Namespace:hostport-1821 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 21:29:10.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 21:29:10.749: INFO: ExecWithOptions: Clientset creation
Sep  3 21:29:10.749: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1821/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.18.0.2+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.18.0.2, port: 54323 09/03/22 21:29:10.838
Sep  3 21:29:10.838: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.18.0.2:54323/hostname] Namespace:hostport-1821 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 21:29:10.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 21:29:10.838: INFO: ExecWithOptions: Clientset creation
Sep  3 21:29:10.838: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1821/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.18.0.2%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.18.0.2, port: 54323 UDP 09/03/22 21:29:10.899
Sep  3 21:29:10.899: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.18.0.2 54323] Namespace:hostport-1821 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 21:29:10.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 21:29:10.899: INFO: ExecWithOptions: Clientset creation
Sep  3 21:29:10.899: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1821/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.18.0.2+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Sep  3 21:29:15.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-1821" for this suite. 09/03/22 21:29:15.97
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":176,"skipped":3127,"failed":0}
------------------------------
• [SLOW TEST] [15.310 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:29:00.663
    Sep  3 21:29:00.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename hostport 09/03/22 21:29:00.664
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:29:00.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:29:00.672
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/03/22 21:29:00.678
    Sep  3 21:29:00.685: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1821" to be "running and ready"
    Sep  3 21:29:00.690: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.924613ms
    Sep  3 21:29:00.690: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:29:02.692: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007657988s
    Sep  3 21:29:02.693: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  3 21:29:02.693: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.18.0.2 on the node which pod1 resides and expect scheduled 09/03/22 21:29:02.693
    Sep  3 21:29:02.697: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1821" to be "running and ready"
    Sep  3 21:29:02.718: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.702757ms
    Sep  3 21:29:02.718: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:29:04.722: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.024802736s
    Sep  3 21:29:04.722: INFO: The phase of Pod pod2 is Running (Ready = false)
    Sep  3 21:29:06.721: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.024263258s
    Sep  3 21:29:06.721: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  3 21:29:06.721: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.18.0.2 but use UDP protocol on the node which pod2 resides 09/03/22 21:29:06.722
    Sep  3 21:29:06.726: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1821" to be "running and ready"
    Sep  3 21:29:06.727: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.714607ms
    Sep  3 21:29:06.728: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:29:08.731: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005613423s
    Sep  3 21:29:08.731: INFO: The phase of Pod pod3 is Running (Ready = true)
    Sep  3 21:29:08.731: INFO: Pod "pod3" satisfied condition "running and ready"
    Sep  3 21:29:08.735: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1821" to be "running and ready"
    Sep  3 21:29:08.741: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.313221ms
    Sep  3 21:29:08.741: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:29:10.745: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.00981684s
    Sep  3 21:29:10.746: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Sep  3 21:29:10.746: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/03/22 21:29:10.748
    Sep  3 21:29:10.748: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.18.0.2 http://127.0.0.1:54323/hostname] Namespace:hostport-1821 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 21:29:10.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 21:29:10.749: INFO: ExecWithOptions: Clientset creation
    Sep  3 21:29:10.749: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1821/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.18.0.2+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.18.0.2, port: 54323 09/03/22 21:29:10.838
    Sep  3 21:29:10.838: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.18.0.2:54323/hostname] Namespace:hostport-1821 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 21:29:10.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 21:29:10.838: INFO: ExecWithOptions: Clientset creation
    Sep  3 21:29:10.838: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1821/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.18.0.2%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.18.0.2, port: 54323 UDP 09/03/22 21:29:10.899
    Sep  3 21:29:10.899: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.18.0.2 54323] Namespace:hostport-1821 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 21:29:10.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 21:29:10.899: INFO: ExecWithOptions: Clientset creation
    Sep  3 21:29:10.899: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1821/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.18.0.2+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Sep  3 21:29:15.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-1821" for this suite. 09/03/22 21:29:15.97
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:29:15.976
Sep  3 21:29:15.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename resourcequota 09/03/22 21:29:15.977
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:29:15.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:29:15.99
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 09/03/22 21:29:15.993
STEP: Getting a ResourceQuota 09/03/22 21:29:15.996
STEP: Listing all ResourceQuotas with LabelSelector 09/03/22 21:29:15.999
STEP: Patching the ResourceQuota 09/03/22 21:29:16.001
STEP: Deleting a Collection of ResourceQuotas 09/03/22 21:29:16.006
STEP: Verifying the deleted ResourceQuota 09/03/22 21:29:16.015
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Sep  3 21:29:16.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4098" for this suite. 09/03/22 21:29:16.019
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":177,"skipped":3143,"failed":0}
------------------------------
• [0.047 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:29:15.976
    Sep  3 21:29:15.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename resourcequota 09/03/22 21:29:15.977
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:29:15.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:29:15.99
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 09/03/22 21:29:15.993
    STEP: Getting a ResourceQuota 09/03/22 21:29:15.996
    STEP: Listing all ResourceQuotas with LabelSelector 09/03/22 21:29:15.999
    STEP: Patching the ResourceQuota 09/03/22 21:29:16.001
    STEP: Deleting a Collection of ResourceQuotas 09/03/22 21:29:16.006
    STEP: Verifying the deleted ResourceQuota 09/03/22 21:29:16.015
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Sep  3 21:29:16.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4098" for this suite. 09/03/22 21:29:16.019
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:29:16.026
Sep  3 21:29:16.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename deployment 09/03/22 21:29:16.027
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:29:16.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:29:16.042
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Sep  3 21:29:16.044: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  3 21:29:16.048: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  3 21:29:21.057: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/03/22 21:29:21.057
Sep  3 21:29:21.057: INFO: Creating deployment "test-rolling-update-deployment"
Sep  3 21:29:21.083: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  3 21:29:21.130: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Sep  3 21:29:23.139: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  3 21:29:23.141: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  3 21:29:23.147: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7565  37c47b7a-df15-445a-ac7c-c0c2e97831d2 13129 1 2022-09-03 21:29:21 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-09-03 21:29:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a10708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-09-03 21:29:21 +0000 UTC,LastTransitionTime:2022-09-03 21:29:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2022-09-03 21:29:22 +0000 UTC,LastTransitionTime:2022-09-03 21:29:21 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  3 21:29:23.150: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-7565  0ee152a5-dba4-472e-a4e8-97dc6b85b322 13119 1 2022-09-03 21:29:21 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 37c47b7a-df15-445a-ac7c-c0c2e97831d2 0xc003a10c17 0xc003a10c18}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:29:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37c47b7a-df15-445a-ac7c-c0c2e97831d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:29:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a10cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:29:23.150: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  3 21:29:23.150: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7565  d95979c6-461f-45fb-b14d-3c2b3a51f30c 13128 2 2022-09-03 21:29:16 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 37c47b7a-df15-445a-ac7c-c0c2e97831d2 0xc003a10ae7 0xc003a10ae8}] [] [{e2e.test Update apps/v1 2022-09-03 21:29:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37c47b7a-df15-445a-ac7c-c0c2e97831d2\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:29:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a10ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:29:23.152: INFO: Pod "test-rolling-update-deployment-78f575d8ff-nj99w" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-nj99w test-rolling-update-deployment-78f575d8ff- deployment-7565  4fdca2f9-7eca-4447-bbbb-b8cea204c65d 13118 0 2022-09-03 21:29:21 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 0ee152a5-dba4-472e-a4e8-97dc6b85b322 0xc003a110e7 0xc003a110e8}] [] [{kube-controller-manager Update v1 2022-09-03 21:29:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ee152a5-dba4-472e-a4e8-97dc6b85b322\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:29:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pjg2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pjg2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:29:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:29:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:29:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.174,StartTime:2022-09-03 21:29:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:29:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://79e43e207ed13e64cc0a9c4598cb1f2de3f87c6a1a9b698fee67092b001f6d45,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Sep  3 21:29:23.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7565" for this suite. 09/03/22 21:29:23.155
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":178,"skipped":3175,"failed":0}
------------------------------
• [SLOW TEST] [7.132 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:29:16.026
    Sep  3 21:29:16.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename deployment 09/03/22 21:29:16.027
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:29:16.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:29:16.042
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Sep  3 21:29:16.044: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Sep  3 21:29:16.048: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  3 21:29:21.057: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/03/22 21:29:21.057
    Sep  3 21:29:21.057: INFO: Creating deployment "test-rolling-update-deployment"
    Sep  3 21:29:21.083: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Sep  3 21:29:21.130: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
    Sep  3 21:29:23.139: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Sep  3 21:29:23.141: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  3 21:29:23.147: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7565  37c47b7a-df15-445a-ac7c-c0c2e97831d2 13129 1 2022-09-03 21:29:21 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-09-03 21:29:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a10708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-09-03 21:29:21 +0000 UTC,LastTransitionTime:2022-09-03 21:29:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2022-09-03 21:29:22 +0000 UTC,LastTransitionTime:2022-09-03 21:29:21 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  3 21:29:23.150: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-7565  0ee152a5-dba4-472e-a4e8-97dc6b85b322 13119 1 2022-09-03 21:29:21 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 37c47b7a-df15-445a-ac7c-c0c2e97831d2 0xc003a10c17 0xc003a10c18}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:29:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37c47b7a-df15-445a-ac7c-c0c2e97831d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:29:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a10cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:29:23.150: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Sep  3 21:29:23.150: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7565  d95979c6-461f-45fb-b14d-3c2b3a51f30c 13128 2 2022-09-03 21:29:16 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 37c47b7a-df15-445a-ac7c-c0c2e97831d2 0xc003a10ae7 0xc003a10ae8}] [] [{e2e.test Update apps/v1 2022-09-03 21:29:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37c47b7a-df15-445a-ac7c-c0c2e97831d2\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:29:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a10ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:29:23.152: INFO: Pod "test-rolling-update-deployment-78f575d8ff-nj99w" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-nj99w test-rolling-update-deployment-78f575d8ff- deployment-7565  4fdca2f9-7eca-4447-bbbb-b8cea204c65d 13118 0 2022-09-03 21:29:21 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 0ee152a5-dba4-472e-a4e8-97dc6b85b322 0xc003a110e7 0xc003a110e8}] [] [{kube-controller-manager Update v1 2022-09-03 21:29:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ee152a5-dba4-472e-a4e8-97dc6b85b322\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:29:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pjg2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pjg2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:29:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:29:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:29:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.174,StartTime:2022-09-03 21:29:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:29:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://79e43e207ed13e64cc0a9c4598cb1f2de3f87c6a1a9b698fee67092b001f6d45,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Sep  3 21:29:23.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7565" for this suite. 09/03/22 21:29:23.155
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:29:23.159
Sep  3 21:29:23.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pods 09/03/22 21:29:23.16
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:29:23.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:29:23.178
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Sep  3 21:29:23.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: creating the pod 09/03/22 21:29:23.182
STEP: submitting the pod to kubernetes 09/03/22 21:29:23.182
Sep  3 21:29:23.192: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-e18bd435-cf5b-4dfe-84b8-09376fb8b8ff" in namespace "pods-3502" to be "running and ready"
Sep  3 21:29:23.195: INFO: Pod "pod-logs-websocket-e18bd435-cf5b-4dfe-84b8-09376fb8b8ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168005ms
Sep  3 21:29:23.195: INFO: The phase of Pod pod-logs-websocket-e18bd435-cf5b-4dfe-84b8-09376fb8b8ff is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:29:25.197: INFO: Pod "pod-logs-websocket-e18bd435-cf5b-4dfe-84b8-09376fb8b8ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.004702134s
Sep  3 21:29:25.197: INFO: The phase of Pod pod-logs-websocket-e18bd435-cf5b-4dfe-84b8-09376fb8b8ff is Running (Ready = true)
Sep  3 21:29:25.197: INFO: Pod "pod-logs-websocket-e18bd435-cf5b-4dfe-84b8-09376fb8b8ff" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Sep  3 21:29:25.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3502" for this suite. 09/03/22 21:29:25.209
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":179,"skipped":3197,"failed":0}
------------------------------
• [2.056 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:29:23.159
    Sep  3 21:29:23.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pods 09/03/22 21:29:23.16
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:29:23.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:29:23.178
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Sep  3 21:29:23.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: creating the pod 09/03/22 21:29:23.182
    STEP: submitting the pod to kubernetes 09/03/22 21:29:23.182
    Sep  3 21:29:23.192: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-e18bd435-cf5b-4dfe-84b8-09376fb8b8ff" in namespace "pods-3502" to be "running and ready"
    Sep  3 21:29:23.195: INFO: Pod "pod-logs-websocket-e18bd435-cf5b-4dfe-84b8-09376fb8b8ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168005ms
    Sep  3 21:29:23.195: INFO: The phase of Pod pod-logs-websocket-e18bd435-cf5b-4dfe-84b8-09376fb8b8ff is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:29:25.197: INFO: Pod "pod-logs-websocket-e18bd435-cf5b-4dfe-84b8-09376fb8b8ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.004702134s
    Sep  3 21:29:25.197: INFO: The phase of Pod pod-logs-websocket-e18bd435-cf5b-4dfe-84b8-09376fb8b8ff is Running (Ready = true)
    Sep  3 21:29:25.197: INFO: Pod "pod-logs-websocket-e18bd435-cf5b-4dfe-84b8-09376fb8b8ff" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Sep  3 21:29:25.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3502" for this suite. 09/03/22 21:29:25.209
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:29:25.216
Sep  3 21:29:25.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 21:29:25.217
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:29:25.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:29:25.228
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-856ad9d1-e80c-4672-93d3-087fa68d0cf3 09/03/22 21:29:25.232
STEP: Creating the pod 09/03/22 21:29:25.235
Sep  3 21:29:25.239: INFO: Waiting up to 5m0s for pod "pod-configmaps-b3776540-9d8e-4a65-8f56-8444c91d7895" in namespace "configmap-3619" to be "running and ready"
Sep  3 21:29:25.241: INFO: Pod "pod-configmaps-b3776540-9d8e-4a65-8f56-8444c91d7895": Phase="Pending", Reason="", readiness=false. Elapsed: 2.275705ms
Sep  3 21:29:25.241: INFO: The phase of Pod pod-configmaps-b3776540-9d8e-4a65-8f56-8444c91d7895 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:29:27.244: INFO: Pod "pod-configmaps-b3776540-9d8e-4a65-8f56-8444c91d7895": Phase="Running", Reason="", readiness=true. Elapsed: 2.005230036s
Sep  3 21:29:27.244: INFO: The phase of Pod pod-configmaps-b3776540-9d8e-4a65-8f56-8444c91d7895 is Running (Ready = true)
Sep  3 21:29:27.244: INFO: Pod "pod-configmaps-b3776540-9d8e-4a65-8f56-8444c91d7895" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-856ad9d1-e80c-4672-93d3-087fa68d0cf3 09/03/22 21:29:27.249
STEP: waiting to observe update in volume 09/03/22 21:29:27.252
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 21:30:37.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3619" for this suite. 09/03/22 21:30:37.467
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":180,"skipped":3210,"failed":0}
------------------------------
• [SLOW TEST] [72.258 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:29:25.216
    Sep  3 21:29:25.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 21:29:25.217
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:29:25.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:29:25.228
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-856ad9d1-e80c-4672-93d3-087fa68d0cf3 09/03/22 21:29:25.232
    STEP: Creating the pod 09/03/22 21:29:25.235
    Sep  3 21:29:25.239: INFO: Waiting up to 5m0s for pod "pod-configmaps-b3776540-9d8e-4a65-8f56-8444c91d7895" in namespace "configmap-3619" to be "running and ready"
    Sep  3 21:29:25.241: INFO: Pod "pod-configmaps-b3776540-9d8e-4a65-8f56-8444c91d7895": Phase="Pending", Reason="", readiness=false. Elapsed: 2.275705ms
    Sep  3 21:29:25.241: INFO: The phase of Pod pod-configmaps-b3776540-9d8e-4a65-8f56-8444c91d7895 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:29:27.244: INFO: Pod "pod-configmaps-b3776540-9d8e-4a65-8f56-8444c91d7895": Phase="Running", Reason="", readiness=true. Elapsed: 2.005230036s
    Sep  3 21:29:27.244: INFO: The phase of Pod pod-configmaps-b3776540-9d8e-4a65-8f56-8444c91d7895 is Running (Ready = true)
    Sep  3 21:29:27.244: INFO: Pod "pod-configmaps-b3776540-9d8e-4a65-8f56-8444c91d7895" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-856ad9d1-e80c-4672-93d3-087fa68d0cf3 09/03/22 21:29:27.249
    STEP: waiting to observe update in volume 09/03/22 21:29:27.252
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 21:30:37.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3619" for this suite. 09/03/22 21:30:37.467
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:30:37.478
Sep  3 21:30:37.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename watch 09/03/22 21:30:37.48
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:30:37.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:30:37.5
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 09/03/22 21:30:37.502
STEP: starting a background goroutine to produce watch events 09/03/22 21:30:37.506
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/03/22 21:30:37.506
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Sep  3 21:30:40.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9881" for this suite. 09/03/22 21:30:40.333
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":181,"skipped":3210,"failed":0}
------------------------------
• [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:30:37.478
    Sep  3 21:30:37.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename watch 09/03/22 21:30:37.48
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:30:37.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:30:37.5
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 09/03/22 21:30:37.502
    STEP: starting a background goroutine to produce watch events 09/03/22 21:30:37.506
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/03/22 21:30:37.506
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Sep  3 21:30:40.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9881" for this suite. 09/03/22 21:30:40.333
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:30:40.399
Sep  3 21:30:40.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename deployment 09/03/22 21:30:40.402
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:30:40.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:30:40.413
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Sep  3 21:30:40.415: INFO: Creating simple deployment test-new-deployment
Sep  3 21:30:40.425: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 09/03/22 21:30:42.434
STEP: updating a scale subresource 09/03/22 21:30:42.436
STEP: verifying the deployment Spec.Replicas was modified 09/03/22 21:30:42.441
STEP: Patch a scale subresource 09/03/22 21:30:42.448
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  3 21:30:42.503: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-3372  77993bc1-6917-4818-a2b4-bc29b30a9b76 13450 3 2022-09-03 21:30:40 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-09-03 21:30:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b83208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2022-09-03 21:30:41 +0000 UTC,LastTransitionTime:2022-09-03 21:30:40 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-09-03 21:30:42 +0000 UTC,LastTransitionTime:2022-09-03 21:30:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  3 21:30:42.514: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-3372  7f20574b-38ac-46d2-b9d0-9afbb7e07de4 13452 3 2022-09-03 21:30:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 77993bc1-6917-4818-a2b4-bc29b30a9b76 0xc003fac4e7 0xc003fac4e8}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77993bc1-6917-4818-a2b4-bc29b30a9b76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:30:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fac578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:30:42.520: INFO: Pod "test-new-deployment-845c8977d9-p5kjt" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-p5kjt test-new-deployment-845c8977d9- deployment-3372  61dfb004-6e6c-41fa-ad97-6ba72493f2ef 13435 0 2022-09-03 21:30:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 7f20574b-38ac-46d2-b9d0-9afbb7e07de4 0xc003b83687 0xc003b83688}] [] [{kube-controller-manager Update v1 2022-09-03 21:30:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f20574b-38ac-46d2-b9d0-9afbb7e07de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:30:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkwk4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkwk4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.177,StartTime:2022-09-03 21:30:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:30:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5ec76f420aaf5b984833c8c943a703b2f343d43b8dac2c952c43f00a8359fb78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:30:42.520: INFO: Pod "test-new-deployment-845c8977d9-sth28" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-sth28 test-new-deployment-845c8977d9- deployment-3372  47babb89-512d-4c39-a15c-839d3edfbf65 13451 0 2022-09-03 21:30:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 7f20574b-38ac-46d2-b9d0-9afbb7e07de4 0xc003b83870 0xc003b83871}] [] [{kube-controller-manager Update v1 2022-09-03 21:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f20574b-38ac-46d2-b9d0-9afbb7e07de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:30:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gmznz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gmznz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:,StartTime:2022-09-03 21:30:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Sep  3 21:30:42.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3372" for this suite. 09/03/22 21:30:42.537
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":182,"skipped":3230,"failed":0}
------------------------------
• [2.155 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:30:40.399
    Sep  3 21:30:40.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename deployment 09/03/22 21:30:40.402
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:30:40.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:30:40.413
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Sep  3 21:30:40.415: INFO: Creating simple deployment test-new-deployment
    Sep  3 21:30:40.425: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 09/03/22 21:30:42.434
    STEP: updating a scale subresource 09/03/22 21:30:42.436
    STEP: verifying the deployment Spec.Replicas was modified 09/03/22 21:30:42.441
    STEP: Patch a scale subresource 09/03/22 21:30:42.448
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  3 21:30:42.503: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-3372  77993bc1-6917-4818-a2b4-bc29b30a9b76 13450 3 2022-09-03 21:30:40 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-09-03 21:30:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b83208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2022-09-03 21:30:41 +0000 UTC,LastTransitionTime:2022-09-03 21:30:40 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-09-03 21:30:42 +0000 UTC,LastTransitionTime:2022-09-03 21:30:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  3 21:30:42.514: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-3372  7f20574b-38ac-46d2-b9d0-9afbb7e07de4 13452 3 2022-09-03 21:30:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 77993bc1-6917-4818-a2b4-bc29b30a9b76 0xc003fac4e7 0xc003fac4e8}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77993bc1-6917-4818-a2b4-bc29b30a9b76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:30:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fac578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:30:42.520: INFO: Pod "test-new-deployment-845c8977d9-p5kjt" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-p5kjt test-new-deployment-845c8977d9- deployment-3372  61dfb004-6e6c-41fa-ad97-6ba72493f2ef 13435 0 2022-09-03 21:30:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 7f20574b-38ac-46d2-b9d0-9afbb7e07de4 0xc003b83687 0xc003b83688}] [] [{kube-controller-manager Update v1 2022-09-03 21:30:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f20574b-38ac-46d2-b9d0-9afbb7e07de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:30:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkwk4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkwk4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.177,StartTime:2022-09-03 21:30:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:30:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5ec76f420aaf5b984833c8c943a703b2f343d43b8dac2c952c43f00a8359fb78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:30:42.520: INFO: Pod "test-new-deployment-845c8977d9-sth28" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-sth28 test-new-deployment-845c8977d9- deployment-3372  47babb89-512d-4c39-a15c-839d3edfbf65 13451 0 2022-09-03 21:30:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 7f20574b-38ac-46d2-b9d0-9afbb7e07de4 0xc003b83870 0xc003b83871}] [] [{kube-controller-manager Update v1 2022-09-03 21:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f20574b-38ac-46d2-b9d0-9afbb7e07de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:30:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gmznz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gmznz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:30:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:,StartTime:2022-09-03 21:30:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Sep  3 21:30:42.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3372" for this suite. 09/03/22 21:30:42.537
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:30:42.556
Sep  3 21:30:42.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename cronjob 09/03/22 21:30:42.557
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:30:42.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:30:42.597
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 09/03/22 21:30:42.599
STEP: Ensuring more than one job is running at a time 09/03/22 21:30:42.604
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/03/22 21:32:00.608
STEP: Removing cronjob 09/03/22 21:32:00.61
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Sep  3 21:32:00.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9058" for this suite. 09/03/22 21:32:00.623
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":183,"skipped":3241,"failed":0}
------------------------------
• [SLOW TEST] [78.087 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:30:42.556
    Sep  3 21:30:42.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename cronjob 09/03/22 21:30:42.557
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:30:42.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:30:42.597
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 09/03/22 21:30:42.599
    STEP: Ensuring more than one job is running at a time 09/03/22 21:30:42.604
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/03/22 21:32:00.608
    STEP: Removing cronjob 09/03/22 21:32:00.61
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Sep  3 21:32:00.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9058" for this suite. 09/03/22 21:32:00.623
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:32:00.65
Sep  3 21:32:00.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename job 09/03/22 21:32:00.652
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:00.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:00.717
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 09/03/22 21:32:00.721
STEP: Ensuring job reaches completions 09/03/22 21:32:00.734
STEP: Ensuring pods with index for job exist 09/03/22 21:32:08.736
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Sep  3 21:32:08.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4312" for this suite. 09/03/22 21:32:08.741
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":184,"skipped":3254,"failed":0}
------------------------------
• [SLOW TEST] [8.095 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:32:00.65
    Sep  3 21:32:00.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename job 09/03/22 21:32:00.652
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:00.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:00.717
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 09/03/22 21:32:00.721
    STEP: Ensuring job reaches completions 09/03/22 21:32:00.734
    STEP: Ensuring pods with index for job exist 09/03/22 21:32:08.736
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Sep  3 21:32:08.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4312" for this suite. 09/03/22 21:32:08.741
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:32:08.746
Sep  3 21:32:08.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 21:32:08.747
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:08.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:08.757
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 09/03/22 21:32:08.759
Sep  3 21:32:08.763: INFO: Waiting up to 5m0s for pod "pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f" in namespace "emptydir-9929" to be "Succeeded or Failed"
Sep  3 21:32:08.765: INFO: Pod "pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.352105ms
Sep  3 21:32:10.770: INFO: Pod "pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006466857s
Sep  3 21:32:12.769: INFO: Pod "pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005574086s
STEP: Saw pod success 09/03/22 21:32:12.769
Sep  3 21:32:12.769: INFO: Pod "pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f" satisfied condition "Succeeded or Failed"
Sep  3 21:32:12.773: INFO: Trying to get logs from node kind-worker2 pod pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f container test-container: <nil>
STEP: delete the pod 09/03/22 21:32:12.786
Sep  3 21:32:12.793: INFO: Waiting for pod pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f to disappear
Sep  3 21:32:12.794: INFO: Pod pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 21:32:12.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9929" for this suite. 09/03/22 21:32:12.797
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":185,"skipped":3255,"failed":0}
------------------------------
• [4.054 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:32:08.746
    Sep  3 21:32:08.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 21:32:08.747
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:08.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:08.757
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 09/03/22 21:32:08.759
    Sep  3 21:32:08.763: INFO: Waiting up to 5m0s for pod "pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f" in namespace "emptydir-9929" to be "Succeeded or Failed"
    Sep  3 21:32:08.765: INFO: Pod "pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.352105ms
    Sep  3 21:32:10.770: INFO: Pod "pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006466857s
    Sep  3 21:32:12.769: INFO: Pod "pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005574086s
    STEP: Saw pod success 09/03/22 21:32:12.769
    Sep  3 21:32:12.769: INFO: Pod "pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f" satisfied condition "Succeeded or Failed"
    Sep  3 21:32:12.773: INFO: Trying to get logs from node kind-worker2 pod pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f container test-container: <nil>
    STEP: delete the pod 09/03/22 21:32:12.786
    Sep  3 21:32:12.793: INFO: Waiting for pod pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f to disappear
    Sep  3 21:32:12.794: INFO: Pod pod-abb07d55-aa5d-488e-95d8-c0b13ee7802f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 21:32:12.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9929" for this suite. 09/03/22 21:32:12.797
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:32:12.804
Sep  3 21:32:12.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubelet-test 09/03/22 21:32:12.805
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:12.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:12.818
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Sep  3 21:32:12.824: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1" in namespace "kubelet-test-7830" to be "running and ready"
Sep  3 21:32:12.830: INFO: Pod "busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.429212ms
Sep  3 21:32:12.830: INFO: The phase of Pod busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:32:14.834: INFO: Pod "busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009106952s
Sep  3 21:32:14.834: INFO: The phase of Pod busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:32:16.834: INFO: Pod "busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1": Phase="Running", Reason="", readiness=true. Elapsed: 4.009322583s
Sep  3 21:32:16.834: INFO: The phase of Pod busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1 is Running (Ready = true)
Sep  3 21:32:16.834: INFO: Pod "busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Sep  3 21:32:16.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7830" for this suite. 09/03/22 21:32:16.844
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":186,"skipped":3276,"failed":0}
------------------------------
• [4.043 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:32:12.804
    Sep  3 21:32:12.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubelet-test 09/03/22 21:32:12.805
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:12.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:12.818
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Sep  3 21:32:12.824: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1" in namespace "kubelet-test-7830" to be "running and ready"
    Sep  3 21:32:12.830: INFO: Pod "busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.429212ms
    Sep  3 21:32:12.830: INFO: The phase of Pod busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:32:14.834: INFO: Pod "busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009106952s
    Sep  3 21:32:14.834: INFO: The phase of Pod busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:32:16.834: INFO: Pod "busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1": Phase="Running", Reason="", readiness=true. Elapsed: 4.009322583s
    Sep  3 21:32:16.834: INFO: The phase of Pod busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1 is Running (Ready = true)
    Sep  3 21:32:16.834: INFO: Pod "busybox-readonly-fs7d16cd8a-3864-471e-a3b9-93ecd93b7dd1" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Sep  3 21:32:16.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7830" for this suite. 09/03/22 21:32:16.844
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:32:16.853
Sep  3 21:32:16.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename events 09/03/22 21:32:16.854
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:16.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:16.866
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 09/03/22 21:32:16.868
STEP: get a list of Events with a label in the current namespace 09/03/22 21:32:16.876
STEP: delete a list of events 09/03/22 21:32:16.878
Sep  3 21:32:16.878: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/03/22 21:32:16.886
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Sep  3 21:32:16.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9128" for this suite. 09/03/22 21:32:16.892
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":187,"skipped":3314,"failed":0}
------------------------------
• [0.043 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:32:16.853
    Sep  3 21:32:16.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename events 09/03/22 21:32:16.854
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:16.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:16.866
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 09/03/22 21:32:16.868
    STEP: get a list of Events with a label in the current namespace 09/03/22 21:32:16.876
    STEP: delete a list of events 09/03/22 21:32:16.878
    Sep  3 21:32:16.878: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/03/22 21:32:16.886
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Sep  3 21:32:16.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9128" for this suite. 09/03/22 21:32:16.892
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:32:16.898
Sep  3 21:32:16.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:32:16.899
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:16.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:16.924
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 09/03/22 21:32:16.928
Sep  3 21:32:16.929: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2924 proxy --unix-socket=/tmp/kubectl-proxy-unix796077770/test'
STEP: retrieving proxy /api/ output 09/03/22 21:32:16.993
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:32:16.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2924" for this suite. 09/03/22 21:32:16.998
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":188,"skipped":3334,"failed":0}
------------------------------
• [0.103 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:32:16.898
    Sep  3 21:32:16.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:32:16.899
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:16.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:16.924
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 09/03/22 21:32:16.928
    Sep  3 21:32:16.929: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-2924 proxy --unix-socket=/tmp/kubectl-proxy-unix796077770/test'
    STEP: retrieving proxy /api/ output 09/03/22 21:32:16.993
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:32:16.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2924" for this suite. 09/03/22 21:32:16.998
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:32:17.003
Sep  3 21:32:17.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 21:32:17.004
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:17.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:17.015
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Sep  3 21:32:17.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1599" for this suite. 09/03/22 21:32:17.039
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":189,"skipped":3368,"failed":0}
------------------------------
• [0.039 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:32:17.003
    Sep  3 21:32:17.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 21:32:17.004
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:17.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:17.015
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 21:32:17.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1599" for this suite. 09/03/22 21:32:17.039
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:32:17.044
Sep  3 21:32:17.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 21:32:17.045
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:17.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:17.058
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/03/22 21:32:17.06
Sep  3 21:32:17.067: INFO: Waiting up to 5m0s for pod "pod-189071ca-0c24-48bb-86d8-86f71e45fa7a" in namespace "emptydir-5557" to be "Succeeded or Failed"
Sep  3 21:32:17.072: INFO: Pod "pod-189071ca-0c24-48bb-86d8-86f71e45fa7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.966012ms
Sep  3 21:32:19.075: INFO: Pod "pod-189071ca-0c24-48bb-86d8-86f71e45fa7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00837645s
Sep  3 21:32:21.079: INFO: Pod "pod-189071ca-0c24-48bb-86d8-86f71e45fa7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012314683s
STEP: Saw pod success 09/03/22 21:32:21.079
Sep  3 21:32:21.079: INFO: Pod "pod-189071ca-0c24-48bb-86d8-86f71e45fa7a" satisfied condition "Succeeded or Failed"
Sep  3 21:32:21.084: INFO: Trying to get logs from node kind-worker2 pod pod-189071ca-0c24-48bb-86d8-86f71e45fa7a container test-container: <nil>
STEP: delete the pod 09/03/22 21:32:21.088
Sep  3 21:32:21.102: INFO: Waiting for pod pod-189071ca-0c24-48bb-86d8-86f71e45fa7a to disappear
Sep  3 21:32:21.105: INFO: Pod pod-189071ca-0c24-48bb-86d8-86f71e45fa7a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 21:32:21.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5557" for this suite. 09/03/22 21:32:21.108
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":190,"skipped":3416,"failed":0}
------------------------------
• [4.067 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:32:17.044
    Sep  3 21:32:17.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 21:32:17.045
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:17.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:17.058
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/03/22 21:32:17.06
    Sep  3 21:32:17.067: INFO: Waiting up to 5m0s for pod "pod-189071ca-0c24-48bb-86d8-86f71e45fa7a" in namespace "emptydir-5557" to be "Succeeded or Failed"
    Sep  3 21:32:17.072: INFO: Pod "pod-189071ca-0c24-48bb-86d8-86f71e45fa7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.966012ms
    Sep  3 21:32:19.075: INFO: Pod "pod-189071ca-0c24-48bb-86d8-86f71e45fa7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00837645s
    Sep  3 21:32:21.079: INFO: Pod "pod-189071ca-0c24-48bb-86d8-86f71e45fa7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012314683s
    STEP: Saw pod success 09/03/22 21:32:21.079
    Sep  3 21:32:21.079: INFO: Pod "pod-189071ca-0c24-48bb-86d8-86f71e45fa7a" satisfied condition "Succeeded or Failed"
    Sep  3 21:32:21.084: INFO: Trying to get logs from node kind-worker2 pod pod-189071ca-0c24-48bb-86d8-86f71e45fa7a container test-container: <nil>
    STEP: delete the pod 09/03/22 21:32:21.088
    Sep  3 21:32:21.102: INFO: Waiting for pod pod-189071ca-0c24-48bb-86d8-86f71e45fa7a to disappear
    Sep  3 21:32:21.105: INFO: Pod pod-189071ca-0c24-48bb-86d8-86f71e45fa7a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 21:32:21.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5557" for this suite. 09/03/22 21:32:21.108
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:32:21.117
Sep  3 21:32:21.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename runtimeclass 09/03/22 21:32:21.118
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:21.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:21.144
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-2738-delete-me 09/03/22 21:32:21.151
STEP: Waiting for the RuntimeClass to disappear 09/03/22 21:32:21.155
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Sep  3 21:32:21.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2738" for this suite. 09/03/22 21:32:21.163
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":191,"skipped":3426,"failed":0}
------------------------------
• [0.050 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:32:21.117
    Sep  3 21:32:21.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename runtimeclass 09/03/22 21:32:21.118
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:21.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:21.144
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-2738-delete-me 09/03/22 21:32:21.151
    STEP: Waiting for the RuntimeClass to disappear 09/03/22 21:32:21.155
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Sep  3 21:32:21.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2738" for this suite. 09/03/22 21:32:21.163
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:32:21.181
Sep  3 21:32:21.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 21:32:21.183
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:21.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:21.208
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-45a2061d-35dc-48be-acba-0fbdc85985e1 09/03/22 21:32:21.213
STEP: Creating a pod to test consume configMaps 09/03/22 21:32:21.216
Sep  3 21:32:21.223: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5" in namespace "configmap-1240" to be "Succeeded or Failed"
Sep  3 21:32:21.225: INFO: Pod "pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.522305ms
Sep  3 21:32:23.228: INFO: Pod "pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005277736s
Sep  3 21:32:25.229: INFO: Pod "pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006000561s
STEP: Saw pod success 09/03/22 21:32:25.229
Sep  3 21:32:25.229: INFO: Pod "pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5" satisfied condition "Succeeded or Failed"
Sep  3 21:32:25.231: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5 container agnhost-container: <nil>
STEP: delete the pod 09/03/22 21:32:25.236
Sep  3 21:32:25.247: INFO: Waiting for pod pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5 to disappear
Sep  3 21:32:25.250: INFO: Pod pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 21:32:25.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1240" for this suite. 09/03/22 21:32:25.255
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":192,"skipped":3451,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:32:21.181
    Sep  3 21:32:21.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 21:32:21.183
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:21.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:21.208
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-45a2061d-35dc-48be-acba-0fbdc85985e1 09/03/22 21:32:21.213
    STEP: Creating a pod to test consume configMaps 09/03/22 21:32:21.216
    Sep  3 21:32:21.223: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5" in namespace "configmap-1240" to be "Succeeded or Failed"
    Sep  3 21:32:21.225: INFO: Pod "pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.522305ms
    Sep  3 21:32:23.228: INFO: Pod "pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005277736s
    Sep  3 21:32:25.229: INFO: Pod "pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006000561s
    STEP: Saw pod success 09/03/22 21:32:25.229
    Sep  3 21:32:25.229: INFO: Pod "pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5" satisfied condition "Succeeded or Failed"
    Sep  3 21:32:25.231: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5 container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 21:32:25.236
    Sep  3 21:32:25.247: INFO: Waiting for pod pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5 to disappear
    Sep  3 21:32:25.250: INFO: Pod pod-configmaps-7cd0e9b5-37a9-471e-8537-a7a0c40547f5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 21:32:25.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1240" for this suite. 09/03/22 21:32:25.255
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:32:25.264
Sep  3 21:32:25.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename deployment 09/03/22 21:32:25.265
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:25.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:25.278
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Sep  3 21:32:25.283: INFO: Creating deployment "webserver-deployment"
Sep  3 21:32:25.288: INFO: Waiting for observed generation 1
Sep  3 21:32:27.315: INFO: Waiting for all required pods to come up
Sep  3 21:32:27.342: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 09/03/22 21:32:27.342
Sep  3 21:32:27.342: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4bp4g" in namespace "deployment-2123" to be "running"
Sep  3 21:32:27.342: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4h2bg" in namespace "deployment-2123" to be "running"
Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6s4bs" in namespace "deployment-2123" to be "running"
Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6twtr" in namespace "deployment-2123" to be "running"
Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6wvcw" in namespace "deployment-2123" to be "running"
Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-fknpn" in namespace "deployment-2123" to be "running"
Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jn87p" in namespace "deployment-2123" to be "running"
Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mz6vh" in namespace "deployment-2123" to be "running"
Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qk68n" in namespace "deployment-2123" to be "running"
Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-4bp4g": Phase="Pending", Reason="", readiness=false. Elapsed: 96.471323ms
Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-qk68n": Phase="Pending", Reason="", readiness=false. Elapsed: 95.736422ms
Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-6s4bs": Phase="Pending", Reason="", readiness=false. Elapsed: 96.387523ms
Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-6wvcw": Phase="Pending", Reason="", readiness=false. Elapsed: 96.282422ms
Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-fknpn": Phase="Pending", Reason="", readiness=false. Elapsed: 96.267122ms
Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-jn87p": Phase="Pending", Reason="", readiness=false. Elapsed: 96.236322ms
Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-6twtr": Phase="Pending", Reason="", readiness=false. Elapsed: 96.564823ms
Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-4h2bg": Phase="Pending", Reason="", readiness=false. Elapsed: 96.515223ms
Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-mz6vh": Phase="Pending", Reason="", readiness=false. Elapsed: 96.263923ms
Sep  3 21:32:29.446: INFO: Pod "webserver-deployment-845c8977d9-6wvcw": Phase="Running", Reason="", readiness=true. Elapsed: 2.103386258s
Sep  3 21:32:29.446: INFO: Pod "webserver-deployment-845c8977d9-6wvcw" satisfied condition "running"
Sep  3 21:32:29.447: INFO: Pod "webserver-deployment-845c8977d9-mz6vh": Phase="Running", Reason="", readiness=true. Elapsed: 2.103783759s
Sep  3 21:32:29.447: INFO: Pod "webserver-deployment-845c8977d9-mz6vh" satisfied condition "running"
Sep  3 21:32:29.447: INFO: Pod "webserver-deployment-845c8977d9-4bp4g": Phase="Running", Reason="", readiness=true. Elapsed: 2.105353063s
Sep  3 21:32:29.448: INFO: Pod "webserver-deployment-845c8977d9-4bp4g" satisfied condition "running"
Sep  3 21:32:29.448: INFO: Pod "webserver-deployment-845c8977d9-qk68n": Phase="Running", Reason="", readiness=true. Elapsed: 2.104807862s
Sep  3 21:32:29.448: INFO: Pod "webserver-deployment-845c8977d9-qk68n" satisfied condition "running"
Sep  3 21:32:29.448: INFO: Pod "webserver-deployment-845c8977d9-6twtr": Phase="Running", Reason="", readiness=true. Elapsed: 2.105857564s
Sep  3 21:32:29.449: INFO: Pod "webserver-deployment-845c8977d9-6twtr" satisfied condition "running"
Sep  3 21:32:29.449: INFO: Pod "webserver-deployment-845c8977d9-fknpn": Phase="Running", Reason="", readiness=true. Elapsed: 2.106312665s
Sep  3 21:32:29.449: INFO: Pod "webserver-deployment-845c8977d9-fknpn" satisfied condition "running"
Sep  3 21:32:29.450: INFO: Pod "webserver-deployment-845c8977d9-jn87p": Phase="Running", Reason="", readiness=true. Elapsed: 2.106626666s
Sep  3 21:32:29.450: INFO: Pod "webserver-deployment-845c8977d9-jn87p" satisfied condition "running"
Sep  3 21:32:29.450: INFO: Pod "webserver-deployment-845c8977d9-4h2bg": Phase="Running", Reason="", readiness=true. Elapsed: 2.107670968s
Sep  3 21:32:29.450: INFO: Pod "webserver-deployment-845c8977d9-4h2bg" satisfied condition "running"
Sep  3 21:32:29.451: INFO: Pod "webserver-deployment-845c8977d9-6s4bs": Phase="Running", Reason="", readiness=true. Elapsed: 2.108043069s
Sep  3 21:32:29.451: INFO: Pod "webserver-deployment-845c8977d9-6s4bs" satisfied condition "running"
Sep  3 21:32:29.451: INFO: Waiting for deployment "webserver-deployment" to complete
Sep  3 21:32:29.465: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep  3 21:32:29.476: INFO: Updating deployment webserver-deployment
Sep  3 21:32:29.477: INFO: Waiting for observed generation 2
Sep  3 21:32:31.490: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  3 21:32:31.494: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  3 21:32:31.498: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  3 21:32:31.512: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  3 21:32:31.512: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  3 21:32:31.514: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  3 21:32:31.534: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep  3 21:32:31.534: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep  3 21:32:31.556: INFO: Updating deployment webserver-deployment
Sep  3 21:32:31.556: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep  3 21:32:31.565: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  3 21:32:33.587: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  3 21:32:33.601: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2123  e097a711-3131-463a-9780-694506a84e4f 14215 3 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038be828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-09-03 21:32:31 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2022-09-03 21:32:31 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep  3 21:32:33.607: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-2123  d0e95f30-f340-4527-b281-842cdde65730 14212 3 2022-09-03 21:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e097a711-3131-463a-9780-694506a84e4f 0xc0038bec57 0xc0038bec58}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e097a711-3131-463a-9780-694506a84e4f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038becf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:32:33.613: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep  3 21:32:33.613: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-2123  1c1a1375-9060-4dd7-9203-21d3a0f527c6 14210 3 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e097a711-3131-463a-9780-694506a84e4f 0xc0038bed57 0xc0038bed58}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e097a711-3131-463a-9780-694506a84e4f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038bede8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:32:33.620: INFO: Pod "webserver-deployment-69b7448995-dnfp7" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-dnfp7 webserver-deployment-69b7448995- deployment-2123  fda0cfee-34db-4c63-b137-78557d54ab99 14190 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bf307 0xc0038bf308}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjpkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjpkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.620: INFO: Pod "webserver-deployment-69b7448995-gvfvh" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-gvfvh webserver-deployment-69b7448995- deployment-2123  ddad1ce6-21c7-4b97-b4f5-3d14af653feb 14167 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bf490 0xc0038bf491}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-66f98,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-66f98,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:,StartTime:2022-09-03 21:32:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.621: INFO: Pod "webserver-deployment-69b7448995-jvsg8" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-jvsg8 webserver-deployment-69b7448995- deployment-2123  1830ca8d-fe6c-415e-9b32-61339f887f62 14102 0 2022-09-03 21:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bf670 0xc0038bf671}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbvq2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbvq2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:,StartTime:2022-09-03 21:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.621: INFO: Pod "webserver-deployment-69b7448995-lv5ph" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-lv5ph webserver-deployment-69b7448995- deployment-2123  b7fa1f38-61a7-40ba-9b26-34a87436a46c 14135 0 2022-09-03 21:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bf860 0xc0038bf861}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jg4s5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jg4s5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.191,StartTime:2022-09-03 21:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.621: INFO: Pod "webserver-deployment-69b7448995-mnscw" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-mnscw webserver-deployment-69b7448995- deployment-2123  8f2b7fd7-11d6-4afb-89d1-767a7f6b7074 14207 0 2022-09-03 21:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bfa70 0xc0038bfa71}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jmpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jmpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:10.244.2.54,StartTime:2022-09-03 21:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.626: INFO: Pod "webserver-deployment-69b7448995-pxcmv" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-pxcmv webserver-deployment-69b7448995- deployment-2123  ec01a927-841e-448f-86d5-c2ffde52ef26 14199 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bfc80 0xc0038bfc81}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7bmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7bmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.628: INFO: Pod "webserver-deployment-69b7448995-q79nn" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-q79nn webserver-deployment-69b7448995- deployment-2123  5a10e59f-c1b3-4f02-bcaa-4123a35503c6 14107 0 2022-09-03 21:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bfdf0 0xc0038bfdf1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-88b2v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-88b2v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:,StartTime:2022-09-03 21:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.628: INFO: Pod "webserver-deployment-69b7448995-r7vgz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-r7vgz webserver-deployment-69b7448995- deployment-2123  5441b586-2158-448f-9538-0f111fecfe9f 14168 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bffd0 0xc0038bffd1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ff7lp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ff7lp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.629: INFO: Pod "webserver-deployment-69b7448995-rlqpr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-rlqpr webserver-deployment-69b7448995- deployment-2123  3282381f-dadb-4bf9-9673-24b4dcfe359e 14202 0 2022-09-03 21:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc003a2e150 0xc003a2e151}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z76vt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z76vt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:10.244.2.55,StartTime:2022-09-03 21:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.629: INFO: Pod "webserver-deployment-69b7448995-s2cgs" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-s2cgs webserver-deployment-69b7448995- deployment-2123  34b3e641-ceac-4205-98aa-a49a05df8da6 14204 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc003a2e360 0xc003a2e361}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4xm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4xm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.629: INFO: Pod "webserver-deployment-69b7448995-wtpkp" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-wtpkp webserver-deployment-69b7448995- deployment-2123  9ca3f9b4-448b-483c-a254-a359a45a4fc6 14164 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc003a2e4d0 0xc003a2e4d1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4jqj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4jqj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.639: INFO: Pod "webserver-deployment-69b7448995-xjnxd" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-xjnxd webserver-deployment-69b7448995- deployment-2123  de0679a6-e990-4dc7-a149-7ad4ec854403 14194 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc003a2e640 0xc003a2e641}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5fgdw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5fgdw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.639: INFO: Pod "webserver-deployment-69b7448995-zq7n6" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zq7n6 webserver-deployment-69b7448995- deployment-2123  641e203f-4913-49cc-9572-bbecc1f930b5 14191 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc003a2e7b0 0xc003a2e7b1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pb5fv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pb5fv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.639: INFO: Pod "webserver-deployment-845c8977d9-2thn4" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-2thn4 webserver-deployment-845c8977d9- deployment-2123  23aa6ac7-a5da-41f2-bdde-c57f38fffc15 14201 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2e920 0xc003a2e921}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wc2fm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wc2fm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.640: INFO: Pod "webserver-deployment-845c8977d9-4bp4g" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4bp4g webserver-deployment-845c8977d9- deployment-2123  4cb36f93-fdca-4cfe-af12-7504c1635658 14046 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2ea80 0xc003a2ea81}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h27tc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h27tc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.190,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e5ca98be78bfe1fb3bf6185bade1ab51c06cd7845812ed429cfe853472e12e58,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.640: INFO: Pod "webserver-deployment-845c8977d9-4h2bg" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4h2bg webserver-deployment-845c8977d9- deployment-2123  7da2ffe8-5aa2-430c-ad18-ce38fa554d9d 14043 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2ec60 0xc003a2ec61}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6fzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6fzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.189,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f3198634ff1425af97f9523f9815d8a8ec10fa8d0e69710102ebc30b631cdb49,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.641: INFO: Pod "webserver-deployment-845c8977d9-5mprt" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5mprt webserver-deployment-845c8977d9- deployment-2123  11cc547b-34b2-413e-9814-f23cb9066a9b 14171 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2ee40 0xc003a2ee41}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g58f9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g58f9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.641: INFO: Pod "webserver-deployment-845c8977d9-6s4bs" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6s4bs webserver-deployment-845c8977d9- deployment-2123  9db56e92-15e2-40a8-bc6e-febde2184494 14021 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2efa0 0xc003a2efa1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fsvzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fsvzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:10.244.2.50,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6b6bdb47097adf80fa05b3091b4450ec11726927eda8e071c9e2c6ea9ae42925,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.642: INFO: Pod "webserver-deployment-845c8977d9-6twtr" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6twtr webserver-deployment-845c8977d9- deployment-2123  e1c5fcab-f52c-4ce8-b002-46ff829f810a 14053 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2f180 0xc003a2f181}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-txp8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-txp8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:10.244.2.51,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://23da9f71118530295b05a4e4d1f20e88b5b24266978bcc1087205ede69ca7abd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.642: INFO: Pod "webserver-deployment-845c8977d9-6wvcw" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6wvcw webserver-deployment-845c8977d9- deployment-2123  14a51645-2a76-4cf1-a3ad-05257451bd3e 14017 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2f360 0xc003a2f361}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhfvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhfvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.187,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://67b278cbae658fda82756624f6a9b7b21b83e251fd89a7449151032d4e53bbbb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.643: INFO: Pod "webserver-deployment-845c8977d9-7z6gl" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7z6gl webserver-deployment-845c8977d9- deployment-2123  5fa88422-b4f0-47ee-aa95-294b560c9d8e 14214 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2f540 0xc003a2f541}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vc62d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vc62d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:,StartTime:2022-09-03 21:32:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.646: INFO: Pod "webserver-deployment-845c8977d9-966lq" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-966lq webserver-deployment-845c8977d9- deployment-2123  d2f067cc-7e6b-48f3-8a58-52a5a70cb3c5 14182 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2f700 0xc003a2f701}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwk84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwk84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.646: INFO: Pod "webserver-deployment-845c8977d9-ccnkg" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ccnkg webserver-deployment-845c8977d9- deployment-2123  bef82ea9-ce43-4ca9-9fbe-d602b1dafe6d 14184 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2f870 0xc003a2f871}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4w76h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4w76h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.647: INFO: Pod "webserver-deployment-845c8977d9-fknpn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-fknpn webserver-deployment-845c8977d9- deployment-2123  b9272673-c607-4b09-b538-f2f56771abec 14023 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2f9d0 0xc003a2f9d1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qdpcz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qdpcz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:10.244.2.49,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://71cc6baa29ef7b67233aee07af94e6946381396f383f73825f5c98988b1906ee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.647: INFO: Pod "webserver-deployment-845c8977d9-fmt6t" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-fmt6t webserver-deployment-845c8977d9- deployment-2123  0f6f04a4-3c90-4185-a686-04b2a59e302e 14186 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2fbc0 0xc003a2fbc1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2tq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2tq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.648: INFO: Pod "webserver-deployment-845c8977d9-gn9t5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gn9t5 webserver-deployment-845c8977d9- deployment-2123  1c0b3d8e-a7d6-495c-b14b-74c921e46726 14156 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2fd20 0xc003a2fd21}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9lmbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9lmbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.648: INFO: Pod "webserver-deployment-845c8977d9-h2td4" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-h2td4 webserver-deployment-845c8977d9- deployment-2123  6eb56894-3e6e-4b44-815f-f8874fae0142 14172 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2fe80 0xc003a2fe81}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kltrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kltrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.649: INFO: Pod "webserver-deployment-845c8977d9-hrznl" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hrznl webserver-deployment-845c8977d9- deployment-2123  3a9ab958-b422-47b4-95bc-04d4ff566553 14189 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2ffe0 0xc003a2ffe1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzxj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzxj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.661: INFO: Pod "webserver-deployment-845c8977d9-jn87p" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jn87p webserver-deployment-845c8977d9- deployment-2123  daba6180-a0ae-463b-aa9f-8fec052b18fe 14040 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003b08140 0xc003b08141}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w9mmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w9mmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.188,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b80ad0118fbac1a177593957c7969bf2ec2ad032f1e3899bb53868fec0af746f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.661: INFO: Pod "webserver-deployment-845c8977d9-kp9gp" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kp9gp webserver-deployment-845c8977d9- deployment-2123  444ca7bf-721e-48b8-84d8-945ad2badb9f 14197 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003b08330 0xc003b08331}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mzf25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mzf25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.661: INFO: Pod "webserver-deployment-845c8977d9-l2gcn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-l2gcn webserver-deployment-845c8977d9- deployment-2123  2bfa9293-7537-4661-99bc-6752b2653ab2 14200 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003b08490 0xc003b08491}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mfvzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mfvzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.661: INFO: Pod "webserver-deployment-845c8977d9-qlh9n" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qlh9n webserver-deployment-845c8977d9- deployment-2123  08598776-1b9f-4a5b-a5f8-29109ebe814d 14008 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003b085f0 0xc003b085f1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-scws6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-scws6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.186,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f1711f9ed526b28b1e2a93a5685b57207e57491b0260db3edf9b25da42efeb24,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 21:32:33.662: INFO: Pod "webserver-deployment-845c8977d9-rhgfq" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rhgfq webserver-deployment-845c8977d9- deployment-2123  1a5e9876-595f-4e59-b089-c6778ddf4888 14154 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003b087d0 0xc003b087d1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdh4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdh4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:,StartTime:2022-09-03 21:32:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Sep  3 21:32:33.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2123" for this suite. 09/03/22 21:32:33.677
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":193,"skipped":3469,"failed":0}
------------------------------
• [SLOW TEST] [8.421 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:32:25.264
    Sep  3 21:32:25.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename deployment 09/03/22 21:32:25.265
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:25.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:25.278
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Sep  3 21:32:25.283: INFO: Creating deployment "webserver-deployment"
    Sep  3 21:32:25.288: INFO: Waiting for observed generation 1
    Sep  3 21:32:27.315: INFO: Waiting for all required pods to come up
    Sep  3 21:32:27.342: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 09/03/22 21:32:27.342
    Sep  3 21:32:27.342: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4bp4g" in namespace "deployment-2123" to be "running"
    Sep  3 21:32:27.342: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4h2bg" in namespace "deployment-2123" to be "running"
    Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6s4bs" in namespace "deployment-2123" to be "running"
    Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6twtr" in namespace "deployment-2123" to be "running"
    Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6wvcw" in namespace "deployment-2123" to be "running"
    Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-fknpn" in namespace "deployment-2123" to be "running"
    Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jn87p" in namespace "deployment-2123" to be "running"
    Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mz6vh" in namespace "deployment-2123" to be "running"
    Sep  3 21:32:27.343: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qk68n" in namespace "deployment-2123" to be "running"
    Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-4bp4g": Phase="Pending", Reason="", readiness=false. Elapsed: 96.471323ms
    Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-qk68n": Phase="Pending", Reason="", readiness=false. Elapsed: 95.736422ms
    Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-6s4bs": Phase="Pending", Reason="", readiness=false. Elapsed: 96.387523ms
    Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-6wvcw": Phase="Pending", Reason="", readiness=false. Elapsed: 96.282422ms
    Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-fknpn": Phase="Pending", Reason="", readiness=false. Elapsed: 96.267122ms
    Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-jn87p": Phase="Pending", Reason="", readiness=false. Elapsed: 96.236322ms
    Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-6twtr": Phase="Pending", Reason="", readiness=false. Elapsed: 96.564823ms
    Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-4h2bg": Phase="Pending", Reason="", readiness=false. Elapsed: 96.515223ms
    Sep  3 21:32:27.439: INFO: Pod "webserver-deployment-845c8977d9-mz6vh": Phase="Pending", Reason="", readiness=false. Elapsed: 96.263923ms
    Sep  3 21:32:29.446: INFO: Pod "webserver-deployment-845c8977d9-6wvcw": Phase="Running", Reason="", readiness=true. Elapsed: 2.103386258s
    Sep  3 21:32:29.446: INFO: Pod "webserver-deployment-845c8977d9-6wvcw" satisfied condition "running"
    Sep  3 21:32:29.447: INFO: Pod "webserver-deployment-845c8977d9-mz6vh": Phase="Running", Reason="", readiness=true. Elapsed: 2.103783759s
    Sep  3 21:32:29.447: INFO: Pod "webserver-deployment-845c8977d9-mz6vh" satisfied condition "running"
    Sep  3 21:32:29.447: INFO: Pod "webserver-deployment-845c8977d9-4bp4g": Phase="Running", Reason="", readiness=true. Elapsed: 2.105353063s
    Sep  3 21:32:29.448: INFO: Pod "webserver-deployment-845c8977d9-4bp4g" satisfied condition "running"
    Sep  3 21:32:29.448: INFO: Pod "webserver-deployment-845c8977d9-qk68n": Phase="Running", Reason="", readiness=true. Elapsed: 2.104807862s
    Sep  3 21:32:29.448: INFO: Pod "webserver-deployment-845c8977d9-qk68n" satisfied condition "running"
    Sep  3 21:32:29.448: INFO: Pod "webserver-deployment-845c8977d9-6twtr": Phase="Running", Reason="", readiness=true. Elapsed: 2.105857564s
    Sep  3 21:32:29.449: INFO: Pod "webserver-deployment-845c8977d9-6twtr" satisfied condition "running"
    Sep  3 21:32:29.449: INFO: Pod "webserver-deployment-845c8977d9-fknpn": Phase="Running", Reason="", readiness=true. Elapsed: 2.106312665s
    Sep  3 21:32:29.449: INFO: Pod "webserver-deployment-845c8977d9-fknpn" satisfied condition "running"
    Sep  3 21:32:29.450: INFO: Pod "webserver-deployment-845c8977d9-jn87p": Phase="Running", Reason="", readiness=true. Elapsed: 2.106626666s
    Sep  3 21:32:29.450: INFO: Pod "webserver-deployment-845c8977d9-jn87p" satisfied condition "running"
    Sep  3 21:32:29.450: INFO: Pod "webserver-deployment-845c8977d9-4h2bg": Phase="Running", Reason="", readiness=true. Elapsed: 2.107670968s
    Sep  3 21:32:29.450: INFO: Pod "webserver-deployment-845c8977d9-4h2bg" satisfied condition "running"
    Sep  3 21:32:29.451: INFO: Pod "webserver-deployment-845c8977d9-6s4bs": Phase="Running", Reason="", readiness=true. Elapsed: 2.108043069s
    Sep  3 21:32:29.451: INFO: Pod "webserver-deployment-845c8977d9-6s4bs" satisfied condition "running"
    Sep  3 21:32:29.451: INFO: Waiting for deployment "webserver-deployment" to complete
    Sep  3 21:32:29.465: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Sep  3 21:32:29.476: INFO: Updating deployment webserver-deployment
    Sep  3 21:32:29.477: INFO: Waiting for observed generation 2
    Sep  3 21:32:31.490: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Sep  3 21:32:31.494: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Sep  3 21:32:31.498: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  3 21:32:31.512: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Sep  3 21:32:31.512: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Sep  3 21:32:31.514: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  3 21:32:31.534: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Sep  3 21:32:31.534: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Sep  3 21:32:31.556: INFO: Updating deployment webserver-deployment
    Sep  3 21:32:31.556: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Sep  3 21:32:31.565: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Sep  3 21:32:33.587: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  3 21:32:33.601: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-2123  e097a711-3131-463a-9780-694506a84e4f 14215 3 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038be828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-09-03 21:32:31 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2022-09-03 21:32:31 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Sep  3 21:32:33.607: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-2123  d0e95f30-f340-4527-b281-842cdde65730 14212 3 2022-09-03 21:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e097a711-3131-463a-9780-694506a84e4f 0xc0038bec57 0xc0038bec58}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e097a711-3131-463a-9780-694506a84e4f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038becf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:32:33.613: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Sep  3 21:32:33.613: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-2123  1c1a1375-9060-4dd7-9203-21d3a0f527c6 14210 3 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e097a711-3131-463a-9780-694506a84e4f 0xc0038bed57 0xc0038bed58}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e097a711-3131-463a-9780-694506a84e4f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038bede8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:32:33.620: INFO: Pod "webserver-deployment-69b7448995-dnfp7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-dnfp7 webserver-deployment-69b7448995- deployment-2123  fda0cfee-34db-4c63-b137-78557d54ab99 14190 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bf307 0xc0038bf308}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjpkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjpkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.620: INFO: Pod "webserver-deployment-69b7448995-gvfvh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-gvfvh webserver-deployment-69b7448995- deployment-2123  ddad1ce6-21c7-4b97-b4f5-3d14af653feb 14167 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bf490 0xc0038bf491}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-66f98,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-66f98,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:,StartTime:2022-09-03 21:32:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.621: INFO: Pod "webserver-deployment-69b7448995-jvsg8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-jvsg8 webserver-deployment-69b7448995- deployment-2123  1830ca8d-fe6c-415e-9b32-61339f887f62 14102 0 2022-09-03 21:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bf670 0xc0038bf671}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbvq2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbvq2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:,StartTime:2022-09-03 21:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.621: INFO: Pod "webserver-deployment-69b7448995-lv5ph" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-lv5ph webserver-deployment-69b7448995- deployment-2123  b7fa1f38-61a7-40ba-9b26-34a87436a46c 14135 0 2022-09-03 21:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bf860 0xc0038bf861}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jg4s5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jg4s5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.191,StartTime:2022-09-03 21:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.621: INFO: Pod "webserver-deployment-69b7448995-mnscw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-mnscw webserver-deployment-69b7448995- deployment-2123  8f2b7fd7-11d6-4afb-89d1-767a7f6b7074 14207 0 2022-09-03 21:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bfa70 0xc0038bfa71}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jmpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jmpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:10.244.2.54,StartTime:2022-09-03 21:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.626: INFO: Pod "webserver-deployment-69b7448995-pxcmv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-pxcmv webserver-deployment-69b7448995- deployment-2123  ec01a927-841e-448f-86d5-c2ffde52ef26 14199 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bfc80 0xc0038bfc81}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7bmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7bmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.628: INFO: Pod "webserver-deployment-69b7448995-q79nn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-q79nn webserver-deployment-69b7448995- deployment-2123  5a10e59f-c1b3-4f02-bcaa-4123a35503c6 14107 0 2022-09-03 21:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bfdf0 0xc0038bfdf1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-88b2v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-88b2v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:,StartTime:2022-09-03 21:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.628: INFO: Pod "webserver-deployment-69b7448995-r7vgz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-r7vgz webserver-deployment-69b7448995- deployment-2123  5441b586-2158-448f-9538-0f111fecfe9f 14168 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc0038bffd0 0xc0038bffd1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ff7lp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ff7lp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.629: INFO: Pod "webserver-deployment-69b7448995-rlqpr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-rlqpr webserver-deployment-69b7448995- deployment-2123  3282381f-dadb-4bf9-9673-24b4dcfe359e 14202 0 2022-09-03 21:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc003a2e150 0xc003a2e151}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z76vt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z76vt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:10.244.2.55,StartTime:2022-09-03 21:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.629: INFO: Pod "webserver-deployment-69b7448995-s2cgs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-s2cgs webserver-deployment-69b7448995- deployment-2123  34b3e641-ceac-4205-98aa-a49a05df8da6 14204 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc003a2e360 0xc003a2e361}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4xm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4xm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.629: INFO: Pod "webserver-deployment-69b7448995-wtpkp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-wtpkp webserver-deployment-69b7448995- deployment-2123  9ca3f9b4-448b-483c-a254-a359a45a4fc6 14164 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc003a2e4d0 0xc003a2e4d1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4jqj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4jqj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.639: INFO: Pod "webserver-deployment-69b7448995-xjnxd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-xjnxd webserver-deployment-69b7448995- deployment-2123  de0679a6-e990-4dc7-a149-7ad4ec854403 14194 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc003a2e640 0xc003a2e641}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5fgdw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5fgdw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.639: INFO: Pod "webserver-deployment-69b7448995-zq7n6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zq7n6 webserver-deployment-69b7448995- deployment-2123  641e203f-4913-49cc-9572-bbecc1f930b5 14191 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 d0e95f30-f340-4527-b281-842cdde65730 0xc003a2e7b0 0xc003a2e7b1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0e95f30-f340-4527-b281-842cdde65730\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pb5fv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pb5fv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.639: INFO: Pod "webserver-deployment-845c8977d9-2thn4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-2thn4 webserver-deployment-845c8977d9- deployment-2123  23aa6ac7-a5da-41f2-bdde-c57f38fffc15 14201 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2e920 0xc003a2e921}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wc2fm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wc2fm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.640: INFO: Pod "webserver-deployment-845c8977d9-4bp4g" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4bp4g webserver-deployment-845c8977d9- deployment-2123  4cb36f93-fdca-4cfe-af12-7504c1635658 14046 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2ea80 0xc003a2ea81}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h27tc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h27tc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.190,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e5ca98be78bfe1fb3bf6185bade1ab51c06cd7845812ed429cfe853472e12e58,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.640: INFO: Pod "webserver-deployment-845c8977d9-4h2bg" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4h2bg webserver-deployment-845c8977d9- deployment-2123  7da2ffe8-5aa2-430c-ad18-ce38fa554d9d 14043 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2ec60 0xc003a2ec61}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6fzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6fzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.189,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f3198634ff1425af97f9523f9815d8a8ec10fa8d0e69710102ebc30b631cdb49,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.641: INFO: Pod "webserver-deployment-845c8977d9-5mprt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5mprt webserver-deployment-845c8977d9- deployment-2123  11cc547b-34b2-413e-9814-f23cb9066a9b 14171 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2ee40 0xc003a2ee41}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g58f9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g58f9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.641: INFO: Pod "webserver-deployment-845c8977d9-6s4bs" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6s4bs webserver-deployment-845c8977d9- deployment-2123  9db56e92-15e2-40a8-bc6e-febde2184494 14021 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2efa0 0xc003a2efa1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fsvzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fsvzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:10.244.2.50,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6b6bdb47097adf80fa05b3091b4450ec11726927eda8e071c9e2c6ea9ae42925,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.642: INFO: Pod "webserver-deployment-845c8977d9-6twtr" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6twtr webserver-deployment-845c8977d9- deployment-2123  e1c5fcab-f52c-4ce8-b002-46ff829f810a 14053 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2f180 0xc003a2f181}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-txp8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-txp8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:10.244.2.51,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://23da9f71118530295b05a4e4d1f20e88b5b24266978bcc1087205ede69ca7abd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.642: INFO: Pod "webserver-deployment-845c8977d9-6wvcw" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6wvcw webserver-deployment-845c8977d9- deployment-2123  14a51645-2a76-4cf1-a3ad-05257451bd3e 14017 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2f360 0xc003a2f361}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhfvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhfvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.187,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://67b278cbae658fda82756624f6a9b7b21b83e251fd89a7449151032d4e53bbbb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.643: INFO: Pod "webserver-deployment-845c8977d9-7z6gl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7z6gl webserver-deployment-845c8977d9- deployment-2123  5fa88422-b4f0-47ee-aa95-294b560c9d8e 14214 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2f540 0xc003a2f541}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vc62d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vc62d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:,StartTime:2022-09-03 21:32:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.646: INFO: Pod "webserver-deployment-845c8977d9-966lq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-966lq webserver-deployment-845c8977d9- deployment-2123  d2f067cc-7e6b-48f3-8a58-52a5a70cb3c5 14182 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2f700 0xc003a2f701}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwk84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwk84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.646: INFO: Pod "webserver-deployment-845c8977d9-ccnkg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ccnkg webserver-deployment-845c8977d9- deployment-2123  bef82ea9-ce43-4ca9-9fbe-d602b1dafe6d 14184 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2f870 0xc003a2f871}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4w76h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4w76h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.647: INFO: Pod "webserver-deployment-845c8977d9-fknpn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-fknpn webserver-deployment-845c8977d9- deployment-2123  b9272673-c607-4b09-b538-f2f56771abec 14023 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2f9d0 0xc003a2f9d1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qdpcz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qdpcz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:10.244.2.49,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://71cc6baa29ef7b67233aee07af94e6946381396f383f73825f5c98988b1906ee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.647: INFO: Pod "webserver-deployment-845c8977d9-fmt6t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-fmt6t webserver-deployment-845c8977d9- deployment-2123  0f6f04a4-3c90-4185-a686-04b2a59e302e 14186 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2fbc0 0xc003a2fbc1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2tq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2tq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.648: INFO: Pod "webserver-deployment-845c8977d9-gn9t5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gn9t5 webserver-deployment-845c8977d9- deployment-2123  1c0b3d8e-a7d6-495c-b14b-74c921e46726 14156 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2fd20 0xc003a2fd21}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9lmbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9lmbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.648: INFO: Pod "webserver-deployment-845c8977d9-h2td4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-h2td4 webserver-deployment-845c8977d9- deployment-2123  6eb56894-3e6e-4b44-815f-f8874fae0142 14172 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2fe80 0xc003a2fe81}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kltrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kltrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.649: INFO: Pod "webserver-deployment-845c8977d9-hrznl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hrznl webserver-deployment-845c8977d9- deployment-2123  3a9ab958-b422-47b4-95bc-04d4ff566553 14189 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003a2ffe0 0xc003a2ffe1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzxj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzxj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.661: INFO: Pod "webserver-deployment-845c8977d9-jn87p" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jn87p webserver-deployment-845c8977d9- deployment-2123  daba6180-a0ae-463b-aa9f-8fec052b18fe 14040 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003b08140 0xc003b08141}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w9mmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w9mmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.188,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b80ad0118fbac1a177593957c7969bf2ec2ad032f1e3899bb53868fec0af746f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.661: INFO: Pod "webserver-deployment-845c8977d9-kp9gp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kp9gp webserver-deployment-845c8977d9- deployment-2123  444ca7bf-721e-48b8-84d8-945ad2badb9f 14197 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003b08330 0xc003b08331}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mzf25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mzf25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.661: INFO: Pod "webserver-deployment-845c8977d9-l2gcn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-l2gcn webserver-deployment-845c8977d9- deployment-2123  2bfa9293-7537-4661-99bc-6752b2653ab2 14200 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003b08490 0xc003b08491}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mfvzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mfvzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.661: INFO: Pod "webserver-deployment-845c8977d9-qlh9n" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qlh9n webserver-deployment-845c8977d9- deployment-2123  08598776-1b9f-4a5b-a5f8-29109ebe814d 14008 0 2022-09-03 21:32:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003b085f0 0xc003b085f1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-scws6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-scws6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.186,StartTime:2022-09-03 21:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:32:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f1711f9ed526b28b1e2a93a5685b57207e57491b0260db3edf9b25da42efeb24,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  3 21:32:33.662: INFO: Pod "webserver-deployment-845c8977d9-rhgfq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rhgfq webserver-deployment-845c8977d9- deployment-2123  1a5e9876-595f-4e59-b089-c6778ddf4888 14154 0 2022-09-03 21:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1c1a1375-9060-4dd7-9203-21d3a0f527c6 0xc003b087d0 0xc003b087d1}] [] [{kube-controller-manager Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c1a1375-9060-4dd7-9203-21d3a0f527c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdh4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdh4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:32:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:,StartTime:2022-09-03 21:32:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Sep  3 21:32:33.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2123" for this suite. 09/03/22 21:32:33.677
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:32:33.699
Sep  3 21:32:33.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 21:32:33.7
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:33.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:33.74
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-59620e21-c0b2-4d8e-9fbe-e22b71f6d84b 09/03/22 21:32:33.745
STEP: Creating a pod to test consume configMaps 09/03/22 21:32:33.749
Sep  3 21:32:33.792: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25" in namespace "configmap-3654" to be "Succeeded or Failed"
Sep  3 21:32:33.802: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25": Phase="Pending", Reason="", readiness=false. Elapsed: 10.191423ms
Sep  3 21:32:35.822: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030165587s
Sep  3 21:32:37.808: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016024371s
Sep  3 21:32:39.836: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044708654s
Sep  3 21:32:41.808: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016584905s
Sep  3 21:32:43.805: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.013482314s
STEP: Saw pod success 09/03/22 21:32:43.805
Sep  3 21:32:43.805: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25" satisfied condition "Succeeded or Failed"
Sep  3 21:32:43.807: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25 container agnhost-container: <nil>
STEP: delete the pod 09/03/22 21:32:43.812
Sep  3 21:32:43.819: INFO: Waiting for pod pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25 to disappear
Sep  3 21:32:43.822: INFO: Pod pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 21:32:43.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3654" for this suite. 09/03/22 21:32:43.83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":194,"skipped":3490,"failed":0}
------------------------------
• [SLOW TEST] [10.135 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:32:33.699
    Sep  3 21:32:33.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 21:32:33.7
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:33.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:33.74
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-59620e21-c0b2-4d8e-9fbe-e22b71f6d84b 09/03/22 21:32:33.745
    STEP: Creating a pod to test consume configMaps 09/03/22 21:32:33.749
    Sep  3 21:32:33.792: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25" in namespace "configmap-3654" to be "Succeeded or Failed"
    Sep  3 21:32:33.802: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25": Phase="Pending", Reason="", readiness=false. Elapsed: 10.191423ms
    Sep  3 21:32:35.822: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030165587s
    Sep  3 21:32:37.808: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016024371s
    Sep  3 21:32:39.836: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044708654s
    Sep  3 21:32:41.808: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016584905s
    Sep  3 21:32:43.805: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.013482314s
    STEP: Saw pod success 09/03/22 21:32:43.805
    Sep  3 21:32:43.805: INFO: Pod "pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25" satisfied condition "Succeeded or Failed"
    Sep  3 21:32:43.807: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25 container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 21:32:43.812
    Sep  3 21:32:43.819: INFO: Waiting for pod pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25 to disappear
    Sep  3 21:32:43.822: INFO: Pod pod-configmaps-6e3e3d18-cd64-456b-8728-7fbd9a2f5f25 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 21:32:43.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3654" for this suite. 09/03/22 21:32:43.83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:32:43.84
Sep  3 21:32:43.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename statefulset 09/03/22 21:32:43.841
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:43.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:43.859
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5868 09/03/22 21:32:43.864
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 09/03/22 21:32:43.872
STEP: Creating pod with conflicting port in namespace statefulset-5868 09/03/22 21:32:43.877
STEP: Waiting until pod test-pod will start running in namespace statefulset-5868 09/03/22 21:32:43.882
Sep  3 21:32:43.882: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5868" to be "running"
Sep  3 21:32:43.884: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015505ms
Sep  3 21:32:45.887: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004853726s
Sep  3 21:32:47.887: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00465584s
Sep  3 21:32:49.888: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006283958s
Sep  3 21:32:49.888: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-5868 09/03/22 21:32:49.889
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5868 09/03/22 21:32:49.892
Sep  3 21:32:49.906: INFO: Observed stateful pod in namespace: statefulset-5868, name: ss-0, uid: c4af4879-6a0f-4d21-ae33-92c2ac02c8cd, status phase: Pending. Waiting for statefulset controller to delete.
Sep  3 21:32:49.921: INFO: Observed stateful pod in namespace: statefulset-5868, name: ss-0, uid: c4af4879-6a0f-4d21-ae33-92c2ac02c8cd, status phase: Failed. Waiting for statefulset controller to delete.
Sep  3 21:32:49.937: INFO: Observed stateful pod in namespace: statefulset-5868, name: ss-0, uid: c4af4879-6a0f-4d21-ae33-92c2ac02c8cd, status phase: Failed. Waiting for statefulset controller to delete.
Sep  3 21:32:49.940: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5868
STEP: Removing pod with conflicting port in namespace statefulset-5868 09/03/22 21:32:49.94
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5868 and will be in running state 09/03/22 21:32:49.984
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Sep  3 21:32:52.024: INFO: Deleting all statefulset in ns statefulset-5868
Sep  3 21:32:52.029: INFO: Scaling statefulset ss to 0
Sep  3 21:33:02.060: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 21:33:02.065: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Sep  3 21:33:02.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5868" for this suite. 09/03/22 21:33:02.089
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":195,"skipped":3507,"failed":0}
------------------------------
• [SLOW TEST] [18.252 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:32:43.84
    Sep  3 21:32:43.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename statefulset 09/03/22 21:32:43.841
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:32:43.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:32:43.859
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5868 09/03/22 21:32:43.864
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 09/03/22 21:32:43.872
    STEP: Creating pod with conflicting port in namespace statefulset-5868 09/03/22 21:32:43.877
    STEP: Waiting until pod test-pod will start running in namespace statefulset-5868 09/03/22 21:32:43.882
    Sep  3 21:32:43.882: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5868" to be "running"
    Sep  3 21:32:43.884: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015505ms
    Sep  3 21:32:45.887: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004853726s
    Sep  3 21:32:47.887: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00465584s
    Sep  3 21:32:49.888: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006283958s
    Sep  3 21:32:49.888: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-5868 09/03/22 21:32:49.889
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5868 09/03/22 21:32:49.892
    Sep  3 21:32:49.906: INFO: Observed stateful pod in namespace: statefulset-5868, name: ss-0, uid: c4af4879-6a0f-4d21-ae33-92c2ac02c8cd, status phase: Pending. Waiting for statefulset controller to delete.
    Sep  3 21:32:49.921: INFO: Observed stateful pod in namespace: statefulset-5868, name: ss-0, uid: c4af4879-6a0f-4d21-ae33-92c2ac02c8cd, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  3 21:32:49.937: INFO: Observed stateful pod in namespace: statefulset-5868, name: ss-0, uid: c4af4879-6a0f-4d21-ae33-92c2ac02c8cd, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  3 21:32:49.940: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5868
    STEP: Removing pod with conflicting port in namespace statefulset-5868 09/03/22 21:32:49.94
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5868 and will be in running state 09/03/22 21:32:49.984
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Sep  3 21:32:52.024: INFO: Deleting all statefulset in ns statefulset-5868
    Sep  3 21:32:52.029: INFO: Scaling statefulset ss to 0
    Sep  3 21:33:02.060: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  3 21:33:02.065: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Sep  3 21:33:02.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5868" for this suite. 09/03/22 21:33:02.089
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:33:02.108
Sep  3 21:33:02.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename replication-controller 09/03/22 21:33:02.109
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:33:02.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:33:02.125
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 09/03/22 21:33:02.129
STEP: waiting for RC to be added 09/03/22 21:33:02.134
STEP: waiting for available Replicas 09/03/22 21:33:02.134
STEP: patching ReplicationController 09/03/22 21:33:03.142
STEP: waiting for RC to be modified 09/03/22 21:33:03.148
STEP: patching ReplicationController status 09/03/22 21:33:03.148
STEP: waiting for RC to be modified 09/03/22 21:33:03.152
STEP: waiting for available Replicas 09/03/22 21:33:03.152
STEP: fetching ReplicationController status 09/03/22 21:33:03.155
STEP: patching ReplicationController scale 09/03/22 21:33:03.158
STEP: waiting for RC to be modified 09/03/22 21:33:03.164
STEP: waiting for ReplicationController's scale to be the max amount 09/03/22 21:33:03.165
STEP: fetching ReplicationController; ensuring that it's patched 09/03/22 21:33:03.889
STEP: updating ReplicationController status 09/03/22 21:33:03.891
STEP: waiting for RC to be modified 09/03/22 21:33:03.895
STEP: listing all ReplicationControllers 09/03/22 21:33:03.896
STEP: checking that ReplicationController has expected values 09/03/22 21:33:03.899
STEP: deleting ReplicationControllers by collection 09/03/22 21:33:03.9
STEP: waiting for ReplicationController to have a DELETED watchEvent 09/03/22 21:33:03.905
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Sep  3 21:33:03.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8599" for this suite. 09/03/22 21:33:03.926
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":196,"skipped":3577,"failed":0}
------------------------------
• [1.824 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:33:02.108
    Sep  3 21:33:02.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename replication-controller 09/03/22 21:33:02.109
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:33:02.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:33:02.125
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 09/03/22 21:33:02.129
    STEP: waiting for RC to be added 09/03/22 21:33:02.134
    STEP: waiting for available Replicas 09/03/22 21:33:02.134
    STEP: patching ReplicationController 09/03/22 21:33:03.142
    STEP: waiting for RC to be modified 09/03/22 21:33:03.148
    STEP: patching ReplicationController status 09/03/22 21:33:03.148
    STEP: waiting for RC to be modified 09/03/22 21:33:03.152
    STEP: waiting for available Replicas 09/03/22 21:33:03.152
    STEP: fetching ReplicationController status 09/03/22 21:33:03.155
    STEP: patching ReplicationController scale 09/03/22 21:33:03.158
    STEP: waiting for RC to be modified 09/03/22 21:33:03.164
    STEP: waiting for ReplicationController's scale to be the max amount 09/03/22 21:33:03.165
    STEP: fetching ReplicationController; ensuring that it's patched 09/03/22 21:33:03.889
    STEP: updating ReplicationController status 09/03/22 21:33:03.891
    STEP: waiting for RC to be modified 09/03/22 21:33:03.895
    STEP: listing all ReplicationControllers 09/03/22 21:33:03.896
    STEP: checking that ReplicationController has expected values 09/03/22 21:33:03.899
    STEP: deleting ReplicationControllers by collection 09/03/22 21:33:03.9
    STEP: waiting for ReplicationController to have a DELETED watchEvent 09/03/22 21:33:03.905
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Sep  3 21:33:03.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8599" for this suite. 09/03/22 21:33:03.926
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:33:03.934
Sep  3 21:33:03.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename cronjob 09/03/22 21:33:03.935
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:33:03.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:33:03.96
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 09/03/22 21:33:03.962
STEP: Ensuring no jobs are scheduled 09/03/22 21:33:03.965
STEP: Ensuring no job exists by listing jobs explicitly 09/03/22 21:38:03.976
STEP: Removing cronjob 09/03/22 21:38:03.979
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Sep  3 21:38:03.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4023" for this suite. 09/03/22 21:38:03.985
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":197,"skipped":3587,"failed":0}
------------------------------
• [SLOW TEST] [300.055 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:33:03.934
    Sep  3 21:33:03.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename cronjob 09/03/22 21:33:03.935
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:33:03.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:33:03.96
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 09/03/22 21:33:03.962
    STEP: Ensuring no jobs are scheduled 09/03/22 21:33:03.965
    STEP: Ensuring no job exists by listing jobs explicitly 09/03/22 21:38:03.976
    STEP: Removing cronjob 09/03/22 21:38:03.979
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Sep  3 21:38:03.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4023" for this suite. 09/03/22 21:38:03.985
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:38:03.992
Sep  3 21:38:03.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename runtimeclass 09/03/22 21:38:03.994
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:38:04.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:38:04.006
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 09/03/22 21:38:04.008
STEP: getting /apis/node.k8s.io 09/03/22 21:38:04.012
STEP: getting /apis/node.k8s.io/v1 09/03/22 21:38:04.013
STEP: creating 09/03/22 21:38:04.014
STEP: watching 09/03/22 21:38:04.023
Sep  3 21:38:04.024: INFO: starting watch
STEP: getting 09/03/22 21:38:04.029
STEP: listing 09/03/22 21:38:04.032
STEP: patching 09/03/22 21:38:04.035
STEP: updating 09/03/22 21:38:04.038
Sep  3 21:38:04.041: INFO: waiting for watch events with expected annotations
STEP: deleting 09/03/22 21:38:04.042
STEP: deleting a collection 09/03/22 21:38:04.048
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Sep  3 21:38:04.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4613" for this suite. 09/03/22 21:38:04.059
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":198,"skipped":3610,"failed":0}
------------------------------
• [0.071 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:38:03.992
    Sep  3 21:38:03.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename runtimeclass 09/03/22 21:38:03.994
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:38:04.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:38:04.006
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 09/03/22 21:38:04.008
    STEP: getting /apis/node.k8s.io 09/03/22 21:38:04.012
    STEP: getting /apis/node.k8s.io/v1 09/03/22 21:38:04.013
    STEP: creating 09/03/22 21:38:04.014
    STEP: watching 09/03/22 21:38:04.023
    Sep  3 21:38:04.024: INFO: starting watch
    STEP: getting 09/03/22 21:38:04.029
    STEP: listing 09/03/22 21:38:04.032
    STEP: patching 09/03/22 21:38:04.035
    STEP: updating 09/03/22 21:38:04.038
    Sep  3 21:38:04.041: INFO: waiting for watch events with expected annotations
    STEP: deleting 09/03/22 21:38:04.042
    STEP: deleting a collection 09/03/22 21:38:04.048
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Sep  3 21:38:04.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4613" for this suite. 09/03/22 21:38:04.059
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:38:04.065
Sep  3 21:38:04.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 21:38:04.066
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:38:04.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:38:04.078
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 21:38:04.086
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:38:04.919
STEP: Deploying the webhook pod 09/03/22 21:38:04.925
STEP: Wait for the deployment to be ready 09/03/22 21:38:04.931
Sep  3 21:38:04.939: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 21:38:06.945
STEP: Verifying the service has paired with the endpoint 09/03/22 21:38:06.963
Sep  3 21:38:07.966: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 09/03/22 21:38:07.968
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/03/22 21:38:07.969
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/03/22 21:38:07.969
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/03/22 21:38:07.97
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/03/22 21:38:07.971
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/03/22 21:38:07.972
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/03/22 21:38:07.973
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:38:07.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5100" for this suite. 09/03/22 21:38:07.976
STEP: Destroying namespace "webhook-5100-markers" for this suite. 09/03/22 21:38:07.979
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":199,"skipped":3625,"failed":0}
------------------------------
• [4.015 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:38:04.065
    Sep  3 21:38:04.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 21:38:04.066
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:38:04.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:38:04.078
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 21:38:04.086
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:38:04.919
    STEP: Deploying the webhook pod 09/03/22 21:38:04.925
    STEP: Wait for the deployment to be ready 09/03/22 21:38:04.931
    Sep  3 21:38:04.939: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 21:38:06.945
    STEP: Verifying the service has paired with the endpoint 09/03/22 21:38:06.963
    Sep  3 21:38:07.966: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 09/03/22 21:38:07.968
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/03/22 21:38:07.969
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/03/22 21:38:07.969
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/03/22 21:38:07.97
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/03/22 21:38:07.971
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/03/22 21:38:07.972
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/03/22 21:38:07.973
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:38:07.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5100" for this suite. 09/03/22 21:38:07.976
    STEP: Destroying namespace "webhook-5100-markers" for this suite. 09/03/22 21:38:07.979
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:38:08.098
Sep  3 21:38:08.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename replicaset 09/03/22 21:38:08.099
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:38:08.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:38:08.119
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 09/03/22 21:38:08.123
STEP: Verify that the required pods have come up 09/03/22 21:38:08.132
Sep  3 21:38:08.142: INFO: Pod name sample-pod: Found 0 pods out of 3
Sep  3 21:38:13.146: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 09/03/22 21:38:13.146
Sep  3 21:38:13.149: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 09/03/22 21:38:13.149
STEP: DeleteCollection of the ReplicaSets 09/03/22 21:38:13.151
STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/03/22 21:38:13.157
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Sep  3 21:38:13.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1903" for this suite. 09/03/22 21:38:13.165
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":200,"skipped":3645,"failed":0}
------------------------------
• [SLOW TEST] [5.074 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:38:08.098
    Sep  3 21:38:08.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename replicaset 09/03/22 21:38:08.099
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:38:08.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:38:08.119
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 09/03/22 21:38:08.123
    STEP: Verify that the required pods have come up 09/03/22 21:38:08.132
    Sep  3 21:38:08.142: INFO: Pod name sample-pod: Found 0 pods out of 3
    Sep  3 21:38:13.146: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 09/03/22 21:38:13.146
    Sep  3 21:38:13.149: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 09/03/22 21:38:13.149
    STEP: DeleteCollection of the ReplicaSets 09/03/22 21:38:13.151
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/03/22 21:38:13.157
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Sep  3 21:38:13.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1903" for this suite. 09/03/22 21:38:13.165
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:38:13.19
Sep  3 21:38:13.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-runtime 09/03/22 21:38:13.191
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:38:13.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:38:13.46
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 09/03/22 21:38:13.467
STEP: wait for the container to reach Succeeded 09/03/22 21:38:13.509
STEP: get the container status 09/03/22 21:38:17.529
STEP: the container should be terminated 09/03/22 21:38:17.531
STEP: the termination message should be set 09/03/22 21:38:17.531
Sep  3 21:38:17.532: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 09/03/22 21:38:17.532
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Sep  3 21:38:17.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1091" for this suite. 09/03/22 21:38:17.548
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":201,"skipped":3683,"failed":0}
------------------------------
• [4.362 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:38:13.19
    Sep  3 21:38:13.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-runtime 09/03/22 21:38:13.191
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:38:13.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:38:13.46
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 09/03/22 21:38:13.467
    STEP: wait for the container to reach Succeeded 09/03/22 21:38:13.509
    STEP: get the container status 09/03/22 21:38:17.529
    STEP: the container should be terminated 09/03/22 21:38:17.531
    STEP: the termination message should be set 09/03/22 21:38:17.531
    Sep  3 21:38:17.532: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 09/03/22 21:38:17.532
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Sep  3 21:38:17.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1091" for this suite. 09/03/22 21:38:17.548
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:38:17.554
Sep  3 21:38:17.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename cronjob 09/03/22 21:38:17.555
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:38:17.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:38:17.565
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 09/03/22 21:38:17.568
STEP: Ensuring a job is scheduled 09/03/22 21:38:17.571
STEP: Ensuring exactly one is scheduled 09/03/22 21:39:01.574
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/03/22 21:39:01.576
STEP: Ensuring the job is replaced with a new one 09/03/22 21:39:01.577
STEP: Removing cronjob 09/03/22 21:40:01.581
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Sep  3 21:40:01.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9600" for this suite. 09/03/22 21:40:01.587
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":202,"skipped":3710,"failed":0}
------------------------------
• [SLOW TEST] [104.039 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:38:17.554
    Sep  3 21:38:17.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename cronjob 09/03/22 21:38:17.555
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:38:17.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:38:17.565
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 09/03/22 21:38:17.568
    STEP: Ensuring a job is scheduled 09/03/22 21:38:17.571
    STEP: Ensuring exactly one is scheduled 09/03/22 21:39:01.574
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/03/22 21:39:01.576
    STEP: Ensuring the job is replaced with a new one 09/03/22 21:39:01.577
    STEP: Removing cronjob 09/03/22 21:40:01.581
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Sep  3 21:40:01.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9600" for this suite. 09/03/22 21:40:01.587
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:40:01.595
Sep  3 21:40:01.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename limitrange 09/03/22 21:40:01.596
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:40:01.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:40:01.625
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 09/03/22 21:40:01.629
STEP: Setting up watch 09/03/22 21:40:01.629
STEP: Submitting a LimitRange 09/03/22 21:40:01.733
STEP: Verifying LimitRange creation was observed 09/03/22 21:40:01.738
STEP: Fetching the LimitRange to ensure it has proper values 09/03/22 21:40:01.738
Sep  3 21:40:01.740: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  3 21:40:01.740: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 09/03/22 21:40:01.74
STEP: Ensuring Pod has resource requirements applied from LimitRange 09/03/22 21:40:01.743
Sep  3 21:40:01.745: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  3 21:40:01.745: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 09/03/22 21:40:01.745
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/03/22 21:40:01.751
Sep  3 21:40:01.753: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep  3 21:40:01.753: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 09/03/22 21:40:01.753
STEP: Failing to create a Pod with more than max resources 09/03/22 21:40:01.755
STEP: Updating a LimitRange 09/03/22 21:40:01.756
STEP: Verifying LimitRange updating is effective 09/03/22 21:40:01.761
STEP: Creating a Pod with less than former min resources 09/03/22 21:40:03.766
STEP: Failing to create a Pod with more than max resources 09/03/22 21:40:03.769
STEP: Deleting a LimitRange 09/03/22 21:40:03.771
STEP: Verifying the LimitRange was deleted 09/03/22 21:40:03.779
Sep  3 21:40:08.782: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 09/03/22 21:40:08.782
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Sep  3 21:40:08.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4688" for this suite. 09/03/22 21:40:08.794
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":203,"skipped":3733,"failed":0}
------------------------------
• [SLOW TEST] [7.203 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:40:01.595
    Sep  3 21:40:01.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename limitrange 09/03/22 21:40:01.596
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:40:01.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:40:01.625
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 09/03/22 21:40:01.629
    STEP: Setting up watch 09/03/22 21:40:01.629
    STEP: Submitting a LimitRange 09/03/22 21:40:01.733
    STEP: Verifying LimitRange creation was observed 09/03/22 21:40:01.738
    STEP: Fetching the LimitRange to ensure it has proper values 09/03/22 21:40:01.738
    Sep  3 21:40:01.740: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  3 21:40:01.740: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 09/03/22 21:40:01.74
    STEP: Ensuring Pod has resource requirements applied from LimitRange 09/03/22 21:40:01.743
    Sep  3 21:40:01.745: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  3 21:40:01.745: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 09/03/22 21:40:01.745
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/03/22 21:40:01.751
    Sep  3 21:40:01.753: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Sep  3 21:40:01.753: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 09/03/22 21:40:01.753
    STEP: Failing to create a Pod with more than max resources 09/03/22 21:40:01.755
    STEP: Updating a LimitRange 09/03/22 21:40:01.756
    STEP: Verifying LimitRange updating is effective 09/03/22 21:40:01.761
    STEP: Creating a Pod with less than former min resources 09/03/22 21:40:03.766
    STEP: Failing to create a Pod with more than max resources 09/03/22 21:40:03.769
    STEP: Deleting a LimitRange 09/03/22 21:40:03.771
    STEP: Verifying the LimitRange was deleted 09/03/22 21:40:03.779
    Sep  3 21:40:08.782: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 09/03/22 21:40:08.782
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Sep  3 21:40:08.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-4688" for this suite. 09/03/22 21:40:08.794
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:40:08.8
Sep  3 21:40:08.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename deployment 09/03/22 21:40:08.801
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:40:08.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:40:08.812
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Sep  3 21:40:08.814: INFO: Creating deployment "test-recreate-deployment"
Sep  3 21:40:08.817: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  3 21:40:08.821: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  3 21:40:10.827: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  3 21:40:10.829: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  3 21:40:10.834: INFO: Updating deployment test-recreate-deployment
Sep  3 21:40:10.834: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  3 21:40:10.906: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7068  691e2d7e-02f5-409b-bb69-d236e9dac37c 15695 2 2022-09-03 21:40:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d3f948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-09-03 21:40:10 +0000 UTC,LastTransitionTime:2022-09-03 21:40:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2022-09-03 21:40:10 +0000 UTC,LastTransitionTime:2022-09-03 21:40:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep  3 21:40:10.908: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-7068  8b9398b1-dba1-4e03-9cd4-b483fb726c07 15693 1 2022-09-03 21:40:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 691e2d7e-02f5-409b-bb69-d236e9dac37c 0xc001d3fe50 0xc001d3fe51}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"691e2d7e-02f5-409b-bb69-d236e9dac37c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d3fee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:40:10.908: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  3 21:40:10.909: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-7068  42d26b13-c744-448a-92e4-e8951015941f 15683 2 2022-09-03 21:40:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 691e2d7e-02f5-409b-bb69-d236e9dac37c 0xc001d3fd27 0xc001d3fd28}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"691e2d7e-02f5-409b-bb69-d236e9dac37c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d3fde8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:40:10.910: INFO: Pod "test-recreate-deployment-9d58999df-v7hmf" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-v7hmf test-recreate-deployment-9d58999df- deployment-7068  17188211-9c8d-4dc8-a468-03c4d44c456a 15694 0 2022-09-03 21:40:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 8b9398b1-dba1-4e03-9cd4-b483fb726c07 0xc004d73a20 0xc004d73a21}] [] [{kube-controller-manager Update v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b9398b1-dba1-4e03-9cd4-b483fb726c07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4xl6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4xl6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:40:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:40:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:40:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:40:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:,StartTime:2022-09-03 21:40:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Sep  3 21:40:10.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7068" for this suite. 09/03/22 21:40:10.913
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":204,"skipped":3760,"failed":0}
------------------------------
• [2.117 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:40:08.8
    Sep  3 21:40:08.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename deployment 09/03/22 21:40:08.801
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:40:08.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:40:08.812
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Sep  3 21:40:08.814: INFO: Creating deployment "test-recreate-deployment"
    Sep  3 21:40:08.817: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Sep  3 21:40:08.821: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Sep  3 21:40:10.827: INFO: Waiting deployment "test-recreate-deployment" to complete
    Sep  3 21:40:10.829: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Sep  3 21:40:10.834: INFO: Updating deployment test-recreate-deployment
    Sep  3 21:40:10.834: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  3 21:40:10.906: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-7068  691e2d7e-02f5-409b-bb69-d236e9dac37c 15695 2 2022-09-03 21:40:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d3f948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-09-03 21:40:10 +0000 UTC,LastTransitionTime:2022-09-03 21:40:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2022-09-03 21:40:10 +0000 UTC,LastTransitionTime:2022-09-03 21:40:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  3 21:40:10.908: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-7068  8b9398b1-dba1-4e03-9cd4-b483fb726c07 15693 1 2022-09-03 21:40:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 691e2d7e-02f5-409b-bb69-d236e9dac37c 0xc001d3fe50 0xc001d3fe51}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"691e2d7e-02f5-409b-bb69-d236e9dac37c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d3fee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:40:10.908: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Sep  3 21:40:10.909: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-7068  42d26b13-c744-448a-92e4-e8951015941f 15683 2 2022-09-03 21:40:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 691e2d7e-02f5-409b-bb69-d236e9dac37c 0xc001d3fd27 0xc001d3fd28}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"691e2d7e-02f5-409b-bb69-d236e9dac37c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d3fde8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:40:10.910: INFO: Pod "test-recreate-deployment-9d58999df-v7hmf" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-v7hmf test-recreate-deployment-9d58999df- deployment-7068  17188211-9c8d-4dc8-a468-03c4d44c456a 15694 0 2022-09-03 21:40:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 8b9398b1-dba1-4e03-9cd4-b483fb726c07 0xc004d73a20 0xc004d73a21}] [] [{kube-controller-manager Update v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b9398b1-dba1-4e03-9cd4-b483fb726c07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:40:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4xl6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4xl6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:40:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:40:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:40:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:40:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:,StartTime:2022-09-03 21:40:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Sep  3 21:40:10.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7068" for this suite. 09/03/22 21:40:10.913
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:40:10.92
Sep  3 21:40:10.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 21:40:10.921
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:40:10.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:40:10.936
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/03/22 21:40:10.938
Sep  3 21:40:10.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/03/22 21:40:21.645
Sep  3 21:40:21.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 21:40:24.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:40:34.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9586" for this suite. 09/03/22 21:40:34.611
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":205,"skipped":3769,"failed":0}
------------------------------
• [SLOW TEST] [23.695 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:40:10.92
    Sep  3 21:40:10.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 21:40:10.921
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:40:10.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:40:10.936
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/03/22 21:40:10.938
    Sep  3 21:40:10.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/03/22 21:40:21.645
    Sep  3 21:40:21.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 21:40:24.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:40:34.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9586" for this suite. 09/03/22 21:40:34.611
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:40:34.617
Sep  3 21:40:34.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename gc 09/03/22 21:40:34.618
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:40:34.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:40:34.631
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 09/03/22 21:40:34.633
STEP: delete the rc 09/03/22 21:40:39.639
STEP: wait for all pods to be garbage collected 09/03/22 21:40:39.647
STEP: Gathering metrics 09/03/22 21:40:44.652
Sep  3 21:40:44.669: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
Sep  3 21:40:44.672: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 2.62391ms
Sep  3 21:40:44.672: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
Sep  3 21:40:44.672: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
Sep  3 21:40:44.731: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Sep  3 21:40:44.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-833" for this suite. 09/03/22 21:40:44.736
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":206,"skipped":3786,"failed":0}
------------------------------
• [SLOW TEST] [10.122 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:40:34.617
    Sep  3 21:40:34.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename gc 09/03/22 21:40:34.618
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:40:34.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:40:34.631
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 09/03/22 21:40:34.633
    STEP: delete the rc 09/03/22 21:40:39.639
    STEP: wait for all pods to be garbage collected 09/03/22 21:40:39.647
    STEP: Gathering metrics 09/03/22 21:40:44.652
    Sep  3 21:40:44.669: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
    Sep  3 21:40:44.672: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 2.62391ms
    Sep  3 21:40:44.672: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
    Sep  3 21:40:44.672: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
    Sep  3 21:40:44.731: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Sep  3 21:40:44.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-833" for this suite. 09/03/22 21:40:44.736
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:40:44.746
Sep  3 21:40:44.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename subpath 09/03/22 21:40:44.747
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:40:44.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:40:44.758
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/03/22 21:40:44.76
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-ndrn 09/03/22 21:40:44.764
STEP: Creating a pod to test atomic-volume-subpath 09/03/22 21:40:44.764
Sep  3 21:40:44.769: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ndrn" in namespace "subpath-5474" to be "Succeeded or Failed"
Sep  3 21:40:44.772: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.300912ms
Sep  3 21:40:46.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 2.005976328s
Sep  3 21:40:48.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 4.005851734s
Sep  3 21:40:50.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 6.00580604s
Sep  3 21:40:52.776: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 8.006761758s
Sep  3 21:40:54.776: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 10.007288837s
Sep  3 21:40:56.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 12.006609511s
Sep  3 21:40:58.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 14.006095185s
Sep  3 21:41:00.776: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 16.006947916s
Sep  3 21:41:02.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 18.006350328s
Sep  3 21:41:04.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 20.005867447s
Sep  3 21:41:06.776: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=false. Elapsed: 22.006773408s
Sep  3 21:41:08.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005787747s
STEP: Saw pod success 09/03/22 21:41:08.775
Sep  3 21:41:08.775: INFO: Pod "pod-subpath-test-configmap-ndrn" satisfied condition "Succeeded or Failed"
Sep  3 21:41:08.776: INFO: Trying to get logs from node kind-worker2 pod pod-subpath-test-configmap-ndrn container test-container-subpath-configmap-ndrn: <nil>
STEP: delete the pod 09/03/22 21:41:08.789
Sep  3 21:41:08.794: INFO: Waiting for pod pod-subpath-test-configmap-ndrn to disappear
Sep  3 21:41:08.796: INFO: Pod pod-subpath-test-configmap-ndrn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ndrn 09/03/22 21:41:08.796
Sep  3 21:41:08.796: INFO: Deleting pod "pod-subpath-test-configmap-ndrn" in namespace "subpath-5474"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Sep  3 21:41:08.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5474" for this suite. 09/03/22 21:41:08.8
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":207,"skipped":3801,"failed":0}
------------------------------
• [SLOW TEST] [24.057 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:40:44.746
    Sep  3 21:40:44.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename subpath 09/03/22 21:40:44.747
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:40:44.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:40:44.758
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/03/22 21:40:44.76
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-ndrn 09/03/22 21:40:44.764
    STEP: Creating a pod to test atomic-volume-subpath 09/03/22 21:40:44.764
    Sep  3 21:40:44.769: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ndrn" in namespace "subpath-5474" to be "Succeeded or Failed"
    Sep  3 21:40:44.772: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.300912ms
    Sep  3 21:40:46.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 2.005976328s
    Sep  3 21:40:48.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 4.005851734s
    Sep  3 21:40:50.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 6.00580604s
    Sep  3 21:40:52.776: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 8.006761758s
    Sep  3 21:40:54.776: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 10.007288837s
    Sep  3 21:40:56.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 12.006609511s
    Sep  3 21:40:58.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 14.006095185s
    Sep  3 21:41:00.776: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 16.006947916s
    Sep  3 21:41:02.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 18.006350328s
    Sep  3 21:41:04.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=true. Elapsed: 20.005867447s
    Sep  3 21:41:06.776: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Running", Reason="", readiness=false. Elapsed: 22.006773408s
    Sep  3 21:41:08.775: INFO: Pod "pod-subpath-test-configmap-ndrn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005787747s
    STEP: Saw pod success 09/03/22 21:41:08.775
    Sep  3 21:41:08.775: INFO: Pod "pod-subpath-test-configmap-ndrn" satisfied condition "Succeeded or Failed"
    Sep  3 21:41:08.776: INFO: Trying to get logs from node kind-worker2 pod pod-subpath-test-configmap-ndrn container test-container-subpath-configmap-ndrn: <nil>
    STEP: delete the pod 09/03/22 21:41:08.789
    Sep  3 21:41:08.794: INFO: Waiting for pod pod-subpath-test-configmap-ndrn to disappear
    Sep  3 21:41:08.796: INFO: Pod pod-subpath-test-configmap-ndrn no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-ndrn 09/03/22 21:41:08.796
    Sep  3 21:41:08.796: INFO: Deleting pod "pod-subpath-test-configmap-ndrn" in namespace "subpath-5474"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Sep  3 21:41:08.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5474" for this suite. 09/03/22 21:41:08.8
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:41:08.814
Sep  3 21:41:08.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename runtimeclass 09/03/22 21:41:08.815
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:08.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:08.825
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Sep  3 21:41:08.841: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3512 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Sep  3 21:41:08.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3512" for this suite. 09/03/22 21:41:08.883
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":208,"skipped":3856,"failed":0}
------------------------------
• [0.075 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:41:08.814
    Sep  3 21:41:08.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename runtimeclass 09/03/22 21:41:08.815
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:08.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:08.825
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Sep  3 21:41:08.841: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3512 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Sep  3 21:41:08.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3512" for this suite. 09/03/22 21:41:08.883
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:41:08.89
Sep  3 21:41:08.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename deployment 09/03/22 21:41:08.891
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:08.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:08.945
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Sep  3 21:41:08.973: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  3 21:41:13.987: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/03/22 21:41:13.987
Sep  3 21:41:13.988: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/03/22 21:41:14.007
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  3 21:41:14.028: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1280  3bc9f872-c454-43e6-92ae-7f9ca2bb0b58 15978 1 2022-09-03 21:41:13 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-09-03 21:41:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e558a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep  3 21:41:14.032: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-1280  5d91aefb-696a-493c-bf42-7dfe216c2127 15982 1 2022-09-03 21:41:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 3bc9f872-c454-43e6-92ae-7f9ca2bb0b58 0xc003ed41a7 0xc003ed41a8}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:41:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bc9f872-c454-43e6-92ae-7f9ca2bb0b58\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ed4238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:41:14.032: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  3 21:41:14.033: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1280  fa6600a0-b648-438e-a0ef-86eeca99eff6 15981 1 2022-09-03 21:41:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 3bc9f872-c454-43e6-92ae-7f9ca2bb0b58 0xc003ed4077 0xc003ed4078}] [] [{e2e.test Update apps/v1 2022-09-03 21:41:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:41:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-09-03 21:41:14 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"3bc9f872-c454-43e6-92ae-7f9ca2bb0b58\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ed4138 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  3 21:41:14.039: INFO: Pod "test-cleanup-controller-6jzwn" is available:
&Pod{ObjectMeta:{test-cleanup-controller-6jzwn test-cleanup-controller- deployment-1280  b0c341e1-52c7-486b-b0bc-4e299149f7ad 15961 0 2022-09-03 21:41:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller fa6600a0-b648-438e-a0ef-86eeca99eff6 0xc003e55c77 0xc003e55c78}] [] [{kube-controller-manager Update v1 2022-09-03 21:41:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa6600a0-b648-438e-a0ef-86eeca99eff6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:41:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjwvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjwvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:41:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:41:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:41:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:41:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.216,StartTime:2022-09-03 21:41:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:41:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6318768f085be00a492c50f7c185aa0c908889f0ab3160d7ca719b222bc37802,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Sep  3 21:41:14.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1280" for this suite. 09/03/22 21:41:14.044
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":209,"skipped":3863,"failed":0}
------------------------------
• [SLOW TEST] [5.170 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:41:08.89
    Sep  3 21:41:08.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename deployment 09/03/22 21:41:08.891
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:08.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:08.945
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Sep  3 21:41:08.973: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Sep  3 21:41:13.987: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/03/22 21:41:13.987
    Sep  3 21:41:13.988: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/03/22 21:41:14.007
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  3 21:41:14.028: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1280  3bc9f872-c454-43e6-92ae-7f9ca2bb0b58 15978 1 2022-09-03 21:41:13 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-09-03 21:41:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e558a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  3 21:41:14.032: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-1280  5d91aefb-696a-493c-bf42-7dfe216c2127 15982 1 2022-09-03 21:41:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 3bc9f872-c454-43e6-92ae-7f9ca2bb0b58 0xc003ed41a7 0xc003ed41a8}] [] [{kube-controller-manager Update apps/v1 2022-09-03 21:41:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bc9f872-c454-43e6-92ae-7f9ca2bb0b58\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ed4238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:41:14.032: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Sep  3 21:41:14.033: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1280  fa6600a0-b648-438e-a0ef-86eeca99eff6 15981 1 2022-09-03 21:41:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 3bc9f872-c454-43e6-92ae-7f9ca2bb0b58 0xc003ed4077 0xc003ed4078}] [] [{e2e.test Update apps/v1 2022-09-03 21:41:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 21:41:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-09-03 21:41:14 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"3bc9f872-c454-43e6-92ae-7f9ca2bb0b58\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ed4138 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  3 21:41:14.039: INFO: Pod "test-cleanup-controller-6jzwn" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-6jzwn test-cleanup-controller- deployment-1280  b0c341e1-52c7-486b-b0bc-4e299149f7ad 15961 0 2022-09-03 21:41:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller fa6600a0-b648-438e-a0ef-86eeca99eff6 0xc003e55c77 0xc003e55c78}] [] [{kube-controller-manager Update v1 2022-09-03 21:41:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa6600a0-b648-438e-a0ef-86eeca99eff6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 21:41:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjwvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjwvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:41:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:41:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:41:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 21:41:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.216,StartTime:2022-09-03 21:41:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 21:41:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6318768f085be00a492c50f7c185aa0c908889f0ab3160d7ca719b222bc37802,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Sep  3 21:41:14.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1280" for this suite. 09/03/22 21:41:14.044
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:41:14.067
Sep  3 21:41:14.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename job 09/03/22 21:41:14.069
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:14.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:14.104
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 09/03/22 21:41:14.107
STEP: Ensuring job reaches completions 09/03/22 21:41:14.112
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Sep  3 21:41:26.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7101" for this suite. 09/03/22 21:41:26.118
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":210,"skipped":3869,"failed":0}
------------------------------
• [SLOW TEST] [12.055 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:41:14.067
    Sep  3 21:41:14.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename job 09/03/22 21:41:14.069
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:14.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:14.104
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 09/03/22 21:41:14.107
    STEP: Ensuring job reaches completions 09/03/22 21:41:14.112
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Sep  3 21:41:26.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7101" for this suite. 09/03/22 21:41:26.118
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:41:26.127
Sep  3 21:41:26.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename sysctl 09/03/22 21:41:26.128
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:26.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:26.142
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/03/22 21:41:26.145
STEP: Watching for error events or started pod 09/03/22 21:41:26.15
STEP: Waiting for pod completion 09/03/22 21:41:28.153
Sep  3 21:41:28.153: INFO: Waiting up to 3m0s for pod "sysctl-64f8b489-0f4b-42ae-ac39-427b53def85a" in namespace "sysctl-586" to be "completed"
Sep  3 21:41:28.155: INFO: Pod "sysctl-64f8b489-0f4b-42ae-ac39-427b53def85a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.939307ms
Sep  3 21:41:30.157: INFO: Pod "sysctl-64f8b489-0f4b-42ae-ac39-427b53def85a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003878796s
Sep  3 21:41:30.157: INFO: Pod "sysctl-64f8b489-0f4b-42ae-ac39-427b53def85a" satisfied condition "completed"
STEP: Checking that the pod succeeded 09/03/22 21:41:30.159
STEP: Getting logs from the pod 09/03/22 21:41:30.159
STEP: Checking that the sysctl is actually updated 09/03/22 21:41:30.163
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Sep  3 21:41:30.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-586" for this suite. 09/03/22 21:41:30.166
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":211,"skipped":3909,"failed":0}
------------------------------
• [4.043 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:41:26.127
    Sep  3 21:41:26.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename sysctl 09/03/22 21:41:26.128
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:26.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:26.142
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/03/22 21:41:26.145
    STEP: Watching for error events or started pod 09/03/22 21:41:26.15
    STEP: Waiting for pod completion 09/03/22 21:41:28.153
    Sep  3 21:41:28.153: INFO: Waiting up to 3m0s for pod "sysctl-64f8b489-0f4b-42ae-ac39-427b53def85a" in namespace "sysctl-586" to be "completed"
    Sep  3 21:41:28.155: INFO: Pod "sysctl-64f8b489-0f4b-42ae-ac39-427b53def85a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.939307ms
    Sep  3 21:41:30.157: INFO: Pod "sysctl-64f8b489-0f4b-42ae-ac39-427b53def85a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003878796s
    Sep  3 21:41:30.157: INFO: Pod "sysctl-64f8b489-0f4b-42ae-ac39-427b53def85a" satisfied condition "completed"
    STEP: Checking that the pod succeeded 09/03/22 21:41:30.159
    STEP: Getting logs from the pod 09/03/22 21:41:30.159
    STEP: Checking that the sysctl is actually updated 09/03/22 21:41:30.163
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Sep  3 21:41:30.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-586" for this suite. 09/03/22 21:41:30.166
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:41:30.178
Sep  3 21:41:30.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename daemonsets 09/03/22 21:41:30.179
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:30.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:30.193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Sep  3 21:41:30.205: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 09/03/22 21:41:30.208
Sep  3 21:41:30.210: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:41:30.210: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 09/03/22 21:41:30.21
Sep  3 21:41:30.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:41:30.234: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  3 21:41:31.241: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:41:31.241: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  3 21:41:32.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  3 21:41:32.235: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 09/03/22 21:41:32.237
Sep  3 21:41:32.255: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  3 21:41:32.255: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Sep  3 21:41:33.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:41:33.260: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/03/22 21:41:33.26
Sep  3 21:41:33.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:41:33.271: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  3 21:41:34.276: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:41:34.276: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  3 21:41:35.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:41:35.275: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  3 21:41:36.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  3 21:41:36.275: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 09/03/22 21:41:36.279
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3392, will wait for the garbage collector to delete the pods 09/03/22 21:41:36.279
Sep  3 21:41:36.342: INFO: Deleting DaemonSet.extensions daemon-set took: 10.989718ms
Sep  3 21:41:36.442: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.473562ms
Sep  3 21:41:38.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:41:38.345: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  3 21:41:38.347: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16241"},"items":null}

Sep  3 21:41:38.348: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16241"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:41:38.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3392" for this suite. 09/03/22 21:41:38.365
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":212,"skipped":3933,"failed":0}
------------------------------
• [SLOW TEST] [8.192 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:41:30.178
    Sep  3 21:41:30.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename daemonsets 09/03/22 21:41:30.179
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:30.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:30.193
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Sep  3 21:41:30.205: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 09/03/22 21:41:30.208
    Sep  3 21:41:30.210: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:41:30.210: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 09/03/22 21:41:30.21
    Sep  3 21:41:30.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:41:30.234: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  3 21:41:31.241: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:41:31.241: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  3 21:41:32.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  3 21:41:32.235: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 09/03/22 21:41:32.237
    Sep  3 21:41:32.255: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  3 21:41:32.255: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Sep  3 21:41:33.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:41:33.260: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/03/22 21:41:33.26
    Sep  3 21:41:33.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:41:33.271: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  3 21:41:34.276: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:41:34.276: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  3 21:41:35.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:41:35.275: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  3 21:41:36.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  3 21:41:36.275: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 09/03/22 21:41:36.279
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3392, will wait for the garbage collector to delete the pods 09/03/22 21:41:36.279
    Sep  3 21:41:36.342: INFO: Deleting DaemonSet.extensions daemon-set took: 10.989718ms
    Sep  3 21:41:36.442: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.473562ms
    Sep  3 21:41:38.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:41:38.345: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  3 21:41:38.347: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16241"},"items":null}

    Sep  3 21:41:38.348: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16241"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:41:38.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3392" for this suite. 09/03/22 21:41:38.365
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:41:38.373
Sep  3 21:41:38.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename containers 09/03/22 21:41:38.374
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:38.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:38.395
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Sep  3 21:41:38.409: INFO: Waiting up to 5m0s for pod "client-containers-77973d61-8a63-426f-85d3-2452ec61585d" in namespace "containers-51" to be "running"
Sep  3 21:41:38.415: INFO: Pod "client-containers-77973d61-8a63-426f-85d3-2452ec61585d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.104621ms
Sep  3 21:41:40.418: INFO: Pod "client-containers-77973d61-8a63-426f-85d3-2452ec61585d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008052952s
Sep  3 21:41:40.418: INFO: Pod "client-containers-77973d61-8a63-426f-85d3-2452ec61585d" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Sep  3 21:41:40.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-51" for this suite. 09/03/22 21:41:40.425
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":213,"skipped":3947,"failed":0}
------------------------------
• [2.055 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:41:38.373
    Sep  3 21:41:38.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename containers 09/03/22 21:41:38.374
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:38.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:38.395
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Sep  3 21:41:38.409: INFO: Waiting up to 5m0s for pod "client-containers-77973d61-8a63-426f-85d3-2452ec61585d" in namespace "containers-51" to be "running"
    Sep  3 21:41:38.415: INFO: Pod "client-containers-77973d61-8a63-426f-85d3-2452ec61585d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.104621ms
    Sep  3 21:41:40.418: INFO: Pod "client-containers-77973d61-8a63-426f-85d3-2452ec61585d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008052952s
    Sep  3 21:41:40.418: INFO: Pod "client-containers-77973d61-8a63-426f-85d3-2452ec61585d" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Sep  3 21:41:40.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-51" for this suite. 09/03/22 21:41:40.425
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:41:40.428
Sep  3 21:41:40.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 21:41:40.429
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:40.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:40.441
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-1cff2c5d-4fcf-469c-a5d1-08cacd4e1d1f 09/03/22 21:41:40.444
STEP: Creating a pod to test consume configMaps 09/03/22 21:41:40.446
Sep  3 21:41:40.455: INFO: Waiting up to 5m0s for pod "pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca" in namespace "configmap-4513" to be "Succeeded or Failed"
Sep  3 21:41:40.458: INFO: Pod "pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.159313ms
Sep  3 21:41:42.461: INFO: Pod "pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006088122s
Sep  3 21:41:44.461: INFO: Pod "pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006584621s
STEP: Saw pod success 09/03/22 21:41:44.462
Sep  3 21:41:44.462: INFO: Pod "pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca" satisfied condition "Succeeded or Failed"
Sep  3 21:41:44.463: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca container agnhost-container: <nil>
STEP: delete the pod 09/03/22 21:41:44.467
Sep  3 21:41:44.474: INFO: Waiting for pod pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca to disappear
Sep  3 21:41:44.477: INFO: Pod pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 21:41:44.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4513" for this suite. 09/03/22 21:41:44.48
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":214,"skipped":3951,"failed":0}
------------------------------
• [4.054 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:41:40.428
    Sep  3 21:41:40.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 21:41:40.429
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:40.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:40.441
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-1cff2c5d-4fcf-469c-a5d1-08cacd4e1d1f 09/03/22 21:41:40.444
    STEP: Creating a pod to test consume configMaps 09/03/22 21:41:40.446
    Sep  3 21:41:40.455: INFO: Waiting up to 5m0s for pod "pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca" in namespace "configmap-4513" to be "Succeeded or Failed"
    Sep  3 21:41:40.458: INFO: Pod "pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.159313ms
    Sep  3 21:41:42.461: INFO: Pod "pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006088122s
    Sep  3 21:41:44.461: INFO: Pod "pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006584621s
    STEP: Saw pod success 09/03/22 21:41:44.462
    Sep  3 21:41:44.462: INFO: Pod "pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca" satisfied condition "Succeeded or Failed"
    Sep  3 21:41:44.463: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 21:41:44.467
    Sep  3 21:41:44.474: INFO: Waiting for pod pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca to disappear
    Sep  3 21:41:44.477: INFO: Pod pod-configmaps-672f8ad9-36c7-4552-b43e-fdef89ddcdca no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 21:41:44.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4513" for this suite. 09/03/22 21:41:44.48
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:41:44.491
Sep  3 21:41:44.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 21:41:44.492
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:44.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:44.504
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 09/03/22 21:41:44.506
Sep  3 21:41:44.511: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9" in namespace "downward-api-9276" to be "Succeeded or Failed"
Sep  3 21:41:44.514: INFO: Pod "downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.57611ms
Sep  3 21:41:46.519: INFO: Pod "downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007846928s
Sep  3 21:41:48.517: INFO: Pod "downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005884174s
STEP: Saw pod success 09/03/22 21:41:48.517
Sep  3 21:41:48.517: INFO: Pod "downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9" satisfied condition "Succeeded or Failed"
Sep  3 21:41:48.519: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9 container client-container: <nil>
STEP: delete the pod 09/03/22 21:41:48.523
Sep  3 21:41:48.531: INFO: Waiting for pod downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9 to disappear
Sep  3 21:41:48.533: INFO: Pod downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Sep  3 21:41:48.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9276" for this suite. 09/03/22 21:41:48.538
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":215,"skipped":3970,"failed":0}
------------------------------
• [4.053 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:41:44.491
    Sep  3 21:41:44.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 21:41:44.492
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:44.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:44.504
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 09/03/22 21:41:44.506
    Sep  3 21:41:44.511: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9" in namespace "downward-api-9276" to be "Succeeded or Failed"
    Sep  3 21:41:44.514: INFO: Pod "downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.57611ms
    Sep  3 21:41:46.519: INFO: Pod "downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007846928s
    Sep  3 21:41:48.517: INFO: Pod "downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005884174s
    STEP: Saw pod success 09/03/22 21:41:48.517
    Sep  3 21:41:48.517: INFO: Pod "downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9" satisfied condition "Succeeded or Failed"
    Sep  3 21:41:48.519: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9 container client-container: <nil>
    STEP: delete the pod 09/03/22 21:41:48.523
    Sep  3 21:41:48.531: INFO: Waiting for pod downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9 to disappear
    Sep  3 21:41:48.533: INFO: Pod downwardapi-volume-cb1cc023-353a-49ee-946e-51c71e2c66c9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Sep  3 21:41:48.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9276" for this suite. 09/03/22 21:41:48.538
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:41:48.556
Sep  3 21:41:48.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename daemonsets 09/03/22 21:41:48.558
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:48.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:48.568
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 09/03/22 21:41:48.579
STEP: Check that daemon pods launch on every node of the cluster. 09/03/22 21:41:48.582
Sep  3 21:41:48.587: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:41:48.590: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:41:48.590: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 21:41:49.594: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:41:49.597: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:41:49.597: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 21:41:50.593: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:41:50.595: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  3 21:41:50.595: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 09/03/22 21:41:50.597
Sep  3 21:41:50.606: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:41:50.609: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  3 21:41:50.609: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  3 21:41:51.612: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:41:51.614: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  3 21:41:51.614: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  3 21:41:52.612: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:41:52.614: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  3 21:41:52.614: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  3 21:41:53.612: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:41:53.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  3 21:41:53.619: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  3 21:41:54.612: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:41:54.615: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  3 21:41:54.615: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 09/03/22 21:41:54.617
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6858, will wait for the garbage collector to delete the pods 09/03/22 21:41:54.617
Sep  3 21:41:54.673: INFO: Deleting DaemonSet.extensions daemon-set took: 3.344712ms
Sep  3 21:41:54.773: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.193466ms
Sep  3 21:41:57.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 21:41:57.476: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  3 21:41:57.478: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16421"},"items":null}

Sep  3 21:41:57.480: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16421"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:41:57.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6858" for this suite. 09/03/22 21:41:57.49
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":216,"skipped":4013,"failed":0}
------------------------------
• [SLOW TEST] [8.939 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:41:48.556
    Sep  3 21:41:48.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename daemonsets 09/03/22 21:41:48.558
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:48.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:48.568
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 09/03/22 21:41:48.579
    STEP: Check that daemon pods launch on every node of the cluster. 09/03/22 21:41:48.582
    Sep  3 21:41:48.587: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:41:48.590: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:41:48.590: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 21:41:49.594: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:41:49.597: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:41:49.597: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 21:41:50.593: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:41:50.595: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  3 21:41:50.595: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 09/03/22 21:41:50.597
    Sep  3 21:41:50.606: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:41:50.609: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  3 21:41:50.609: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  3 21:41:51.612: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:41:51.614: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  3 21:41:51.614: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  3 21:41:52.612: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:41:52.614: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  3 21:41:52.614: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  3 21:41:53.612: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:41:53.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  3 21:41:53.619: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  3 21:41:54.612: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:41:54.615: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  3 21:41:54.615: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 09/03/22 21:41:54.617
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6858, will wait for the garbage collector to delete the pods 09/03/22 21:41:54.617
    Sep  3 21:41:54.673: INFO: Deleting DaemonSet.extensions daemon-set took: 3.344712ms
    Sep  3 21:41:54.773: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.193466ms
    Sep  3 21:41:57.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 21:41:57.476: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  3 21:41:57.478: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16421"},"items":null}

    Sep  3 21:41:57.480: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16421"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:41:57.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6858" for this suite. 09/03/22 21:41:57.49
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:41:57.503
Sep  3 21:41:57.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename replicaset 09/03/22 21:41:57.504
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:57.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:57.521
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/03/22 21:41:57.523
Sep  3 21:41:57.529: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-3385" to be "running and ready"
Sep  3 21:41:57.530: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.634202ms
Sep  3 21:41:57.530: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:41:59.533: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.004828258s
Sep  3 21:41:59.534: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Sep  3 21:41:59.534: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 09/03/22 21:41:59.535
STEP: Then the orphan pod is adopted 09/03/22 21:41:59.539
STEP: When the matched label of one of its pods change 09/03/22 21:42:00.545
Sep  3 21:42:00.547: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 09/03/22 21:42:00.554
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Sep  3 21:42:01.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3385" for this suite. 09/03/22 21:42:01.561
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":217,"skipped":4057,"failed":0}
------------------------------
• [4.061 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:41:57.503
    Sep  3 21:41:57.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename replicaset 09/03/22 21:41:57.504
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:41:57.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:41:57.521
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/03/22 21:41:57.523
    Sep  3 21:41:57.529: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-3385" to be "running and ready"
    Sep  3 21:41:57.530: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.634202ms
    Sep  3 21:41:57.530: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:41:59.533: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.004828258s
    Sep  3 21:41:59.534: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Sep  3 21:41:59.534: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 09/03/22 21:41:59.535
    STEP: Then the orphan pod is adopted 09/03/22 21:41:59.539
    STEP: When the matched label of one of its pods change 09/03/22 21:42:00.545
    Sep  3 21:42:00.547: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/03/22 21:42:00.554
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Sep  3 21:42:01.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3385" for this suite. 09/03/22 21:42:01.561
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:42:01.568
Sep  3 21:42:01.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 21:42:01.569
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:01.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:01.58
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 21:42:01.59
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:42:01.945
STEP: Deploying the webhook pod 09/03/22 21:42:01.952
STEP: Wait for the deployment to be ready 09/03/22 21:42:01.96
Sep  3 21:42:01.973: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 21:42:03.979
STEP: Verifying the service has paired with the endpoint 09/03/22 21:42:03.984
Sep  3 21:42:04.985: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/03/22 21:42:04.988
STEP: create a configmap that should be updated by the webhook 09/03/22 21:42:05.001
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:42:05.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8018" for this suite. 09/03/22 21:42:05.024
STEP: Destroying namespace "webhook-8018-markers" for this suite. 09/03/22 21:42:05.027
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":218,"skipped":4079,"failed":0}
------------------------------
• [3.518 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:42:01.568
    Sep  3 21:42:01.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 21:42:01.569
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:01.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:01.58
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 21:42:01.59
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:42:01.945
    STEP: Deploying the webhook pod 09/03/22 21:42:01.952
    STEP: Wait for the deployment to be ready 09/03/22 21:42:01.96
    Sep  3 21:42:01.973: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 21:42:03.979
    STEP: Verifying the service has paired with the endpoint 09/03/22 21:42:03.984
    Sep  3 21:42:04.985: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/03/22 21:42:04.988
    STEP: create a configmap that should be updated by the webhook 09/03/22 21:42:05.001
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:42:05.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8018" for this suite. 09/03/22 21:42:05.024
    STEP: Destroying namespace "webhook-8018-markers" for this suite. 09/03/22 21:42:05.027
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:42:05.089
Sep  3 21:42:05.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename events 09/03/22 21:42:05.09
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:05.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:05.113
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 09/03/22 21:42:05.117
STEP: listing events in all namespaces 09/03/22 21:42:05.125
STEP: listing events in test namespace 09/03/22 21:42:05.129
STEP: listing events with field selection filtering on source 09/03/22 21:42:05.133
STEP: listing events with field selection filtering on reportingController 09/03/22 21:42:05.135
STEP: getting the test event 09/03/22 21:42:05.138
STEP: patching the test event 09/03/22 21:42:05.14
STEP: getting the test event 09/03/22 21:42:05.148
STEP: updating the test event 09/03/22 21:42:05.15
STEP: getting the test event 09/03/22 21:42:05.155
STEP: deleting the test event 09/03/22 21:42:05.157
STEP: listing events in all namespaces 09/03/22 21:42:05.161
STEP: listing events in test namespace 09/03/22 21:42:05.165
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Sep  3 21:42:05.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2930" for this suite. 09/03/22 21:42:05.169
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":219,"skipped":4081,"failed":0}
------------------------------
• [0.096 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:42:05.089
    Sep  3 21:42:05.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename events 09/03/22 21:42:05.09
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:05.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:05.113
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 09/03/22 21:42:05.117
    STEP: listing events in all namespaces 09/03/22 21:42:05.125
    STEP: listing events in test namespace 09/03/22 21:42:05.129
    STEP: listing events with field selection filtering on source 09/03/22 21:42:05.133
    STEP: listing events with field selection filtering on reportingController 09/03/22 21:42:05.135
    STEP: getting the test event 09/03/22 21:42:05.138
    STEP: patching the test event 09/03/22 21:42:05.14
    STEP: getting the test event 09/03/22 21:42:05.148
    STEP: updating the test event 09/03/22 21:42:05.15
    STEP: getting the test event 09/03/22 21:42:05.155
    STEP: deleting the test event 09/03/22 21:42:05.157
    STEP: listing events in all namespaces 09/03/22 21:42:05.161
    STEP: listing events in test namespace 09/03/22 21:42:05.165
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Sep  3 21:42:05.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-2930" for this suite. 09/03/22 21:42:05.169
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:42:05.189
Sep  3 21:42:05.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 21:42:05.19
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:05.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:05.217
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 09/03/22 21:42:05.219
Sep  3 21:42:05.229: INFO: Waiting up to 5m0s for pod "pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469" in namespace "emptydir-3944" to be "Succeeded or Failed"
Sep  3 21:42:05.239: INFO: Pod "pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469": Phase="Pending", Reason="", readiness=false. Elapsed: 9.538306ms
Sep  3 21:42:07.242: INFO: Pod "pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012777127s
Sep  3 21:42:09.243: INFO: Pod "pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013329086s
STEP: Saw pod success 09/03/22 21:42:09.243
Sep  3 21:42:09.243: INFO: Pod "pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469" satisfied condition "Succeeded or Failed"
Sep  3 21:42:09.245: INFO: Trying to get logs from node kind-worker2 pod pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469 container test-container: <nil>
STEP: delete the pod 09/03/22 21:42:09.249
Sep  3 21:42:09.255: INFO: Waiting for pod pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469 to disappear
Sep  3 21:42:09.258: INFO: Pod pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 21:42:09.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3944" for this suite. 09/03/22 21:42:09.263
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":220,"skipped":4093,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:42:05.189
    Sep  3 21:42:05.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 21:42:05.19
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:05.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:05.217
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/03/22 21:42:05.219
    Sep  3 21:42:05.229: INFO: Waiting up to 5m0s for pod "pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469" in namespace "emptydir-3944" to be "Succeeded or Failed"
    Sep  3 21:42:05.239: INFO: Pod "pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469": Phase="Pending", Reason="", readiness=false. Elapsed: 9.538306ms
    Sep  3 21:42:07.242: INFO: Pod "pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012777127s
    Sep  3 21:42:09.243: INFO: Pod "pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013329086s
    STEP: Saw pod success 09/03/22 21:42:09.243
    Sep  3 21:42:09.243: INFO: Pod "pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469" satisfied condition "Succeeded or Failed"
    Sep  3 21:42:09.245: INFO: Trying to get logs from node kind-worker2 pod pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469 container test-container: <nil>
    STEP: delete the pod 09/03/22 21:42:09.249
    Sep  3 21:42:09.255: INFO: Waiting for pod pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469 to disappear
    Sep  3 21:42:09.258: INFO: Pod pod-f632f0fe-af79-42d1-9a8d-1360f0d1d469 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 21:42:09.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3944" for this suite. 09/03/22 21:42:09.263
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:42:09.273
Sep  3 21:42:09.273: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 21:42:09.274
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:09.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:09.285
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 21:42:09.296
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:42:09.996
STEP: Deploying the webhook pod 09/03/22 21:42:10
STEP: Wait for the deployment to be ready 09/03/22 21:42:10.007
Sep  3 21:42:10.021: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 21:42:12.027
STEP: Verifying the service has paired with the endpoint 09/03/22 21:42:12.037
Sep  3 21:42:13.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 09/03/22 21:42:13.091
STEP: Creating a configMap that should be mutated 09/03/22 21:42:13.102
STEP: Deleting the collection of validation webhooks 09/03/22 21:42:13.145
STEP: Creating a configMap that should not be mutated 09/03/22 21:42:13.171
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:42:13.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5975" for this suite. 09/03/22 21:42:13.307
STEP: Destroying namespace "webhook-5975-markers" for this suite. 09/03/22 21:42:13.309
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":221,"skipped":4112,"failed":0}
------------------------------
• [4.110 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:42:09.273
    Sep  3 21:42:09.273: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 21:42:09.274
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:09.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:09.285
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 21:42:09.296
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:42:09.996
    STEP: Deploying the webhook pod 09/03/22 21:42:10
    STEP: Wait for the deployment to be ready 09/03/22 21:42:10.007
    Sep  3 21:42:10.021: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 21:42:12.027
    STEP: Verifying the service has paired with the endpoint 09/03/22 21:42:12.037
    Sep  3 21:42:13.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 09/03/22 21:42:13.091
    STEP: Creating a configMap that should be mutated 09/03/22 21:42:13.102
    STEP: Deleting the collection of validation webhooks 09/03/22 21:42:13.145
    STEP: Creating a configMap that should not be mutated 09/03/22 21:42:13.171
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:42:13.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5975" for this suite. 09/03/22 21:42:13.307
    STEP: Destroying namespace "webhook-5975-markers" for this suite. 09/03/22 21:42:13.309
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:42:13.397
Sep  3 21:42:13.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename statefulset 09/03/22 21:42:13.4
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:13.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:13.415
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-759 09/03/22 21:42:13.418
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Sep  3 21:42:13.438: INFO: Found 0 stateful pods, waiting for 1
Sep  3 21:42:23.441: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 09/03/22 21:42:23.445
W0903 21:42:23.465602      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  3 21:42:23.492: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 21:42:23.492: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Sep  3 21:42:33.498: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 21:42:33.498: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 09/03/22 21:42:33.502
STEP: Delete all of the StatefulSets 09/03/22 21:42:33.504
STEP: Verify that StatefulSets have been deleted 09/03/22 21:42:33.508
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Sep  3 21:42:33.511: INFO: Deleting all statefulset in ns statefulset-759
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Sep  3 21:42:33.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-759" for this suite. 09/03/22 21:42:33.53
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":222,"skipped":4121,"failed":0}
------------------------------
• [SLOW TEST] [20.139 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:42:13.397
    Sep  3 21:42:13.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename statefulset 09/03/22 21:42:13.4
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:13.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:13.415
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-759 09/03/22 21:42:13.418
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Sep  3 21:42:13.438: INFO: Found 0 stateful pods, waiting for 1
    Sep  3 21:42:23.441: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 09/03/22 21:42:23.445
    W0903 21:42:23.465602      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  3 21:42:23.492: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 21:42:23.492: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Sep  3 21:42:33.498: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 21:42:33.498: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 09/03/22 21:42:33.502
    STEP: Delete all of the StatefulSets 09/03/22 21:42:33.504
    STEP: Verify that StatefulSets have been deleted 09/03/22 21:42:33.508
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Sep  3 21:42:33.511: INFO: Deleting all statefulset in ns statefulset-759
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Sep  3 21:42:33.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-759" for this suite. 09/03/22 21:42:33.53
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:42:33.546
Sep  3 21:42:33.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 21:42:33.547
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:33.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:33.605
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 09/03/22 21:42:33.609
Sep  3 21:42:33.627: INFO: Waiting up to 5m0s for pod "downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27" in namespace "downward-api-1815" to be "Succeeded or Failed"
Sep  3 21:42:33.631: INFO: Pod "downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033914ms
Sep  3 21:42:35.634: INFO: Pod "downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007000179s
Sep  3 21:42:37.634: INFO: Pod "downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006827486s
STEP: Saw pod success 09/03/22 21:42:37.634
Sep  3 21:42:37.634: INFO: Pod "downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27" satisfied condition "Succeeded or Failed"
Sep  3 21:42:37.635: INFO: Trying to get logs from node kind-worker2 pod downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27 container dapi-container: <nil>
STEP: delete the pod 09/03/22 21:42:37.64
Sep  3 21:42:37.648: INFO: Waiting for pod downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27 to disappear
Sep  3 21:42:37.650: INFO: Pod downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Sep  3 21:42:37.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1815" for this suite. 09/03/22 21:42:37.653
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":223,"skipped":4156,"failed":0}
------------------------------
• [4.110 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:42:33.546
    Sep  3 21:42:33.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 21:42:33.547
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:33.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:33.605
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 09/03/22 21:42:33.609
    Sep  3 21:42:33.627: INFO: Waiting up to 5m0s for pod "downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27" in namespace "downward-api-1815" to be "Succeeded or Failed"
    Sep  3 21:42:33.631: INFO: Pod "downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033914ms
    Sep  3 21:42:35.634: INFO: Pod "downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007000179s
    Sep  3 21:42:37.634: INFO: Pod "downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006827486s
    STEP: Saw pod success 09/03/22 21:42:37.634
    Sep  3 21:42:37.634: INFO: Pod "downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27" satisfied condition "Succeeded or Failed"
    Sep  3 21:42:37.635: INFO: Trying to get logs from node kind-worker2 pod downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27 container dapi-container: <nil>
    STEP: delete the pod 09/03/22 21:42:37.64
    Sep  3 21:42:37.648: INFO: Waiting for pod downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27 to disappear
    Sep  3 21:42:37.650: INFO: Pod downward-api-412ff793-94dd-4b13-b498-0e3cf177bb27 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Sep  3 21:42:37.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1815" for this suite. 09/03/22 21:42:37.653
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:42:37.671
Sep  3 21:42:37.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 21:42:37.672
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:37.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:37.694
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 09/03/22 21:42:37.698
STEP: fetching the ConfigMap 09/03/22 21:42:37.701
STEP: patching the ConfigMap 09/03/22 21:42:37.703
STEP: listing all ConfigMaps in all namespaces with a label selector 09/03/22 21:42:37.706
STEP: deleting the ConfigMap by collection with a label selector 09/03/22 21:42:37.708
STEP: listing all ConfigMaps in test namespace 09/03/22 21:42:37.712
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 21:42:37.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8253" for this suite. 09/03/22 21:42:37.716
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":224,"skipped":4167,"failed":0}
------------------------------
• [0.048 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:42:37.671
    Sep  3 21:42:37.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 21:42:37.672
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:37.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:37.694
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 09/03/22 21:42:37.698
    STEP: fetching the ConfigMap 09/03/22 21:42:37.701
    STEP: patching the ConfigMap 09/03/22 21:42:37.703
    STEP: listing all ConfigMaps in all namespaces with a label selector 09/03/22 21:42:37.706
    STEP: deleting the ConfigMap by collection with a label selector 09/03/22 21:42:37.708
    STEP: listing all ConfigMaps in test namespace 09/03/22 21:42:37.712
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 21:42:37.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8253" for this suite. 09/03/22 21:42:37.716
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:42:37.719
Sep  3 21:42:37.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubelet-test 09/03/22 21:42:37.72
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:37.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:37.733
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 09/03/22 21:42:37.74
Sep  3 21:42:37.740: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases117a156d-4b45-4178-9ca3-e190f5e120da" in namespace "kubelet-test-4332" to be "completed"
Sep  3 21:42:37.741: INFO: Pod "agnhost-host-aliases117a156d-4b45-4178-9ca3-e190f5e120da": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869706ms
Sep  3 21:42:39.744: INFO: Pod "agnhost-host-aliases117a156d-4b45-4178-9ca3-e190f5e120da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004244621s
Sep  3 21:42:41.745: INFO: Pod "agnhost-host-aliases117a156d-4b45-4178-9ca3-e190f5e120da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005607533s
Sep  3 21:42:41.745: INFO: Pod "agnhost-host-aliases117a156d-4b45-4178-9ca3-e190f5e120da" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Sep  3 21:42:41.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4332" for this suite. 09/03/22 21:42:41.752
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":225,"skipped":4171,"failed":0}
------------------------------
• [4.036 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:42:37.719
    Sep  3 21:42:37.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubelet-test 09/03/22 21:42:37.72
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:37.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:37.733
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 09/03/22 21:42:37.74
    Sep  3 21:42:37.740: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases117a156d-4b45-4178-9ca3-e190f5e120da" in namespace "kubelet-test-4332" to be "completed"
    Sep  3 21:42:37.741: INFO: Pod "agnhost-host-aliases117a156d-4b45-4178-9ca3-e190f5e120da": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869706ms
    Sep  3 21:42:39.744: INFO: Pod "agnhost-host-aliases117a156d-4b45-4178-9ca3-e190f5e120da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004244621s
    Sep  3 21:42:41.745: INFO: Pod "agnhost-host-aliases117a156d-4b45-4178-9ca3-e190f5e120da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005607533s
    Sep  3 21:42:41.745: INFO: Pod "agnhost-host-aliases117a156d-4b45-4178-9ca3-e190f5e120da" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Sep  3 21:42:41.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4332" for this suite. 09/03/22 21:42:41.752
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:42:41.756
Sep  3 21:42:41.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename gc 09/03/22 21:42:41.757
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:41.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:41.769
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 09/03/22 21:42:41.774
STEP: delete the rc 09/03/22 21:42:46.804
STEP: wait for the rc to be deleted 09/03/22 21:42:46.833
Sep  3 21:42:48.214: INFO: 80 pods remaining
Sep  3 21:42:48.214: INFO: 80 pods has nil DeletionTimestamp
Sep  3 21:42:48.214: INFO: 
Sep  3 21:42:48.864: INFO: 71 pods remaining
Sep  3 21:42:48.864: INFO: 71 pods has nil DeletionTimestamp
Sep  3 21:42:48.864: INFO: 
Sep  3 21:42:49.878: INFO: 60 pods remaining
Sep  3 21:42:49.878: INFO: 60 pods has nil DeletionTimestamp
Sep  3 21:42:49.878: INFO: 
Sep  3 21:42:50.914: INFO: 40 pods remaining
Sep  3 21:42:50.914: INFO: 40 pods has nil DeletionTimestamp
Sep  3 21:42:50.938: INFO: 
Sep  3 21:42:51.942: INFO: 33 pods remaining
Sep  3 21:42:51.942: INFO: 30 pods has nil DeletionTimestamp
Sep  3 21:42:51.942: INFO: 
Sep  3 21:42:52.907: INFO: 17 pods remaining
Sep  3 21:42:52.907: INFO: 17 pods has nil DeletionTimestamp
Sep  3 21:42:52.907: INFO: 
STEP: Gathering metrics 09/03/22 21:42:53.84
Sep  3 21:42:53.860: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
Sep  3 21:42:53.863: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 3.358213ms
Sep  3 21:42:53.864: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
Sep  3 21:42:53.864: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
Sep  3 21:42:54.074: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Sep  3 21:42:54.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-564" for this suite. 09/03/22 21:42:54.081
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":226,"skipped":4189,"failed":0}
------------------------------
• [SLOW TEST] [12.337 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:42:41.756
    Sep  3 21:42:41.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename gc 09/03/22 21:42:41.757
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:41.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:41.769
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 09/03/22 21:42:41.774
    STEP: delete the rc 09/03/22 21:42:46.804
    STEP: wait for the rc to be deleted 09/03/22 21:42:46.833
    Sep  3 21:42:48.214: INFO: 80 pods remaining
    Sep  3 21:42:48.214: INFO: 80 pods has nil DeletionTimestamp
    Sep  3 21:42:48.214: INFO: 
    Sep  3 21:42:48.864: INFO: 71 pods remaining
    Sep  3 21:42:48.864: INFO: 71 pods has nil DeletionTimestamp
    Sep  3 21:42:48.864: INFO: 
    Sep  3 21:42:49.878: INFO: 60 pods remaining
    Sep  3 21:42:49.878: INFO: 60 pods has nil DeletionTimestamp
    Sep  3 21:42:49.878: INFO: 
    Sep  3 21:42:50.914: INFO: 40 pods remaining
    Sep  3 21:42:50.914: INFO: 40 pods has nil DeletionTimestamp
    Sep  3 21:42:50.938: INFO: 
    Sep  3 21:42:51.942: INFO: 33 pods remaining
    Sep  3 21:42:51.942: INFO: 30 pods has nil DeletionTimestamp
    Sep  3 21:42:51.942: INFO: 
    Sep  3 21:42:52.907: INFO: 17 pods remaining
    Sep  3 21:42:52.907: INFO: 17 pods has nil DeletionTimestamp
    Sep  3 21:42:52.907: INFO: 
    STEP: Gathering metrics 09/03/22 21:42:53.84
    Sep  3 21:42:53.860: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
    Sep  3 21:42:53.863: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 3.358213ms
    Sep  3 21:42:53.864: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
    Sep  3 21:42:53.864: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
    Sep  3 21:42:54.074: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Sep  3 21:42:54.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-564" for this suite. 09/03/22 21:42:54.081
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:42:54.104
Sep  3 21:42:54.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename watch 09/03/22 21:42:54.14
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:54.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:54.205
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 09/03/22 21:42:54.213
STEP: creating a watch on configmaps with label B 09/03/22 21:42:54.215
STEP: creating a watch on configmaps with label A or B 09/03/22 21:42:54.217
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/03/22 21:42:54.218
Sep  3 21:42:54.223: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17632 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:42:54.223: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17632 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/03/22 21:42:54.223
Sep  3 21:42:54.249: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17634 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:42:54.249: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17634 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/03/22 21:42:54.249
Sep  3 21:42:54.267: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17635 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:42:54.267: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17635 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/03/22 21:42:54.268
Sep  3 21:42:54.291: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17636 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:42:54.291: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17636 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/03/22 21:42:54.291
Sep  3 21:42:54.307: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9801  4a20390e-01bb-41e1-9bdb-9cf64ab322cb 17637 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:42:54.307: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9801  4a20390e-01bb-41e1-9bdb-9cf64ab322cb 17637 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/03/22 21:43:04.315
Sep  3 21:43:04.320: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9801  4a20390e-01bb-41e1-9bdb-9cf64ab322cb 17853 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 21:43:04.320: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9801  4a20390e-01bb-41e1-9bdb-9cf64ab322cb 17853 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Sep  3 21:43:14.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9801" for this suite. 09/03/22 21:43:14.339
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":227,"skipped":4196,"failed":0}
------------------------------
• [SLOW TEST] [20.251 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:42:54.104
    Sep  3 21:42:54.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename watch 09/03/22 21:42:54.14
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:42:54.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:42:54.205
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 09/03/22 21:42:54.213
    STEP: creating a watch on configmaps with label B 09/03/22 21:42:54.215
    STEP: creating a watch on configmaps with label A or B 09/03/22 21:42:54.217
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/03/22 21:42:54.218
    Sep  3 21:42:54.223: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17632 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:42:54.223: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17632 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/03/22 21:42:54.223
    Sep  3 21:42:54.249: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17634 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:42:54.249: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17634 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/03/22 21:42:54.249
    Sep  3 21:42:54.267: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17635 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:42:54.267: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17635 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/03/22 21:42:54.268
    Sep  3 21:42:54.291: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17636 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:42:54.291: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9801  64d337d8-d2bd-4c37-a79e-25677ab12678 17636 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/03/22 21:42:54.291
    Sep  3 21:42:54.307: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9801  4a20390e-01bb-41e1-9bdb-9cf64ab322cb 17637 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:42:54.307: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9801  4a20390e-01bb-41e1-9bdb-9cf64ab322cb 17637 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/03/22 21:43:04.315
    Sep  3 21:43:04.320: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9801  4a20390e-01bb-41e1-9bdb-9cf64ab322cb 17853 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  3 21:43:04.320: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9801  4a20390e-01bb-41e1-9bdb-9cf64ab322cb 17853 0 2022-09-03 21:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-09-03 21:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Sep  3 21:43:14.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9801" for this suite. 09/03/22 21:43:14.339
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:43:14.364
Sep  3 21:43:14.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename custom-resource-definition 09/03/22 21:43:14.38
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:43:14.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:43:14.425
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 09/03/22 21:43:14.436
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/03/22 21:43:14.44
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/03/22 21:43:14.44
STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/03/22 21:43:14.441
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/03/22 21:43:14.445
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/03/22 21:43:14.445
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/03/22 21:43:14.448
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:43:14.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-664" for this suite. 09/03/22 21:43:14.452
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":228,"skipped":4198,"failed":0}
------------------------------
• [0.096 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:43:14.364
    Sep  3 21:43:14.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename custom-resource-definition 09/03/22 21:43:14.38
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:43:14.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:43:14.425
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 09/03/22 21:43:14.436
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/03/22 21:43:14.44
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/03/22 21:43:14.44
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/03/22 21:43:14.441
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/03/22 21:43:14.445
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/03/22 21:43:14.445
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/03/22 21:43:14.448
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:43:14.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-664" for this suite. 09/03/22 21:43:14.452
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:43:14.462
Sep  3 21:43:14.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename replication-controller 09/03/22 21:43:14.464
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:43:14.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:43:14.503
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 09/03/22 21:43:14.506
Sep  3 21:43:14.523: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6675" to be "running and ready"
Sep  3 21:43:14.535: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 11.633341ms
Sep  3 21:43:14.535: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:43:16.540: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016441124s
Sep  3 21:43:16.540: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:43:18.557: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033507564s
Sep  3 21:43:18.557: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:43:20.541: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017903389s
Sep  3 21:43:20.541: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:43:22.539: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015848661s
Sep  3 21:43:22.539: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:43:24.659: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 10.136171899s
Sep  3 21:43:24.660: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:43:26.545: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 12.022149812s
Sep  3 21:43:26.546: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:43:28.539: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 14.015736037s
Sep  3 21:43:28.539: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:43:30.548: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024737222s
Sep  3 21:43:30.549: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:43:32.540: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01691433s
Sep  3 21:43:32.540: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:43:34.540: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016816166s
Sep  3 21:43:34.540: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:43:36.539: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 22.015406097s
Sep  3 21:43:36.539: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Sep  3 21:43:36.539: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 09/03/22 21:43:36.541
STEP: Then the orphan pod is adopted 09/03/22 21:43:36.546
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Sep  3 21:43:37.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6675" for this suite. 09/03/22 21:43:37.561
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":229,"skipped":4203,"failed":0}
------------------------------
• [SLOW TEST] [23.102 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:43:14.462
    Sep  3 21:43:14.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename replication-controller 09/03/22 21:43:14.464
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:43:14.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:43:14.503
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 09/03/22 21:43:14.506
    Sep  3 21:43:14.523: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6675" to be "running and ready"
    Sep  3 21:43:14.535: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 11.633341ms
    Sep  3 21:43:14.535: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:43:16.540: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016441124s
    Sep  3 21:43:16.540: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:43:18.557: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033507564s
    Sep  3 21:43:18.557: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:43:20.541: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017903389s
    Sep  3 21:43:20.541: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:43:22.539: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015848661s
    Sep  3 21:43:22.539: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:43:24.659: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 10.136171899s
    Sep  3 21:43:24.660: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:43:26.545: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 12.022149812s
    Sep  3 21:43:26.546: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:43:28.539: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 14.015736037s
    Sep  3 21:43:28.539: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:43:30.548: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024737222s
    Sep  3 21:43:30.549: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:43:32.540: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01691433s
    Sep  3 21:43:32.540: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:43:34.540: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016816166s
    Sep  3 21:43:34.540: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:43:36.539: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 22.015406097s
    Sep  3 21:43:36.539: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Sep  3 21:43:36.539: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 09/03/22 21:43:36.541
    STEP: Then the orphan pod is adopted 09/03/22 21:43:36.546
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Sep  3 21:43:37.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6675" for this suite. 09/03/22 21:43:37.561
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:43:37.567
Sep  3 21:43:37.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 21:43:37.568
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:43:37.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:43:37.588
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 09/03/22 21:43:37.602
Sep  3 21:43:37.616: INFO: Waiting up to 5m0s for pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf" in namespace "downward-api-3534" to be "Succeeded or Failed"
Sep  3 21:43:37.625: INFO: Pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.853242ms
Sep  3 21:43:39.628: INFO: Pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011534692s
Sep  3 21:43:41.629: INFO: Pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012702121s
Sep  3 21:43:43.630: INFO: Pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014095892s
Sep  3 21:43:45.628: INFO: Pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011412359s
STEP: Saw pod success 09/03/22 21:43:45.628
Sep  3 21:43:45.628: INFO: Pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf" satisfied condition "Succeeded or Failed"
Sep  3 21:43:45.630: INFO: Trying to get logs from node kind-worker2 pod downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf container dapi-container: <nil>
STEP: delete the pod 09/03/22 21:43:45.636
Sep  3 21:43:45.642: INFO: Waiting for pod downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf to disappear
Sep  3 21:43:45.644: INFO: Pod downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Sep  3 21:43:45.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3534" for this suite. 09/03/22 21:43:45.647
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":230,"skipped":4217,"failed":0}
------------------------------
• [SLOW TEST] [8.084 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:43:37.567
    Sep  3 21:43:37.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 21:43:37.568
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:43:37.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:43:37.588
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 09/03/22 21:43:37.602
    Sep  3 21:43:37.616: INFO: Waiting up to 5m0s for pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf" in namespace "downward-api-3534" to be "Succeeded or Failed"
    Sep  3 21:43:37.625: INFO: Pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.853242ms
    Sep  3 21:43:39.628: INFO: Pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011534692s
    Sep  3 21:43:41.629: INFO: Pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012702121s
    Sep  3 21:43:43.630: INFO: Pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014095892s
    Sep  3 21:43:45.628: INFO: Pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011412359s
    STEP: Saw pod success 09/03/22 21:43:45.628
    Sep  3 21:43:45.628: INFO: Pod "downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf" satisfied condition "Succeeded or Failed"
    Sep  3 21:43:45.630: INFO: Trying to get logs from node kind-worker2 pod downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf container dapi-container: <nil>
    STEP: delete the pod 09/03/22 21:43:45.636
    Sep  3 21:43:45.642: INFO: Waiting for pod downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf to disappear
    Sep  3 21:43:45.644: INFO: Pod downward-api-2b4bcd0d-2166-461d-b711-359db0edf2cf no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Sep  3 21:43:45.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3534" for this suite. 09/03/22 21:43:45.647
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:43:45.655
Sep  3 21:43:45.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename namespaces 09/03/22 21:43:45.656
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:43:45.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:43:45.666
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 09/03/22 21:43:45.668
STEP: patching the Namespace 09/03/22 21:43:45.676
STEP: get the Namespace and ensuring it has the label 09/03/22 21:43:45.681
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:43:45.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5888" for this suite. 09/03/22 21:43:45.685
STEP: Destroying namespace "nspatchtest-ecc1c111-bc20-4eba-b48d-cfaf885c4c81-8962" for this suite. 09/03/22 21:43:45.688
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":231,"skipped":4231,"failed":0}
------------------------------
• [0.037 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:43:45.655
    Sep  3 21:43:45.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename namespaces 09/03/22 21:43:45.656
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:43:45.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:43:45.666
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 09/03/22 21:43:45.668
    STEP: patching the Namespace 09/03/22 21:43:45.676
    STEP: get the Namespace and ensuring it has the label 09/03/22 21:43:45.681
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:43:45.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5888" for this suite. 09/03/22 21:43:45.685
    STEP: Destroying namespace "nspatchtest-ecc1c111-bc20-4eba-b48d-cfaf885c4c81-8962" for this suite. 09/03/22 21:43:45.688
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:43:45.705
Sep  3 21:43:45.706: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename proxy 09/03/22 21:43:45.707
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:43:45.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:43:45.718
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 09/03/22 21:43:45.73
STEP: creating replication controller proxy-service-fndqc in namespace proxy-8720 09/03/22 21:43:45.73
I0903 21:43:45.751288      24 runners.go:193] Created replication controller with name: proxy-service-fndqc, namespace: proxy-8720, replica count: 1
I0903 21:43:46.801974      24 runners.go:193] proxy-service-fndqc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0903 21:43:47.802473      24 runners.go:193] proxy-service-fndqc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0903 21:43:48.803202      24 runners.go:193] proxy-service-fndqc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 21:43:48.806: INFO: setup took 3.086056282s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/03/22 21:43:48.806
Sep  3 21:43:48.823: INFO: (0) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 15.924835ms)
Sep  3 21:43:48.825: INFO: (0) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 18.374642ms)
Sep  3 21:43:48.830: INFO: (0) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 21.92835ms)
Sep  3 21:43:48.833: INFO: (0) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 25.476657ms)
Sep  3 21:43:48.835: INFO: (0) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 27.916163ms)
Sep  3 21:43:48.839: INFO: (0) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 31.693772ms)
Sep  3 21:43:48.839: INFO: (0) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 31.953972ms)
Sep  3 21:43:48.840: INFO: (0) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 32.358673ms)
Sep  3 21:43:48.840: INFO: (0) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 32.616574ms)
Sep  3 21:43:48.840: INFO: (0) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 32.967774ms)
Sep  3 21:43:48.841: INFO: (0) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 33.951676ms)
Sep  3 21:43:48.841: INFO: (0) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 33.209575ms)
Sep  3 21:43:48.842: INFO: (0) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 34.315978ms)
Sep  3 21:43:48.842: INFO: (0) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 35.43498ms)
Sep  3 21:43:48.847: INFO: (0) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 40.810392ms)
Sep  3 21:43:48.848: INFO: (0) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 40.880292ms)
Sep  3 21:43:48.883: INFO: (1) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 35.14708ms)
Sep  3 21:43:48.884: INFO: (1) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 35.55068ms)
Sep  3 21:43:48.884: INFO: (1) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 35.857681ms)
Sep  3 21:43:48.885: INFO: (1) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 36.900983ms)
Sep  3 21:43:48.886: INFO: (1) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 37.623685ms)
Sep  3 21:43:48.891: INFO: (1) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 42.491796ms)
Sep  3 21:43:48.891: INFO: (1) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 42.552797ms)
Sep  3 21:43:48.891: INFO: (1) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 42.521196ms)
Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 42.670697ms)
Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 42.712297ms)
Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 42.775997ms)
Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 42.978797ms)
Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 43.358798ms)
Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 43.629398ms)
Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 43.786099ms)
Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 44.163ms)
Sep  3 21:43:48.916: INFO: (2) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 22.753751ms)
Sep  3 21:43:48.916: INFO: (2) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 23.881854ms)
Sep  3 21:43:48.916: INFO: (2) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 23.721754ms)
Sep  3 21:43:48.917: INFO: (2) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 23.720354ms)
Sep  3 21:43:48.917: INFO: (2) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 23.757253ms)
Sep  3 21:43:48.918: INFO: (2) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 24.526556ms)
Sep  3 21:43:48.918: INFO: (2) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 25.240057ms)
Sep  3 21:43:48.919: INFO: (2) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 25.579058ms)
Sep  3 21:43:48.920: INFO: (2) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 26.66996ms)
Sep  3 21:43:48.922: INFO: (2) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 28.786865ms)
Sep  3 21:43:48.925: INFO: (2) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 31.871272ms)
Sep  3 21:43:48.926: INFO: (2) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 32.736974ms)
Sep  3 21:43:48.930: INFO: (2) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 36.903283ms)
Sep  3 21:43:48.931: INFO: (2) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 38.065186ms)
Sep  3 21:43:48.931: INFO: (2) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 38.213687ms)
Sep  3 21:43:48.931: INFO: (2) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 38.494887ms)
Sep  3 21:43:48.944: INFO: (3) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 11.872826ms)
Sep  3 21:43:48.944: INFO: (3) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 12.794529ms)
Sep  3 21:43:48.945: INFO: (3) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 13.42383ms)
Sep  3 21:43:48.946: INFO: (3) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 14.810633ms)
Sep  3 21:43:48.946: INFO: (3) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 14.243633ms)
Sep  3 21:43:48.947: INFO: (3) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 15.166635ms)
Sep  3 21:43:48.948: INFO: (3) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 15.879636ms)
Sep  3 21:43:48.948: INFO: (3) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 16.400437ms)
Sep  3 21:43:48.948: INFO: (3) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 16.556937ms)
Sep  3 21:43:48.948: INFO: (3) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 16.302837ms)
Sep  3 21:43:48.957: INFO: (3) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 26.18346ms)
Sep  3 21:43:48.958: INFO: (3) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 26.088459ms)
Sep  3 21:43:48.958: INFO: (3) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 26.39196ms)
Sep  3 21:43:48.959: INFO: (3) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 27.014361ms)
Sep  3 21:43:48.959: INFO: (3) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 27.205361ms)
Sep  3 21:43:48.959: INFO: (3) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 27.584162ms)
Sep  3 21:43:48.968: INFO: (4) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 8.377119ms)
Sep  3 21:43:48.969: INFO: (4) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 8.009018ms)
Sep  3 21:43:48.969: INFO: (4) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 9.781022ms)
Sep  3 21:43:48.971: INFO: (4) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 10.178023ms)
Sep  3 21:43:48.971: INFO: (4) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 10.861324ms)
Sep  3 21:43:48.971: INFO: (4) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 11.476226ms)
Sep  3 21:43:48.972: INFO: (4) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 11.245126ms)
Sep  3 21:43:48.973: INFO: (4) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 12.884929ms)
Sep  3 21:43:48.974: INFO: (4) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 13.763331ms)
Sep  3 21:43:48.975: INFO: (4) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 15.208135ms)
Sep  3 21:43:48.976: INFO: (4) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 16.340837ms)
Sep  3 21:43:48.977: INFO: (4) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 16.751938ms)
Sep  3 21:43:48.977: INFO: (4) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 16.994139ms)
Sep  3 21:43:48.977: INFO: (4) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 17.065939ms)
Sep  3 21:43:48.977: INFO: (4) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 17.32494ms)
Sep  3 21:43:48.977: INFO: (4) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 17.131339ms)
Sep  3 21:43:48.985: INFO: (5) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 7.366416ms)
Sep  3 21:43:48.987: INFO: (5) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 9.19312ms)
Sep  3 21:43:48.987: INFO: (5) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 9.405022ms)
Sep  3 21:43:48.988: INFO: (5) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 10.302523ms)
Sep  3 21:43:48.988: INFO: (5) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 10.499323ms)
Sep  3 21:43:48.989: INFO: (5) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 11.343825ms)
Sep  3 21:43:48.989: INFO: (5) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 11.650927ms)
Sep  3 21:43:48.990: INFO: (5) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 12.656629ms)
Sep  3 21:43:48.991: INFO: (5) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 13.04263ms)
Sep  3 21:43:48.991: INFO: (5) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 13.26503ms)
Sep  3 21:43:48.992: INFO: (5) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 14.253732ms)
Sep  3 21:43:48.992: INFO: (5) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 14.090231ms)
Sep  3 21:43:48.992: INFO: (5) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 14.524433ms)
Sep  3 21:43:48.993: INFO: (5) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 15.057534ms)
Sep  3 21:43:48.993: INFO: (5) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 15.529435ms)
Sep  3 21:43:48.994: INFO: (5) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 16.172037ms)
Sep  3 21:43:49.003: INFO: (6) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 8.356219ms)
Sep  3 21:43:49.003: INFO: (6) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 9.418822ms)
Sep  3 21:43:49.004: INFO: (6) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 9.976322ms)
Sep  3 21:43:49.007: INFO: (6) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 12.380428ms)
Sep  3 21:43:49.007: INFO: (6) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 12.195427ms)
Sep  3 21:43:49.007: INFO: (6) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 12.701329ms)
Sep  3 21:43:49.007: INFO: (6) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 13.012329ms)
Sep  3 21:43:49.008: INFO: (6) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 13.424731ms)
Sep  3 21:43:49.008: INFO: (6) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 14.147232ms)
Sep  3 21:43:49.009: INFO: (6) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 14.143432ms)
Sep  3 21:43:49.009: INFO: (6) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 14.378732ms)
Sep  3 21:43:49.009: INFO: (6) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 15.304535ms)
Sep  3 21:43:49.010: INFO: (6) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 15.781336ms)
Sep  3 21:43:49.011: INFO: (6) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 16.648138ms)
Sep  3 21:43:49.011: INFO: (6) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 17.014539ms)
Sep  3 21:43:49.011: INFO: (6) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 16.769638ms)
Sep  3 21:43:49.019: INFO: (7) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 7.438517ms)
Sep  3 21:43:49.021: INFO: (7) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 9.660521ms)
Sep  3 21:43:49.028: INFO: (7) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 15.990336ms)
Sep  3 21:43:49.028: INFO: (7) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 16.295537ms)
Sep  3 21:43:49.028: INFO: (7) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 16.680238ms)
Sep  3 21:43:49.029: INFO: (7) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 16.721638ms)
Sep  3 21:43:49.029: INFO: (7) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 16.944438ms)
Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 17.93994ms)
Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 18.303041ms)
Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 18.063741ms)
Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 17.84994ms)
Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 17.90364ms)
Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 18.126141ms)
Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 18.474541ms)
Sep  3 21:43:49.032: INFO: (7) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 20.718747ms)
Sep  3 21:43:49.032: INFO: (7) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 20.429146ms)
Sep  3 21:43:49.038: INFO: (8) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 5.394212ms)
Sep  3 21:43:49.039: INFO: (8) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 5.936713ms)
Sep  3 21:43:49.039: INFO: (8) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 6.212114ms)
Sep  3 21:43:49.041: INFO: (8) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 8.186818ms)
Sep  3 21:43:49.043: INFO: (8) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 10.213623ms)
Sep  3 21:43:49.044: INFO: (8) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 11.330026ms)
Sep  3 21:43:49.045: INFO: (8) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 11.103226ms)
Sep  3 21:43:49.045: INFO: (8) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 11.371926ms)
Sep  3 21:43:49.045: INFO: (8) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 11.436226ms)
Sep  3 21:43:49.048: INFO: (8) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 14.349133ms)
Sep  3 21:43:49.049: INFO: (8) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 15.152334ms)
Sep  3 21:43:49.049: INFO: (8) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 15.649435ms)
Sep  3 21:43:49.049: INFO: (8) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 16.775937ms)
Sep  3 21:43:49.052: INFO: (8) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 18.10764ms)
Sep  3 21:43:49.053: INFO: (8) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 19.425044ms)
Sep  3 21:43:49.054: INFO: (8) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 20.185146ms)
Sep  3 21:43:49.062: INFO: (9) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 6.927815ms)
Sep  3 21:43:49.062: INFO: (9) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 7.652918ms)
Sep  3 21:43:49.064: INFO: (9) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 9.476322ms)
Sep  3 21:43:49.065: INFO: (9) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 10.791825ms)
Sep  3 21:43:49.065: INFO: (9) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 10.500124ms)
Sep  3 21:43:49.068: INFO: (9) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 13.43173ms)
Sep  3 21:43:49.068: INFO: (9) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 13.51973ms)
Sep  3 21:43:49.068: INFO: (9) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 14.011232ms)
Sep  3 21:43:49.068: INFO: (9) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 14.100832ms)
Sep  3 21:43:49.070: INFO: (9) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 14.917633ms)
Sep  3 21:43:49.070: INFO: (9) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 15.405135ms)
Sep  3 21:43:49.070: INFO: (9) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 15.529635ms)
Sep  3 21:43:49.070: INFO: (9) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 15.772035ms)
Sep  3 21:43:49.071: INFO: (9) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 16.304436ms)
Sep  3 21:43:49.071: INFO: (9) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 16.759838ms)
Sep  3 21:43:49.071: INFO: (9) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 16.410337ms)
Sep  3 21:43:49.077: INFO: (10) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 5.418712ms)
Sep  3 21:43:49.078: INFO: (10) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 6.429214ms)
Sep  3 21:43:49.078: INFO: (10) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 6.449514ms)
Sep  3 21:43:49.079: INFO: (10) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 7.582818ms)
Sep  3 21:43:49.080: INFO: (10) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 8.020218ms)
Sep  3 21:43:49.080: INFO: (10) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 8.64252ms)
Sep  3 21:43:49.081: INFO: (10) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 8.957521ms)
Sep  3 21:43:49.083: INFO: (10) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 10.427924ms)
Sep  3 21:43:49.083: INFO: (10) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 10.645424ms)
Sep  3 21:43:49.083: INFO: (10) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 11.185425ms)
Sep  3 21:43:49.086: INFO: (10) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 13.732431ms)
Sep  3 21:43:49.088: INFO: (10) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 16.105037ms)
Sep  3 21:43:49.089: INFO: (10) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 16.531437ms)
Sep  3 21:43:49.088: INFO: (10) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 16.359137ms)
Sep  3 21:43:49.089: INFO: (10) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 17.225539ms)
Sep  3 21:43:49.089: INFO: (10) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 17.299039ms)
Sep  3 21:43:49.103: INFO: (11) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 12.578228ms)
Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 12.952729ms)
Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 13.017729ms)
Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 13.107929ms)
Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 13.22693ms)
Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 14.096732ms)
Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 14.244732ms)
Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 13.44373ms)
Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 13.60993ms)
Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 13.745931ms)
Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 14.019132ms)
Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 13.605731ms)
Sep  3 21:43:49.105: INFO: (11) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 14.103832ms)
Sep  3 21:43:49.105: INFO: (11) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 14.551333ms)
Sep  3 21:43:49.105: INFO: (11) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 15.278934ms)
Sep  3 21:43:49.106: INFO: (11) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 15.148334ms)
Sep  3 21:43:49.112: INFO: (12) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 5.071612ms)
Sep  3 21:43:49.112: INFO: (12) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 5.364212ms)
Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 10.206223ms)
Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 10.295923ms)
Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 10.747725ms)
Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 11.090225ms)
Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 11.280526ms)
Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 10.245424ms)
Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 11.362426ms)
Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 10.235723ms)
Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 10.484424ms)
Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 10.444024ms)
Sep  3 21:43:49.118: INFO: (12) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 11.125125ms)
Sep  3 21:43:49.119: INFO: (12) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 12.404628ms)
Sep  3 21:43:49.119: INFO: (12) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 12.415528ms)
Sep  3 21:43:49.120: INFO: (12) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 14.067132ms)
Sep  3 21:43:49.128: INFO: (13) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 6.942616ms)
Sep  3 21:43:49.128: INFO: (13) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 7.257017ms)
Sep  3 21:43:49.129: INFO: (13) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 7.715317ms)
Sep  3 21:43:49.131: INFO: (13) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 9.309721ms)
Sep  3 21:43:49.134: INFO: (13) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 12.691029ms)
Sep  3 21:43:49.134: INFO: (13) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 13.082929ms)
Sep  3 21:43:49.134: INFO: (13) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 13.141629ms)
Sep  3 21:43:49.136: INFO: (13) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 14.873733ms)
Sep  3 21:43:49.136: INFO: (13) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 15.424335ms)
Sep  3 21:43:49.136: INFO: (13) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 14.783833ms)
Sep  3 21:43:49.136: INFO: (13) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 14.917234ms)
Sep  3 21:43:49.137: INFO: (13) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 15.316034ms)
Sep  3 21:43:49.137: INFO: (13) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 15.791336ms)
Sep  3 21:43:49.137: INFO: (13) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 16.193837ms)
Sep  3 21:43:49.137: INFO: (13) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 16.146236ms)
Sep  3 21:43:49.138: INFO: (13) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 17.070839ms)
Sep  3 21:43:49.148: INFO: (14) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 10.316323ms)
Sep  3 21:43:49.149: INFO: (14) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 10.716624ms)
Sep  3 21:43:49.149: INFO: (14) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 11.208925ms)
Sep  3 21:43:49.151: INFO: (14) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 12.245127ms)
Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 16.703638ms)
Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 17.003439ms)
Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 17.329239ms)
Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 17.115339ms)
Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 17.376639ms)
Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 18.261842ms)
Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 17.71714ms)
Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 17.68854ms)
Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 17.81674ms)
Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 17.909541ms)
Sep  3 21:43:49.157: INFO: (14) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 18.315242ms)
Sep  3 21:43:49.157: INFO: (14) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 18.392142ms)
Sep  3 21:43:49.164: INFO: (15) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 5.995814ms)
Sep  3 21:43:49.164: INFO: (15) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 7.359917ms)
Sep  3 21:43:49.165: INFO: (15) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 7.714617ms)
Sep  3 21:43:49.167: INFO: (15) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 9.900822ms)
Sep  3 21:43:49.181: INFO: (15) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 22.903151ms)
Sep  3 21:43:49.181: INFO: (15) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 23.623054ms)
Sep  3 21:43:49.181: INFO: (15) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 23.597054ms)
Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 23.783654ms)
Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 24.166655ms)
Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 24.235955ms)
Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 24.356955ms)
Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 24.501356ms)
Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 24.580556ms)
Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 24.648956ms)
Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 25.569058ms)
Sep  3 21:43:49.183: INFO: (15) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 25.438857ms)
Sep  3 21:43:49.194: INFO: (16) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 11.418126ms)
Sep  3 21:43:49.197: INFO: (16) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 13.735831ms)
Sep  3 21:43:49.197: INFO: (16) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 14.268533ms)
Sep  3 21:43:49.203: INFO: (16) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 20.310445ms)
Sep  3 21:43:49.204: INFO: (16) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 20.535946ms)
Sep  3 21:43:49.204: INFO: (16) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 21.019948ms)
Sep  3 21:43:49.204: INFO: (16) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 21.072047ms)
Sep  3 21:43:49.204: INFO: (16) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 21.645849ms)
Sep  3 21:43:49.205: INFO: (16) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 20.964647ms)
Sep  3 21:43:49.205: INFO: (16) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 21.312948ms)
Sep  3 21:43:49.207: INFO: (16) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 23.636954ms)
Sep  3 21:43:49.209: INFO: (16) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 25.645758ms)
Sep  3 21:43:49.209: INFO: (16) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 25.949659ms)
Sep  3 21:43:49.210: INFO: (16) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 26.21256ms)
Sep  3 21:43:49.210: INFO: (16) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 26.66846ms)
Sep  3 21:43:49.210: INFO: (16) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 26.68246ms)
Sep  3 21:43:49.218: INFO: (17) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 6.649115ms)
Sep  3 21:43:49.218: INFO: (17) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 7.030316ms)
Sep  3 21:43:49.219: INFO: (17) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 7.668217ms)
Sep  3 21:43:49.222: INFO: (17) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 11.159025ms)
Sep  3 21:43:49.222: INFO: (17) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 10.476724ms)
Sep  3 21:43:49.222: INFO: (17) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 10.916025ms)
Sep  3 21:43:49.222: INFO: (17) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 11.179325ms)
Sep  3 21:43:49.223: INFO: (17) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 11.477526ms)
Sep  3 21:43:49.223: INFO: (17) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 11.914327ms)
Sep  3 21:43:49.226: INFO: (17) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 14.763034ms)
Sep  3 21:43:49.226: INFO: (17) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 15.365034ms)
Sep  3 21:43:49.226: INFO: (17) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 14.923534ms)
Sep  3 21:43:49.226: INFO: (17) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 15.923036ms)
Sep  3 21:43:49.227: INFO: (17) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 16.054936ms)
Sep  3 21:43:49.227: INFO: (17) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 16.285937ms)
Sep  3 21:43:49.228: INFO: (17) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 16.207537ms)
Sep  3 21:43:49.232: INFO: (18) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 4.691111ms)
Sep  3 21:43:49.238: INFO: (18) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 9.336621ms)
Sep  3 21:43:49.238: INFO: (18) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 9.757622ms)
Sep  3 21:43:49.239: INFO: (18) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 10.309423ms)
Sep  3 21:43:49.239: INFO: (18) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 10.534024ms)
Sep  3 21:43:49.239: INFO: (18) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 10.643824ms)
Sep  3 21:43:49.239: INFO: (18) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 10.825025ms)
Sep  3 21:43:49.239: INFO: (18) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 10.715525ms)
Sep  3 21:43:49.239: INFO: (18) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 10.896825ms)
Sep  3 21:43:49.240: INFO: (18) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 11.429726ms)
Sep  3 21:43:49.242: INFO: (18) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 13.764831ms)
Sep  3 21:43:49.243: INFO: (18) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 14.892633ms)
Sep  3 21:43:49.243: INFO: (18) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 14.984333ms)
Sep  3 21:43:49.244: INFO: (18) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 15.781836ms)
Sep  3 21:43:49.245: INFO: (18) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 16.982538ms)
Sep  3 21:43:49.245: INFO: (18) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 16.956839ms)
Sep  3 21:43:49.255: INFO: (19) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 9.225921ms)
Sep  3 21:43:49.258: INFO: (19) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 12.477228ms)
Sep  3 21:43:49.258: INFO: (19) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 12.962729ms)
Sep  3 21:43:49.260: INFO: (19) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 14.488532ms)
Sep  3 21:43:49.260: INFO: (19) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 14.529033ms)
Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 14.848433ms)
Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 15.032134ms)
Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 15.254634ms)
Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 15.441935ms)
Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 15.607535ms)
Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 15.838235ms)
Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 15.878735ms)
Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 15.910436ms)
Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 16.000536ms)
Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 16.241436ms)
Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 15.819736ms)
STEP: deleting ReplicationController proxy-service-fndqc in namespace proxy-8720, will wait for the garbage collector to delete the pods 09/03/22 21:43:49.262
Sep  3 21:43:49.318: INFO: Deleting ReplicationController proxy-service-fndqc took: 3.520608ms
Sep  3 21:43:49.419: INFO: Terminating ReplicationController proxy-service-fndqc pods took: 100.428526ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Sep  3 21:43:51.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8720" for this suite. 09/03/22 21:43:51.027
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":232,"skipped":4277,"failed":0}
------------------------------
• [SLOW TEST] [5.336 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:43:45.705
    Sep  3 21:43:45.706: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename proxy 09/03/22 21:43:45.707
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:43:45.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:43:45.718
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 09/03/22 21:43:45.73
    STEP: creating replication controller proxy-service-fndqc in namespace proxy-8720 09/03/22 21:43:45.73
    I0903 21:43:45.751288      24 runners.go:193] Created replication controller with name: proxy-service-fndqc, namespace: proxy-8720, replica count: 1
    I0903 21:43:46.801974      24 runners.go:193] proxy-service-fndqc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0903 21:43:47.802473      24 runners.go:193] proxy-service-fndqc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0903 21:43:48.803202      24 runners.go:193] proxy-service-fndqc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  3 21:43:48.806: INFO: setup took 3.086056282s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/03/22 21:43:48.806
    Sep  3 21:43:48.823: INFO: (0) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 15.924835ms)
    Sep  3 21:43:48.825: INFO: (0) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 18.374642ms)
    Sep  3 21:43:48.830: INFO: (0) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 21.92835ms)
    Sep  3 21:43:48.833: INFO: (0) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 25.476657ms)
    Sep  3 21:43:48.835: INFO: (0) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 27.916163ms)
    Sep  3 21:43:48.839: INFO: (0) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 31.693772ms)
    Sep  3 21:43:48.839: INFO: (0) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 31.953972ms)
    Sep  3 21:43:48.840: INFO: (0) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 32.358673ms)
    Sep  3 21:43:48.840: INFO: (0) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 32.616574ms)
    Sep  3 21:43:48.840: INFO: (0) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 32.967774ms)
    Sep  3 21:43:48.841: INFO: (0) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 33.951676ms)
    Sep  3 21:43:48.841: INFO: (0) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 33.209575ms)
    Sep  3 21:43:48.842: INFO: (0) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 34.315978ms)
    Sep  3 21:43:48.842: INFO: (0) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 35.43498ms)
    Sep  3 21:43:48.847: INFO: (0) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 40.810392ms)
    Sep  3 21:43:48.848: INFO: (0) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 40.880292ms)
    Sep  3 21:43:48.883: INFO: (1) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 35.14708ms)
    Sep  3 21:43:48.884: INFO: (1) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 35.55068ms)
    Sep  3 21:43:48.884: INFO: (1) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 35.857681ms)
    Sep  3 21:43:48.885: INFO: (1) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 36.900983ms)
    Sep  3 21:43:48.886: INFO: (1) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 37.623685ms)
    Sep  3 21:43:48.891: INFO: (1) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 42.491796ms)
    Sep  3 21:43:48.891: INFO: (1) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 42.552797ms)
    Sep  3 21:43:48.891: INFO: (1) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 42.521196ms)
    Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 42.670697ms)
    Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 42.712297ms)
    Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 42.775997ms)
    Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 42.978797ms)
    Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 43.358798ms)
    Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 43.629398ms)
    Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 43.786099ms)
    Sep  3 21:43:48.892: INFO: (1) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 44.163ms)
    Sep  3 21:43:48.916: INFO: (2) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 22.753751ms)
    Sep  3 21:43:48.916: INFO: (2) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 23.881854ms)
    Sep  3 21:43:48.916: INFO: (2) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 23.721754ms)
    Sep  3 21:43:48.917: INFO: (2) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 23.720354ms)
    Sep  3 21:43:48.917: INFO: (2) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 23.757253ms)
    Sep  3 21:43:48.918: INFO: (2) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 24.526556ms)
    Sep  3 21:43:48.918: INFO: (2) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 25.240057ms)
    Sep  3 21:43:48.919: INFO: (2) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 25.579058ms)
    Sep  3 21:43:48.920: INFO: (2) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 26.66996ms)
    Sep  3 21:43:48.922: INFO: (2) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 28.786865ms)
    Sep  3 21:43:48.925: INFO: (2) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 31.871272ms)
    Sep  3 21:43:48.926: INFO: (2) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 32.736974ms)
    Sep  3 21:43:48.930: INFO: (2) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 36.903283ms)
    Sep  3 21:43:48.931: INFO: (2) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 38.065186ms)
    Sep  3 21:43:48.931: INFO: (2) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 38.213687ms)
    Sep  3 21:43:48.931: INFO: (2) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 38.494887ms)
    Sep  3 21:43:48.944: INFO: (3) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 11.872826ms)
    Sep  3 21:43:48.944: INFO: (3) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 12.794529ms)
    Sep  3 21:43:48.945: INFO: (3) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 13.42383ms)
    Sep  3 21:43:48.946: INFO: (3) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 14.810633ms)
    Sep  3 21:43:48.946: INFO: (3) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 14.243633ms)
    Sep  3 21:43:48.947: INFO: (3) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 15.166635ms)
    Sep  3 21:43:48.948: INFO: (3) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 15.879636ms)
    Sep  3 21:43:48.948: INFO: (3) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 16.400437ms)
    Sep  3 21:43:48.948: INFO: (3) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 16.556937ms)
    Sep  3 21:43:48.948: INFO: (3) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 16.302837ms)
    Sep  3 21:43:48.957: INFO: (3) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 26.18346ms)
    Sep  3 21:43:48.958: INFO: (3) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 26.088459ms)
    Sep  3 21:43:48.958: INFO: (3) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 26.39196ms)
    Sep  3 21:43:48.959: INFO: (3) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 27.014361ms)
    Sep  3 21:43:48.959: INFO: (3) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 27.205361ms)
    Sep  3 21:43:48.959: INFO: (3) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 27.584162ms)
    Sep  3 21:43:48.968: INFO: (4) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 8.377119ms)
    Sep  3 21:43:48.969: INFO: (4) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 8.009018ms)
    Sep  3 21:43:48.969: INFO: (4) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 9.781022ms)
    Sep  3 21:43:48.971: INFO: (4) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 10.178023ms)
    Sep  3 21:43:48.971: INFO: (4) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 10.861324ms)
    Sep  3 21:43:48.971: INFO: (4) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 11.476226ms)
    Sep  3 21:43:48.972: INFO: (4) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 11.245126ms)
    Sep  3 21:43:48.973: INFO: (4) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 12.884929ms)
    Sep  3 21:43:48.974: INFO: (4) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 13.763331ms)
    Sep  3 21:43:48.975: INFO: (4) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 15.208135ms)
    Sep  3 21:43:48.976: INFO: (4) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 16.340837ms)
    Sep  3 21:43:48.977: INFO: (4) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 16.751938ms)
    Sep  3 21:43:48.977: INFO: (4) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 16.994139ms)
    Sep  3 21:43:48.977: INFO: (4) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 17.065939ms)
    Sep  3 21:43:48.977: INFO: (4) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 17.32494ms)
    Sep  3 21:43:48.977: INFO: (4) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 17.131339ms)
    Sep  3 21:43:48.985: INFO: (5) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 7.366416ms)
    Sep  3 21:43:48.987: INFO: (5) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 9.19312ms)
    Sep  3 21:43:48.987: INFO: (5) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 9.405022ms)
    Sep  3 21:43:48.988: INFO: (5) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 10.302523ms)
    Sep  3 21:43:48.988: INFO: (5) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 10.499323ms)
    Sep  3 21:43:48.989: INFO: (5) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 11.343825ms)
    Sep  3 21:43:48.989: INFO: (5) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 11.650927ms)
    Sep  3 21:43:48.990: INFO: (5) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 12.656629ms)
    Sep  3 21:43:48.991: INFO: (5) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 13.04263ms)
    Sep  3 21:43:48.991: INFO: (5) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 13.26503ms)
    Sep  3 21:43:48.992: INFO: (5) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 14.253732ms)
    Sep  3 21:43:48.992: INFO: (5) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 14.090231ms)
    Sep  3 21:43:48.992: INFO: (5) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 14.524433ms)
    Sep  3 21:43:48.993: INFO: (5) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 15.057534ms)
    Sep  3 21:43:48.993: INFO: (5) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 15.529435ms)
    Sep  3 21:43:48.994: INFO: (5) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 16.172037ms)
    Sep  3 21:43:49.003: INFO: (6) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 8.356219ms)
    Sep  3 21:43:49.003: INFO: (6) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 9.418822ms)
    Sep  3 21:43:49.004: INFO: (6) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 9.976322ms)
    Sep  3 21:43:49.007: INFO: (6) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 12.380428ms)
    Sep  3 21:43:49.007: INFO: (6) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 12.195427ms)
    Sep  3 21:43:49.007: INFO: (6) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 12.701329ms)
    Sep  3 21:43:49.007: INFO: (6) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 13.012329ms)
    Sep  3 21:43:49.008: INFO: (6) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 13.424731ms)
    Sep  3 21:43:49.008: INFO: (6) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 14.147232ms)
    Sep  3 21:43:49.009: INFO: (6) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 14.143432ms)
    Sep  3 21:43:49.009: INFO: (6) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 14.378732ms)
    Sep  3 21:43:49.009: INFO: (6) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 15.304535ms)
    Sep  3 21:43:49.010: INFO: (6) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 15.781336ms)
    Sep  3 21:43:49.011: INFO: (6) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 16.648138ms)
    Sep  3 21:43:49.011: INFO: (6) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 17.014539ms)
    Sep  3 21:43:49.011: INFO: (6) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 16.769638ms)
    Sep  3 21:43:49.019: INFO: (7) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 7.438517ms)
    Sep  3 21:43:49.021: INFO: (7) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 9.660521ms)
    Sep  3 21:43:49.028: INFO: (7) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 15.990336ms)
    Sep  3 21:43:49.028: INFO: (7) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 16.295537ms)
    Sep  3 21:43:49.028: INFO: (7) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 16.680238ms)
    Sep  3 21:43:49.029: INFO: (7) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 16.721638ms)
    Sep  3 21:43:49.029: INFO: (7) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 16.944438ms)
    Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 17.93994ms)
    Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 18.303041ms)
    Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 18.063741ms)
    Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 17.84994ms)
    Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 17.90364ms)
    Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 18.126141ms)
    Sep  3 21:43:49.030: INFO: (7) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 18.474541ms)
    Sep  3 21:43:49.032: INFO: (7) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 20.718747ms)
    Sep  3 21:43:49.032: INFO: (7) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 20.429146ms)
    Sep  3 21:43:49.038: INFO: (8) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 5.394212ms)
    Sep  3 21:43:49.039: INFO: (8) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 5.936713ms)
    Sep  3 21:43:49.039: INFO: (8) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 6.212114ms)
    Sep  3 21:43:49.041: INFO: (8) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 8.186818ms)
    Sep  3 21:43:49.043: INFO: (8) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 10.213623ms)
    Sep  3 21:43:49.044: INFO: (8) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 11.330026ms)
    Sep  3 21:43:49.045: INFO: (8) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 11.103226ms)
    Sep  3 21:43:49.045: INFO: (8) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 11.371926ms)
    Sep  3 21:43:49.045: INFO: (8) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 11.436226ms)
    Sep  3 21:43:49.048: INFO: (8) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 14.349133ms)
    Sep  3 21:43:49.049: INFO: (8) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 15.152334ms)
    Sep  3 21:43:49.049: INFO: (8) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 15.649435ms)
    Sep  3 21:43:49.049: INFO: (8) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 16.775937ms)
    Sep  3 21:43:49.052: INFO: (8) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 18.10764ms)
    Sep  3 21:43:49.053: INFO: (8) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 19.425044ms)
    Sep  3 21:43:49.054: INFO: (8) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 20.185146ms)
    Sep  3 21:43:49.062: INFO: (9) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 6.927815ms)
    Sep  3 21:43:49.062: INFO: (9) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 7.652918ms)
    Sep  3 21:43:49.064: INFO: (9) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 9.476322ms)
    Sep  3 21:43:49.065: INFO: (9) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 10.791825ms)
    Sep  3 21:43:49.065: INFO: (9) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 10.500124ms)
    Sep  3 21:43:49.068: INFO: (9) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 13.43173ms)
    Sep  3 21:43:49.068: INFO: (9) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 13.51973ms)
    Sep  3 21:43:49.068: INFO: (9) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 14.011232ms)
    Sep  3 21:43:49.068: INFO: (9) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 14.100832ms)
    Sep  3 21:43:49.070: INFO: (9) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 14.917633ms)
    Sep  3 21:43:49.070: INFO: (9) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 15.405135ms)
    Sep  3 21:43:49.070: INFO: (9) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 15.529635ms)
    Sep  3 21:43:49.070: INFO: (9) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 15.772035ms)
    Sep  3 21:43:49.071: INFO: (9) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 16.304436ms)
    Sep  3 21:43:49.071: INFO: (9) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 16.759838ms)
    Sep  3 21:43:49.071: INFO: (9) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 16.410337ms)
    Sep  3 21:43:49.077: INFO: (10) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 5.418712ms)
    Sep  3 21:43:49.078: INFO: (10) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 6.429214ms)
    Sep  3 21:43:49.078: INFO: (10) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 6.449514ms)
    Sep  3 21:43:49.079: INFO: (10) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 7.582818ms)
    Sep  3 21:43:49.080: INFO: (10) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 8.020218ms)
    Sep  3 21:43:49.080: INFO: (10) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 8.64252ms)
    Sep  3 21:43:49.081: INFO: (10) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 8.957521ms)
    Sep  3 21:43:49.083: INFO: (10) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 10.427924ms)
    Sep  3 21:43:49.083: INFO: (10) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 10.645424ms)
    Sep  3 21:43:49.083: INFO: (10) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 11.185425ms)
    Sep  3 21:43:49.086: INFO: (10) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 13.732431ms)
    Sep  3 21:43:49.088: INFO: (10) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 16.105037ms)
    Sep  3 21:43:49.089: INFO: (10) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 16.531437ms)
    Sep  3 21:43:49.088: INFO: (10) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 16.359137ms)
    Sep  3 21:43:49.089: INFO: (10) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 17.225539ms)
    Sep  3 21:43:49.089: INFO: (10) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 17.299039ms)
    Sep  3 21:43:49.103: INFO: (11) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 12.578228ms)
    Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 12.952729ms)
    Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 13.017729ms)
    Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 13.107929ms)
    Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 13.22693ms)
    Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 14.096732ms)
    Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 14.244732ms)
    Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 13.44373ms)
    Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 13.60993ms)
    Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 13.745931ms)
    Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 14.019132ms)
    Sep  3 21:43:49.104: INFO: (11) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 13.605731ms)
    Sep  3 21:43:49.105: INFO: (11) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 14.103832ms)
    Sep  3 21:43:49.105: INFO: (11) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 14.551333ms)
    Sep  3 21:43:49.105: INFO: (11) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 15.278934ms)
    Sep  3 21:43:49.106: INFO: (11) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 15.148334ms)
    Sep  3 21:43:49.112: INFO: (12) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 5.071612ms)
    Sep  3 21:43:49.112: INFO: (12) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 5.364212ms)
    Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 10.206223ms)
    Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 10.295923ms)
    Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 10.747725ms)
    Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 11.090225ms)
    Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 11.280526ms)
    Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 10.245424ms)
    Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 11.362426ms)
    Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 10.235723ms)
    Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 10.484424ms)
    Sep  3 21:43:49.117: INFO: (12) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 10.444024ms)
    Sep  3 21:43:49.118: INFO: (12) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 11.125125ms)
    Sep  3 21:43:49.119: INFO: (12) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 12.404628ms)
    Sep  3 21:43:49.119: INFO: (12) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 12.415528ms)
    Sep  3 21:43:49.120: INFO: (12) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 14.067132ms)
    Sep  3 21:43:49.128: INFO: (13) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 6.942616ms)
    Sep  3 21:43:49.128: INFO: (13) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 7.257017ms)
    Sep  3 21:43:49.129: INFO: (13) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 7.715317ms)
    Sep  3 21:43:49.131: INFO: (13) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 9.309721ms)
    Sep  3 21:43:49.134: INFO: (13) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 12.691029ms)
    Sep  3 21:43:49.134: INFO: (13) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 13.082929ms)
    Sep  3 21:43:49.134: INFO: (13) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 13.141629ms)
    Sep  3 21:43:49.136: INFO: (13) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 14.873733ms)
    Sep  3 21:43:49.136: INFO: (13) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 15.424335ms)
    Sep  3 21:43:49.136: INFO: (13) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 14.783833ms)
    Sep  3 21:43:49.136: INFO: (13) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 14.917234ms)
    Sep  3 21:43:49.137: INFO: (13) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 15.316034ms)
    Sep  3 21:43:49.137: INFO: (13) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 15.791336ms)
    Sep  3 21:43:49.137: INFO: (13) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 16.193837ms)
    Sep  3 21:43:49.137: INFO: (13) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 16.146236ms)
    Sep  3 21:43:49.138: INFO: (13) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 17.070839ms)
    Sep  3 21:43:49.148: INFO: (14) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 10.316323ms)
    Sep  3 21:43:49.149: INFO: (14) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 10.716624ms)
    Sep  3 21:43:49.149: INFO: (14) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 11.208925ms)
    Sep  3 21:43:49.151: INFO: (14) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 12.245127ms)
    Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 16.703638ms)
    Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 17.003439ms)
    Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 17.329239ms)
    Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 17.115339ms)
    Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 17.376639ms)
    Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 18.261842ms)
    Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 17.71714ms)
    Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 17.68854ms)
    Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 17.81674ms)
    Sep  3 21:43:49.156: INFO: (14) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 17.909541ms)
    Sep  3 21:43:49.157: INFO: (14) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 18.315242ms)
    Sep  3 21:43:49.157: INFO: (14) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 18.392142ms)
    Sep  3 21:43:49.164: INFO: (15) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 5.995814ms)
    Sep  3 21:43:49.164: INFO: (15) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 7.359917ms)
    Sep  3 21:43:49.165: INFO: (15) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 7.714617ms)
    Sep  3 21:43:49.167: INFO: (15) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 9.900822ms)
    Sep  3 21:43:49.181: INFO: (15) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 22.903151ms)
    Sep  3 21:43:49.181: INFO: (15) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 23.623054ms)
    Sep  3 21:43:49.181: INFO: (15) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 23.597054ms)
    Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 23.783654ms)
    Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 24.166655ms)
    Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 24.235955ms)
    Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 24.356955ms)
    Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 24.501356ms)
    Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 24.580556ms)
    Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 24.648956ms)
    Sep  3 21:43:49.182: INFO: (15) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 25.569058ms)
    Sep  3 21:43:49.183: INFO: (15) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 25.438857ms)
    Sep  3 21:43:49.194: INFO: (16) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 11.418126ms)
    Sep  3 21:43:49.197: INFO: (16) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 13.735831ms)
    Sep  3 21:43:49.197: INFO: (16) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 14.268533ms)
    Sep  3 21:43:49.203: INFO: (16) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 20.310445ms)
    Sep  3 21:43:49.204: INFO: (16) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 20.535946ms)
    Sep  3 21:43:49.204: INFO: (16) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 21.019948ms)
    Sep  3 21:43:49.204: INFO: (16) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 21.072047ms)
    Sep  3 21:43:49.204: INFO: (16) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 21.645849ms)
    Sep  3 21:43:49.205: INFO: (16) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 20.964647ms)
    Sep  3 21:43:49.205: INFO: (16) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 21.312948ms)
    Sep  3 21:43:49.207: INFO: (16) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 23.636954ms)
    Sep  3 21:43:49.209: INFO: (16) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 25.645758ms)
    Sep  3 21:43:49.209: INFO: (16) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 25.949659ms)
    Sep  3 21:43:49.210: INFO: (16) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 26.21256ms)
    Sep  3 21:43:49.210: INFO: (16) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 26.66846ms)
    Sep  3 21:43:49.210: INFO: (16) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 26.68246ms)
    Sep  3 21:43:49.218: INFO: (17) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 6.649115ms)
    Sep  3 21:43:49.218: INFO: (17) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 7.030316ms)
    Sep  3 21:43:49.219: INFO: (17) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 7.668217ms)
    Sep  3 21:43:49.222: INFO: (17) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 11.159025ms)
    Sep  3 21:43:49.222: INFO: (17) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 10.476724ms)
    Sep  3 21:43:49.222: INFO: (17) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 10.916025ms)
    Sep  3 21:43:49.222: INFO: (17) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 11.179325ms)
    Sep  3 21:43:49.223: INFO: (17) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 11.477526ms)
    Sep  3 21:43:49.223: INFO: (17) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 11.914327ms)
    Sep  3 21:43:49.226: INFO: (17) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 14.763034ms)
    Sep  3 21:43:49.226: INFO: (17) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 15.365034ms)
    Sep  3 21:43:49.226: INFO: (17) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 14.923534ms)
    Sep  3 21:43:49.226: INFO: (17) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 15.923036ms)
    Sep  3 21:43:49.227: INFO: (17) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 16.054936ms)
    Sep  3 21:43:49.227: INFO: (17) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 16.285937ms)
    Sep  3 21:43:49.228: INFO: (17) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 16.207537ms)
    Sep  3 21:43:49.232: INFO: (18) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 4.691111ms)
    Sep  3 21:43:49.238: INFO: (18) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 9.336621ms)
    Sep  3 21:43:49.238: INFO: (18) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 9.757622ms)
    Sep  3 21:43:49.239: INFO: (18) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 10.309423ms)
    Sep  3 21:43:49.239: INFO: (18) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 10.534024ms)
    Sep  3 21:43:49.239: INFO: (18) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 10.643824ms)
    Sep  3 21:43:49.239: INFO: (18) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 10.825025ms)
    Sep  3 21:43:49.239: INFO: (18) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 10.715525ms)
    Sep  3 21:43:49.239: INFO: (18) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 10.896825ms)
    Sep  3 21:43:49.240: INFO: (18) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 11.429726ms)
    Sep  3 21:43:49.242: INFO: (18) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 13.764831ms)
    Sep  3 21:43:49.243: INFO: (18) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 14.892633ms)
    Sep  3 21:43:49.243: INFO: (18) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 14.984333ms)
    Sep  3 21:43:49.244: INFO: (18) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 15.781836ms)
    Sep  3 21:43:49.245: INFO: (18) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 16.982538ms)
    Sep  3 21:43:49.245: INFO: (18) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 16.956839ms)
    Sep  3 21:43:49.255: INFO: (19) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s/proxy/rewriteme">test</a> (200; 9.225921ms)
    Sep  3 21:43:49.258: INFO: (19) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:162/proxy/: bar (200; 12.477228ms)
    Sep  3 21:43:49.258: INFO: (19) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:462/proxy/: tls qux (200; 12.962729ms)
    Sep  3 21:43:49.260: INFO: (19) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:162/proxy/: bar (200; 14.488532ms)
    Sep  3 21:43:49.260: INFO: (19) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:160/proxy/: foo (200; 14.529033ms)
    Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname1/proxy/: tls baz (200; 14.848433ms)
    Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/services/https:proxy-service-fndqc:tlsportname2/proxy/: tls qux (200; 15.032134ms)
    Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:1080/proxy/rewriteme">test<... (200; 15.254634ms)
    Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname1/proxy/: foo (200; 15.441935ms)
    Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname1/proxy/: foo (200; 15.607535ms)
    Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/pods/proxy-service-fndqc-knn7s:160/proxy/: foo (200; 15.838235ms)
    Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/http:proxy-service-fndqc-knn7s:1080/proxy/rewriteme">... (200; 15.878735ms)
    Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/: <a href="/api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:443/proxy/tlsrewritem... (200; 15.910436ms)
    Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/services/proxy-service-fndqc:portname2/proxy/: bar (200; 16.000536ms)
    Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/pods/https:proxy-service-fndqc-knn7s:460/proxy/: tls baz (200; 16.241436ms)
    Sep  3 21:43:49.261: INFO: (19) /api/v1/namespaces/proxy-8720/services/http:proxy-service-fndqc:portname2/proxy/: bar (200; 15.819736ms)
    STEP: deleting ReplicationController proxy-service-fndqc in namespace proxy-8720, will wait for the garbage collector to delete the pods 09/03/22 21:43:49.262
    Sep  3 21:43:49.318: INFO: Deleting ReplicationController proxy-service-fndqc took: 3.520608ms
    Sep  3 21:43:49.419: INFO: Terminating ReplicationController proxy-service-fndqc pods took: 100.428526ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Sep  3 21:43:51.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-8720" for this suite. 09/03/22 21:43:51.027
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:43:51.045
Sep  3 21:43:51.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename statefulset 09/03/22 21:43:51.049
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:43:51.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:43:51.092
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6771 09/03/22 21:43:51.097
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-6771 09/03/22 21:43:51.104
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6771 09/03/22 21:43:51.112
Sep  3 21:43:51.116: INFO: Found 0 stateful pods, waiting for 1
Sep  3 21:44:01.119: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/03/22 21:44:01.119
Sep  3 21:44:01.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 21:44:01.260: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 21:44:01.260: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 21:44:01.260: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 21:44:01.262: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  3 21:44:11.267: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 21:44:11.267: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 21:44:11.278: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep  3 21:44:11.278: INFO: ss-0  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:43:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:43:51 +0000 UTC  }]
Sep  3 21:44:11.278: INFO: 
Sep  3 21:44:11.278: INFO: StatefulSet ss has not reached scale 3, at 1
Sep  3 21:44:12.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994926919s
Sep  3 21:44:13.285: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.9921186s
Sep  3 21:44:14.288: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988517879s
Sep  3 21:44:15.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98563516s
Sep  3 21:44:16.294: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982686541s
Sep  3 21:44:17.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978969919s
Sep  3 21:44:18.304: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97212319s
Sep  3 21:44:19.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967526966s
Sep  3 21:44:20.316: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.066614ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6771 09/03/22 21:44:21.316
Sep  3 21:44:21.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 21:44:21.460: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 21:44:21.460: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 21:44:21.460: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 21:44:21.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 21:44:21.645: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  3 21:44:21.645: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 21:44:21.645: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 21:44:21.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 21:44:21.776: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  3 21:44:21.777: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 21:44:21.777: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 21:44:21.779: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 21:44:21.780: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 21:44:21.780: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 09/03/22 21:44:21.78
Sep  3 21:44:21.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 21:44:21.925: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 21:44:21.925: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 21:44:21.925: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 21:44:21.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 21:44:22.119: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 21:44:22.119: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 21:44:22.119: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 21:44:22.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 21:44:22.257: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 21:44:22.257: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 21:44:22.257: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 21:44:22.257: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 21:44:22.259: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  3 21:44:32.268: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 21:44:32.268: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 21:44:32.268: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 21:44:32.279: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep  3 21:44:32.279: INFO: ss-0  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:43:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:43:51 +0000 UTC  }]
Sep  3 21:44:32.279: INFO: ss-1  kind-worker   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:11 +0000 UTC  }]
Sep  3 21:44:32.279: INFO: ss-2  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:11 +0000 UTC  }]
Sep  3 21:44:32.279: INFO: 
Sep  3 21:44:32.279: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  3 21:44:33.282: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep  3 21:44:33.282: INFO: ss-1  kind-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:11 +0000 UTC  }]
Sep  3 21:44:33.282: INFO: 
Sep  3 21:44:33.282: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  3 21:44:34.284: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993142436s
Sep  3 21:44:35.287: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.990117055s
Sep  3 21:44:36.290: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.987374963s
Sep  3 21:44:37.293: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.984131162s
Sep  3 21:44:38.298: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.98099706s
Sep  3 21:44:39.301: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.975927254s
Sep  3 21:44:40.304: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.973092553s
Sep  3 21:44:41.307: INFO: Verifying statefulset ss doesn't scale past 0 for another 970.436554ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6771 09/03/22 21:44:42.308
Sep  3 21:44:42.310: INFO: Scaling statefulset ss to 0
Sep  3 21:44:42.315: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Sep  3 21:44:42.317: INFO: Deleting all statefulset in ns statefulset-6771
Sep  3 21:44:42.319: INFO: Scaling statefulset ss to 0
Sep  3 21:44:42.325: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 21:44:42.327: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Sep  3 21:44:42.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6771" for this suite. 09/03/22 21:44:42.339
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":233,"skipped":4282,"failed":0}
------------------------------
• [SLOW TEST] [51.299 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:43:51.045
    Sep  3 21:43:51.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename statefulset 09/03/22 21:43:51.049
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:43:51.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:43:51.092
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6771 09/03/22 21:43:51.097
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-6771 09/03/22 21:43:51.104
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6771 09/03/22 21:43:51.112
    Sep  3 21:43:51.116: INFO: Found 0 stateful pods, waiting for 1
    Sep  3 21:44:01.119: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/03/22 21:44:01.119
    Sep  3 21:44:01.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  3 21:44:01.260: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  3 21:44:01.260: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  3 21:44:01.260: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  3 21:44:01.262: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  3 21:44:11.267: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  3 21:44:11.267: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  3 21:44:11.278: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Sep  3 21:44:11.278: INFO: ss-0  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:43:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:43:51 +0000 UTC  }]
    Sep  3 21:44:11.278: INFO: 
    Sep  3 21:44:11.278: INFO: StatefulSet ss has not reached scale 3, at 1
    Sep  3 21:44:12.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994926919s
    Sep  3 21:44:13.285: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.9921186s
    Sep  3 21:44:14.288: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988517879s
    Sep  3 21:44:15.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98563516s
    Sep  3 21:44:16.294: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982686541s
    Sep  3 21:44:17.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978969919s
    Sep  3 21:44:18.304: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97212319s
    Sep  3 21:44:19.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967526966s
    Sep  3 21:44:20.316: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.066614ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6771 09/03/22 21:44:21.316
    Sep  3 21:44:21.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  3 21:44:21.460: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  3 21:44:21.460: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  3 21:44:21.460: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  3 21:44:21.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  3 21:44:21.645: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  3 21:44:21.645: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  3 21:44:21.645: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  3 21:44:21.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  3 21:44:21.776: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  3 21:44:21.777: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  3 21:44:21.777: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  3 21:44:21.779: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 21:44:21.780: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  3 21:44:21.780: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 09/03/22 21:44:21.78
    Sep  3 21:44:21.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  3 21:44:21.925: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  3 21:44:21.925: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  3 21:44:21.925: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  3 21:44:21.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  3 21:44:22.119: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  3 21:44:22.119: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  3 21:44:22.119: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  3 21:44:22.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=statefulset-6771 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  3 21:44:22.257: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  3 21:44:22.257: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  3 21:44:22.257: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  3 21:44:22.257: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  3 21:44:22.259: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Sep  3 21:44:32.268: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  3 21:44:32.268: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  3 21:44:32.268: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  3 21:44:32.279: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Sep  3 21:44:32.279: INFO: ss-0  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:43:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:43:51 +0000 UTC  }]
    Sep  3 21:44:32.279: INFO: ss-1  kind-worker   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:11 +0000 UTC  }]
    Sep  3 21:44:32.279: INFO: ss-2  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:11 +0000 UTC  }]
    Sep  3 21:44:32.279: INFO: 
    Sep  3 21:44:32.279: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep  3 21:44:33.282: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
    Sep  3 21:44:33.282: INFO: ss-1  kind-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:44:11 +0000 UTC  }]
    Sep  3 21:44:33.282: INFO: 
    Sep  3 21:44:33.282: INFO: StatefulSet ss has not reached scale 0, at 1
    Sep  3 21:44:34.284: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993142436s
    Sep  3 21:44:35.287: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.990117055s
    Sep  3 21:44:36.290: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.987374963s
    Sep  3 21:44:37.293: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.984131162s
    Sep  3 21:44:38.298: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.98099706s
    Sep  3 21:44:39.301: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.975927254s
    Sep  3 21:44:40.304: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.973092553s
    Sep  3 21:44:41.307: INFO: Verifying statefulset ss doesn't scale past 0 for another 970.436554ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6771 09/03/22 21:44:42.308
    Sep  3 21:44:42.310: INFO: Scaling statefulset ss to 0
    Sep  3 21:44:42.315: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Sep  3 21:44:42.317: INFO: Deleting all statefulset in ns statefulset-6771
    Sep  3 21:44:42.319: INFO: Scaling statefulset ss to 0
    Sep  3 21:44:42.325: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  3 21:44:42.327: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Sep  3 21:44:42.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6771" for this suite. 09/03/22 21:44:42.339
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:44:42.346
Sep  3 21:44:42.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename svc-latency 09/03/22 21:44:42.347
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:44:42.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:44:42.362
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Sep  3 21:44:42.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3777 09/03/22 21:44:42.365
I0903 21:44:42.369001      24 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3777, replica count: 1
I0903 21:44:43.419557      24 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 21:44:43.537: INFO: Created: latency-svc-km85v
Sep  3 21:44:43.551: INFO: Got endpoints: latency-svc-km85v [29.74498ms]
Sep  3 21:44:43.572: INFO: Created: latency-svc-7jz7r
Sep  3 21:44:43.600: INFO: Got endpoints: latency-svc-7jz7r [47.917129ms]
Sep  3 21:44:43.600: INFO: Created: latency-svc-226gx
Sep  3 21:44:43.624: INFO: Got endpoints: latency-svc-226gx [71.966494ms]
Sep  3 21:44:43.627: INFO: Created: latency-svc-bvswr
Sep  3 21:44:43.642: INFO: Got endpoints: latency-svc-bvswr [89.950343ms]
Sep  3 21:44:43.650: INFO: Created: latency-svc-j62nv
Sep  3 21:44:43.657: INFO: Created: latency-svc-gl6qj
Sep  3 21:44:43.659: INFO: Got endpoints: latency-svc-j62nv [106.738087ms]
Sep  3 21:44:43.669: INFO: Got endpoints: latency-svc-gl6qj [116.367914ms]
Sep  3 21:44:43.683: INFO: Created: latency-svc-qwd8t
Sep  3 21:44:43.689: INFO: Got endpoints: latency-svc-qwd8t [136.286367ms]
Sep  3 21:44:43.694: INFO: Created: latency-svc-vbgdd
Sep  3 21:44:43.704: INFO: Got endpoints: latency-svc-vbgdd [152.22801ms]
Sep  3 21:44:43.713: INFO: Created: latency-svc-2jjhb
Sep  3 21:44:43.718: INFO: Created: latency-svc-jjl4d
Sep  3 21:44:43.724: INFO: Got endpoints: latency-svc-2jjhb [169.971858ms]
Sep  3 21:44:43.735: INFO: Created: latency-svc-ptlqc
Sep  3 21:44:43.737: INFO: Got endpoints: latency-svc-jjl4d [184.564897ms]
Sep  3 21:44:43.754: INFO: Got endpoints: latency-svc-ptlqc [200.545436ms]
Sep  3 21:44:43.757: INFO: Created: latency-svc-mcsfd
Sep  3 21:44:43.767: INFO: Got endpoints: latency-svc-mcsfd [213.610465ms]
Sep  3 21:44:43.770: INFO: Created: latency-svc-sz5cl
Sep  3 21:44:43.776: INFO: Got endpoints: latency-svc-sz5cl [222.943785ms]
Sep  3 21:44:43.782: INFO: Created: latency-svc-9lrb7
Sep  3 21:44:43.786: INFO: Got endpoints: latency-svc-9lrb7 [233.028407ms]
Sep  3 21:44:43.793: INFO: Created: latency-svc-7ntf9
Sep  3 21:44:43.800: INFO: Got endpoints: latency-svc-7ntf9 [246.822338ms]
Sep  3 21:44:43.806: INFO: Created: latency-svc-jdr5k
Sep  3 21:44:43.807: INFO: Got endpoints: latency-svc-jdr5k [254.266054ms]
Sep  3 21:44:43.816: INFO: Created: latency-svc-lp6kz
Sep  3 21:44:43.818: INFO: Created: latency-svc-zjcr6
Sep  3 21:44:43.825: INFO: Got endpoints: latency-svc-zjcr6 [224.765966ms]
Sep  3 21:44:43.834: INFO: Created: latency-svc-8fwb5
Sep  3 21:44:43.836: INFO: Created: latency-svc-86vxm
Sep  3 21:44:43.837: INFO: Got endpoints: latency-svc-8fwb5 [194.236277ms]
Sep  3 21:44:43.839: INFO: Got endpoints: latency-svc-lp6kz [214.891231ms]
Sep  3 21:44:43.845: INFO: Got endpoints: latency-svc-86vxm [185.026748ms]
Sep  3 21:44:43.846: INFO: Created: latency-svc-mkvd6
Sep  3 21:44:43.852: INFO: Got endpoints: latency-svc-mkvd6 [183.12104ms]
Sep  3 21:44:43.859: INFO: Created: latency-svc-dvzck
Sep  3 21:44:43.862: INFO: Created: latency-svc-rrwxm
Sep  3 21:44:43.866: INFO: Got endpoints: latency-svc-dvzck [176.855916ms]
Sep  3 21:44:43.871: INFO: Got endpoints: latency-svc-rrwxm [166.132485ms]
Sep  3 21:44:43.872: INFO: Created: latency-svc-qcdsj
Sep  3 21:44:43.879: INFO: Got endpoints: latency-svc-qcdsj [155.235751ms]
Sep  3 21:44:43.883: INFO: Created: latency-svc-2tpp5
Sep  3 21:44:43.887: INFO: Got endpoints: latency-svc-2tpp5 [149.926633ms]
Sep  3 21:44:43.893: INFO: Created: latency-svc-flkkl
Sep  3 21:44:43.898: INFO: Got endpoints: latency-svc-flkkl [144.747116ms]
Sep  3 21:44:43.963: INFO: Created: latency-svc-kjfq5
Sep  3 21:44:43.986: INFO: Created: latency-svc-94whs
Sep  3 21:44:43.986: INFO: Created: latency-svc-7rkkk
Sep  3 21:44:43.987: INFO: Created: latency-svc-9spst
Sep  3 21:44:43.987: INFO: Created: latency-svc-xk5lf
Sep  3 21:44:43.987: INFO: Created: latency-svc-xwhvd
Sep  3 21:44:43.987: INFO: Created: latency-svc-fxrf8
Sep  3 21:44:43.987: INFO: Created: latency-svc-thk8m
Sep  3 21:44:43.987: INFO: Created: latency-svc-qff7v
Sep  3 21:44:43.987: INFO: Created: latency-svc-rphdj
Sep  3 21:44:43.987: INFO: Created: latency-svc-dbf2x
Sep  3 21:44:43.987: INFO: Created: latency-svc-qfx55
Sep  3 21:44:43.988: INFO: Created: latency-svc-8b7xg
Sep  3 21:44:43.988: INFO: Created: latency-svc-m5k6v
Sep  3 21:44:43.988: INFO: Created: latency-svc-dlld5
Sep  3 21:44:43.992: INFO: Got endpoints: latency-svc-94whs [185.111705ms]
Sep  3 21:44:43.992: INFO: Got endpoints: latency-svc-kjfq5 [104.87273ms]
Sep  3 21:44:43.993: INFO: Got endpoints: latency-svc-xk5lf [114.30395ms]
Sep  3 21:44:44.023: INFO: Got endpoints: latency-svc-7rkkk [124.601372ms]
Sep  3 21:44:44.024: INFO: Got endpoints: latency-svc-dlld5 [257.037662ms]
Sep  3 21:44:44.033: INFO: Got endpoints: latency-svc-m5k6v [256.282861ms]
Sep  3 21:44:44.036: INFO: Got endpoints: latency-svc-dbf2x [235.412015ms]
Sep  3 21:44:44.036: INFO: Got endpoints: latency-svc-8b7xg [249.584146ms]
Sep  3 21:44:44.045: INFO: Created: latency-svc-4k5s7
Sep  3 21:44:44.045: INFO: Got endpoints: latency-svc-qff7v [200.624938ms]
Sep  3 21:44:44.049: INFO: Got endpoints: latency-svc-qfx55 [212.413365ms]
Sep  3 21:44:44.049: INFO: Got endpoints: latency-svc-xwhvd [224.645191ms]
Sep  3 21:44:44.061: INFO: Got endpoints: latency-svc-rphdj [221.299684ms]
Sep  3 21:44:44.061: INFO: Got endpoints: latency-svc-thk8m [194.836626ms]
Sep  3 21:44:44.062: INFO: Got endpoints: latency-svc-fxrf8 [209.843659ms]
Sep  3 21:44:44.063: INFO: Created: latency-svc-nn826
Sep  3 21:44:44.071: INFO: Created: latency-svc-wp2c5
Sep  3 21:44:44.074: INFO: Created: latency-svc-5qjwz
Sep  3 21:44:44.078: INFO: Created: latency-svc-l7wt6
Sep  3 21:44:44.082: INFO: Created: latency-svc-sq28w
Sep  3 21:44:44.087: INFO: Created: latency-svc-2gb6r
Sep  3 21:44:44.091: INFO: Got endpoints: latency-svc-9spst [220.113582ms]
Sep  3 21:44:44.097: INFO: Created: latency-svc-9zl45
Sep  3 21:44:44.100: INFO: Created: latency-svc-bw9hb
Sep  3 21:44:44.103: INFO: Created: latency-svc-h42vd
Sep  3 21:44:44.107: INFO: Created: latency-svc-kkn48
Sep  3 21:44:44.111: INFO: Created: latency-svc-q2bhh
Sep  3 21:44:44.116: INFO: Created: latency-svc-z9f66
Sep  3 21:44:44.124: INFO: Created: latency-svc-57ln7
Sep  3 21:44:44.130: INFO: Created: latency-svc-xh7vv
Sep  3 21:44:44.142: INFO: Got endpoints: latency-svc-4k5s7 [149.842628ms]
Sep  3 21:44:44.150: INFO: Created: latency-svc-ck5ls
Sep  3 21:44:44.191: INFO: Got endpoints: latency-svc-nn826 [167.119865ms]
Sep  3 21:44:44.198: INFO: Created: latency-svc-ltr78
Sep  3 21:44:44.239: INFO: Got endpoints: latency-svc-wp2c5 [215.087171ms]
Sep  3 21:44:44.247: INFO: Created: latency-svc-w9ssv
Sep  3 21:44:44.287: INFO: Got endpoints: latency-svc-5qjwz [293.299741ms]
Sep  3 21:44:44.295: INFO: Created: latency-svc-dcl8l
Sep  3 21:44:44.338: INFO: Got endpoints: latency-svc-l7wt6 [345.128255ms]
Sep  3 21:44:44.348: INFO: Created: latency-svc-45xbf
Sep  3 21:44:44.387: INFO: Got endpoints: latency-svc-sq28w [354.508475ms]
Sep  3 21:44:44.395: INFO: Created: latency-svc-njwzb
Sep  3 21:44:44.441: INFO: Got endpoints: latency-svc-2gb6r [404.531585ms]
Sep  3 21:44:44.447: INFO: Created: latency-svc-kbdrx
Sep  3 21:44:44.490: INFO: Got endpoints: latency-svc-9zl45 [453.733692ms]
Sep  3 21:44:44.496: INFO: Created: latency-svc-kmzvv
Sep  3 21:44:44.545: INFO: Got endpoints: latency-svc-bw9hb [499.626292ms]
Sep  3 21:44:44.556: INFO: Created: latency-svc-t2x7c
Sep  3 21:44:44.593: INFO: Got endpoints: latency-svc-h42vd [543.495589ms]
Sep  3 21:44:44.604: INFO: Created: latency-svc-z5sr5
Sep  3 21:44:44.642: INFO: Got endpoints: latency-svc-kkn48 [592.410096ms]
Sep  3 21:44:44.659: INFO: Created: latency-svc-grjwh
Sep  3 21:44:44.691: INFO: Got endpoints: latency-svc-q2bhh [629.868378ms]
Sep  3 21:44:44.702: INFO: Created: latency-svc-9s2d8
Sep  3 21:44:44.739: INFO: Got endpoints: latency-svc-z9f66 [677.152381ms]
Sep  3 21:44:44.747: INFO: Created: latency-svc-6cj4p
Sep  3 21:44:44.788: INFO: Got endpoints: latency-svc-57ln7 [726.92049ms]
Sep  3 21:44:44.796: INFO: Created: latency-svc-56gjg
Sep  3 21:44:44.837: INFO: Got endpoints: latency-svc-xh7vv [745.840831ms]
Sep  3 21:44:44.845: INFO: Created: latency-svc-qtvm5
Sep  3 21:44:44.888: INFO: Got endpoints: latency-svc-ck5ls [745.734431ms]
Sep  3 21:44:44.897: INFO: Created: latency-svc-dxmh5
Sep  3 21:44:44.940: INFO: Got endpoints: latency-svc-ltr78 [749.399139ms]
Sep  3 21:44:44.946: INFO: Created: latency-svc-wtkxz
Sep  3 21:44:44.988: INFO: Got endpoints: latency-svc-w9ssv [748.092436ms]
Sep  3 21:44:44.995: INFO: Created: latency-svc-gld2w
Sep  3 21:44:45.038: INFO: Got endpoints: latency-svc-dcl8l [751.088742ms]
Sep  3 21:44:45.045: INFO: Created: latency-svc-kp2xm
Sep  3 21:44:45.090: INFO: Got endpoints: latency-svc-45xbf [752.448245ms]
Sep  3 21:44:45.096: INFO: Created: latency-svc-pgwvc
Sep  3 21:44:45.140: INFO: Got endpoints: latency-svc-njwzb [752.822446ms]
Sep  3 21:44:45.146: INFO: Created: latency-svc-7mwfm
Sep  3 21:44:45.191: INFO: Got endpoints: latency-svc-kbdrx [750.485541ms]
Sep  3 21:44:45.201: INFO: Created: latency-svc-59w9d
Sep  3 21:44:45.239: INFO: Got endpoints: latency-svc-kmzvv [748.660437ms]
Sep  3 21:44:45.245: INFO: Created: latency-svc-cchqm
Sep  3 21:44:45.287: INFO: Got endpoints: latency-svc-t2x7c [741.885022ms]
Sep  3 21:44:45.296: INFO: Created: latency-svc-6jqpw
Sep  3 21:44:45.341: INFO: Got endpoints: latency-svc-z5sr5 [746.545832ms]
Sep  3 21:44:45.351: INFO: Created: latency-svc-vprjx
Sep  3 21:44:45.390: INFO: Got endpoints: latency-svc-grjwh [748.131736ms]
Sep  3 21:44:45.398: INFO: Created: latency-svc-64vmg
Sep  3 21:44:45.437: INFO: Got endpoints: latency-svc-9s2d8 [745.558531ms]
Sep  3 21:44:45.443: INFO: Created: latency-svc-c8d4r
Sep  3 21:44:45.488: INFO: Got endpoints: latency-svc-6cj4p [747.986836ms]
Sep  3 21:44:45.502: INFO: Created: latency-svc-rpc8b
Sep  3 21:44:45.542: INFO: Got endpoints: latency-svc-56gjg [753.519848ms]
Sep  3 21:44:45.557: INFO: Created: latency-svc-65w88
Sep  3 21:44:45.587: INFO: Got endpoints: latency-svc-qtvm5 [749.88294ms]
Sep  3 21:44:45.612: INFO: Created: latency-svc-cthv7
Sep  3 21:44:45.647: INFO: Got endpoints: latency-svc-dxmh5 [758.936959ms]
Sep  3 21:44:45.665: INFO: Created: latency-svc-cgpjx
Sep  3 21:44:45.695: INFO: Got endpoints: latency-svc-wtkxz [754.020149ms]
Sep  3 21:44:45.708: INFO: Created: latency-svc-8gbkq
Sep  3 21:44:45.739: INFO: Got endpoints: latency-svc-gld2w [751.716244ms]
Sep  3 21:44:45.746: INFO: Created: latency-svc-s9x9x
Sep  3 21:44:45.789: INFO: Got endpoints: latency-svc-kp2xm [750.995742ms]
Sep  3 21:44:45.796: INFO: Created: latency-svc-75tjk
Sep  3 21:44:45.837: INFO: Got endpoints: latency-svc-pgwvc [746.641733ms]
Sep  3 21:44:45.846: INFO: Created: latency-svc-mgspx
Sep  3 21:44:45.889: INFO: Got endpoints: latency-svc-7mwfm [747.956336ms]
Sep  3 21:44:45.896: INFO: Created: latency-svc-jrkzh
Sep  3 21:44:45.937: INFO: Got endpoints: latency-svc-59w9d [745.794531ms]
Sep  3 21:44:45.946: INFO: Created: latency-svc-vnhxv
Sep  3 21:44:45.991: INFO: Got endpoints: latency-svc-cchqm [751.829744ms]
Sep  3 21:44:45.998: INFO: Created: latency-svc-h8nrm
Sep  3 21:44:46.040: INFO: Got endpoints: latency-svc-6jqpw [752.624446ms]
Sep  3 21:44:46.048: INFO: Created: latency-svc-cvjnx
Sep  3 21:44:46.087: INFO: Got endpoints: latency-svc-vprjx [745.47403ms]
Sep  3 21:44:46.095: INFO: Created: latency-svc-hkf9s
Sep  3 21:44:46.140: INFO: Got endpoints: latency-svc-64vmg [750.115441ms]
Sep  3 21:44:46.147: INFO: Created: latency-svc-q8zvf
Sep  3 21:44:46.187: INFO: Got endpoints: latency-svc-c8d4r [749.83344ms]
Sep  3 21:44:46.197: INFO: Created: latency-svc-7qpq8
Sep  3 21:44:46.238: INFO: Got endpoints: latency-svc-rpc8b [750.515441ms]
Sep  3 21:44:46.248: INFO: Created: latency-svc-s7hr7
Sep  3 21:44:46.288: INFO: Got endpoints: latency-svc-65w88 [746.139532ms]
Sep  3 21:44:46.301: INFO: Created: latency-svc-kgdj5
Sep  3 21:44:46.338: INFO: Got endpoints: latency-svc-cthv7 [750.861542ms]
Sep  3 21:44:46.348: INFO: Created: latency-svc-nl2l7
Sep  3 21:44:46.387: INFO: Got endpoints: latency-svc-cgpjx [739.920618ms]
Sep  3 21:44:46.394: INFO: Created: latency-svc-gv45v
Sep  3 21:44:46.437: INFO: Got endpoints: latency-svc-8gbkq [742.079723ms]
Sep  3 21:44:46.453: INFO: Created: latency-svc-q4rhv
Sep  3 21:44:46.488: INFO: Got endpoints: latency-svc-s9x9x [748.428036ms]
Sep  3 21:44:46.496: INFO: Created: latency-svc-wtkst
Sep  3 21:44:46.539: INFO: Got endpoints: latency-svc-75tjk [749.64004ms]
Sep  3 21:44:46.588: INFO: Created: latency-svc-mnvdt
Sep  3 21:44:46.606: INFO: Got endpoints: latency-svc-mgspx [768.07848ms]
Sep  3 21:44:46.634: INFO: Created: latency-svc-r6s9s
Sep  3 21:44:46.647: INFO: Got endpoints: latency-svc-jrkzh [757.015856ms]
Sep  3 21:44:46.662: INFO: Created: latency-svc-bwb76
Sep  3 21:44:46.689: INFO: Got endpoints: latency-svc-vnhxv [751.776144ms]
Sep  3 21:44:46.696: INFO: Created: latency-svc-9zr8r
Sep  3 21:44:46.740: INFO: Got endpoints: latency-svc-h8nrm [749.641739ms]
Sep  3 21:44:46.747: INFO: Created: latency-svc-qj54j
Sep  3 21:44:46.790: INFO: Got endpoints: latency-svc-cvjnx [747.908135ms]
Sep  3 21:44:46.796: INFO: Created: latency-svc-4hpk6
Sep  3 21:44:46.840: INFO: Got endpoints: latency-svc-hkf9s [753.499848ms]
Sep  3 21:44:46.847: INFO: Created: latency-svc-fs7zc
Sep  3 21:44:46.888: INFO: Got endpoints: latency-svc-q8zvf [747.697635ms]
Sep  3 21:44:46.898: INFO: Created: latency-svc-wchmt
Sep  3 21:44:46.939: INFO: Got endpoints: latency-svc-7qpq8 [751.387843ms]
Sep  3 21:44:46.951: INFO: Created: latency-svc-4gkvs
Sep  3 21:44:46.990: INFO: Got endpoints: latency-svc-s7hr7 [751.358344ms]
Sep  3 21:44:46.995: INFO: Created: latency-svc-r65vh
Sep  3 21:44:47.039: INFO: Got endpoints: latency-svc-kgdj5 [750.811042ms]
Sep  3 21:44:47.044: INFO: Created: latency-svc-wrsbq
Sep  3 21:44:47.090: INFO: Got endpoints: latency-svc-nl2l7 [751.172443ms]
Sep  3 21:44:47.097: INFO: Created: latency-svc-b55nx
Sep  3 21:44:47.138: INFO: Got endpoints: latency-svc-gv45v [750.722441ms]
Sep  3 21:44:47.159: INFO: Created: latency-svc-xms4s
Sep  3 21:44:47.194: INFO: Got endpoints: latency-svc-q4rhv [756.565554ms]
Sep  3 21:44:47.202: INFO: Created: latency-svc-ldlj8
Sep  3 21:44:47.242: INFO: Got endpoints: latency-svc-wtkst [753.598248ms]
Sep  3 21:44:47.251: INFO: Created: latency-svc-kbnzk
Sep  3 21:44:47.287: INFO: Got endpoints: latency-svc-mnvdt [748.472837ms]
Sep  3 21:44:47.297: INFO: Created: latency-svc-tb24j
Sep  3 21:44:47.342: INFO: Got endpoints: latency-svc-r6s9s [735.969709ms]
Sep  3 21:44:47.363: INFO: Created: latency-svc-n6gw6
Sep  3 21:44:47.389: INFO: Got endpoints: latency-svc-bwb76 [742.369624ms]
Sep  3 21:44:47.409: INFO: Created: latency-svc-8zmjp
Sep  3 21:44:47.440: INFO: Got endpoints: latency-svc-9zr8r [750.921342ms]
Sep  3 21:44:47.462: INFO: Created: latency-svc-zw9qt
Sep  3 21:44:47.489: INFO: Got endpoints: latency-svc-qj54j [748.627238ms]
Sep  3 21:44:47.500: INFO: Created: latency-svc-cw6cc
Sep  3 21:44:47.547: INFO: Got endpoints: latency-svc-4hpk6 [756.701354ms]
Sep  3 21:44:47.564: INFO: Created: latency-svc-jl8zp
Sep  3 21:44:47.598: INFO: Got endpoints: latency-svc-fs7zc [756.922755ms]
Sep  3 21:44:47.614: INFO: Created: latency-svc-p8rv5
Sep  3 21:44:47.643: INFO: Got endpoints: latency-svc-wchmt [754.40255ms]
Sep  3 21:44:47.661: INFO: Created: latency-svc-sbxdv
Sep  3 21:44:47.688: INFO: Got endpoints: latency-svc-4gkvs [748.773137ms]
Sep  3 21:44:47.698: INFO: Created: latency-svc-p5hwh
Sep  3 21:44:47.738: INFO: Got endpoints: latency-svc-r65vh [747.898935ms]
Sep  3 21:44:47.750: INFO: Created: latency-svc-dln97
Sep  3 21:44:47.790: INFO: Got endpoints: latency-svc-wrsbq [750.528442ms]
Sep  3 21:44:47.796: INFO: Created: latency-svc-qrmb4
Sep  3 21:44:47.838: INFO: Got endpoints: latency-svc-b55nx [748.137236ms]
Sep  3 21:44:47.847: INFO: Created: latency-svc-m2pkx
Sep  3 21:44:47.889: INFO: Got endpoints: latency-svc-xms4s [750.419941ms]
Sep  3 21:44:47.895: INFO: Created: latency-svc-4vtbj
Sep  3 21:44:47.937: INFO: Got endpoints: latency-svc-ldlj8 [742.891424ms]
Sep  3 21:44:47.949: INFO: Created: latency-svc-7nhts
Sep  3 21:44:47.987: INFO: Got endpoints: latency-svc-kbnzk [745.69023ms]
Sep  3 21:44:47.996: INFO: Created: latency-svc-m68cf
Sep  3 21:44:48.038: INFO: Got endpoints: latency-svc-tb24j [750.584842ms]
Sep  3 21:44:48.047: INFO: Created: latency-svc-cz5ft
Sep  3 21:44:48.089: INFO: Got endpoints: latency-svc-n6gw6 [746.726533ms]
Sep  3 21:44:48.096: INFO: Created: latency-svc-k7btn
Sep  3 21:44:48.138: INFO: Got endpoints: latency-svc-8zmjp [748.426537ms]
Sep  3 21:44:48.146: INFO: Created: latency-svc-7mt92
Sep  3 21:44:48.189: INFO: Got endpoints: latency-svc-zw9qt [748.043336ms]
Sep  3 21:44:48.196: INFO: Created: latency-svc-s4bl2
Sep  3 21:44:48.237: INFO: Got endpoints: latency-svc-cw6cc [747.716135ms]
Sep  3 21:44:48.246: INFO: Created: latency-svc-pfj92
Sep  3 21:44:48.337: INFO: Got endpoints: latency-svc-jl8zp [788.157424ms]
Sep  3 21:44:48.347: INFO: Created: latency-svc-7lc88
Sep  3 21:44:48.391: INFO: Got endpoints: latency-svc-p8rv5 [792.715133ms]
Sep  3 21:44:48.397: INFO: Created: latency-svc-5qh44
Sep  3 21:44:48.442: INFO: Got endpoints: latency-svc-sbxdv [798.970847ms]
Sep  3 21:44:48.450: INFO: Created: latency-svc-c8jqm
Sep  3 21:44:48.487: INFO: Got endpoints: latency-svc-p5hwh [799.126247ms]
Sep  3 21:44:48.495: INFO: Created: latency-svc-pkxn6
Sep  3 21:44:48.540: INFO: Got endpoints: latency-svc-dln97 [801.836653ms]
Sep  3 21:44:48.565: INFO: Created: latency-svc-5r64v
Sep  3 21:44:48.606: INFO: Got endpoints: latency-svc-qrmb4 [815.859684ms]
Sep  3 21:44:48.621: INFO: Created: latency-svc-xt4ss
Sep  3 21:44:48.640: INFO: Got endpoints: latency-svc-m2pkx [802.125054ms]
Sep  3 21:44:48.657: INFO: Created: latency-svc-zhr6s
Sep  3 21:44:48.691: INFO: Got endpoints: latency-svc-4vtbj [802.002454ms]
Sep  3 21:44:48.704: INFO: Created: latency-svc-fh4mx
Sep  3 21:44:48.738: INFO: Got endpoints: latency-svc-7nhts [800.630351ms]
Sep  3 21:44:48.746: INFO: Created: latency-svc-k7qt5
Sep  3 21:44:48.789: INFO: Got endpoints: latency-svc-m68cf [801.042952ms]
Sep  3 21:44:48.796: INFO: Created: latency-svc-kwqqv
Sep  3 21:44:48.838: INFO: Got endpoints: latency-svc-cz5ft [798.782647ms]
Sep  3 21:44:48.844: INFO: Created: latency-svc-2sktw
Sep  3 21:44:48.889: INFO: Got endpoints: latency-svc-k7btn [799.324248ms]
Sep  3 21:44:48.895: INFO: Created: latency-svc-wpbkh
Sep  3 21:44:48.938: INFO: Got endpoints: latency-svc-7mt92 [799.576648ms]
Sep  3 21:44:48.943: INFO: Created: latency-svc-s4xpp
Sep  3 21:44:48.988: INFO: Got endpoints: latency-svc-s4bl2 [799.335548ms]
Sep  3 21:44:48.998: INFO: Created: latency-svc-7gw55
Sep  3 21:44:49.042: INFO: Got endpoints: latency-svc-pfj92 [804.267958ms]
Sep  3 21:44:49.049: INFO: Created: latency-svc-jhjfs
Sep  3 21:44:49.092: INFO: Got endpoints: latency-svc-7lc88 [754.875751ms]
Sep  3 21:44:49.103: INFO: Created: latency-svc-mk8mr
Sep  3 21:44:49.144: INFO: Got endpoints: latency-svc-5qh44 [753.022447ms]
Sep  3 21:44:49.150: INFO: Created: latency-svc-nl2qh
Sep  3 21:44:49.190: INFO: Got endpoints: latency-svc-c8jqm [747.913735ms]
Sep  3 21:44:49.204: INFO: Created: latency-svc-94twt
Sep  3 21:44:49.239: INFO: Got endpoints: latency-svc-pkxn6 [751.945244ms]
Sep  3 21:44:49.249: INFO: Created: latency-svc-dvlw2
Sep  3 21:44:49.288: INFO: Got endpoints: latency-svc-5r64v [747.023034ms]
Sep  3 21:44:49.298: INFO: Created: latency-svc-d4s6f
Sep  3 21:44:49.338: INFO: Got endpoints: latency-svc-xt4ss [732.315502ms]
Sep  3 21:44:49.346: INFO: Created: latency-svc-tvgn8
Sep  3 21:44:49.401: INFO: Got endpoints: latency-svc-zhr6s [760.994164ms]
Sep  3 21:44:49.421: INFO: Created: latency-svc-b4zpj
Sep  3 21:44:49.453: INFO: Got endpoints: latency-svc-fh4mx [756.162954ms]
Sep  3 21:44:49.494: INFO: Created: latency-svc-qf7d2
Sep  3 21:44:49.496: INFO: Got endpoints: latency-svc-k7qt5 [757.472656ms]
Sep  3 21:44:49.525: INFO: Created: latency-svc-9cbbn
Sep  3 21:44:49.559: INFO: Got endpoints: latency-svc-kwqqv [769.832183ms]
Sep  3 21:44:49.589: INFO: Created: latency-svc-7fmps
Sep  3 21:44:49.597: INFO: Got endpoints: latency-svc-2sktw [759.309561ms]
Sep  3 21:44:49.605: INFO: Created: latency-svc-xmknq
Sep  3 21:44:49.642: INFO: Got endpoints: latency-svc-wpbkh [753.037146ms]
Sep  3 21:44:49.657: INFO: Created: latency-svc-cjr4r
Sep  3 21:44:49.696: INFO: Got endpoints: latency-svc-s4xpp [757.594157ms]
Sep  3 21:44:49.707: INFO: Created: latency-svc-r2vt7
Sep  3 21:44:49.740: INFO: Got endpoints: latency-svc-7gw55 [751.998845ms]
Sep  3 21:44:49.748: INFO: Created: latency-svc-7vhpw
Sep  3 21:44:49.794: INFO: Got endpoints: latency-svc-jhjfs [751.368843ms]
Sep  3 21:44:49.802: INFO: Created: latency-svc-fs5sx
Sep  3 21:44:49.837: INFO: Got endpoints: latency-svc-mk8mr [744.918229ms]
Sep  3 21:44:49.847: INFO: Created: latency-svc-95pfx
Sep  3 21:44:49.891: INFO: Got endpoints: latency-svc-nl2qh [746.591732ms]
Sep  3 21:44:49.897: INFO: Created: latency-svc-9fqln
Sep  3 21:44:49.938: INFO: Got endpoints: latency-svc-94twt [747.606735ms]
Sep  3 21:44:49.958: INFO: Created: latency-svc-2c2zz
Sep  3 21:44:49.990: INFO: Got endpoints: latency-svc-dvlw2 [750.982642ms]
Sep  3 21:44:49.998: INFO: Created: latency-svc-vswh4
Sep  3 21:44:50.039: INFO: Got endpoints: latency-svc-d4s6f [751.077243ms]
Sep  3 21:44:50.045: INFO: Created: latency-svc-qnf2k
Sep  3 21:44:50.091: INFO: Got endpoints: latency-svc-tvgn8 [752.660546ms]
Sep  3 21:44:50.099: INFO: Created: latency-svc-b4bfs
Sep  3 21:44:50.138: INFO: Got endpoints: latency-svc-b4zpj [736.669511ms]
Sep  3 21:44:50.149: INFO: Created: latency-svc-ddl75
Sep  3 21:44:50.188: INFO: Got endpoints: latency-svc-qf7d2 [734.625707ms]
Sep  3 21:44:50.195: INFO: Created: latency-svc-g7k4h
Sep  3 21:44:50.239: INFO: Got endpoints: latency-svc-9cbbn [743.439926ms]
Sep  3 21:44:50.249: INFO: Created: latency-svc-qdvld
Sep  3 21:44:50.289: INFO: Got endpoints: latency-svc-7fmps [729.992296ms]
Sep  3 21:44:50.298: INFO: Created: latency-svc-2pln9
Sep  3 21:44:50.338: INFO: Got endpoints: latency-svc-xmknq [740.944021ms]
Sep  3 21:44:50.347: INFO: Created: latency-svc-j766d
Sep  3 21:44:50.389: INFO: Got endpoints: latency-svc-cjr4r [746.653833ms]
Sep  3 21:44:50.397: INFO: Created: latency-svc-hmb8k
Sep  3 21:44:50.439: INFO: Got endpoints: latency-svc-r2vt7 [742.447524ms]
Sep  3 21:44:50.446: INFO: Created: latency-svc-ww9qf
Sep  3 21:44:50.488: INFO: Got endpoints: latency-svc-7vhpw [746.917433ms]
Sep  3 21:44:50.493: INFO: Created: latency-svc-vnbb7
Sep  3 21:44:50.537: INFO: Got endpoints: latency-svc-fs5sx [742.891625ms]
Sep  3 21:44:50.558: INFO: Created: latency-svc-vzsf4
Sep  3 21:44:50.598: INFO: Got endpoints: latency-svc-95pfx [760.273663ms]
Sep  3 21:44:50.615: INFO: Created: latency-svc-64bk8
Sep  3 21:44:50.638: INFO: Got endpoints: latency-svc-9fqln [747.368535ms]
Sep  3 21:44:50.672: INFO: Created: latency-svc-nwrlv
Sep  3 21:44:50.688: INFO: Got endpoints: latency-svc-2c2zz [749.79394ms]
Sep  3 21:44:50.696: INFO: Created: latency-svc-jmpqq
Sep  3 21:44:50.739: INFO: Got endpoints: latency-svc-vswh4 [748.036236ms]
Sep  3 21:44:50.753: INFO: Created: latency-svc-wp2px
Sep  3 21:44:50.789: INFO: Got endpoints: latency-svc-qnf2k [749.512739ms]
Sep  3 21:44:50.798: INFO: Created: latency-svc-7pgl5
Sep  3 21:44:50.837: INFO: Got endpoints: latency-svc-b4bfs [746.069131ms]
Sep  3 21:44:50.856: INFO: Created: latency-svc-znd5z
Sep  3 21:44:50.887: INFO: Got endpoints: latency-svc-ddl75 [748.495837ms]
Sep  3 21:44:50.895: INFO: Created: latency-svc-s6kgp
Sep  3 21:44:50.938: INFO: Got endpoints: latency-svc-g7k4h [749.83774ms]
Sep  3 21:44:50.945: INFO: Created: latency-svc-phfqb
Sep  3 21:44:50.988: INFO: Got endpoints: latency-svc-qdvld [748.102536ms]
Sep  3 21:44:50.997: INFO: Created: latency-svc-t6t48
Sep  3 21:44:51.039: INFO: Got endpoints: latency-svc-2pln9 [749.551439ms]
Sep  3 21:44:51.046: INFO: Created: latency-svc-n79j8
Sep  3 21:44:51.092: INFO: Got endpoints: latency-svc-j766d [752.880847ms]
Sep  3 21:44:51.107: INFO: Created: latency-svc-8mhxv
Sep  3 21:44:51.139: INFO: Got endpoints: latency-svc-hmb8k [749.729939ms]
Sep  3 21:44:51.145: INFO: Created: latency-svc-6c7h2
Sep  3 21:44:51.190: INFO: Got endpoints: latency-svc-ww9qf [750.850742ms]
Sep  3 21:44:51.203: INFO: Created: latency-svc-rlpv2
Sep  3 21:44:51.238: INFO: Got endpoints: latency-svc-vnbb7 [749.822639ms]
Sep  3 21:44:51.246: INFO: Created: latency-svc-2c4c8
Sep  3 21:44:51.289: INFO: Got endpoints: latency-svc-vzsf4 [751.882644ms]
Sep  3 21:44:51.301: INFO: Created: latency-svc-8kjlk
Sep  3 21:44:51.338: INFO: Got endpoints: latency-svc-64bk8 [738.621315ms]
Sep  3 21:44:51.346: INFO: Created: latency-svc-qskg5
Sep  3 21:44:51.390: INFO: Got endpoints: latency-svc-nwrlv [751.609344ms]
Sep  3 21:44:51.397: INFO: Created: latency-svc-9tzl2
Sep  3 21:44:51.437: INFO: Got endpoints: latency-svc-jmpqq [748.590937ms]
Sep  3 21:44:51.489: INFO: Got endpoints: latency-svc-wp2px [750.427841ms]
Sep  3 21:44:51.551: INFO: Got endpoints: latency-svc-7pgl5 [761.885366ms]
Sep  3 21:44:51.591: INFO: Got endpoints: latency-svc-znd5z [753.416548ms]
Sep  3 21:44:51.650: INFO: Got endpoints: latency-svc-s6kgp [762.171867ms]
Sep  3 21:44:51.694: INFO: Got endpoints: latency-svc-phfqb [755.687852ms]
Sep  3 21:44:51.739: INFO: Got endpoints: latency-svc-t6t48 [750.919742ms]
Sep  3 21:44:51.790: INFO: Got endpoints: latency-svc-n79j8 [750.947048ms]
Sep  3 21:44:51.837: INFO: Got endpoints: latency-svc-8mhxv [745.496143ms]
Sep  3 21:44:51.887: INFO: Got endpoints: latency-svc-6c7h2 [747.235454ms]
Sep  3 21:44:51.938: INFO: Got endpoints: latency-svc-rlpv2 [748.125665ms]
Sep  3 21:44:51.990: INFO: Got endpoints: latency-svc-2c4c8 [752.175182ms]
Sep  3 21:44:52.039: INFO: Got endpoints: latency-svc-8kjlk [749.454683ms]
Sep  3 21:44:52.088: INFO: Got endpoints: latency-svc-qskg5 [750.031991ms]
Sep  3 21:44:52.140: INFO: Got endpoints: latency-svc-9tzl2 [750.0443ms]
Sep  3 21:44:52.141: INFO: Latencies: [47.917129ms 71.966494ms 89.950343ms 104.87273ms 106.738087ms 114.30395ms 116.367914ms 124.601372ms 136.286367ms 144.747116ms 149.842628ms 149.926633ms 152.22801ms 155.235751ms 166.132485ms 167.119865ms 169.971858ms 176.855916ms 183.12104ms 184.564897ms 185.026748ms 185.111705ms 194.236277ms 194.836626ms 200.545436ms 200.624938ms 209.843659ms 212.413365ms 213.610465ms 214.891231ms 215.087171ms 220.113582ms 221.299684ms 222.943785ms 224.645191ms 224.765966ms 233.028407ms 235.412015ms 246.822338ms 249.584146ms 254.266054ms 256.282861ms 257.037662ms 293.299741ms 345.128255ms 354.508475ms 404.531585ms 453.733692ms 499.626292ms 543.495589ms 592.410096ms 629.868378ms 677.152381ms 726.92049ms 729.992296ms 732.315502ms 734.625707ms 735.969709ms 736.669511ms 738.621315ms 739.920618ms 740.944021ms 741.885022ms 742.079723ms 742.369624ms 742.447524ms 742.891424ms 742.891625ms 743.439926ms 744.918229ms 745.47403ms 745.496143ms 745.558531ms 745.69023ms 745.734431ms 745.794531ms 745.840831ms 746.069131ms 746.139532ms 746.545832ms 746.591732ms 746.641733ms 746.653833ms 746.726533ms 746.917433ms 747.023034ms 747.235454ms 747.368535ms 747.606735ms 747.697635ms 747.716135ms 747.898935ms 747.908135ms 747.913735ms 747.956336ms 747.986836ms 748.036236ms 748.043336ms 748.092436ms 748.102536ms 748.125665ms 748.131736ms 748.137236ms 748.426537ms 748.428036ms 748.472837ms 748.495837ms 748.590937ms 748.627238ms 748.660437ms 748.773137ms 749.399139ms 749.454683ms 749.512739ms 749.551439ms 749.64004ms 749.641739ms 749.729939ms 749.79394ms 749.822639ms 749.83344ms 749.83774ms 749.88294ms 750.031991ms 750.0443ms 750.115441ms 750.419941ms 750.427841ms 750.485541ms 750.515441ms 750.528442ms 750.584842ms 750.722441ms 750.811042ms 750.850742ms 750.861542ms 750.919742ms 750.921342ms 750.947048ms 750.982642ms 750.995742ms 751.077243ms 751.088742ms 751.172443ms 751.358344ms 751.368843ms 751.387843ms 751.609344ms 751.716244ms 751.776144ms 751.829744ms 751.882644ms 751.945244ms 751.998845ms 752.175182ms 752.448245ms 752.624446ms 752.660546ms 752.822446ms 752.880847ms 753.022447ms 753.037146ms 753.416548ms 753.499848ms 753.519848ms 753.598248ms 754.020149ms 754.40255ms 754.875751ms 755.687852ms 756.162954ms 756.565554ms 756.701354ms 756.922755ms 757.015856ms 757.472656ms 757.594157ms 758.936959ms 759.309561ms 760.273663ms 760.994164ms 761.885366ms 762.171867ms 768.07848ms 769.832183ms 788.157424ms 792.715133ms 798.782647ms 798.970847ms 799.126247ms 799.324248ms 799.335548ms 799.576648ms 800.630351ms 801.042952ms 801.836653ms 802.002454ms 802.125054ms 804.267958ms 815.859684ms]
Sep  3 21:44:52.141: INFO: 50 %ile: 748.125665ms
Sep  3 21:44:52.141: INFO: 90 %ile: 760.994164ms
Sep  3 21:44:52.141: INFO: 99 %ile: 804.267958ms
Sep  3 21:44:52.141: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Sep  3 21:44:52.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3777" for this suite. 09/03/22 21:44:52.146
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":234,"skipped":4303,"failed":0}
------------------------------
• [SLOW TEST] [9.804 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:44:42.346
    Sep  3 21:44:42.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename svc-latency 09/03/22 21:44:42.347
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:44:42.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:44:42.362
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Sep  3 21:44:42.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-3777 09/03/22 21:44:42.365
    I0903 21:44:42.369001      24 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3777, replica count: 1
    I0903 21:44:43.419557      24 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  3 21:44:43.537: INFO: Created: latency-svc-km85v
    Sep  3 21:44:43.551: INFO: Got endpoints: latency-svc-km85v [29.74498ms]
    Sep  3 21:44:43.572: INFO: Created: latency-svc-7jz7r
    Sep  3 21:44:43.600: INFO: Got endpoints: latency-svc-7jz7r [47.917129ms]
    Sep  3 21:44:43.600: INFO: Created: latency-svc-226gx
    Sep  3 21:44:43.624: INFO: Got endpoints: latency-svc-226gx [71.966494ms]
    Sep  3 21:44:43.627: INFO: Created: latency-svc-bvswr
    Sep  3 21:44:43.642: INFO: Got endpoints: latency-svc-bvswr [89.950343ms]
    Sep  3 21:44:43.650: INFO: Created: latency-svc-j62nv
    Sep  3 21:44:43.657: INFO: Created: latency-svc-gl6qj
    Sep  3 21:44:43.659: INFO: Got endpoints: latency-svc-j62nv [106.738087ms]
    Sep  3 21:44:43.669: INFO: Got endpoints: latency-svc-gl6qj [116.367914ms]
    Sep  3 21:44:43.683: INFO: Created: latency-svc-qwd8t
    Sep  3 21:44:43.689: INFO: Got endpoints: latency-svc-qwd8t [136.286367ms]
    Sep  3 21:44:43.694: INFO: Created: latency-svc-vbgdd
    Sep  3 21:44:43.704: INFO: Got endpoints: latency-svc-vbgdd [152.22801ms]
    Sep  3 21:44:43.713: INFO: Created: latency-svc-2jjhb
    Sep  3 21:44:43.718: INFO: Created: latency-svc-jjl4d
    Sep  3 21:44:43.724: INFO: Got endpoints: latency-svc-2jjhb [169.971858ms]
    Sep  3 21:44:43.735: INFO: Created: latency-svc-ptlqc
    Sep  3 21:44:43.737: INFO: Got endpoints: latency-svc-jjl4d [184.564897ms]
    Sep  3 21:44:43.754: INFO: Got endpoints: latency-svc-ptlqc [200.545436ms]
    Sep  3 21:44:43.757: INFO: Created: latency-svc-mcsfd
    Sep  3 21:44:43.767: INFO: Got endpoints: latency-svc-mcsfd [213.610465ms]
    Sep  3 21:44:43.770: INFO: Created: latency-svc-sz5cl
    Sep  3 21:44:43.776: INFO: Got endpoints: latency-svc-sz5cl [222.943785ms]
    Sep  3 21:44:43.782: INFO: Created: latency-svc-9lrb7
    Sep  3 21:44:43.786: INFO: Got endpoints: latency-svc-9lrb7 [233.028407ms]
    Sep  3 21:44:43.793: INFO: Created: latency-svc-7ntf9
    Sep  3 21:44:43.800: INFO: Got endpoints: latency-svc-7ntf9 [246.822338ms]
    Sep  3 21:44:43.806: INFO: Created: latency-svc-jdr5k
    Sep  3 21:44:43.807: INFO: Got endpoints: latency-svc-jdr5k [254.266054ms]
    Sep  3 21:44:43.816: INFO: Created: latency-svc-lp6kz
    Sep  3 21:44:43.818: INFO: Created: latency-svc-zjcr6
    Sep  3 21:44:43.825: INFO: Got endpoints: latency-svc-zjcr6 [224.765966ms]
    Sep  3 21:44:43.834: INFO: Created: latency-svc-8fwb5
    Sep  3 21:44:43.836: INFO: Created: latency-svc-86vxm
    Sep  3 21:44:43.837: INFO: Got endpoints: latency-svc-8fwb5 [194.236277ms]
    Sep  3 21:44:43.839: INFO: Got endpoints: latency-svc-lp6kz [214.891231ms]
    Sep  3 21:44:43.845: INFO: Got endpoints: latency-svc-86vxm [185.026748ms]
    Sep  3 21:44:43.846: INFO: Created: latency-svc-mkvd6
    Sep  3 21:44:43.852: INFO: Got endpoints: latency-svc-mkvd6 [183.12104ms]
    Sep  3 21:44:43.859: INFO: Created: latency-svc-dvzck
    Sep  3 21:44:43.862: INFO: Created: latency-svc-rrwxm
    Sep  3 21:44:43.866: INFO: Got endpoints: latency-svc-dvzck [176.855916ms]
    Sep  3 21:44:43.871: INFO: Got endpoints: latency-svc-rrwxm [166.132485ms]
    Sep  3 21:44:43.872: INFO: Created: latency-svc-qcdsj
    Sep  3 21:44:43.879: INFO: Got endpoints: latency-svc-qcdsj [155.235751ms]
    Sep  3 21:44:43.883: INFO: Created: latency-svc-2tpp5
    Sep  3 21:44:43.887: INFO: Got endpoints: latency-svc-2tpp5 [149.926633ms]
    Sep  3 21:44:43.893: INFO: Created: latency-svc-flkkl
    Sep  3 21:44:43.898: INFO: Got endpoints: latency-svc-flkkl [144.747116ms]
    Sep  3 21:44:43.963: INFO: Created: latency-svc-kjfq5
    Sep  3 21:44:43.986: INFO: Created: latency-svc-94whs
    Sep  3 21:44:43.986: INFO: Created: latency-svc-7rkkk
    Sep  3 21:44:43.987: INFO: Created: latency-svc-9spst
    Sep  3 21:44:43.987: INFO: Created: latency-svc-xk5lf
    Sep  3 21:44:43.987: INFO: Created: latency-svc-xwhvd
    Sep  3 21:44:43.987: INFO: Created: latency-svc-fxrf8
    Sep  3 21:44:43.987: INFO: Created: latency-svc-thk8m
    Sep  3 21:44:43.987: INFO: Created: latency-svc-qff7v
    Sep  3 21:44:43.987: INFO: Created: latency-svc-rphdj
    Sep  3 21:44:43.987: INFO: Created: latency-svc-dbf2x
    Sep  3 21:44:43.987: INFO: Created: latency-svc-qfx55
    Sep  3 21:44:43.988: INFO: Created: latency-svc-8b7xg
    Sep  3 21:44:43.988: INFO: Created: latency-svc-m5k6v
    Sep  3 21:44:43.988: INFO: Created: latency-svc-dlld5
    Sep  3 21:44:43.992: INFO: Got endpoints: latency-svc-94whs [185.111705ms]
    Sep  3 21:44:43.992: INFO: Got endpoints: latency-svc-kjfq5 [104.87273ms]
    Sep  3 21:44:43.993: INFO: Got endpoints: latency-svc-xk5lf [114.30395ms]
    Sep  3 21:44:44.023: INFO: Got endpoints: latency-svc-7rkkk [124.601372ms]
    Sep  3 21:44:44.024: INFO: Got endpoints: latency-svc-dlld5 [257.037662ms]
    Sep  3 21:44:44.033: INFO: Got endpoints: latency-svc-m5k6v [256.282861ms]
    Sep  3 21:44:44.036: INFO: Got endpoints: latency-svc-dbf2x [235.412015ms]
    Sep  3 21:44:44.036: INFO: Got endpoints: latency-svc-8b7xg [249.584146ms]
    Sep  3 21:44:44.045: INFO: Created: latency-svc-4k5s7
    Sep  3 21:44:44.045: INFO: Got endpoints: latency-svc-qff7v [200.624938ms]
    Sep  3 21:44:44.049: INFO: Got endpoints: latency-svc-qfx55 [212.413365ms]
    Sep  3 21:44:44.049: INFO: Got endpoints: latency-svc-xwhvd [224.645191ms]
    Sep  3 21:44:44.061: INFO: Got endpoints: latency-svc-rphdj [221.299684ms]
    Sep  3 21:44:44.061: INFO: Got endpoints: latency-svc-thk8m [194.836626ms]
    Sep  3 21:44:44.062: INFO: Got endpoints: latency-svc-fxrf8 [209.843659ms]
    Sep  3 21:44:44.063: INFO: Created: latency-svc-nn826
    Sep  3 21:44:44.071: INFO: Created: latency-svc-wp2c5
    Sep  3 21:44:44.074: INFO: Created: latency-svc-5qjwz
    Sep  3 21:44:44.078: INFO: Created: latency-svc-l7wt6
    Sep  3 21:44:44.082: INFO: Created: latency-svc-sq28w
    Sep  3 21:44:44.087: INFO: Created: latency-svc-2gb6r
    Sep  3 21:44:44.091: INFO: Got endpoints: latency-svc-9spst [220.113582ms]
    Sep  3 21:44:44.097: INFO: Created: latency-svc-9zl45
    Sep  3 21:44:44.100: INFO: Created: latency-svc-bw9hb
    Sep  3 21:44:44.103: INFO: Created: latency-svc-h42vd
    Sep  3 21:44:44.107: INFO: Created: latency-svc-kkn48
    Sep  3 21:44:44.111: INFO: Created: latency-svc-q2bhh
    Sep  3 21:44:44.116: INFO: Created: latency-svc-z9f66
    Sep  3 21:44:44.124: INFO: Created: latency-svc-57ln7
    Sep  3 21:44:44.130: INFO: Created: latency-svc-xh7vv
    Sep  3 21:44:44.142: INFO: Got endpoints: latency-svc-4k5s7 [149.842628ms]
    Sep  3 21:44:44.150: INFO: Created: latency-svc-ck5ls
    Sep  3 21:44:44.191: INFO: Got endpoints: latency-svc-nn826 [167.119865ms]
    Sep  3 21:44:44.198: INFO: Created: latency-svc-ltr78
    Sep  3 21:44:44.239: INFO: Got endpoints: latency-svc-wp2c5 [215.087171ms]
    Sep  3 21:44:44.247: INFO: Created: latency-svc-w9ssv
    Sep  3 21:44:44.287: INFO: Got endpoints: latency-svc-5qjwz [293.299741ms]
    Sep  3 21:44:44.295: INFO: Created: latency-svc-dcl8l
    Sep  3 21:44:44.338: INFO: Got endpoints: latency-svc-l7wt6 [345.128255ms]
    Sep  3 21:44:44.348: INFO: Created: latency-svc-45xbf
    Sep  3 21:44:44.387: INFO: Got endpoints: latency-svc-sq28w [354.508475ms]
    Sep  3 21:44:44.395: INFO: Created: latency-svc-njwzb
    Sep  3 21:44:44.441: INFO: Got endpoints: latency-svc-2gb6r [404.531585ms]
    Sep  3 21:44:44.447: INFO: Created: latency-svc-kbdrx
    Sep  3 21:44:44.490: INFO: Got endpoints: latency-svc-9zl45 [453.733692ms]
    Sep  3 21:44:44.496: INFO: Created: latency-svc-kmzvv
    Sep  3 21:44:44.545: INFO: Got endpoints: latency-svc-bw9hb [499.626292ms]
    Sep  3 21:44:44.556: INFO: Created: latency-svc-t2x7c
    Sep  3 21:44:44.593: INFO: Got endpoints: latency-svc-h42vd [543.495589ms]
    Sep  3 21:44:44.604: INFO: Created: latency-svc-z5sr5
    Sep  3 21:44:44.642: INFO: Got endpoints: latency-svc-kkn48 [592.410096ms]
    Sep  3 21:44:44.659: INFO: Created: latency-svc-grjwh
    Sep  3 21:44:44.691: INFO: Got endpoints: latency-svc-q2bhh [629.868378ms]
    Sep  3 21:44:44.702: INFO: Created: latency-svc-9s2d8
    Sep  3 21:44:44.739: INFO: Got endpoints: latency-svc-z9f66 [677.152381ms]
    Sep  3 21:44:44.747: INFO: Created: latency-svc-6cj4p
    Sep  3 21:44:44.788: INFO: Got endpoints: latency-svc-57ln7 [726.92049ms]
    Sep  3 21:44:44.796: INFO: Created: latency-svc-56gjg
    Sep  3 21:44:44.837: INFO: Got endpoints: latency-svc-xh7vv [745.840831ms]
    Sep  3 21:44:44.845: INFO: Created: latency-svc-qtvm5
    Sep  3 21:44:44.888: INFO: Got endpoints: latency-svc-ck5ls [745.734431ms]
    Sep  3 21:44:44.897: INFO: Created: latency-svc-dxmh5
    Sep  3 21:44:44.940: INFO: Got endpoints: latency-svc-ltr78 [749.399139ms]
    Sep  3 21:44:44.946: INFO: Created: latency-svc-wtkxz
    Sep  3 21:44:44.988: INFO: Got endpoints: latency-svc-w9ssv [748.092436ms]
    Sep  3 21:44:44.995: INFO: Created: latency-svc-gld2w
    Sep  3 21:44:45.038: INFO: Got endpoints: latency-svc-dcl8l [751.088742ms]
    Sep  3 21:44:45.045: INFO: Created: latency-svc-kp2xm
    Sep  3 21:44:45.090: INFO: Got endpoints: latency-svc-45xbf [752.448245ms]
    Sep  3 21:44:45.096: INFO: Created: latency-svc-pgwvc
    Sep  3 21:44:45.140: INFO: Got endpoints: latency-svc-njwzb [752.822446ms]
    Sep  3 21:44:45.146: INFO: Created: latency-svc-7mwfm
    Sep  3 21:44:45.191: INFO: Got endpoints: latency-svc-kbdrx [750.485541ms]
    Sep  3 21:44:45.201: INFO: Created: latency-svc-59w9d
    Sep  3 21:44:45.239: INFO: Got endpoints: latency-svc-kmzvv [748.660437ms]
    Sep  3 21:44:45.245: INFO: Created: latency-svc-cchqm
    Sep  3 21:44:45.287: INFO: Got endpoints: latency-svc-t2x7c [741.885022ms]
    Sep  3 21:44:45.296: INFO: Created: latency-svc-6jqpw
    Sep  3 21:44:45.341: INFO: Got endpoints: latency-svc-z5sr5 [746.545832ms]
    Sep  3 21:44:45.351: INFO: Created: latency-svc-vprjx
    Sep  3 21:44:45.390: INFO: Got endpoints: latency-svc-grjwh [748.131736ms]
    Sep  3 21:44:45.398: INFO: Created: latency-svc-64vmg
    Sep  3 21:44:45.437: INFO: Got endpoints: latency-svc-9s2d8 [745.558531ms]
    Sep  3 21:44:45.443: INFO: Created: latency-svc-c8d4r
    Sep  3 21:44:45.488: INFO: Got endpoints: latency-svc-6cj4p [747.986836ms]
    Sep  3 21:44:45.502: INFO: Created: latency-svc-rpc8b
    Sep  3 21:44:45.542: INFO: Got endpoints: latency-svc-56gjg [753.519848ms]
    Sep  3 21:44:45.557: INFO: Created: latency-svc-65w88
    Sep  3 21:44:45.587: INFO: Got endpoints: latency-svc-qtvm5 [749.88294ms]
    Sep  3 21:44:45.612: INFO: Created: latency-svc-cthv7
    Sep  3 21:44:45.647: INFO: Got endpoints: latency-svc-dxmh5 [758.936959ms]
    Sep  3 21:44:45.665: INFO: Created: latency-svc-cgpjx
    Sep  3 21:44:45.695: INFO: Got endpoints: latency-svc-wtkxz [754.020149ms]
    Sep  3 21:44:45.708: INFO: Created: latency-svc-8gbkq
    Sep  3 21:44:45.739: INFO: Got endpoints: latency-svc-gld2w [751.716244ms]
    Sep  3 21:44:45.746: INFO: Created: latency-svc-s9x9x
    Sep  3 21:44:45.789: INFO: Got endpoints: latency-svc-kp2xm [750.995742ms]
    Sep  3 21:44:45.796: INFO: Created: latency-svc-75tjk
    Sep  3 21:44:45.837: INFO: Got endpoints: latency-svc-pgwvc [746.641733ms]
    Sep  3 21:44:45.846: INFO: Created: latency-svc-mgspx
    Sep  3 21:44:45.889: INFO: Got endpoints: latency-svc-7mwfm [747.956336ms]
    Sep  3 21:44:45.896: INFO: Created: latency-svc-jrkzh
    Sep  3 21:44:45.937: INFO: Got endpoints: latency-svc-59w9d [745.794531ms]
    Sep  3 21:44:45.946: INFO: Created: latency-svc-vnhxv
    Sep  3 21:44:45.991: INFO: Got endpoints: latency-svc-cchqm [751.829744ms]
    Sep  3 21:44:45.998: INFO: Created: latency-svc-h8nrm
    Sep  3 21:44:46.040: INFO: Got endpoints: latency-svc-6jqpw [752.624446ms]
    Sep  3 21:44:46.048: INFO: Created: latency-svc-cvjnx
    Sep  3 21:44:46.087: INFO: Got endpoints: latency-svc-vprjx [745.47403ms]
    Sep  3 21:44:46.095: INFO: Created: latency-svc-hkf9s
    Sep  3 21:44:46.140: INFO: Got endpoints: latency-svc-64vmg [750.115441ms]
    Sep  3 21:44:46.147: INFO: Created: latency-svc-q8zvf
    Sep  3 21:44:46.187: INFO: Got endpoints: latency-svc-c8d4r [749.83344ms]
    Sep  3 21:44:46.197: INFO: Created: latency-svc-7qpq8
    Sep  3 21:44:46.238: INFO: Got endpoints: latency-svc-rpc8b [750.515441ms]
    Sep  3 21:44:46.248: INFO: Created: latency-svc-s7hr7
    Sep  3 21:44:46.288: INFO: Got endpoints: latency-svc-65w88 [746.139532ms]
    Sep  3 21:44:46.301: INFO: Created: latency-svc-kgdj5
    Sep  3 21:44:46.338: INFO: Got endpoints: latency-svc-cthv7 [750.861542ms]
    Sep  3 21:44:46.348: INFO: Created: latency-svc-nl2l7
    Sep  3 21:44:46.387: INFO: Got endpoints: latency-svc-cgpjx [739.920618ms]
    Sep  3 21:44:46.394: INFO: Created: latency-svc-gv45v
    Sep  3 21:44:46.437: INFO: Got endpoints: latency-svc-8gbkq [742.079723ms]
    Sep  3 21:44:46.453: INFO: Created: latency-svc-q4rhv
    Sep  3 21:44:46.488: INFO: Got endpoints: latency-svc-s9x9x [748.428036ms]
    Sep  3 21:44:46.496: INFO: Created: latency-svc-wtkst
    Sep  3 21:44:46.539: INFO: Got endpoints: latency-svc-75tjk [749.64004ms]
    Sep  3 21:44:46.588: INFO: Created: latency-svc-mnvdt
    Sep  3 21:44:46.606: INFO: Got endpoints: latency-svc-mgspx [768.07848ms]
    Sep  3 21:44:46.634: INFO: Created: latency-svc-r6s9s
    Sep  3 21:44:46.647: INFO: Got endpoints: latency-svc-jrkzh [757.015856ms]
    Sep  3 21:44:46.662: INFO: Created: latency-svc-bwb76
    Sep  3 21:44:46.689: INFO: Got endpoints: latency-svc-vnhxv [751.776144ms]
    Sep  3 21:44:46.696: INFO: Created: latency-svc-9zr8r
    Sep  3 21:44:46.740: INFO: Got endpoints: latency-svc-h8nrm [749.641739ms]
    Sep  3 21:44:46.747: INFO: Created: latency-svc-qj54j
    Sep  3 21:44:46.790: INFO: Got endpoints: latency-svc-cvjnx [747.908135ms]
    Sep  3 21:44:46.796: INFO: Created: latency-svc-4hpk6
    Sep  3 21:44:46.840: INFO: Got endpoints: latency-svc-hkf9s [753.499848ms]
    Sep  3 21:44:46.847: INFO: Created: latency-svc-fs7zc
    Sep  3 21:44:46.888: INFO: Got endpoints: latency-svc-q8zvf [747.697635ms]
    Sep  3 21:44:46.898: INFO: Created: latency-svc-wchmt
    Sep  3 21:44:46.939: INFO: Got endpoints: latency-svc-7qpq8 [751.387843ms]
    Sep  3 21:44:46.951: INFO: Created: latency-svc-4gkvs
    Sep  3 21:44:46.990: INFO: Got endpoints: latency-svc-s7hr7 [751.358344ms]
    Sep  3 21:44:46.995: INFO: Created: latency-svc-r65vh
    Sep  3 21:44:47.039: INFO: Got endpoints: latency-svc-kgdj5 [750.811042ms]
    Sep  3 21:44:47.044: INFO: Created: latency-svc-wrsbq
    Sep  3 21:44:47.090: INFO: Got endpoints: latency-svc-nl2l7 [751.172443ms]
    Sep  3 21:44:47.097: INFO: Created: latency-svc-b55nx
    Sep  3 21:44:47.138: INFO: Got endpoints: latency-svc-gv45v [750.722441ms]
    Sep  3 21:44:47.159: INFO: Created: latency-svc-xms4s
    Sep  3 21:44:47.194: INFO: Got endpoints: latency-svc-q4rhv [756.565554ms]
    Sep  3 21:44:47.202: INFO: Created: latency-svc-ldlj8
    Sep  3 21:44:47.242: INFO: Got endpoints: latency-svc-wtkst [753.598248ms]
    Sep  3 21:44:47.251: INFO: Created: latency-svc-kbnzk
    Sep  3 21:44:47.287: INFO: Got endpoints: latency-svc-mnvdt [748.472837ms]
    Sep  3 21:44:47.297: INFO: Created: latency-svc-tb24j
    Sep  3 21:44:47.342: INFO: Got endpoints: latency-svc-r6s9s [735.969709ms]
    Sep  3 21:44:47.363: INFO: Created: latency-svc-n6gw6
    Sep  3 21:44:47.389: INFO: Got endpoints: latency-svc-bwb76 [742.369624ms]
    Sep  3 21:44:47.409: INFO: Created: latency-svc-8zmjp
    Sep  3 21:44:47.440: INFO: Got endpoints: latency-svc-9zr8r [750.921342ms]
    Sep  3 21:44:47.462: INFO: Created: latency-svc-zw9qt
    Sep  3 21:44:47.489: INFO: Got endpoints: latency-svc-qj54j [748.627238ms]
    Sep  3 21:44:47.500: INFO: Created: latency-svc-cw6cc
    Sep  3 21:44:47.547: INFO: Got endpoints: latency-svc-4hpk6 [756.701354ms]
    Sep  3 21:44:47.564: INFO: Created: latency-svc-jl8zp
    Sep  3 21:44:47.598: INFO: Got endpoints: latency-svc-fs7zc [756.922755ms]
    Sep  3 21:44:47.614: INFO: Created: latency-svc-p8rv5
    Sep  3 21:44:47.643: INFO: Got endpoints: latency-svc-wchmt [754.40255ms]
    Sep  3 21:44:47.661: INFO: Created: latency-svc-sbxdv
    Sep  3 21:44:47.688: INFO: Got endpoints: latency-svc-4gkvs [748.773137ms]
    Sep  3 21:44:47.698: INFO: Created: latency-svc-p5hwh
    Sep  3 21:44:47.738: INFO: Got endpoints: latency-svc-r65vh [747.898935ms]
    Sep  3 21:44:47.750: INFO: Created: latency-svc-dln97
    Sep  3 21:44:47.790: INFO: Got endpoints: latency-svc-wrsbq [750.528442ms]
    Sep  3 21:44:47.796: INFO: Created: latency-svc-qrmb4
    Sep  3 21:44:47.838: INFO: Got endpoints: latency-svc-b55nx [748.137236ms]
    Sep  3 21:44:47.847: INFO: Created: latency-svc-m2pkx
    Sep  3 21:44:47.889: INFO: Got endpoints: latency-svc-xms4s [750.419941ms]
    Sep  3 21:44:47.895: INFO: Created: latency-svc-4vtbj
    Sep  3 21:44:47.937: INFO: Got endpoints: latency-svc-ldlj8 [742.891424ms]
    Sep  3 21:44:47.949: INFO: Created: latency-svc-7nhts
    Sep  3 21:44:47.987: INFO: Got endpoints: latency-svc-kbnzk [745.69023ms]
    Sep  3 21:44:47.996: INFO: Created: latency-svc-m68cf
    Sep  3 21:44:48.038: INFO: Got endpoints: latency-svc-tb24j [750.584842ms]
    Sep  3 21:44:48.047: INFO: Created: latency-svc-cz5ft
    Sep  3 21:44:48.089: INFO: Got endpoints: latency-svc-n6gw6 [746.726533ms]
    Sep  3 21:44:48.096: INFO: Created: latency-svc-k7btn
    Sep  3 21:44:48.138: INFO: Got endpoints: latency-svc-8zmjp [748.426537ms]
    Sep  3 21:44:48.146: INFO: Created: latency-svc-7mt92
    Sep  3 21:44:48.189: INFO: Got endpoints: latency-svc-zw9qt [748.043336ms]
    Sep  3 21:44:48.196: INFO: Created: latency-svc-s4bl2
    Sep  3 21:44:48.237: INFO: Got endpoints: latency-svc-cw6cc [747.716135ms]
    Sep  3 21:44:48.246: INFO: Created: latency-svc-pfj92
    Sep  3 21:44:48.337: INFO: Got endpoints: latency-svc-jl8zp [788.157424ms]
    Sep  3 21:44:48.347: INFO: Created: latency-svc-7lc88
    Sep  3 21:44:48.391: INFO: Got endpoints: latency-svc-p8rv5 [792.715133ms]
    Sep  3 21:44:48.397: INFO: Created: latency-svc-5qh44
    Sep  3 21:44:48.442: INFO: Got endpoints: latency-svc-sbxdv [798.970847ms]
    Sep  3 21:44:48.450: INFO: Created: latency-svc-c8jqm
    Sep  3 21:44:48.487: INFO: Got endpoints: latency-svc-p5hwh [799.126247ms]
    Sep  3 21:44:48.495: INFO: Created: latency-svc-pkxn6
    Sep  3 21:44:48.540: INFO: Got endpoints: latency-svc-dln97 [801.836653ms]
    Sep  3 21:44:48.565: INFO: Created: latency-svc-5r64v
    Sep  3 21:44:48.606: INFO: Got endpoints: latency-svc-qrmb4 [815.859684ms]
    Sep  3 21:44:48.621: INFO: Created: latency-svc-xt4ss
    Sep  3 21:44:48.640: INFO: Got endpoints: latency-svc-m2pkx [802.125054ms]
    Sep  3 21:44:48.657: INFO: Created: latency-svc-zhr6s
    Sep  3 21:44:48.691: INFO: Got endpoints: latency-svc-4vtbj [802.002454ms]
    Sep  3 21:44:48.704: INFO: Created: latency-svc-fh4mx
    Sep  3 21:44:48.738: INFO: Got endpoints: latency-svc-7nhts [800.630351ms]
    Sep  3 21:44:48.746: INFO: Created: latency-svc-k7qt5
    Sep  3 21:44:48.789: INFO: Got endpoints: latency-svc-m68cf [801.042952ms]
    Sep  3 21:44:48.796: INFO: Created: latency-svc-kwqqv
    Sep  3 21:44:48.838: INFO: Got endpoints: latency-svc-cz5ft [798.782647ms]
    Sep  3 21:44:48.844: INFO: Created: latency-svc-2sktw
    Sep  3 21:44:48.889: INFO: Got endpoints: latency-svc-k7btn [799.324248ms]
    Sep  3 21:44:48.895: INFO: Created: latency-svc-wpbkh
    Sep  3 21:44:48.938: INFO: Got endpoints: latency-svc-7mt92 [799.576648ms]
    Sep  3 21:44:48.943: INFO: Created: latency-svc-s4xpp
    Sep  3 21:44:48.988: INFO: Got endpoints: latency-svc-s4bl2 [799.335548ms]
    Sep  3 21:44:48.998: INFO: Created: latency-svc-7gw55
    Sep  3 21:44:49.042: INFO: Got endpoints: latency-svc-pfj92 [804.267958ms]
    Sep  3 21:44:49.049: INFO: Created: latency-svc-jhjfs
    Sep  3 21:44:49.092: INFO: Got endpoints: latency-svc-7lc88 [754.875751ms]
    Sep  3 21:44:49.103: INFO: Created: latency-svc-mk8mr
    Sep  3 21:44:49.144: INFO: Got endpoints: latency-svc-5qh44 [753.022447ms]
    Sep  3 21:44:49.150: INFO: Created: latency-svc-nl2qh
    Sep  3 21:44:49.190: INFO: Got endpoints: latency-svc-c8jqm [747.913735ms]
    Sep  3 21:44:49.204: INFO: Created: latency-svc-94twt
    Sep  3 21:44:49.239: INFO: Got endpoints: latency-svc-pkxn6 [751.945244ms]
    Sep  3 21:44:49.249: INFO: Created: latency-svc-dvlw2
    Sep  3 21:44:49.288: INFO: Got endpoints: latency-svc-5r64v [747.023034ms]
    Sep  3 21:44:49.298: INFO: Created: latency-svc-d4s6f
    Sep  3 21:44:49.338: INFO: Got endpoints: latency-svc-xt4ss [732.315502ms]
    Sep  3 21:44:49.346: INFO: Created: latency-svc-tvgn8
    Sep  3 21:44:49.401: INFO: Got endpoints: latency-svc-zhr6s [760.994164ms]
    Sep  3 21:44:49.421: INFO: Created: latency-svc-b4zpj
    Sep  3 21:44:49.453: INFO: Got endpoints: latency-svc-fh4mx [756.162954ms]
    Sep  3 21:44:49.494: INFO: Created: latency-svc-qf7d2
    Sep  3 21:44:49.496: INFO: Got endpoints: latency-svc-k7qt5 [757.472656ms]
    Sep  3 21:44:49.525: INFO: Created: latency-svc-9cbbn
    Sep  3 21:44:49.559: INFO: Got endpoints: latency-svc-kwqqv [769.832183ms]
    Sep  3 21:44:49.589: INFO: Created: latency-svc-7fmps
    Sep  3 21:44:49.597: INFO: Got endpoints: latency-svc-2sktw [759.309561ms]
    Sep  3 21:44:49.605: INFO: Created: latency-svc-xmknq
    Sep  3 21:44:49.642: INFO: Got endpoints: latency-svc-wpbkh [753.037146ms]
    Sep  3 21:44:49.657: INFO: Created: latency-svc-cjr4r
    Sep  3 21:44:49.696: INFO: Got endpoints: latency-svc-s4xpp [757.594157ms]
    Sep  3 21:44:49.707: INFO: Created: latency-svc-r2vt7
    Sep  3 21:44:49.740: INFO: Got endpoints: latency-svc-7gw55 [751.998845ms]
    Sep  3 21:44:49.748: INFO: Created: latency-svc-7vhpw
    Sep  3 21:44:49.794: INFO: Got endpoints: latency-svc-jhjfs [751.368843ms]
    Sep  3 21:44:49.802: INFO: Created: latency-svc-fs5sx
    Sep  3 21:44:49.837: INFO: Got endpoints: latency-svc-mk8mr [744.918229ms]
    Sep  3 21:44:49.847: INFO: Created: latency-svc-95pfx
    Sep  3 21:44:49.891: INFO: Got endpoints: latency-svc-nl2qh [746.591732ms]
    Sep  3 21:44:49.897: INFO: Created: latency-svc-9fqln
    Sep  3 21:44:49.938: INFO: Got endpoints: latency-svc-94twt [747.606735ms]
    Sep  3 21:44:49.958: INFO: Created: latency-svc-2c2zz
    Sep  3 21:44:49.990: INFO: Got endpoints: latency-svc-dvlw2 [750.982642ms]
    Sep  3 21:44:49.998: INFO: Created: latency-svc-vswh4
    Sep  3 21:44:50.039: INFO: Got endpoints: latency-svc-d4s6f [751.077243ms]
    Sep  3 21:44:50.045: INFO: Created: latency-svc-qnf2k
    Sep  3 21:44:50.091: INFO: Got endpoints: latency-svc-tvgn8 [752.660546ms]
    Sep  3 21:44:50.099: INFO: Created: latency-svc-b4bfs
    Sep  3 21:44:50.138: INFO: Got endpoints: latency-svc-b4zpj [736.669511ms]
    Sep  3 21:44:50.149: INFO: Created: latency-svc-ddl75
    Sep  3 21:44:50.188: INFO: Got endpoints: latency-svc-qf7d2 [734.625707ms]
    Sep  3 21:44:50.195: INFO: Created: latency-svc-g7k4h
    Sep  3 21:44:50.239: INFO: Got endpoints: latency-svc-9cbbn [743.439926ms]
    Sep  3 21:44:50.249: INFO: Created: latency-svc-qdvld
    Sep  3 21:44:50.289: INFO: Got endpoints: latency-svc-7fmps [729.992296ms]
    Sep  3 21:44:50.298: INFO: Created: latency-svc-2pln9
    Sep  3 21:44:50.338: INFO: Got endpoints: latency-svc-xmknq [740.944021ms]
    Sep  3 21:44:50.347: INFO: Created: latency-svc-j766d
    Sep  3 21:44:50.389: INFO: Got endpoints: latency-svc-cjr4r [746.653833ms]
    Sep  3 21:44:50.397: INFO: Created: latency-svc-hmb8k
    Sep  3 21:44:50.439: INFO: Got endpoints: latency-svc-r2vt7 [742.447524ms]
    Sep  3 21:44:50.446: INFO: Created: latency-svc-ww9qf
    Sep  3 21:44:50.488: INFO: Got endpoints: latency-svc-7vhpw [746.917433ms]
    Sep  3 21:44:50.493: INFO: Created: latency-svc-vnbb7
    Sep  3 21:44:50.537: INFO: Got endpoints: latency-svc-fs5sx [742.891625ms]
    Sep  3 21:44:50.558: INFO: Created: latency-svc-vzsf4
    Sep  3 21:44:50.598: INFO: Got endpoints: latency-svc-95pfx [760.273663ms]
    Sep  3 21:44:50.615: INFO: Created: latency-svc-64bk8
    Sep  3 21:44:50.638: INFO: Got endpoints: latency-svc-9fqln [747.368535ms]
    Sep  3 21:44:50.672: INFO: Created: latency-svc-nwrlv
    Sep  3 21:44:50.688: INFO: Got endpoints: latency-svc-2c2zz [749.79394ms]
    Sep  3 21:44:50.696: INFO: Created: latency-svc-jmpqq
    Sep  3 21:44:50.739: INFO: Got endpoints: latency-svc-vswh4 [748.036236ms]
    Sep  3 21:44:50.753: INFO: Created: latency-svc-wp2px
    Sep  3 21:44:50.789: INFO: Got endpoints: latency-svc-qnf2k [749.512739ms]
    Sep  3 21:44:50.798: INFO: Created: latency-svc-7pgl5
    Sep  3 21:44:50.837: INFO: Got endpoints: latency-svc-b4bfs [746.069131ms]
    Sep  3 21:44:50.856: INFO: Created: latency-svc-znd5z
    Sep  3 21:44:50.887: INFO: Got endpoints: latency-svc-ddl75 [748.495837ms]
    Sep  3 21:44:50.895: INFO: Created: latency-svc-s6kgp
    Sep  3 21:44:50.938: INFO: Got endpoints: latency-svc-g7k4h [749.83774ms]
    Sep  3 21:44:50.945: INFO: Created: latency-svc-phfqb
    Sep  3 21:44:50.988: INFO: Got endpoints: latency-svc-qdvld [748.102536ms]
    Sep  3 21:44:50.997: INFO: Created: latency-svc-t6t48
    Sep  3 21:44:51.039: INFO: Got endpoints: latency-svc-2pln9 [749.551439ms]
    Sep  3 21:44:51.046: INFO: Created: latency-svc-n79j8
    Sep  3 21:44:51.092: INFO: Got endpoints: latency-svc-j766d [752.880847ms]
    Sep  3 21:44:51.107: INFO: Created: latency-svc-8mhxv
    Sep  3 21:44:51.139: INFO: Got endpoints: latency-svc-hmb8k [749.729939ms]
    Sep  3 21:44:51.145: INFO: Created: latency-svc-6c7h2
    Sep  3 21:44:51.190: INFO: Got endpoints: latency-svc-ww9qf [750.850742ms]
    Sep  3 21:44:51.203: INFO: Created: latency-svc-rlpv2
    Sep  3 21:44:51.238: INFO: Got endpoints: latency-svc-vnbb7 [749.822639ms]
    Sep  3 21:44:51.246: INFO: Created: latency-svc-2c4c8
    Sep  3 21:44:51.289: INFO: Got endpoints: latency-svc-vzsf4 [751.882644ms]
    Sep  3 21:44:51.301: INFO: Created: latency-svc-8kjlk
    Sep  3 21:44:51.338: INFO: Got endpoints: latency-svc-64bk8 [738.621315ms]
    Sep  3 21:44:51.346: INFO: Created: latency-svc-qskg5
    Sep  3 21:44:51.390: INFO: Got endpoints: latency-svc-nwrlv [751.609344ms]
    Sep  3 21:44:51.397: INFO: Created: latency-svc-9tzl2
    Sep  3 21:44:51.437: INFO: Got endpoints: latency-svc-jmpqq [748.590937ms]
    Sep  3 21:44:51.489: INFO: Got endpoints: latency-svc-wp2px [750.427841ms]
    Sep  3 21:44:51.551: INFO: Got endpoints: latency-svc-7pgl5 [761.885366ms]
    Sep  3 21:44:51.591: INFO: Got endpoints: latency-svc-znd5z [753.416548ms]
    Sep  3 21:44:51.650: INFO: Got endpoints: latency-svc-s6kgp [762.171867ms]
    Sep  3 21:44:51.694: INFO: Got endpoints: latency-svc-phfqb [755.687852ms]
    Sep  3 21:44:51.739: INFO: Got endpoints: latency-svc-t6t48 [750.919742ms]
    Sep  3 21:44:51.790: INFO: Got endpoints: latency-svc-n79j8 [750.947048ms]
    Sep  3 21:44:51.837: INFO: Got endpoints: latency-svc-8mhxv [745.496143ms]
    Sep  3 21:44:51.887: INFO: Got endpoints: latency-svc-6c7h2 [747.235454ms]
    Sep  3 21:44:51.938: INFO: Got endpoints: latency-svc-rlpv2 [748.125665ms]
    Sep  3 21:44:51.990: INFO: Got endpoints: latency-svc-2c4c8 [752.175182ms]
    Sep  3 21:44:52.039: INFO: Got endpoints: latency-svc-8kjlk [749.454683ms]
    Sep  3 21:44:52.088: INFO: Got endpoints: latency-svc-qskg5 [750.031991ms]
    Sep  3 21:44:52.140: INFO: Got endpoints: latency-svc-9tzl2 [750.0443ms]
    Sep  3 21:44:52.141: INFO: Latencies: [47.917129ms 71.966494ms 89.950343ms 104.87273ms 106.738087ms 114.30395ms 116.367914ms 124.601372ms 136.286367ms 144.747116ms 149.842628ms 149.926633ms 152.22801ms 155.235751ms 166.132485ms 167.119865ms 169.971858ms 176.855916ms 183.12104ms 184.564897ms 185.026748ms 185.111705ms 194.236277ms 194.836626ms 200.545436ms 200.624938ms 209.843659ms 212.413365ms 213.610465ms 214.891231ms 215.087171ms 220.113582ms 221.299684ms 222.943785ms 224.645191ms 224.765966ms 233.028407ms 235.412015ms 246.822338ms 249.584146ms 254.266054ms 256.282861ms 257.037662ms 293.299741ms 345.128255ms 354.508475ms 404.531585ms 453.733692ms 499.626292ms 543.495589ms 592.410096ms 629.868378ms 677.152381ms 726.92049ms 729.992296ms 732.315502ms 734.625707ms 735.969709ms 736.669511ms 738.621315ms 739.920618ms 740.944021ms 741.885022ms 742.079723ms 742.369624ms 742.447524ms 742.891424ms 742.891625ms 743.439926ms 744.918229ms 745.47403ms 745.496143ms 745.558531ms 745.69023ms 745.734431ms 745.794531ms 745.840831ms 746.069131ms 746.139532ms 746.545832ms 746.591732ms 746.641733ms 746.653833ms 746.726533ms 746.917433ms 747.023034ms 747.235454ms 747.368535ms 747.606735ms 747.697635ms 747.716135ms 747.898935ms 747.908135ms 747.913735ms 747.956336ms 747.986836ms 748.036236ms 748.043336ms 748.092436ms 748.102536ms 748.125665ms 748.131736ms 748.137236ms 748.426537ms 748.428036ms 748.472837ms 748.495837ms 748.590937ms 748.627238ms 748.660437ms 748.773137ms 749.399139ms 749.454683ms 749.512739ms 749.551439ms 749.64004ms 749.641739ms 749.729939ms 749.79394ms 749.822639ms 749.83344ms 749.83774ms 749.88294ms 750.031991ms 750.0443ms 750.115441ms 750.419941ms 750.427841ms 750.485541ms 750.515441ms 750.528442ms 750.584842ms 750.722441ms 750.811042ms 750.850742ms 750.861542ms 750.919742ms 750.921342ms 750.947048ms 750.982642ms 750.995742ms 751.077243ms 751.088742ms 751.172443ms 751.358344ms 751.368843ms 751.387843ms 751.609344ms 751.716244ms 751.776144ms 751.829744ms 751.882644ms 751.945244ms 751.998845ms 752.175182ms 752.448245ms 752.624446ms 752.660546ms 752.822446ms 752.880847ms 753.022447ms 753.037146ms 753.416548ms 753.499848ms 753.519848ms 753.598248ms 754.020149ms 754.40255ms 754.875751ms 755.687852ms 756.162954ms 756.565554ms 756.701354ms 756.922755ms 757.015856ms 757.472656ms 757.594157ms 758.936959ms 759.309561ms 760.273663ms 760.994164ms 761.885366ms 762.171867ms 768.07848ms 769.832183ms 788.157424ms 792.715133ms 798.782647ms 798.970847ms 799.126247ms 799.324248ms 799.335548ms 799.576648ms 800.630351ms 801.042952ms 801.836653ms 802.002454ms 802.125054ms 804.267958ms 815.859684ms]
    Sep  3 21:44:52.141: INFO: 50 %ile: 748.125665ms
    Sep  3 21:44:52.141: INFO: 90 %ile: 760.994164ms
    Sep  3 21:44:52.141: INFO: 99 %ile: 804.267958ms
    Sep  3 21:44:52.141: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Sep  3 21:44:52.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-3777" for this suite. 09/03/22 21:44:52.146
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:44:52.158
Sep  3 21:44:52.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename resourcequota 09/03/22 21:44:52.16
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:44:52.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:44:52.184
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 09/03/22 21:44:52.189
STEP: Creating a ResourceQuota 09/03/22 21:44:57.195
STEP: Ensuring resource quota status is calculated 09/03/22 21:44:57.199
STEP: Creating a ReplicationController 09/03/22 21:44:59.207
STEP: Ensuring resource quota status captures replication controller creation 09/03/22 21:44:59.242
STEP: Deleting a ReplicationController 09/03/22 21:45:01.245
STEP: Ensuring resource quota status released usage 09/03/22 21:45:01.248
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Sep  3 21:45:03.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4323" for this suite. 09/03/22 21:45:03.254
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":235,"skipped":4320,"failed":0}
------------------------------
• [SLOW TEST] [11.100 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:44:52.158
    Sep  3 21:44:52.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename resourcequota 09/03/22 21:44:52.16
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:44:52.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:44:52.184
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 09/03/22 21:44:52.189
    STEP: Creating a ResourceQuota 09/03/22 21:44:57.195
    STEP: Ensuring resource quota status is calculated 09/03/22 21:44:57.199
    STEP: Creating a ReplicationController 09/03/22 21:44:59.207
    STEP: Ensuring resource quota status captures replication controller creation 09/03/22 21:44:59.242
    STEP: Deleting a ReplicationController 09/03/22 21:45:01.245
    STEP: Ensuring resource quota status released usage 09/03/22 21:45:01.248
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Sep  3 21:45:03.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4323" for this suite. 09/03/22 21:45:03.254
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:45:03.263
Sep  3 21:45:03.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 21:45:03.264
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:03.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:03.28
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-3724/configmap-test-06d3d9cf-5d6b-4f63-9c32-67cdfc758d9f 09/03/22 21:45:03.282
STEP: Creating a pod to test consume configMaps 09/03/22 21:45:03.285
Sep  3 21:45:03.290: INFO: Waiting up to 5m0s for pod "pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898" in namespace "configmap-3724" to be "Succeeded or Failed"
Sep  3 21:45:03.294: INFO: Pod "pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898": Phase="Pending", Reason="", readiness=false. Elapsed: 4.847135ms
Sep  3 21:45:05.300: INFO: Pod "pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010375588s
Sep  3 21:45:07.300: INFO: Pod "pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010324043s
STEP: Saw pod success 09/03/22 21:45:07.3
Sep  3 21:45:07.301: INFO: Pod "pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898" satisfied condition "Succeeded or Failed"
Sep  3 21:45:07.303: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898 container env-test: <nil>
STEP: delete the pod 09/03/22 21:45:07.308
Sep  3 21:45:07.315: INFO: Waiting for pod pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898 to disappear
Sep  3 21:45:07.317: INFO: Pod pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 21:45:07.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3724" for this suite. 09/03/22 21:45:07.319
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":236,"skipped":4365,"failed":0}
------------------------------
• [4.061 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:45:03.263
    Sep  3 21:45:03.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 21:45:03.264
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:03.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:03.28
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-3724/configmap-test-06d3d9cf-5d6b-4f63-9c32-67cdfc758d9f 09/03/22 21:45:03.282
    STEP: Creating a pod to test consume configMaps 09/03/22 21:45:03.285
    Sep  3 21:45:03.290: INFO: Waiting up to 5m0s for pod "pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898" in namespace "configmap-3724" to be "Succeeded or Failed"
    Sep  3 21:45:03.294: INFO: Pod "pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898": Phase="Pending", Reason="", readiness=false. Elapsed: 4.847135ms
    Sep  3 21:45:05.300: INFO: Pod "pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010375588s
    Sep  3 21:45:07.300: INFO: Pod "pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010324043s
    STEP: Saw pod success 09/03/22 21:45:07.3
    Sep  3 21:45:07.301: INFO: Pod "pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898" satisfied condition "Succeeded or Failed"
    Sep  3 21:45:07.303: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898 container env-test: <nil>
    STEP: delete the pod 09/03/22 21:45:07.308
    Sep  3 21:45:07.315: INFO: Waiting for pod pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898 to disappear
    Sep  3 21:45:07.317: INFO: Pod pod-configmaps-14d14e00-be0c-48cd-abc9-4369b3ec7898 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 21:45:07.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3724" for this suite. 09/03/22 21:45:07.319
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:45:07.324
Sep  3 21:45:07.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pods 09/03/22 21:45:07.325
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:07.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:07.336
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 09/03/22 21:45:07.339
Sep  3 21:45:07.344: INFO: Waiting up to 5m0s for pod "pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf" in namespace "pods-896" to be "running and ready"
Sep  3 21:45:07.351: INFO: Pod "pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.13632ms
Sep  3 21:45:07.351: INFO: The phase of Pod pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:45:09.353: INFO: Pod "pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009760131s
Sep  3 21:45:09.353: INFO: The phase of Pod pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf is Running (Ready = true)
Sep  3 21:45:09.353: INFO: Pod "pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf" satisfied condition "running and ready"
Sep  3 21:45:09.357: INFO: Pod pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf has hostIP: 172.18.0.3
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Sep  3 21:45:09.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-896" for this suite. 09/03/22 21:45:09.359
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":237,"skipped":4376,"failed":0}
------------------------------
• [2.038 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:45:07.324
    Sep  3 21:45:07.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pods 09/03/22 21:45:07.325
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:07.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:07.336
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 09/03/22 21:45:07.339
    Sep  3 21:45:07.344: INFO: Waiting up to 5m0s for pod "pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf" in namespace "pods-896" to be "running and ready"
    Sep  3 21:45:07.351: INFO: Pod "pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.13632ms
    Sep  3 21:45:07.351: INFO: The phase of Pod pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:45:09.353: INFO: Pod "pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009760131s
    Sep  3 21:45:09.353: INFO: The phase of Pod pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf is Running (Ready = true)
    Sep  3 21:45:09.353: INFO: Pod "pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf" satisfied condition "running and ready"
    Sep  3 21:45:09.357: INFO: Pod pod-hostip-5f82d005-abf7-43ae-bcf0-62d5652ba4bf has hostIP: 172.18.0.3
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Sep  3 21:45:09.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-896" for this suite. 09/03/22 21:45:09.359
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:45:09.37
Sep  3 21:45:09.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pods 09/03/22 21:45:09.372
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:09.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:09.382
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 09/03/22 21:45:09.389
STEP: watching for Pod to be ready 09/03/22 21:45:09.395
Sep  3 21:45:09.398: INFO: observed Pod pod-test in namespace pods-3153 in phase Pending with labels: map[test-pod-static:true] & conditions []
Sep  3 21:45:09.400: INFO: observed Pod pod-test in namespace pods-3153 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC  }]
Sep  3 21:45:09.408: INFO: observed Pod pod-test in namespace pods-3153 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC  }]
Sep  3 21:45:12.254: INFO: Found Pod pod-test in namespace pods-3153 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 09/03/22 21:45:12.257
STEP: getting the Pod and ensuring that it's patched 09/03/22 21:45:12.264
STEP: replacing the Pod's status Ready condition to False 09/03/22 21:45:12.269
STEP: check the Pod again to ensure its Ready conditions are False 09/03/22 21:45:12.275
STEP: deleting the Pod via a Collection with a LabelSelector 09/03/22 21:45:12.276
STEP: watching for the Pod to be deleted 09/03/22 21:45:12.281
Sep  3 21:45:12.283: INFO: observed event type MODIFIED
Sep  3 21:45:13.461: INFO: observed event type MODIFIED
Sep  3 21:45:15.282: INFO: observed event type MODIFIED
Sep  3 21:45:15.309: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Sep  3 21:45:15.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3153" for this suite. 09/03/22 21:45:15.321
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":238,"skipped":4415,"failed":0}
------------------------------
• [SLOW TEST] [5.961 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:45:09.37
    Sep  3 21:45:09.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pods 09/03/22 21:45:09.372
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:09.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:09.382
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 09/03/22 21:45:09.389
    STEP: watching for Pod to be ready 09/03/22 21:45:09.395
    Sep  3 21:45:09.398: INFO: observed Pod pod-test in namespace pods-3153 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Sep  3 21:45:09.400: INFO: observed Pod pod-test in namespace pods-3153 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC  }]
    Sep  3 21:45:09.408: INFO: observed Pod pod-test in namespace pods-3153 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC  }]
    Sep  3 21:45:12.254: INFO: Found Pod pod-test in namespace pods-3153 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-03 21:45:09 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 09/03/22 21:45:12.257
    STEP: getting the Pod and ensuring that it's patched 09/03/22 21:45:12.264
    STEP: replacing the Pod's status Ready condition to False 09/03/22 21:45:12.269
    STEP: check the Pod again to ensure its Ready conditions are False 09/03/22 21:45:12.275
    STEP: deleting the Pod via a Collection with a LabelSelector 09/03/22 21:45:12.276
    STEP: watching for the Pod to be deleted 09/03/22 21:45:12.281
    Sep  3 21:45:12.283: INFO: observed event type MODIFIED
    Sep  3 21:45:13.461: INFO: observed event type MODIFIED
    Sep  3 21:45:15.282: INFO: observed event type MODIFIED
    Sep  3 21:45:15.309: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Sep  3 21:45:15.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3153" for this suite. 09/03/22 21:45:15.321
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:45:15.333
Sep  3 21:45:15.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:45:15.334
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:15.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:15.353
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Sep  3 21:45:15.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1578 version'
Sep  3 21:45:15.417: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Sep  3 21:45:15.417: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:44:59Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-09-01T23:30:43Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:45:15.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1578" for this suite. 09/03/22 21:45:15.42
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":239,"skipped":4456,"failed":0}
------------------------------
• [0.091 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:45:15.333
    Sep  3 21:45:15.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:45:15.334
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:15.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:15.353
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Sep  3 21:45:15.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1578 version'
    Sep  3 21:45:15.417: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Sep  3 21:45:15.417: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:44:59Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-09-01T23:30:43Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:45:15.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1578" for this suite. 09/03/22 21:45:15.42
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:45:15.424
Sep  3 21:45:15.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-probe 09/03/22 21:45:15.425
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:15.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:15.436
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Sep  3 21:45:15.444: INFO: Waiting up to 5m0s for pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f" in namespace "container-probe-6384" to be "running and ready"
Sep  3 21:45:15.449: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.104201ms
Sep  3 21:45:15.449: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:45:17.459: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 2.015201905s
Sep  3 21:45:17.459: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
Sep  3 21:45:19.463: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 4.019015313s
Sep  3 21:45:19.463: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
Sep  3 21:45:21.460: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 6.016061156s
Sep  3 21:45:21.460: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
Sep  3 21:45:23.459: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 8.014721459s
Sep  3 21:45:23.459: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
Sep  3 21:45:25.455: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 10.011046014s
Sep  3 21:45:25.455: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
Sep  3 21:45:27.458: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 12.013980437s
Sep  3 21:45:27.458: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
Sep  3 21:45:29.463: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 14.019086468s
Sep  3 21:45:29.463: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
Sep  3 21:45:31.461: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 16.017213074s
Sep  3 21:45:31.461: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
Sep  3 21:45:33.454: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 18.009850397s
Sep  3 21:45:33.454: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
Sep  3 21:45:35.457: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 20.013092929s
Sep  3 21:45:35.457: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
Sep  3 21:45:37.458: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=true. Elapsed: 22.014141446s
Sep  3 21:45:37.458: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = true)
Sep  3 21:45:37.458: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f" satisfied condition "running and ready"
Sep  3 21:45:37.463: INFO: Container started at 2022-09-03 21:45:16 +0000 UTC, pod became ready at 2022-09-03 21:45:35 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Sep  3 21:45:37.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6384" for this suite. 09/03/22 21:45:37.465
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":240,"skipped":4463,"failed":0}
------------------------------
• [SLOW TEST] [22.045 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:45:15.424
    Sep  3 21:45:15.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-probe 09/03/22 21:45:15.425
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:15.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:15.436
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Sep  3 21:45:15.444: INFO: Waiting up to 5m0s for pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f" in namespace "container-probe-6384" to be "running and ready"
    Sep  3 21:45:15.449: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.104201ms
    Sep  3 21:45:15.449: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:45:17.459: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 2.015201905s
    Sep  3 21:45:17.459: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
    Sep  3 21:45:19.463: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 4.019015313s
    Sep  3 21:45:19.463: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
    Sep  3 21:45:21.460: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 6.016061156s
    Sep  3 21:45:21.460: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
    Sep  3 21:45:23.459: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 8.014721459s
    Sep  3 21:45:23.459: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
    Sep  3 21:45:25.455: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 10.011046014s
    Sep  3 21:45:25.455: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
    Sep  3 21:45:27.458: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 12.013980437s
    Sep  3 21:45:27.458: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
    Sep  3 21:45:29.463: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 14.019086468s
    Sep  3 21:45:29.463: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
    Sep  3 21:45:31.461: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 16.017213074s
    Sep  3 21:45:31.461: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
    Sep  3 21:45:33.454: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 18.009850397s
    Sep  3 21:45:33.454: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
    Sep  3 21:45:35.457: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=false. Elapsed: 20.013092929s
    Sep  3 21:45:35.457: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = false)
    Sep  3 21:45:37.458: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f": Phase="Running", Reason="", readiness=true. Elapsed: 22.014141446s
    Sep  3 21:45:37.458: INFO: The phase of Pod test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f is Running (Ready = true)
    Sep  3 21:45:37.458: INFO: Pod "test-webserver-7bb79a2a-bbf5-46be-a670-bef095b72b7f" satisfied condition "running and ready"
    Sep  3 21:45:37.463: INFO: Container started at 2022-09-03 21:45:16 +0000 UTC, pod became ready at 2022-09-03 21:45:35 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Sep  3 21:45:37.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6384" for this suite. 09/03/22 21:45:37.465
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:45:37.484
Sep  3 21:45:37.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:45:37.485
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:37.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:37.497
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-305ff2f9-3ac7-4e91-a711-d55399812a7c 09/03/22 21:45:37.5
STEP: Creating a pod to test consume configMaps 09/03/22 21:45:37.502
Sep  3 21:45:37.508: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d" in namespace "projected-3314" to be "Succeeded or Failed"
Sep  3 21:45:37.509: INFO: Pod "pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.789112ms
Sep  3 21:45:39.511: INFO: Pod "pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003914536s
Sep  3 21:45:41.513: INFO: Pod "pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005908774s
STEP: Saw pod success 09/03/22 21:45:41.513
Sep  3 21:45:41.514: INFO: Pod "pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d" satisfied condition "Succeeded or Failed"
Sep  3 21:45:41.516: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d container agnhost-container: <nil>
STEP: delete the pod 09/03/22 21:45:41.52
Sep  3 21:45:41.525: INFO: Waiting for pod pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d to disappear
Sep  3 21:45:41.527: INFO: Pod pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Sep  3 21:45:41.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3314" for this suite. 09/03/22 21:45:41.53
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":241,"skipped":4488,"failed":0}
------------------------------
• [4.050 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:45:37.484
    Sep  3 21:45:37.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:45:37.485
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:37.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:37.497
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-305ff2f9-3ac7-4e91-a711-d55399812a7c 09/03/22 21:45:37.5
    STEP: Creating a pod to test consume configMaps 09/03/22 21:45:37.502
    Sep  3 21:45:37.508: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d" in namespace "projected-3314" to be "Succeeded or Failed"
    Sep  3 21:45:37.509: INFO: Pod "pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.789112ms
    Sep  3 21:45:39.511: INFO: Pod "pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003914536s
    Sep  3 21:45:41.513: INFO: Pod "pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005908774s
    STEP: Saw pod success 09/03/22 21:45:41.513
    Sep  3 21:45:41.514: INFO: Pod "pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d" satisfied condition "Succeeded or Failed"
    Sep  3 21:45:41.516: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 21:45:41.52
    Sep  3 21:45:41.525: INFO: Waiting for pod pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d to disappear
    Sep  3 21:45:41.527: INFO: Pod pod-projected-configmaps-6cb0641a-8923-4c95-aa8e-a59215cb369d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Sep  3 21:45:41.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3314" for this suite. 09/03/22 21:45:41.53
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:45:41.54
Sep  3 21:45:41.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pods 09/03/22 21:45:41.54
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:41.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:41.553
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 09/03/22 21:45:41.555
STEP: submitting the pod to kubernetes 09/03/22 21:45:41.556
Sep  3 21:45:41.560: INFO: Waiting up to 5m0s for pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1" in namespace "pods-8202" to be "running and ready"
Sep  3 21:45:41.564: INFO: Pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.909416ms
Sep  3 21:45:41.565: INFO: The phase of Pod pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:45:43.568: INFO: Pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007725989s
Sep  3 21:45:43.568: INFO: The phase of Pod pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1 is Running (Ready = true)
Sep  3 21:45:43.568: INFO: Pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/03/22 21:45:43.57
STEP: updating the pod 09/03/22 21:45:43.572
Sep  3 21:45:44.081: INFO: Successfully updated pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1"
Sep  3 21:45:44.082: INFO: Waiting up to 5m0s for pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1" in namespace "pods-8202" to be "running"
Sep  3 21:45:44.083: INFO: Pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1": Phase="Running", Reason="", readiness=true. Elapsed: 1.791607ms
Sep  3 21:45:44.083: INFO: Pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 09/03/22 21:45:44.083
Sep  3 21:45:44.085: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Sep  3 21:45:44.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8202" for this suite. 09/03/22 21:45:44.088
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":242,"skipped":4532,"failed":0}
------------------------------
• [2.552 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:45:41.54
    Sep  3 21:45:41.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pods 09/03/22 21:45:41.54
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:41.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:41.553
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 09/03/22 21:45:41.555
    STEP: submitting the pod to kubernetes 09/03/22 21:45:41.556
    Sep  3 21:45:41.560: INFO: Waiting up to 5m0s for pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1" in namespace "pods-8202" to be "running and ready"
    Sep  3 21:45:41.564: INFO: Pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.909416ms
    Sep  3 21:45:41.565: INFO: The phase of Pod pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:45:43.568: INFO: Pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007725989s
    Sep  3 21:45:43.568: INFO: The phase of Pod pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1 is Running (Ready = true)
    Sep  3 21:45:43.568: INFO: Pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/03/22 21:45:43.57
    STEP: updating the pod 09/03/22 21:45:43.572
    Sep  3 21:45:44.081: INFO: Successfully updated pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1"
    Sep  3 21:45:44.082: INFO: Waiting up to 5m0s for pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1" in namespace "pods-8202" to be "running"
    Sep  3 21:45:44.083: INFO: Pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1": Phase="Running", Reason="", readiness=true. Elapsed: 1.791607ms
    Sep  3 21:45:44.083: INFO: Pod "pod-update-3e29d2e9-b0db-4561-8925-3e15b3f1fef1" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 09/03/22 21:45:44.083
    Sep  3 21:45:44.085: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Sep  3 21:45:44.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8202" for this suite. 09/03/22 21:45:44.088
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:45:44.094
Sep  3 21:45:44.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:45:44.095
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:44.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:44.106
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-c8fe8a90-3d58-498b-8866-af255e09dfa2 09/03/22 21:45:44.108
STEP: Creating a pod to test consume secrets 09/03/22 21:45:44.111
Sep  3 21:45:44.119: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773" in namespace "projected-4568" to be "Succeeded or Failed"
Sep  3 21:45:44.124: INFO: Pod "pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773": Phase="Pending", Reason="", readiness=false. Elapsed: 4.439518ms
Sep  3 21:45:46.127: INFO: Pod "pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008094191s
Sep  3 21:45:48.127: INFO: Pod "pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007764212s
STEP: Saw pod success 09/03/22 21:45:48.127
Sep  3 21:45:48.127: INFO: Pod "pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773" satisfied condition "Succeeded or Failed"
Sep  3 21:45:48.129: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/03/22 21:45:48.133
Sep  3 21:45:48.139: INFO: Waiting for pod pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773 to disappear
Sep  3 21:45:48.141: INFO: Pod pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Sep  3 21:45:48.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4568" for this suite. 09/03/22 21:45:48.143
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":243,"skipped":4562,"failed":0}
------------------------------
• [4.054 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:45:44.094
    Sep  3 21:45:44.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:45:44.095
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:44.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:44.106
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-c8fe8a90-3d58-498b-8866-af255e09dfa2 09/03/22 21:45:44.108
    STEP: Creating a pod to test consume secrets 09/03/22 21:45:44.111
    Sep  3 21:45:44.119: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773" in namespace "projected-4568" to be "Succeeded or Failed"
    Sep  3 21:45:44.124: INFO: Pod "pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773": Phase="Pending", Reason="", readiness=false. Elapsed: 4.439518ms
    Sep  3 21:45:46.127: INFO: Pod "pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008094191s
    Sep  3 21:45:48.127: INFO: Pod "pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007764212s
    STEP: Saw pod success 09/03/22 21:45:48.127
    Sep  3 21:45:48.127: INFO: Pod "pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773" satisfied condition "Succeeded or Failed"
    Sep  3 21:45:48.129: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 21:45:48.133
    Sep  3 21:45:48.139: INFO: Waiting for pod pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773 to disappear
    Sep  3 21:45:48.141: INFO: Pod pod-projected-secrets-4e07b296-e3b2-44ee-8b88-a8fd8cb10773 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Sep  3 21:45:48.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4568" for this suite. 09/03/22 21:45:48.143
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:45:48.15
Sep  3 21:45:48.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename statefulset 09/03/22 21:45:48.151
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:48.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:48.162
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5640 09/03/22 21:45:48.165
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-5640 09/03/22 21:45:48.174
Sep  3 21:45:48.181: INFO: Found 0 stateful pods, waiting for 1
Sep  3 21:45:58.185: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 09/03/22 21:45:58.189
STEP: Getting /status 09/03/22 21:45:58.193
Sep  3 21:45:58.199: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 09/03/22 21:45:58.199
Sep  3 21:45:58.205: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 09/03/22 21:45:58.205
Sep  3 21:45:58.207: INFO: Observed &StatefulSet event: ADDED
Sep  3 21:45:58.207: INFO: Found Statefulset ss in namespace statefulset-5640 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  3 21:45:58.207: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 09/03/22 21:45:58.207
Sep  3 21:45:58.207: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  3 21:45:58.211: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 09/03/22 21:45:58.211
Sep  3 21:45:58.213: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Sep  3 21:45:58.213: INFO: Deleting all statefulset in ns statefulset-5640
Sep  3 21:45:58.214: INFO: Scaling statefulset ss to 0
Sep  3 21:46:08.227: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 21:46:08.229: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Sep  3 21:46:08.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5640" for this suite. 09/03/22 21:46:08.245
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":244,"skipped":4575,"failed":0}
------------------------------
• [SLOW TEST] [20.100 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:45:48.15
    Sep  3 21:45:48.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename statefulset 09/03/22 21:45:48.151
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:45:48.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:45:48.162
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5640 09/03/22 21:45:48.165
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-5640 09/03/22 21:45:48.174
    Sep  3 21:45:48.181: INFO: Found 0 stateful pods, waiting for 1
    Sep  3 21:45:58.185: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 09/03/22 21:45:58.189
    STEP: Getting /status 09/03/22 21:45:58.193
    Sep  3 21:45:58.199: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 09/03/22 21:45:58.199
    Sep  3 21:45:58.205: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 09/03/22 21:45:58.205
    Sep  3 21:45:58.207: INFO: Observed &StatefulSet event: ADDED
    Sep  3 21:45:58.207: INFO: Found Statefulset ss in namespace statefulset-5640 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  3 21:45:58.207: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 09/03/22 21:45:58.207
    Sep  3 21:45:58.207: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  3 21:45:58.211: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 09/03/22 21:45:58.211
    Sep  3 21:45:58.213: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Sep  3 21:45:58.213: INFO: Deleting all statefulset in ns statefulset-5640
    Sep  3 21:45:58.214: INFO: Scaling statefulset ss to 0
    Sep  3 21:46:08.227: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  3 21:46:08.229: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Sep  3 21:46:08.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5640" for this suite. 09/03/22 21:46:08.245
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:46:08.254
Sep  3 21:46:08.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename controllerrevisions 09/03/22 21:46:08.255
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:08.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:08.269
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-zbb82-daemon-set" 09/03/22 21:46:08.285
STEP: Check that daemon pods launch on every node of the cluster. 09/03/22 21:46:08.289
Sep  3 21:46:08.292: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:46:08.296: INFO: Number of nodes with available pods controlled by daemonset e2e-zbb82-daemon-set: 0
Sep  3 21:46:08.296: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 21:46:09.300: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:46:09.302: INFO: Number of nodes with available pods controlled by daemonset e2e-zbb82-daemon-set: 0
Sep  3 21:46:09.302: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 21:46:10.301: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 21:46:10.304: INFO: Number of nodes with available pods controlled by daemonset e2e-zbb82-daemon-set: 2
Sep  3 21:46:10.304: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-zbb82-daemon-set
STEP: Confirm DaemonSet "e2e-zbb82-daemon-set" successfully created with "daemonset-name=e2e-zbb82-daemon-set" label 09/03/22 21:46:10.306
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-zbb82-daemon-set" 09/03/22 21:46:10.31
Sep  3 21:46:10.312: INFO: Located ControllerRevision: "e2e-zbb82-daemon-set-6b4cdf44ff"
STEP: Patching ControllerRevision "e2e-zbb82-daemon-set-6b4cdf44ff" 09/03/22 21:46:10.314
Sep  3 21:46:10.318: INFO: e2e-zbb82-daemon-set-6b4cdf44ff has been patched
STEP: Create a new ControllerRevision 09/03/22 21:46:10.318
Sep  3 21:46:10.322: INFO: Created ControllerRevision: e2e-zbb82-daemon-set-c997f6dff
STEP: Confirm that there are two ControllerRevisions 09/03/22 21:46:10.323
Sep  3 21:46:10.323: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  3 21:46:10.325: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-zbb82-daemon-set-6b4cdf44ff" 09/03/22 21:46:10.325
STEP: Confirm that there is only one ControllerRevision 09/03/22 21:46:10.328
Sep  3 21:46:10.328: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  3 21:46:10.329: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-zbb82-daemon-set-c997f6dff" 09/03/22 21:46:10.331
Sep  3 21:46:10.335: INFO: e2e-zbb82-daemon-set-c997f6dff has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 09/03/22 21:46:10.335
W0903 21:46:10.340127      24 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 09/03/22 21:46:10.34
Sep  3 21:46:10.340: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  3 21:46:11.342: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  3 21:46:11.345: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-zbb82-daemon-set-c997f6dff=updated" 09/03/22 21:46:11.345
STEP: Confirm that there is only one ControllerRevision 09/03/22 21:46:11.351
Sep  3 21:46:11.351: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  3 21:46:11.353: INFO: Found 1 ControllerRevisions
Sep  3 21:46:11.355: INFO: ControllerRevision "e2e-zbb82-daemon-set-58d4d7f54b" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-zbb82-daemon-set" 09/03/22 21:46:11.357
STEP: deleting DaemonSet.extensions e2e-zbb82-daemon-set in namespace controllerrevisions-3716, will wait for the garbage collector to delete the pods 09/03/22 21:46:11.357
Sep  3 21:46:11.414: INFO: Deleting DaemonSet.extensions e2e-zbb82-daemon-set took: 3.853409ms
Sep  3 21:46:11.514: INFO: Terminating DaemonSet.extensions e2e-zbb82-daemon-set pods took: 100.15195ms
Sep  3 21:46:12.917: INFO: Number of nodes with available pods controlled by daemonset e2e-zbb82-daemon-set: 0
Sep  3 21:46:12.917: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-zbb82-daemon-set
Sep  3 21:46:12.919: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20443"},"items":null}

Sep  3 21:46:12.920: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20443"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:46:12.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-3716" for this suite. 09/03/22 21:46:12.928
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":245,"skipped":4576,"failed":0}
------------------------------
• [4.677 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:46:08.254
    Sep  3 21:46:08.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename controllerrevisions 09/03/22 21:46:08.255
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:08.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:08.269
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-zbb82-daemon-set" 09/03/22 21:46:08.285
    STEP: Check that daemon pods launch on every node of the cluster. 09/03/22 21:46:08.289
    Sep  3 21:46:08.292: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:46:08.296: INFO: Number of nodes with available pods controlled by daemonset e2e-zbb82-daemon-set: 0
    Sep  3 21:46:08.296: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 21:46:09.300: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:46:09.302: INFO: Number of nodes with available pods controlled by daemonset e2e-zbb82-daemon-set: 0
    Sep  3 21:46:09.302: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 21:46:10.301: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 21:46:10.304: INFO: Number of nodes with available pods controlled by daemonset e2e-zbb82-daemon-set: 2
    Sep  3 21:46:10.304: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-zbb82-daemon-set
    STEP: Confirm DaemonSet "e2e-zbb82-daemon-set" successfully created with "daemonset-name=e2e-zbb82-daemon-set" label 09/03/22 21:46:10.306
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-zbb82-daemon-set" 09/03/22 21:46:10.31
    Sep  3 21:46:10.312: INFO: Located ControllerRevision: "e2e-zbb82-daemon-set-6b4cdf44ff"
    STEP: Patching ControllerRevision "e2e-zbb82-daemon-set-6b4cdf44ff" 09/03/22 21:46:10.314
    Sep  3 21:46:10.318: INFO: e2e-zbb82-daemon-set-6b4cdf44ff has been patched
    STEP: Create a new ControllerRevision 09/03/22 21:46:10.318
    Sep  3 21:46:10.322: INFO: Created ControllerRevision: e2e-zbb82-daemon-set-c997f6dff
    STEP: Confirm that there are two ControllerRevisions 09/03/22 21:46:10.323
    Sep  3 21:46:10.323: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  3 21:46:10.325: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-zbb82-daemon-set-6b4cdf44ff" 09/03/22 21:46:10.325
    STEP: Confirm that there is only one ControllerRevision 09/03/22 21:46:10.328
    Sep  3 21:46:10.328: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  3 21:46:10.329: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-zbb82-daemon-set-c997f6dff" 09/03/22 21:46:10.331
    Sep  3 21:46:10.335: INFO: e2e-zbb82-daemon-set-c997f6dff has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 09/03/22 21:46:10.335
    W0903 21:46:10.340127      24 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 09/03/22 21:46:10.34
    Sep  3 21:46:10.340: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  3 21:46:11.342: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  3 21:46:11.345: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-zbb82-daemon-set-c997f6dff=updated" 09/03/22 21:46:11.345
    STEP: Confirm that there is only one ControllerRevision 09/03/22 21:46:11.351
    Sep  3 21:46:11.351: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  3 21:46:11.353: INFO: Found 1 ControllerRevisions
    Sep  3 21:46:11.355: INFO: ControllerRevision "e2e-zbb82-daemon-set-58d4d7f54b" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-zbb82-daemon-set" 09/03/22 21:46:11.357
    STEP: deleting DaemonSet.extensions e2e-zbb82-daemon-set in namespace controllerrevisions-3716, will wait for the garbage collector to delete the pods 09/03/22 21:46:11.357
    Sep  3 21:46:11.414: INFO: Deleting DaemonSet.extensions e2e-zbb82-daemon-set took: 3.853409ms
    Sep  3 21:46:11.514: INFO: Terminating DaemonSet.extensions e2e-zbb82-daemon-set pods took: 100.15195ms
    Sep  3 21:46:12.917: INFO: Number of nodes with available pods controlled by daemonset e2e-zbb82-daemon-set: 0
    Sep  3 21:46:12.917: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-zbb82-daemon-set
    Sep  3 21:46:12.919: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20443"},"items":null}

    Sep  3 21:46:12.920: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20443"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:46:12.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-3716" for this suite. 09/03/22 21:46:12.928
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:46:12.94
Sep  3 21:46:12.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 21:46:12.941
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:12.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:12.97
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-5563 09/03/22 21:46:12.972
STEP: creating service affinity-clusterip in namespace services-5563 09/03/22 21:46:12.972
STEP: creating replication controller affinity-clusterip in namespace services-5563 09/03/22 21:46:12.976
I0903 21:46:12.998267      24 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5563, replica count: 3
I0903 21:46:16.049205      24 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 21:46:16.052: INFO: Creating new exec pod
Sep  3 21:46:16.055: INFO: Waiting up to 5m0s for pod "execpod-affinityv7wwk" in namespace "services-5563" to be "running"
Sep  3 21:46:16.058: INFO: Pod "execpod-affinityv7wwk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.378305ms
Sep  3 21:46:18.062: INFO: Pod "execpod-affinityv7wwk": Phase="Running", Reason="", readiness=true. Elapsed: 2.006035115s
Sep  3 21:46:18.062: INFO: Pod "execpod-affinityv7wwk" satisfied condition "running"
Sep  3 21:46:19.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5563 exec execpod-affinityv7wwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Sep  3 21:46:19.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep  3 21:46:19.222: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 21:46:19.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5563 exec execpod-affinityv7wwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.62.12 80'
Sep  3 21:46:19.378: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.62.12 80\nConnection to 10.96.62.12 80 port [tcp/http] succeeded!\n"
Sep  3 21:46:19.378: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 21:46:19.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5563 exec execpod-affinityv7wwk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.62.12:80/ ; done'
Sep  3 21:46:19.629: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n"
Sep  3 21:46:19.629: INFO: stdout: "\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p"
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
Sep  3 21:46:19.629: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5563, will wait for the garbage collector to delete the pods 09/03/22 21:46:19.639
Sep  3 21:46:19.698: INFO: Deleting ReplicationController affinity-clusterip took: 3.608907ms
Sep  3 21:46:19.799: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.117097ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 21:46:21.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5563" for this suite. 09/03/22 21:46:21.96
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":246,"skipped":4596,"failed":0}
------------------------------
• [SLOW TEST] [9.029 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:46:12.94
    Sep  3 21:46:12.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 21:46:12.941
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:12.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:12.97
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-5563 09/03/22 21:46:12.972
    STEP: creating service affinity-clusterip in namespace services-5563 09/03/22 21:46:12.972
    STEP: creating replication controller affinity-clusterip in namespace services-5563 09/03/22 21:46:12.976
    I0903 21:46:12.998267      24 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5563, replica count: 3
    I0903 21:46:16.049205      24 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  3 21:46:16.052: INFO: Creating new exec pod
    Sep  3 21:46:16.055: INFO: Waiting up to 5m0s for pod "execpod-affinityv7wwk" in namespace "services-5563" to be "running"
    Sep  3 21:46:16.058: INFO: Pod "execpod-affinityv7wwk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.378305ms
    Sep  3 21:46:18.062: INFO: Pod "execpod-affinityv7wwk": Phase="Running", Reason="", readiness=true. Elapsed: 2.006035115s
    Sep  3 21:46:18.062: INFO: Pod "execpod-affinityv7wwk" satisfied condition "running"
    Sep  3 21:46:19.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5563 exec execpod-affinityv7wwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Sep  3 21:46:19.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Sep  3 21:46:19.222: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 21:46:19.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5563 exec execpod-affinityv7wwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.62.12 80'
    Sep  3 21:46:19.378: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.62.12 80\nConnection to 10.96.62.12 80 port [tcp/http] succeeded!\n"
    Sep  3 21:46:19.378: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 21:46:19.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5563 exec execpod-affinityv7wwk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.62.12:80/ ; done'
    Sep  3 21:46:19.629: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.62.12:80/\n"
    Sep  3 21:46:19.629: INFO: stdout: "\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p\naffinity-clusterip-tzv2p"
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Received response from host: affinity-clusterip-tzv2p
    Sep  3 21:46:19.629: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-5563, will wait for the garbage collector to delete the pods 09/03/22 21:46:19.639
    Sep  3 21:46:19.698: INFO: Deleting ReplicationController affinity-clusterip took: 3.608907ms
    Sep  3 21:46:19.799: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.117097ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 21:46:21.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5563" for this suite. 09/03/22 21:46:21.96
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:46:21.969
Sep  3 21:46:21.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-runtime 09/03/22 21:46:21.972
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:21.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:21.996
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 09/03/22 21:46:21.999
STEP: wait for the container to reach Succeeded 09/03/22 21:46:22.006
STEP: get the container status 09/03/22 21:46:26.021
STEP: the container should be terminated 09/03/22 21:46:26.03
STEP: the termination message should be set 09/03/22 21:46:26.03
Sep  3 21:46:26.030: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 09/03/22 21:46:26.03
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Sep  3 21:46:26.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9677" for this suite. 09/03/22 21:46:26.04
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":247,"skipped":4599,"failed":0}
------------------------------
• [4.074 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:46:21.969
    Sep  3 21:46:21.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-runtime 09/03/22 21:46:21.972
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:21.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:21.996
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 09/03/22 21:46:21.999
    STEP: wait for the container to reach Succeeded 09/03/22 21:46:22.006
    STEP: get the container status 09/03/22 21:46:26.021
    STEP: the container should be terminated 09/03/22 21:46:26.03
    STEP: the termination message should be set 09/03/22 21:46:26.03
    Sep  3 21:46:26.030: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 09/03/22 21:46:26.03
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Sep  3 21:46:26.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9677" for this suite. 09/03/22 21:46:26.04
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:46:26.047
Sep  3 21:46:26.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename custom-resource-definition 09/03/22 21:46:26.048
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:26.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:26.066
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Sep  3 21:46:26.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:46:29.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7293" for this suite. 09/03/22 21:46:29.192
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":248,"skipped":4645,"failed":0}
------------------------------
• [3.148 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:46:26.047
    Sep  3 21:46:26.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename custom-resource-definition 09/03/22 21:46:26.048
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:26.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:26.066
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Sep  3 21:46:26.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:46:29.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7293" for this suite. 09/03/22 21:46:29.192
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:46:29.196
Sep  3 21:46:29.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:46:29.197
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:29.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:29.208
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 09/03/22 21:46:29.21
Sep  3 21:46:29.211: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep  3 21:46:29.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 create -f -'
Sep  3 21:46:30.126: INFO: stderr: ""
Sep  3 21:46:30.126: INFO: stdout: "service/agnhost-replica created\n"
Sep  3 21:46:30.126: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep  3 21:46:30.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 create -f -'
Sep  3 21:46:31.193: INFO: stderr: ""
Sep  3 21:46:31.193: INFO: stdout: "service/agnhost-primary created\n"
Sep  3 21:46:31.197: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  3 21:46:31.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 create -f -'
Sep  3 21:46:31.582: INFO: stderr: ""
Sep  3 21:46:31.583: INFO: stdout: "service/frontend created\n"
Sep  3 21:46:31.583: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep  3 21:46:31.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 create -f -'
Sep  3 21:46:31.763: INFO: stderr: ""
Sep  3 21:46:31.763: INFO: stdout: "deployment.apps/frontend created\n"
Sep  3 21:46:31.763: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  3 21:46:31.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 create -f -'
Sep  3 21:46:32.071: INFO: stderr: ""
Sep  3 21:46:32.071: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep  3 21:46:32.071: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  3 21:46:32.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 create -f -'
Sep  3 21:46:32.874: INFO: stderr: ""
Sep  3 21:46:32.874: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 09/03/22 21:46:32.874
Sep  3 21:46:32.875: INFO: Waiting for all frontend pods to be Running.
Sep  3 21:46:37.934: INFO: Waiting for frontend to serve content.
Sep  3 21:46:37.942: INFO: Trying to add a new entry to the guestbook.
Sep  3 21:46:37.955: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 09/03/22 21:46:37.961
Sep  3 21:46:37.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 delete --grace-period=0 --force -f -'
Sep  3 21:46:38.104: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 21:46:38.104: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 09/03/22 21:46:38.105
Sep  3 21:46:38.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 delete --grace-period=0 --force -f -'
Sep  3 21:46:38.356: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 21:46:38.356: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/03/22 21:46:38.356
Sep  3 21:46:38.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 delete --grace-period=0 --force -f -'
Sep  3 21:46:38.466: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 21:46:38.466: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/03/22 21:46:38.467
Sep  3 21:46:38.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 delete --grace-period=0 --force -f -'
Sep  3 21:46:38.537: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 21:46:38.537: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/03/22 21:46:38.538
Sep  3 21:46:38.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 delete --grace-period=0 --force -f -'
Sep  3 21:46:38.758: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 21:46:38.758: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/03/22 21:46:38.758
Sep  3 21:46:38.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 delete --grace-period=0 --force -f -'
Sep  3 21:46:39.023: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 21:46:39.023: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:46:39.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5089" for this suite. 09/03/22 21:46:39.03
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":249,"skipped":4649,"failed":0}
------------------------------
• [SLOW TEST] [9.855 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:46:29.196
    Sep  3 21:46:29.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:46:29.197
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:29.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:29.208
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 09/03/22 21:46:29.21
    Sep  3 21:46:29.211: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Sep  3 21:46:29.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 create -f -'
    Sep  3 21:46:30.126: INFO: stderr: ""
    Sep  3 21:46:30.126: INFO: stdout: "service/agnhost-replica created\n"
    Sep  3 21:46:30.126: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Sep  3 21:46:30.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 create -f -'
    Sep  3 21:46:31.193: INFO: stderr: ""
    Sep  3 21:46:31.193: INFO: stdout: "service/agnhost-primary created\n"
    Sep  3 21:46:31.197: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Sep  3 21:46:31.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 create -f -'
    Sep  3 21:46:31.582: INFO: stderr: ""
    Sep  3 21:46:31.583: INFO: stdout: "service/frontend created\n"
    Sep  3 21:46:31.583: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Sep  3 21:46:31.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 create -f -'
    Sep  3 21:46:31.763: INFO: stderr: ""
    Sep  3 21:46:31.763: INFO: stdout: "deployment.apps/frontend created\n"
    Sep  3 21:46:31.763: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  3 21:46:31.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 create -f -'
    Sep  3 21:46:32.071: INFO: stderr: ""
    Sep  3 21:46:32.071: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Sep  3 21:46:32.071: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  3 21:46:32.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 create -f -'
    Sep  3 21:46:32.874: INFO: stderr: ""
    Sep  3 21:46:32.874: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 09/03/22 21:46:32.874
    Sep  3 21:46:32.875: INFO: Waiting for all frontend pods to be Running.
    Sep  3 21:46:37.934: INFO: Waiting for frontend to serve content.
    Sep  3 21:46:37.942: INFO: Trying to add a new entry to the guestbook.
    Sep  3 21:46:37.955: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 09/03/22 21:46:37.961
    Sep  3 21:46:37.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 delete --grace-period=0 --force -f -'
    Sep  3 21:46:38.104: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  3 21:46:38.104: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 09/03/22 21:46:38.105
    Sep  3 21:46:38.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 delete --grace-period=0 --force -f -'
    Sep  3 21:46:38.356: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  3 21:46:38.356: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/03/22 21:46:38.356
    Sep  3 21:46:38.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 delete --grace-period=0 --force -f -'
    Sep  3 21:46:38.466: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  3 21:46:38.466: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/03/22 21:46:38.467
    Sep  3 21:46:38.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 delete --grace-period=0 --force -f -'
    Sep  3 21:46:38.537: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  3 21:46:38.537: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/03/22 21:46:38.538
    Sep  3 21:46:38.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 delete --grace-period=0 --force -f -'
    Sep  3 21:46:38.758: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  3 21:46:38.758: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/03/22 21:46:38.758
    Sep  3 21:46:38.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-5089 delete --grace-period=0 --force -f -'
    Sep  3 21:46:39.023: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  3 21:46:39.023: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:46:39.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5089" for this suite. 09/03/22 21:46:39.03
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:46:39.053
Sep  3 21:46:39.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pods 09/03/22 21:46:39.066
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:39.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:39.13
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 09/03/22 21:46:39.138
Sep  3 21:46:39.150: INFO: Waiting up to 5m0s for pod "pod-fhhcv" in namespace "pods-2201" to be "running"
Sep  3 21:46:39.165: INFO: Pod "pod-fhhcv": Phase="Pending", Reason="", readiness=false. Elapsed: 14.754434ms
Sep  3 21:46:41.168: INFO: Pod "pod-fhhcv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017827205s
Sep  3 21:46:43.168: INFO: Pod "pod-fhhcv": Phase="Running", Reason="", readiness=true. Elapsed: 4.017808569s
Sep  3 21:46:43.168: INFO: Pod "pod-fhhcv" satisfied condition "running"
STEP: patching /status 09/03/22 21:46:43.168
Sep  3 21:46:43.177: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Sep  3 21:46:43.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2201" for this suite. 09/03/22 21:46:43.182
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":250,"skipped":4669,"failed":0}
------------------------------
• [4.132 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:46:39.053
    Sep  3 21:46:39.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pods 09/03/22 21:46:39.066
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:39.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:39.13
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 09/03/22 21:46:39.138
    Sep  3 21:46:39.150: INFO: Waiting up to 5m0s for pod "pod-fhhcv" in namespace "pods-2201" to be "running"
    Sep  3 21:46:39.165: INFO: Pod "pod-fhhcv": Phase="Pending", Reason="", readiness=false. Elapsed: 14.754434ms
    Sep  3 21:46:41.168: INFO: Pod "pod-fhhcv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017827205s
    Sep  3 21:46:43.168: INFO: Pod "pod-fhhcv": Phase="Running", Reason="", readiness=true. Elapsed: 4.017808569s
    Sep  3 21:46:43.168: INFO: Pod "pod-fhhcv" satisfied condition "running"
    STEP: patching /status 09/03/22 21:46:43.168
    Sep  3 21:46:43.177: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Sep  3 21:46:43.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2201" for this suite. 09/03/22 21:46:43.182
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:46:43.187
Sep  3 21:46:43.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename security-context-test 09/03/22 21:46:43.187
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:43.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:43.202
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Sep  3 21:46:43.209: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133" in namespace "security-context-test-607" to be "Succeeded or Failed"
Sep  3 21:46:43.217: INFO: Pod "busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133": Phase="Pending", Reason="", readiness=false. Elapsed: 7.488517ms
Sep  3 21:46:45.220: INFO: Pod "busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010255975s
Sep  3 21:46:47.220: INFO: Pod "busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010510773s
Sep  3 21:46:47.220: INFO: Pod "busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133" satisfied condition "Succeeded or Failed"
Sep  3 21:46:47.225: INFO: Got logs for pod "busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Sep  3 21:46:47.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-607" for this suite. 09/03/22 21:46:47.228
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":251,"skipped":4710,"failed":0}
------------------------------
• [4.045 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:46:43.187
    Sep  3 21:46:43.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename security-context-test 09/03/22 21:46:43.187
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:43.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:43.202
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Sep  3 21:46:43.209: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133" in namespace "security-context-test-607" to be "Succeeded or Failed"
    Sep  3 21:46:43.217: INFO: Pod "busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133": Phase="Pending", Reason="", readiness=false. Elapsed: 7.488517ms
    Sep  3 21:46:45.220: INFO: Pod "busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010255975s
    Sep  3 21:46:47.220: INFO: Pod "busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010510773s
    Sep  3 21:46:47.220: INFO: Pod "busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133" satisfied condition "Succeeded or Failed"
    Sep  3 21:46:47.225: INFO: Got logs for pod "busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Sep  3 21:46:47.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-607" for this suite. 09/03/22 21:46:47.228
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:46:47.237
Sep  3 21:46:47.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename tables 09/03/22 21:46:47.238
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:47.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:47.248
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Sep  3 21:46:47.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3634" for this suite. 09/03/22 21:46:47.256
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":252,"skipped":4722,"failed":0}
------------------------------
• [0.024 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:46:47.237
    Sep  3 21:46:47.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename tables 09/03/22 21:46:47.238
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:47.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:47.248
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Sep  3 21:46:47.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-3634" for this suite. 09/03/22 21:46:47.256
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:46:47.265
Sep  3 21:46:47.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename sched-pred 09/03/22 21:46:47.266
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:47.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:47.277
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Sep  3 21:46:47.280: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  3 21:46:47.288: INFO: Waiting for terminating namespaces to be deleted...
Sep  3 21:46:47.291: INFO: 
Logging pods the apiserver thinks is on node kind-worker before test
Sep  3 21:46:47.295: INFO: kindnet-xhxcg from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
Sep  3 21:46:47.295: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  3 21:46:47.295: INFO: kube-proxy-bxm4p from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
Sep  3 21:46:47.295: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 21:46:47.295: INFO: sonobuoy from sonobuoy started at 2022-09-03 20:37:57 +0000 UTC (1 container statuses recorded)
Sep  3 21:46:47.295: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  3 21:46:47.295: INFO: sonobuoy-e2e-job-31a458d0b0c040d4 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
Sep  3 21:46:47.295: INFO: 	Container e2e ready: true, restart count 0
Sep  3 21:46:47.295: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 21:46:47.295: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-jdmz7 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
Sep  3 21:46:47.295: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 21:46:47.295: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 21:46:47.295: INFO: 
Logging pods the apiserver thinks is on node kind-worker2 before test
Sep  3 21:46:47.300: INFO: kindnet-zlt4d from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
Sep  3 21:46:47.300: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  3 21:46:47.300: INFO: kube-proxy-wz2c5 from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
Sep  3 21:46:47.300: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 21:46:47.300: INFO: pod-fhhcv from pods-2201 started at 2022-09-03 21:46:39 +0000 UTC (1 container statuses recorded)
Sep  3 21:46:47.300: INFO: 	Container agnhost ready: true, restart count 0
Sep  3 21:46:47.300: INFO: busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133 from security-context-test-607 started at 2022-09-03 21:46:43 +0000 UTC (1 container statuses recorded)
Sep  3 21:46:47.300: INFO: 	Container busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133 ready: false, restart count 0
Sep  3 21:46:47.300: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-nwk5b from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
Sep  3 21:46:47.300: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 21:46:47.300: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/03/22 21:46:47.301
Sep  3 21:46:47.306: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-495" to be "running"
Sep  3 21:46:47.308: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115403ms
Sep  3 21:46:49.311: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004290203s
Sep  3 21:46:49.311: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/03/22 21:46:49.312
STEP: Trying to apply a random label on the found node. 09/03/22 21:46:49.323
STEP: verifying the node has the label kubernetes.io/e2e-3dc41b79-e424-4e1e-84cb-abed530ed895 95 09/03/22 21:46:49.329
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/03/22 21:46:49.338
Sep  3 21:46:49.343: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-495" to be "not pending"
Sep  3 21:46:49.352: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.076812ms
Sep  3 21:46:51.355: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011849713s
Sep  3 21:46:51.355: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.18.0.3 on the node which pod4 resides and expect not scheduled 09/03/22 21:46:51.355
Sep  3 21:46:51.359: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-495" to be "not pending"
Sep  3 21:46:51.364: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.523807ms
Sep  3 21:46:53.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00874909s
Sep  3 21:46:55.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008977623s
Sep  3 21:46:57.371: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012054964s
Sep  3 21:46:59.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01013399s
Sep  3 21:47:01.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00795462s
Sep  3 21:47:03.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00877345s
Sep  3 21:47:05.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008158777s
Sep  3 21:47:07.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008462606s
Sep  3 21:47:09.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010527799s
Sep  3 21:47:11.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009108617s
Sep  3 21:47:13.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009332937s
Sep  3 21:47:15.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008245955s
Sep  3 21:47:17.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008807348s
Sep  3 21:47:19.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.00941665s
Sep  3 21:47:21.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009968351s
Sep  3 21:47:23.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.009229949s
Sep  3 21:47:25.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.00810467s
Sep  3 21:47:27.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008455067s
Sep  3 21:47:29.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008430864s
Sep  3 21:47:31.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010428963s
Sep  3 21:47:33.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008788999s
Sep  3 21:47:35.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008903458s
Sep  3 21:47:37.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008541916s
Sep  3 21:47:39.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008692274s
Sep  3 21:47:41.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008574454s
Sep  3 21:47:43.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.00844031s
Sep  3 21:47:45.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008195665s
Sep  3 21:47:47.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008416522s
Sep  3 21:47:49.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008733104s
Sep  3 21:47:51.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008317439s
Sep  3 21:47:53.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008191674s
Sep  3 21:47:55.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00850311s
Sep  3 21:47:57.366: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007638726s
Sep  3 21:47:59.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008567456s
Sep  3 21:48:01.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008420083s
Sep  3 21:48:03.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008629812s
Sep  3 21:48:05.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008290539s
Sep  3 21:48:07.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008341367s
Sep  3 21:48:09.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009942599s
Sep  3 21:48:11.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008054323s
Sep  3 21:48:13.370: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011383525s
Sep  3 21:48:15.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.00833275s
Sep  3 21:48:17.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00848868s
Sep  3 21:48:19.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008559402s
Sep  3 21:48:21.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008134547s
Sep  3 21:48:23.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008759643s
Sep  3 21:48:25.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008956338s
Sep  3 21:48:27.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010413236s
Sep  3 21:48:29.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008552189s
Sep  3 21:48:31.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.00823053s
Sep  3 21:48:33.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008631172s
Sep  3 21:48:35.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008144813s
Sep  3 21:48:37.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008013666s
Sep  3 21:48:39.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008969396s
Sep  3 21:48:41.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008371923s
Sep  3 21:48:43.366: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.007832649s
Sep  3 21:48:45.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008636756s
Sep  3 21:48:47.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009273751s
Sep  3 21:48:49.370: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.01135595s
Sep  3 21:48:51.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008337737s
Sep  3 21:48:53.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.008489209s
Sep  3 21:48:55.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008841474s
Sep  3 21:48:57.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008335636s
Sep  3 21:48:59.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.0083003s
Sep  3 21:49:01.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.009107301s
Sep  3 21:49:03.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.008354412s
Sep  3 21:49:05.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008897527s
Sep  3 21:49:07.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.008235939s
Sep  3 21:49:09.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.00845518s
Sep  3 21:49:11.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008916037s
Sep  3 21:49:13.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008019991s
Sep  3 21:49:15.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.008849148s
Sep  3 21:49:17.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008022757s
Sep  3 21:49:19.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009056681s
Sep  3 21:49:21.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.008227102s
Sep  3 21:49:23.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008246924s
Sep  3 21:49:25.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.00814739s
Sep  3 21:49:27.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.008520406s
Sep  3 21:49:29.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.00834412s
Sep  3 21:49:31.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008019534s
Sep  3 21:49:33.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.010165377s
Sep  3 21:49:35.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009611983s
Sep  3 21:49:37.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.00902449s
Sep  3 21:49:39.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.008569796s
Sep  3 21:49:41.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008536227s
Sep  3 21:49:43.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008741654s
Sep  3 21:49:45.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008482079s
Sep  3 21:49:47.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009603708s
Sep  3 21:49:49.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.0089479s
Sep  3 21:49:51.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008154072s
Sep  3 21:49:53.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008015046s
Sep  3 21:49:55.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.00831072s
Sep  3 21:49:57.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008248397s
Sep  3 21:49:59.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008451276s
Sep  3 21:50:01.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009411257s
Sep  3 21:50:03.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008250933s
Sep  3 21:50:05.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008241223s
Sep  3 21:50:07.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.008559122s
Sep  3 21:50:09.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.008854421s
Sep  3 21:50:11.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.00927562s
Sep  3 21:50:13.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.008214016s
Sep  3 21:50:15.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008129752s
Sep  3 21:50:17.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008311088s
Sep  3 21:50:19.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008722725s
Sep  3 21:50:21.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008356943s
Sep  3 21:50:23.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.007913716s
Sep  3 21:50:25.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008218491s
Sep  3 21:50:27.371: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.012663474s
Sep  3 21:50:29.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.008915592s
Sep  3 21:50:31.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008973187s
Sep  3 21:50:33.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008076179s
Sep  3 21:50:35.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.008967275s
Sep  3 21:50:37.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.00877217s
Sep  3 21:50:39.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.009039926s
Sep  3 21:50:41.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.008410782s
Sep  3 21:50:43.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007991537s
Sep  3 21:50:45.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008352802s
Sep  3 21:50:47.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009137919s
Sep  3 21:50:49.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.010561137s
Sep  3 21:50:51.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008723849s
Sep  3 21:50:53.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007986068s
Sep  3 21:50:55.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009319553s
Sep  3 21:50:57.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.015627049s
Sep  3 21:50:59.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.008821718s
Sep  3 21:51:01.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.010590103s
Sep  3 21:51:03.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008870105s
Sep  3 21:51:05.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.008852809s
Sep  3 21:51:07.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008869798s
Sep  3 21:51:09.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008859309s
Sep  3 21:51:11.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00831931s
Sep  3 21:51:13.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009326315s
Sep  3 21:51:15.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009199117s
Sep  3 21:51:17.370: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.01162803s
Sep  3 21:51:19.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008495911s
Sep  3 21:51:21.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008115998s
Sep  3 21:51:23.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009953688s
Sep  3 21:51:25.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008032401s
Sep  3 21:51:27.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008773894s
Sep  3 21:51:29.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007920384s
Sep  3 21:51:31.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007986576s
Sep  3 21:51:33.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009155168s
Sep  3 21:51:35.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008575334s
Sep  3 21:51:37.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008008601s
Sep  3 21:51:39.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.00876017s
Sep  3 21:51:41.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008916034s
Sep  3 21:51:43.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.008376013s
Sep  3 21:51:45.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008431193s
Sep  3 21:51:47.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009738076s
Sep  3 21:51:49.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.008696496s
Sep  3 21:51:51.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008019951s
Sep  3 21:51:51.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009885555s
STEP: removing the label kubernetes.io/e2e-3dc41b79-e424-4e1e-84cb-abed530ed895 off the node kind-worker2 09/03/22 21:51:51.369
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3dc41b79-e424-4e1e-84cb-abed530ed895 09/03/22 21:51:51.376
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:51:51.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-495" for this suite. 09/03/22 21:51:51.386
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":253,"skipped":4731,"failed":0}
------------------------------
• [SLOW TEST] [304.125 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:46:47.265
    Sep  3 21:46:47.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename sched-pred 09/03/22 21:46:47.266
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:46:47.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:46:47.277
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Sep  3 21:46:47.280: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  3 21:46:47.288: INFO: Waiting for terminating namespaces to be deleted...
    Sep  3 21:46:47.291: INFO: 
    Logging pods the apiserver thinks is on node kind-worker before test
    Sep  3 21:46:47.295: INFO: kindnet-xhxcg from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
    Sep  3 21:46:47.295: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  3 21:46:47.295: INFO: kube-proxy-bxm4p from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
    Sep  3 21:46:47.295: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  3 21:46:47.295: INFO: sonobuoy from sonobuoy started at 2022-09-03 20:37:57 +0000 UTC (1 container statuses recorded)
    Sep  3 21:46:47.295: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  3 21:46:47.295: INFO: sonobuoy-e2e-job-31a458d0b0c040d4 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
    Sep  3 21:46:47.295: INFO: 	Container e2e ready: true, restart count 0
    Sep  3 21:46:47.295: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  3 21:46:47.295: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-jdmz7 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
    Sep  3 21:46:47.295: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  3 21:46:47.295: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  3 21:46:47.295: INFO: 
    Logging pods the apiserver thinks is on node kind-worker2 before test
    Sep  3 21:46:47.300: INFO: kindnet-zlt4d from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
    Sep  3 21:46:47.300: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  3 21:46:47.300: INFO: kube-proxy-wz2c5 from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
    Sep  3 21:46:47.300: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  3 21:46:47.300: INFO: pod-fhhcv from pods-2201 started at 2022-09-03 21:46:39 +0000 UTC (1 container statuses recorded)
    Sep  3 21:46:47.300: INFO: 	Container agnhost ready: true, restart count 0
    Sep  3 21:46:47.300: INFO: busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133 from security-context-test-607 started at 2022-09-03 21:46:43 +0000 UTC (1 container statuses recorded)
    Sep  3 21:46:47.300: INFO: 	Container busybox-privileged-false-937a0e93-23df-41b7-a1a6-7a1794ba7133 ready: false, restart count 0
    Sep  3 21:46:47.300: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-nwk5b from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
    Sep  3 21:46:47.300: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  3 21:46:47.300: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/03/22 21:46:47.301
    Sep  3 21:46:47.306: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-495" to be "running"
    Sep  3 21:46:47.308: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115403ms
    Sep  3 21:46:49.311: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004290203s
    Sep  3 21:46:49.311: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/03/22 21:46:49.312
    STEP: Trying to apply a random label on the found node. 09/03/22 21:46:49.323
    STEP: verifying the node has the label kubernetes.io/e2e-3dc41b79-e424-4e1e-84cb-abed530ed895 95 09/03/22 21:46:49.329
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/03/22 21:46:49.338
    Sep  3 21:46:49.343: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-495" to be "not pending"
    Sep  3 21:46:49.352: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.076812ms
    Sep  3 21:46:51.355: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011849713s
    Sep  3 21:46:51.355: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.18.0.3 on the node which pod4 resides and expect not scheduled 09/03/22 21:46:51.355
    Sep  3 21:46:51.359: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-495" to be "not pending"
    Sep  3 21:46:51.364: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.523807ms
    Sep  3 21:46:53.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00874909s
    Sep  3 21:46:55.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008977623s
    Sep  3 21:46:57.371: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012054964s
    Sep  3 21:46:59.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01013399s
    Sep  3 21:47:01.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00795462s
    Sep  3 21:47:03.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00877345s
    Sep  3 21:47:05.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008158777s
    Sep  3 21:47:07.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008462606s
    Sep  3 21:47:09.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010527799s
    Sep  3 21:47:11.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009108617s
    Sep  3 21:47:13.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009332937s
    Sep  3 21:47:15.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008245955s
    Sep  3 21:47:17.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008807348s
    Sep  3 21:47:19.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.00941665s
    Sep  3 21:47:21.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009968351s
    Sep  3 21:47:23.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.009229949s
    Sep  3 21:47:25.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.00810467s
    Sep  3 21:47:27.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008455067s
    Sep  3 21:47:29.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008430864s
    Sep  3 21:47:31.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010428963s
    Sep  3 21:47:33.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008788999s
    Sep  3 21:47:35.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008903458s
    Sep  3 21:47:37.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008541916s
    Sep  3 21:47:39.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008692274s
    Sep  3 21:47:41.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008574454s
    Sep  3 21:47:43.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.00844031s
    Sep  3 21:47:45.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008195665s
    Sep  3 21:47:47.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008416522s
    Sep  3 21:47:49.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008733104s
    Sep  3 21:47:51.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008317439s
    Sep  3 21:47:53.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008191674s
    Sep  3 21:47:55.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00850311s
    Sep  3 21:47:57.366: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007638726s
    Sep  3 21:47:59.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008567456s
    Sep  3 21:48:01.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008420083s
    Sep  3 21:48:03.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008629812s
    Sep  3 21:48:05.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008290539s
    Sep  3 21:48:07.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008341367s
    Sep  3 21:48:09.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009942599s
    Sep  3 21:48:11.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008054323s
    Sep  3 21:48:13.370: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011383525s
    Sep  3 21:48:15.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.00833275s
    Sep  3 21:48:17.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00848868s
    Sep  3 21:48:19.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008559402s
    Sep  3 21:48:21.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008134547s
    Sep  3 21:48:23.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008759643s
    Sep  3 21:48:25.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008956338s
    Sep  3 21:48:27.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010413236s
    Sep  3 21:48:29.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008552189s
    Sep  3 21:48:31.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.00823053s
    Sep  3 21:48:33.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008631172s
    Sep  3 21:48:35.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008144813s
    Sep  3 21:48:37.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008013666s
    Sep  3 21:48:39.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008969396s
    Sep  3 21:48:41.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008371923s
    Sep  3 21:48:43.366: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.007832649s
    Sep  3 21:48:45.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008636756s
    Sep  3 21:48:47.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009273751s
    Sep  3 21:48:49.370: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.01135595s
    Sep  3 21:48:51.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008337737s
    Sep  3 21:48:53.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.008489209s
    Sep  3 21:48:55.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008841474s
    Sep  3 21:48:57.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008335636s
    Sep  3 21:48:59.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.0083003s
    Sep  3 21:49:01.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.009107301s
    Sep  3 21:49:03.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.008354412s
    Sep  3 21:49:05.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008897527s
    Sep  3 21:49:07.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.008235939s
    Sep  3 21:49:09.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.00845518s
    Sep  3 21:49:11.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008916037s
    Sep  3 21:49:13.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008019991s
    Sep  3 21:49:15.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.008849148s
    Sep  3 21:49:17.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008022757s
    Sep  3 21:49:19.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009056681s
    Sep  3 21:49:21.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.008227102s
    Sep  3 21:49:23.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008246924s
    Sep  3 21:49:25.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.00814739s
    Sep  3 21:49:27.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.008520406s
    Sep  3 21:49:29.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.00834412s
    Sep  3 21:49:31.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008019534s
    Sep  3 21:49:33.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.010165377s
    Sep  3 21:49:35.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009611983s
    Sep  3 21:49:37.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.00902449s
    Sep  3 21:49:39.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.008569796s
    Sep  3 21:49:41.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008536227s
    Sep  3 21:49:43.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008741654s
    Sep  3 21:49:45.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008482079s
    Sep  3 21:49:47.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009603708s
    Sep  3 21:49:49.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.0089479s
    Sep  3 21:49:51.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008154072s
    Sep  3 21:49:53.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008015046s
    Sep  3 21:49:55.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.00831072s
    Sep  3 21:49:57.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008248397s
    Sep  3 21:49:59.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008451276s
    Sep  3 21:50:01.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009411257s
    Sep  3 21:50:03.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008250933s
    Sep  3 21:50:05.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008241223s
    Sep  3 21:50:07.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.008559122s
    Sep  3 21:50:09.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.008854421s
    Sep  3 21:50:11.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.00927562s
    Sep  3 21:50:13.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.008214016s
    Sep  3 21:50:15.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008129752s
    Sep  3 21:50:17.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008311088s
    Sep  3 21:50:19.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008722725s
    Sep  3 21:50:21.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008356943s
    Sep  3 21:50:23.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.007913716s
    Sep  3 21:50:25.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008218491s
    Sep  3 21:50:27.371: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.012663474s
    Sep  3 21:50:29.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.008915592s
    Sep  3 21:50:31.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008973187s
    Sep  3 21:50:33.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008076179s
    Sep  3 21:50:35.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.008967275s
    Sep  3 21:50:37.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.00877217s
    Sep  3 21:50:39.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.009039926s
    Sep  3 21:50:41.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.008410782s
    Sep  3 21:50:43.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007991537s
    Sep  3 21:50:45.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008352802s
    Sep  3 21:50:47.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009137919s
    Sep  3 21:50:49.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.010561137s
    Sep  3 21:50:51.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008723849s
    Sep  3 21:50:53.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007986068s
    Sep  3 21:50:55.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009319553s
    Sep  3 21:50:57.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.015627049s
    Sep  3 21:50:59.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.008821718s
    Sep  3 21:51:01.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.010590103s
    Sep  3 21:51:03.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008870105s
    Sep  3 21:51:05.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.008852809s
    Sep  3 21:51:07.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008869798s
    Sep  3 21:51:09.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008859309s
    Sep  3 21:51:11.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00831931s
    Sep  3 21:51:13.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009326315s
    Sep  3 21:51:15.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009199117s
    Sep  3 21:51:17.370: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.01162803s
    Sep  3 21:51:19.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008495911s
    Sep  3 21:51:21.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008115998s
    Sep  3 21:51:23.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009953688s
    Sep  3 21:51:25.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008032401s
    Sep  3 21:51:27.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008773894s
    Sep  3 21:51:29.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007920384s
    Sep  3 21:51:31.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007986576s
    Sep  3 21:51:33.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009155168s
    Sep  3 21:51:35.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008575334s
    Sep  3 21:51:37.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008008601s
    Sep  3 21:51:39.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.00876017s
    Sep  3 21:51:41.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008916034s
    Sep  3 21:51:43.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.008376013s
    Sep  3 21:51:45.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008431193s
    Sep  3 21:51:47.368: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009738076s
    Sep  3 21:51:49.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.008696496s
    Sep  3 21:51:51.367: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008019951s
    Sep  3 21:51:51.369: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009885555s
    STEP: removing the label kubernetes.io/e2e-3dc41b79-e424-4e1e-84cb-abed530ed895 off the node kind-worker2 09/03/22 21:51:51.369
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-3dc41b79-e424-4e1e-84cb-abed530ed895 09/03/22 21:51:51.376
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:51:51.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-495" for this suite. 09/03/22 21:51:51.386
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:51:51.395
Sep  3 21:51:51.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename conformance-tests 09/03/22 21:51:51.397
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:51:51.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:51:51.413
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 09/03/22 21:51:51.415
Sep  3 21:51:51.417: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Sep  3 21:51:51.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-446" for this suite. 09/03/22 21:51:51.423
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":254,"skipped":4734,"failed":0}
------------------------------
• [0.036 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:51:51.395
    Sep  3 21:51:51.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename conformance-tests 09/03/22 21:51:51.397
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:51:51.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:51:51.413
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 09/03/22 21:51:51.415
    Sep  3 21:51:51.417: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Sep  3 21:51:51.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-446" for this suite. 09/03/22 21:51:51.423
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:51:51.442
Sep  3 21:51:51.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:51:51.443
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:51:51.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:51:51.461
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 09/03/22 21:51:51.465
Sep  3 21:51:51.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 create -f -'
Sep  3 21:51:51.696: INFO: stderr: ""
Sep  3 21:51:51.696: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/03/22 21:51:51.696
Sep  3 21:51:51.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  3 21:51:51.813: INFO: stderr: ""
Sep  3 21:51:51.813: INFO: stdout: "update-demo-nautilus-blfpc update-demo-nautilus-lwlg7 "
Sep  3 21:51:51.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-blfpc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  3 21:51:51.875: INFO: stderr: ""
Sep  3 21:51:51.875: INFO: stdout: ""
Sep  3 21:51:51.875: INFO: update-demo-nautilus-blfpc is created but not running
Sep  3 21:51:56.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  3 21:51:56.949: INFO: stderr: ""
Sep  3 21:51:56.949: INFO: stdout: "update-demo-nautilus-blfpc update-demo-nautilus-lwlg7 "
Sep  3 21:51:56.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-blfpc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  3 21:51:57.020: INFO: stderr: ""
Sep  3 21:51:57.020: INFO: stdout: "true"
Sep  3 21:51:57.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-blfpc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  3 21:51:57.099: INFO: stderr: ""
Sep  3 21:51:57.099: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Sep  3 21:51:57.099: INFO: validating pod update-demo-nautilus-blfpc
Sep  3 21:51:57.104: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 21:51:57.104: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 21:51:57.104: INFO: update-demo-nautilus-blfpc is verified up and running
Sep  3 21:51:57.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-lwlg7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  3 21:51:57.181: INFO: stderr: ""
Sep  3 21:51:57.181: INFO: stdout: "true"
Sep  3 21:51:57.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-lwlg7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  3 21:51:57.264: INFO: stderr: ""
Sep  3 21:51:57.264: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Sep  3 21:51:57.264: INFO: validating pod update-demo-nautilus-lwlg7
Sep  3 21:51:57.267: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 21:51:57.267: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 21:51:57.267: INFO: update-demo-nautilus-lwlg7 is verified up and running
STEP: scaling down the replication controller 09/03/22 21:51:57.267
Sep  3 21:51:57.268: INFO: scanned /root for discovery docs: <nil>
Sep  3 21:51:57.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep  3 21:51:58.381: INFO: stderr: ""
Sep  3 21:51:58.381: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/03/22 21:51:58.381
Sep  3 21:51:58.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  3 21:51:58.453: INFO: stderr: ""
Sep  3 21:51:58.453: INFO: stdout: "update-demo-nautilus-blfpc update-demo-nautilus-lwlg7 "
STEP: Replicas for name=update-demo: expected=1 actual=2 09/03/22 21:51:58.453
Sep  3 21:52:03.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  3 21:52:03.550: INFO: stderr: ""
Sep  3 21:52:03.551: INFO: stdout: "update-demo-nautilus-lwlg7 "
Sep  3 21:52:03.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-lwlg7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  3 21:52:03.630: INFO: stderr: ""
Sep  3 21:52:03.630: INFO: stdout: "true"
Sep  3 21:52:03.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-lwlg7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  3 21:52:03.706: INFO: stderr: ""
Sep  3 21:52:03.706: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Sep  3 21:52:03.706: INFO: validating pod update-demo-nautilus-lwlg7
Sep  3 21:52:03.708: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 21:52:03.708: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 21:52:03.708: INFO: update-demo-nautilus-lwlg7 is verified up and running
STEP: scaling up the replication controller 09/03/22 21:52:03.708
Sep  3 21:52:03.710: INFO: scanned /root for discovery docs: <nil>
Sep  3 21:52:03.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep  3 21:52:04.815: INFO: stderr: ""
Sep  3 21:52:04.815: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/03/22 21:52:04.815
Sep  3 21:52:04.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  3 21:52:04.890: INFO: stderr: ""
Sep  3 21:52:04.890: INFO: stdout: "update-demo-nautilus-2dlj9 update-demo-nautilus-lwlg7 "
Sep  3 21:52:04.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-2dlj9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  3 21:52:04.958: INFO: stderr: ""
Sep  3 21:52:04.958: INFO: stdout: "true"
Sep  3 21:52:04.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-2dlj9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  3 21:52:05.028: INFO: stderr: ""
Sep  3 21:52:05.028: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Sep  3 21:52:05.028: INFO: validating pod update-demo-nautilus-2dlj9
Sep  3 21:52:05.031: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 21:52:05.031: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 21:52:05.031: INFO: update-demo-nautilus-2dlj9 is verified up and running
Sep  3 21:52:05.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-lwlg7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  3 21:52:05.110: INFO: stderr: ""
Sep  3 21:52:05.110: INFO: stdout: "true"
Sep  3 21:52:05.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-lwlg7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  3 21:52:05.194: INFO: stderr: ""
Sep  3 21:52:05.194: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Sep  3 21:52:05.194: INFO: validating pod update-demo-nautilus-lwlg7
Sep  3 21:52:05.201: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 21:52:05.201: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 21:52:05.201: INFO: update-demo-nautilus-lwlg7 is verified up and running
STEP: using delete to clean up resources 09/03/22 21:52:05.201
Sep  3 21:52:05.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 delete --grace-period=0 --force -f -'
Sep  3 21:52:05.282: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 21:52:05.282: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  3 21:52:05.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get rc,svc -l name=update-demo --no-headers'
Sep  3 21:52:05.447: INFO: stderr: "No resources found in kubectl-9332 namespace.\n"
Sep  3 21:52:05.447: INFO: stdout: ""
Sep  3 21:52:05.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  3 21:52:05.588: INFO: stderr: ""
Sep  3 21:52:05.588: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:52:05.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9332" for this suite. 09/03/22 21:52:05.592
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":255,"skipped":4760,"failed":0}
------------------------------
• [SLOW TEST] [14.156 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:51:51.442
    Sep  3 21:51:51.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:51:51.443
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:51:51.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:51:51.461
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 09/03/22 21:51:51.465
    Sep  3 21:51:51.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 create -f -'
    Sep  3 21:51:51.696: INFO: stderr: ""
    Sep  3 21:51:51.696: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/03/22 21:51:51.696
    Sep  3 21:51:51.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  3 21:51:51.813: INFO: stderr: ""
    Sep  3 21:51:51.813: INFO: stdout: "update-demo-nautilus-blfpc update-demo-nautilus-lwlg7 "
    Sep  3 21:51:51.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-blfpc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  3 21:51:51.875: INFO: stderr: ""
    Sep  3 21:51:51.875: INFO: stdout: ""
    Sep  3 21:51:51.875: INFO: update-demo-nautilus-blfpc is created but not running
    Sep  3 21:51:56.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  3 21:51:56.949: INFO: stderr: ""
    Sep  3 21:51:56.949: INFO: stdout: "update-demo-nautilus-blfpc update-demo-nautilus-lwlg7 "
    Sep  3 21:51:56.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-blfpc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  3 21:51:57.020: INFO: stderr: ""
    Sep  3 21:51:57.020: INFO: stdout: "true"
    Sep  3 21:51:57.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-blfpc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  3 21:51:57.099: INFO: stderr: ""
    Sep  3 21:51:57.099: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Sep  3 21:51:57.099: INFO: validating pod update-demo-nautilus-blfpc
    Sep  3 21:51:57.104: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  3 21:51:57.104: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  3 21:51:57.104: INFO: update-demo-nautilus-blfpc is verified up and running
    Sep  3 21:51:57.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-lwlg7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  3 21:51:57.181: INFO: stderr: ""
    Sep  3 21:51:57.181: INFO: stdout: "true"
    Sep  3 21:51:57.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-lwlg7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  3 21:51:57.264: INFO: stderr: ""
    Sep  3 21:51:57.264: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Sep  3 21:51:57.264: INFO: validating pod update-demo-nautilus-lwlg7
    Sep  3 21:51:57.267: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  3 21:51:57.267: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  3 21:51:57.267: INFO: update-demo-nautilus-lwlg7 is verified up and running
    STEP: scaling down the replication controller 09/03/22 21:51:57.267
    Sep  3 21:51:57.268: INFO: scanned /root for discovery docs: <nil>
    Sep  3 21:51:57.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Sep  3 21:51:58.381: INFO: stderr: ""
    Sep  3 21:51:58.381: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/03/22 21:51:58.381
    Sep  3 21:51:58.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  3 21:51:58.453: INFO: stderr: ""
    Sep  3 21:51:58.453: INFO: stdout: "update-demo-nautilus-blfpc update-demo-nautilus-lwlg7 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 09/03/22 21:51:58.453
    Sep  3 21:52:03.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  3 21:52:03.550: INFO: stderr: ""
    Sep  3 21:52:03.551: INFO: stdout: "update-demo-nautilus-lwlg7 "
    Sep  3 21:52:03.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-lwlg7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  3 21:52:03.630: INFO: stderr: ""
    Sep  3 21:52:03.630: INFO: stdout: "true"
    Sep  3 21:52:03.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-lwlg7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  3 21:52:03.706: INFO: stderr: ""
    Sep  3 21:52:03.706: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Sep  3 21:52:03.706: INFO: validating pod update-demo-nautilus-lwlg7
    Sep  3 21:52:03.708: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  3 21:52:03.708: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  3 21:52:03.708: INFO: update-demo-nautilus-lwlg7 is verified up and running
    STEP: scaling up the replication controller 09/03/22 21:52:03.708
    Sep  3 21:52:03.710: INFO: scanned /root for discovery docs: <nil>
    Sep  3 21:52:03.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Sep  3 21:52:04.815: INFO: stderr: ""
    Sep  3 21:52:04.815: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/03/22 21:52:04.815
    Sep  3 21:52:04.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  3 21:52:04.890: INFO: stderr: ""
    Sep  3 21:52:04.890: INFO: stdout: "update-demo-nautilus-2dlj9 update-demo-nautilus-lwlg7 "
    Sep  3 21:52:04.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-2dlj9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  3 21:52:04.958: INFO: stderr: ""
    Sep  3 21:52:04.958: INFO: stdout: "true"
    Sep  3 21:52:04.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-2dlj9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  3 21:52:05.028: INFO: stderr: ""
    Sep  3 21:52:05.028: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Sep  3 21:52:05.028: INFO: validating pod update-demo-nautilus-2dlj9
    Sep  3 21:52:05.031: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  3 21:52:05.031: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  3 21:52:05.031: INFO: update-demo-nautilus-2dlj9 is verified up and running
    Sep  3 21:52:05.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-lwlg7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  3 21:52:05.110: INFO: stderr: ""
    Sep  3 21:52:05.110: INFO: stdout: "true"
    Sep  3 21:52:05.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods update-demo-nautilus-lwlg7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  3 21:52:05.194: INFO: stderr: ""
    Sep  3 21:52:05.194: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Sep  3 21:52:05.194: INFO: validating pod update-demo-nautilus-lwlg7
    Sep  3 21:52:05.201: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  3 21:52:05.201: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  3 21:52:05.201: INFO: update-demo-nautilus-lwlg7 is verified up and running
    STEP: using delete to clean up resources 09/03/22 21:52:05.201
    Sep  3 21:52:05.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 delete --grace-period=0 --force -f -'
    Sep  3 21:52:05.282: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  3 21:52:05.282: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  3 21:52:05.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get rc,svc -l name=update-demo --no-headers'
    Sep  3 21:52:05.447: INFO: stderr: "No resources found in kubectl-9332 namespace.\n"
    Sep  3 21:52:05.447: INFO: stdout: ""
    Sep  3 21:52:05.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9332 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  3 21:52:05.588: INFO: stderr: ""
    Sep  3 21:52:05.588: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:52:05.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9332" for this suite. 09/03/22 21:52:05.592
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:05.6
Sep  3 21:52:05.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename gc 09/03/22 21:52:05.601
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:05.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:05.616
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 09/03/22 21:52:05.618
STEP: Wait for the Deployment to create new ReplicaSet 09/03/22 21:52:05.62
STEP: delete the deployment 09/03/22 21:52:06.125
STEP: wait for all rs to be garbage collected 09/03/22 21:52:06.129
STEP: expected 0 rs, got 1 rs 09/03/22 21:52:06.135
STEP: expected 0 pods, got 2 pods 09/03/22 21:52:06.139
STEP: Gathering metrics 09/03/22 21:52:06.647
Sep  3 21:52:06.672: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
Sep  3 21:52:06.674: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 2.908406ms
Sep  3 21:52:06.675: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
Sep  3 21:52:06.675: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
Sep  3 21:52:06.770: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Sep  3 21:52:06.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9380" for this suite. 09/03/22 21:52:06.773
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":256,"skipped":4788,"failed":0}
------------------------------
• [1.177 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:05.6
    Sep  3 21:52:05.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename gc 09/03/22 21:52:05.601
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:05.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:05.616
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 09/03/22 21:52:05.618
    STEP: Wait for the Deployment to create new ReplicaSet 09/03/22 21:52:05.62
    STEP: delete the deployment 09/03/22 21:52:06.125
    STEP: wait for all rs to be garbage collected 09/03/22 21:52:06.129
    STEP: expected 0 rs, got 1 rs 09/03/22 21:52:06.135
    STEP: expected 0 pods, got 2 pods 09/03/22 21:52:06.139
    STEP: Gathering metrics 09/03/22 21:52:06.647
    Sep  3 21:52:06.672: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
    Sep  3 21:52:06.674: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 2.908406ms
    Sep  3 21:52:06.675: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
    Sep  3 21:52:06.675: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
    Sep  3 21:52:06.770: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Sep  3 21:52:06.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9380" for this suite. 09/03/22 21:52:06.773
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:06.777
Sep  3 21:52:06.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:52:06.778
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:06.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:06.791
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 09/03/22 21:52:06.794
Sep  3 21:52:06.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-6108 api-versions'
Sep  3 21:52:06.903: INFO: stderr: ""
Sep  3 21:52:06.903: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:52:06.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6108" for this suite. 09/03/22 21:52:06.908
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":257,"skipped":4788,"failed":0}
------------------------------
• [0.140 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:06.777
    Sep  3 21:52:06.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:52:06.778
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:06.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:06.791
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 09/03/22 21:52:06.794
    Sep  3 21:52:06.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-6108 api-versions'
    Sep  3 21:52:06.903: INFO: stderr: ""
    Sep  3 21:52:06.903: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:52:06.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6108" for this suite. 09/03/22 21:52:06.908
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:06.921
Sep  3 21:52:06.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 21:52:06.924
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:06.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:06.936
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-3c1be41d-d006-4a5c-a45f-ace96fd4a2f6 09/03/22 21:52:06.938
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Sep  3 21:52:06.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2434" for this suite. 09/03/22 21:52:06.942
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":258,"skipped":4819,"failed":0}
------------------------------
• [0.024 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:06.921
    Sep  3 21:52:06.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 21:52:06.924
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:06.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:06.936
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-3c1be41d-d006-4a5c-a45f-ace96fd4a2f6 09/03/22 21:52:06.938
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 21:52:06.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2434" for this suite. 09/03/22 21:52:06.942
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:06.948
Sep  3 21:52:06.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:52:06.949
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:06.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:06.961
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 09/03/22 21:52:06.963
Sep  3 21:52:06.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124" in namespace "projected-4145" to be "Succeeded or Failed"
Sep  3 21:52:06.986: INFO: Pod "downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124": Phase="Pending", Reason="", readiness=false. Elapsed: 16.753735ms
Sep  3 21:52:08.989: INFO: Pod "downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020466468s
Sep  3 21:52:10.990: INFO: Pod "downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021277094s
STEP: Saw pod success 09/03/22 21:52:10.99
Sep  3 21:52:10.990: INFO: Pod "downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124" satisfied condition "Succeeded or Failed"
Sep  3 21:52:10.992: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124 container client-container: <nil>
STEP: delete the pod 09/03/22 21:52:11.006
Sep  3 21:52:11.011: INFO: Waiting for pod downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124 to disappear
Sep  3 21:52:11.013: INFO: Pod downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Sep  3 21:52:11.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4145" for this suite. 09/03/22 21:52:11.016
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":259,"skipped":4830,"failed":0}
------------------------------
• [4.072 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:06.948
    Sep  3 21:52:06.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:52:06.949
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:06.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:06.961
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 09/03/22 21:52:06.963
    Sep  3 21:52:06.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124" in namespace "projected-4145" to be "Succeeded or Failed"
    Sep  3 21:52:06.986: INFO: Pod "downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124": Phase="Pending", Reason="", readiness=false. Elapsed: 16.753735ms
    Sep  3 21:52:08.989: INFO: Pod "downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020466468s
    Sep  3 21:52:10.990: INFO: Pod "downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021277094s
    STEP: Saw pod success 09/03/22 21:52:10.99
    Sep  3 21:52:10.990: INFO: Pod "downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124" satisfied condition "Succeeded or Failed"
    Sep  3 21:52:10.992: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124 container client-container: <nil>
    STEP: delete the pod 09/03/22 21:52:11.006
    Sep  3 21:52:11.011: INFO: Waiting for pod downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124 to disappear
    Sep  3 21:52:11.013: INFO: Pod downwardapi-volume-7b883e6b-7982-4fe3-969c-a5a8865ea124 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Sep  3 21:52:11.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4145" for this suite. 09/03/22 21:52:11.016
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:11.025
Sep  3 21:52:11.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename sched-pred 09/03/22 21:52:11.026
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:11.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:11.037
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Sep  3 21:52:11.039: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  3 21:52:11.044: INFO: Waiting for terminating namespaces to be deleted...
Sep  3 21:52:11.047: INFO: 
Logging pods the apiserver thinks is on node kind-worker before test
Sep  3 21:52:11.051: INFO: kindnet-xhxcg from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
Sep  3 21:52:11.051: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  3 21:52:11.051: INFO: kube-proxy-bxm4p from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
Sep  3 21:52:11.051: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 21:52:11.051: INFO: sonobuoy from sonobuoy started at 2022-09-03 20:37:57 +0000 UTC (1 container statuses recorded)
Sep  3 21:52:11.051: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  3 21:52:11.051: INFO: sonobuoy-e2e-job-31a458d0b0c040d4 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
Sep  3 21:52:11.051: INFO: 	Container e2e ready: true, restart count 0
Sep  3 21:52:11.051: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 21:52:11.051: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-jdmz7 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
Sep  3 21:52:11.051: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 21:52:11.051: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 21:52:11.051: INFO: 
Logging pods the apiserver thinks is on node kind-worker2 before test
Sep  3 21:52:11.056: INFO: kindnet-zlt4d from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
Sep  3 21:52:11.056: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  3 21:52:11.056: INFO: kube-proxy-wz2c5 from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
Sep  3 21:52:11.056: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 21:52:11.056: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-nwk5b from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
Sep  3 21:52:11.056: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 21:52:11.056: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node kind-worker 09/03/22 21:52:11.067
STEP: verifying the node has the label node kind-worker2 09/03/22 21:52:11.082
Sep  3 21:52:11.109: INFO: Pod kindnet-xhxcg requesting resource cpu=100m on Node kind-worker
Sep  3 21:52:11.109: INFO: Pod kindnet-zlt4d requesting resource cpu=100m on Node kind-worker2
Sep  3 21:52:11.109: INFO: Pod kube-proxy-bxm4p requesting resource cpu=0m on Node kind-worker
Sep  3 21:52:11.109: INFO: Pod kube-proxy-wz2c5 requesting resource cpu=0m on Node kind-worker2
Sep  3 21:52:11.109: INFO: Pod sonobuoy requesting resource cpu=0m on Node kind-worker
Sep  3 21:52:11.109: INFO: Pod sonobuoy-e2e-job-31a458d0b0c040d4 requesting resource cpu=0m on Node kind-worker
Sep  3 21:52:11.109: INFO: Pod sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-jdmz7 requesting resource cpu=0m on Node kind-worker
Sep  3 21:52:11.109: INFO: Pod sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-nwk5b requesting resource cpu=0m on Node kind-worker2
STEP: Starting Pods to consume most of the cluster CPU. 09/03/22 21:52:11.109
Sep  3 21:52:11.109: INFO: Creating a pod which consumes cpu=1330m on Node kind-worker
Sep  3 21:52:11.117: INFO: Creating a pod which consumes cpu=1330m on Node kind-worker2
Sep  3 21:52:11.123: INFO: Waiting up to 5m0s for pod "filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea" in namespace "sched-pred-5583" to be "running"
Sep  3 21:52:11.133: INFO: Pod "filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea": Phase="Pending", Reason="", readiness=false. Elapsed: 9.45182ms
Sep  3 21:52:13.136: INFO: Pod "filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea": Phase="Running", Reason="", readiness=true. Elapsed: 2.012488139s
Sep  3 21:52:13.137: INFO: Pod "filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea" satisfied condition "running"
Sep  3 21:52:13.137: INFO: Waiting up to 5m0s for pod "filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb" in namespace "sched-pred-5583" to be "running"
Sep  3 21:52:13.138: INFO: Pod "filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb": Phase="Running", Reason="", readiness=true. Elapsed: 1.807004ms
Sep  3 21:52:13.138: INFO: Pod "filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 09/03/22 21:52:13.138
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea.1711783abc0ee984], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5583/filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea to kind-worker] 09/03/22 21:52:13.141
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea.1711783ada1e4c85], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 09/03/22 21:52:13.141
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea.1711783adb49876a], Reason = [Created], Message = [Created container filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea] 09/03/22 21:52:13.142
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea.1711783ae809627e], Reason = [Started], Message = [Started container filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea] 09/03/22 21:52:13.142
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb.1711783abcef3b07], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5583/filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb to kind-worker2] 09/03/22 21:52:13.142
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb.1711783adff9bec1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 09/03/22 21:52:13.142
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb.1711783ae109f666], Reason = [Created], Message = [Created container filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb] 09/03/22 21:52:13.143
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb.1711783aefb6fd4e], Reason = [Started], Message = [Started container filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb] 09/03/22 21:52:13.143
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1711783b34005eef], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.] 09/03/22 21:52:13.152
STEP: removing the label node off the node kind-worker 09/03/22 21:52:14.15
STEP: verifying the node doesn't have the label node 09/03/22 21:52:14.158
STEP: removing the label node off the node kind-worker2 09/03/22 21:52:14.163
STEP: verifying the node doesn't have the label node 09/03/22 21:52:14.177
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:52:14.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5583" for this suite. 09/03/22 21:52:14.193
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":260,"skipped":4852,"failed":0}
------------------------------
• [3.174 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:11.025
    Sep  3 21:52:11.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename sched-pred 09/03/22 21:52:11.026
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:11.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:11.037
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Sep  3 21:52:11.039: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  3 21:52:11.044: INFO: Waiting for terminating namespaces to be deleted...
    Sep  3 21:52:11.047: INFO: 
    Logging pods the apiserver thinks is on node kind-worker before test
    Sep  3 21:52:11.051: INFO: kindnet-xhxcg from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
    Sep  3 21:52:11.051: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  3 21:52:11.051: INFO: kube-proxy-bxm4p from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
    Sep  3 21:52:11.051: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  3 21:52:11.051: INFO: sonobuoy from sonobuoy started at 2022-09-03 20:37:57 +0000 UTC (1 container statuses recorded)
    Sep  3 21:52:11.051: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  3 21:52:11.051: INFO: sonobuoy-e2e-job-31a458d0b0c040d4 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
    Sep  3 21:52:11.051: INFO: 	Container e2e ready: true, restart count 0
    Sep  3 21:52:11.051: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  3 21:52:11.051: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-jdmz7 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
    Sep  3 21:52:11.051: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  3 21:52:11.051: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  3 21:52:11.051: INFO: 
    Logging pods the apiserver thinks is on node kind-worker2 before test
    Sep  3 21:52:11.056: INFO: kindnet-zlt4d from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
    Sep  3 21:52:11.056: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  3 21:52:11.056: INFO: kube-proxy-wz2c5 from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
    Sep  3 21:52:11.056: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  3 21:52:11.056: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-nwk5b from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
    Sep  3 21:52:11.056: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  3 21:52:11.056: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node kind-worker 09/03/22 21:52:11.067
    STEP: verifying the node has the label node kind-worker2 09/03/22 21:52:11.082
    Sep  3 21:52:11.109: INFO: Pod kindnet-xhxcg requesting resource cpu=100m on Node kind-worker
    Sep  3 21:52:11.109: INFO: Pod kindnet-zlt4d requesting resource cpu=100m on Node kind-worker2
    Sep  3 21:52:11.109: INFO: Pod kube-proxy-bxm4p requesting resource cpu=0m on Node kind-worker
    Sep  3 21:52:11.109: INFO: Pod kube-proxy-wz2c5 requesting resource cpu=0m on Node kind-worker2
    Sep  3 21:52:11.109: INFO: Pod sonobuoy requesting resource cpu=0m on Node kind-worker
    Sep  3 21:52:11.109: INFO: Pod sonobuoy-e2e-job-31a458d0b0c040d4 requesting resource cpu=0m on Node kind-worker
    Sep  3 21:52:11.109: INFO: Pod sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-jdmz7 requesting resource cpu=0m on Node kind-worker
    Sep  3 21:52:11.109: INFO: Pod sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-nwk5b requesting resource cpu=0m on Node kind-worker2
    STEP: Starting Pods to consume most of the cluster CPU. 09/03/22 21:52:11.109
    Sep  3 21:52:11.109: INFO: Creating a pod which consumes cpu=1330m on Node kind-worker
    Sep  3 21:52:11.117: INFO: Creating a pod which consumes cpu=1330m on Node kind-worker2
    Sep  3 21:52:11.123: INFO: Waiting up to 5m0s for pod "filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea" in namespace "sched-pred-5583" to be "running"
    Sep  3 21:52:11.133: INFO: Pod "filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea": Phase="Pending", Reason="", readiness=false. Elapsed: 9.45182ms
    Sep  3 21:52:13.136: INFO: Pod "filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea": Phase="Running", Reason="", readiness=true. Elapsed: 2.012488139s
    Sep  3 21:52:13.137: INFO: Pod "filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea" satisfied condition "running"
    Sep  3 21:52:13.137: INFO: Waiting up to 5m0s for pod "filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb" in namespace "sched-pred-5583" to be "running"
    Sep  3 21:52:13.138: INFO: Pod "filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb": Phase="Running", Reason="", readiness=true. Elapsed: 1.807004ms
    Sep  3 21:52:13.138: INFO: Pod "filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 09/03/22 21:52:13.138
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea.1711783abc0ee984], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5583/filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea to kind-worker] 09/03/22 21:52:13.141
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea.1711783ada1e4c85], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 09/03/22 21:52:13.141
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea.1711783adb49876a], Reason = [Created], Message = [Created container filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea] 09/03/22 21:52:13.142
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea.1711783ae809627e], Reason = [Started], Message = [Started container filler-pod-7d89f096-ff0e-4c3a-9c47-10f636897cea] 09/03/22 21:52:13.142
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb.1711783abcef3b07], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5583/filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb to kind-worker2] 09/03/22 21:52:13.142
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb.1711783adff9bec1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 09/03/22 21:52:13.142
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb.1711783ae109f666], Reason = [Created], Message = [Created container filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb] 09/03/22 21:52:13.143
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb.1711783aefb6fd4e], Reason = [Started], Message = [Started container filler-pod-a8d02c54-b0d3-4e6d-9af0-c1fcd08e97bb] 09/03/22 21:52:13.143
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1711783b34005eef], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.] 09/03/22 21:52:13.152
    STEP: removing the label node off the node kind-worker 09/03/22 21:52:14.15
    STEP: verifying the node doesn't have the label node 09/03/22 21:52:14.158
    STEP: removing the label node off the node kind-worker2 09/03/22 21:52:14.163
    STEP: verifying the node doesn't have the label node 09/03/22 21:52:14.177
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:52:14.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5583" for this suite. 09/03/22 21:52:14.193
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:14.232
Sep  3 21:52:14.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:52:14.235
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:14.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:14.252
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-ae023e3b-76c1-42a8-82da-dfe6e0c7b417 09/03/22 21:52:14.257
STEP: Creating a pod to test consume configMaps 09/03/22 21:52:14.26
Sep  3 21:52:14.268: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde" in namespace "projected-3078" to be "Succeeded or Failed"
Sep  3 21:52:14.278: INFO: Pod "pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde": Phase="Pending", Reason="", readiness=false. Elapsed: 10.249222ms
Sep  3 21:52:16.281: INFO: Pod "pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013189425s
Sep  3 21:52:18.281: INFO: Pod "pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012820221s
STEP: Saw pod success 09/03/22 21:52:18.281
Sep  3 21:52:18.281: INFO: Pod "pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde" satisfied condition "Succeeded or Failed"
Sep  3 21:52:18.283: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde container agnhost-container: <nil>
STEP: delete the pod 09/03/22 21:52:18.287
Sep  3 21:52:18.293: INFO: Waiting for pod pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde to disappear
Sep  3 21:52:18.295: INFO: Pod pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Sep  3 21:52:18.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3078" for this suite. 09/03/22 21:52:18.298
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":261,"skipped":4893,"failed":0}
------------------------------
• [4.070 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:14.232
    Sep  3 21:52:14.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:52:14.235
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:14.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:14.252
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-ae023e3b-76c1-42a8-82da-dfe6e0c7b417 09/03/22 21:52:14.257
    STEP: Creating a pod to test consume configMaps 09/03/22 21:52:14.26
    Sep  3 21:52:14.268: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde" in namespace "projected-3078" to be "Succeeded or Failed"
    Sep  3 21:52:14.278: INFO: Pod "pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde": Phase="Pending", Reason="", readiness=false. Elapsed: 10.249222ms
    Sep  3 21:52:16.281: INFO: Pod "pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013189425s
    Sep  3 21:52:18.281: INFO: Pod "pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012820221s
    STEP: Saw pod success 09/03/22 21:52:18.281
    Sep  3 21:52:18.281: INFO: Pod "pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde" satisfied condition "Succeeded or Failed"
    Sep  3 21:52:18.283: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 21:52:18.287
    Sep  3 21:52:18.293: INFO: Waiting for pod pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde to disappear
    Sep  3 21:52:18.295: INFO: Pod pod-projected-configmaps-64d10b52-e5e4-4030-91ca-dba3cfbf9fde no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Sep  3 21:52:18.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3078" for this suite. 09/03/22 21:52:18.298
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:18.313
Sep  3 21:52:18.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 21:52:18.314
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:18.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:18.327
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 09/03/22 21:52:18.333
STEP: waiting for available Endpoint 09/03/22 21:52:18.336
STEP: listing all Endpoints 09/03/22 21:52:18.337
STEP: updating the Endpoint 09/03/22 21:52:18.339
STEP: fetching the Endpoint 09/03/22 21:52:18.343
STEP: patching the Endpoint 09/03/22 21:52:18.344
STEP: fetching the Endpoint 09/03/22 21:52:18.35
STEP: deleting the Endpoint by Collection 09/03/22 21:52:18.352
STEP: waiting for Endpoint deletion 09/03/22 21:52:18.356
STEP: fetching the Endpoint 09/03/22 21:52:18.358
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 21:52:18.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9282" for this suite. 09/03/22 21:52:18.362
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":262,"skipped":4922,"failed":0}
------------------------------
• [0.053 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:18.313
    Sep  3 21:52:18.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 21:52:18.314
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:18.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:18.327
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 09/03/22 21:52:18.333
    STEP: waiting for available Endpoint 09/03/22 21:52:18.336
    STEP: listing all Endpoints 09/03/22 21:52:18.337
    STEP: updating the Endpoint 09/03/22 21:52:18.339
    STEP: fetching the Endpoint 09/03/22 21:52:18.343
    STEP: patching the Endpoint 09/03/22 21:52:18.344
    STEP: fetching the Endpoint 09/03/22 21:52:18.35
    STEP: deleting the Endpoint by Collection 09/03/22 21:52:18.352
    STEP: waiting for Endpoint deletion 09/03/22 21:52:18.356
    STEP: fetching the Endpoint 09/03/22 21:52:18.358
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 21:52:18.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9282" for this suite. 09/03/22 21:52:18.362
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:18.369
Sep  3 21:52:18.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 21:52:18.37
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:18.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:18.381
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 09/03/22 21:52:18.383
Sep  3 21:52:18.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b" in namespace "downward-api-2529" to be "Succeeded or Failed"
Sep  3 21:52:18.392: INFO: Pod "downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.187009ms
Sep  3 21:52:20.395: INFO: Pod "downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00704221s
Sep  3 21:52:22.394: INFO: Pod "downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006520785s
STEP: Saw pod success 09/03/22 21:52:22.394
Sep  3 21:52:22.395: INFO: Pod "downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b" satisfied condition "Succeeded or Failed"
Sep  3 21:52:22.396: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b container client-container: <nil>
STEP: delete the pod 09/03/22 21:52:22.4
Sep  3 21:52:22.413: INFO: Waiting for pod downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b to disappear
Sep  3 21:52:22.415: INFO: Pod downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Sep  3 21:52:22.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2529" for this suite. 09/03/22 21:52:22.418
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":263,"skipped":4931,"failed":0}
------------------------------
• [4.053 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:18.369
    Sep  3 21:52:18.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 21:52:18.37
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:18.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:18.381
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 09/03/22 21:52:18.383
    Sep  3 21:52:18.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b" in namespace "downward-api-2529" to be "Succeeded or Failed"
    Sep  3 21:52:18.392: INFO: Pod "downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.187009ms
    Sep  3 21:52:20.395: INFO: Pod "downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00704221s
    Sep  3 21:52:22.394: INFO: Pod "downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006520785s
    STEP: Saw pod success 09/03/22 21:52:22.394
    Sep  3 21:52:22.395: INFO: Pod "downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b" satisfied condition "Succeeded or Failed"
    Sep  3 21:52:22.396: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b container client-container: <nil>
    STEP: delete the pod 09/03/22 21:52:22.4
    Sep  3 21:52:22.413: INFO: Waiting for pod downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b to disappear
    Sep  3 21:52:22.415: INFO: Pod downwardapi-volume-f1d81ba0-bb71-4192-adca-524853ec415b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Sep  3 21:52:22.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2529" for this suite. 09/03/22 21:52:22.418
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:22.425
Sep  3 21:52:22.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:52:22.427
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:22.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:22.438
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 09/03/22 21:52:22.441
Sep  3 21:52:22.451: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265" in namespace "projected-1089" to be "Succeeded or Failed"
Sep  3 21:52:22.454: INFO: Pod "downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.432805ms
Sep  3 21:52:24.457: INFO: Pod "downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005991588s
Sep  3 21:52:26.458: INFO: Pod "downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006526465s
STEP: Saw pod success 09/03/22 21:52:26.458
Sep  3 21:52:26.458: INFO: Pod "downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265" satisfied condition "Succeeded or Failed"
Sep  3 21:52:26.460: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265 container client-container: <nil>
STEP: delete the pod 09/03/22 21:52:26.464
Sep  3 21:52:26.471: INFO: Waiting for pod downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265 to disappear
Sep  3 21:52:26.473: INFO: Pod downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Sep  3 21:52:26.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1089" for this suite. 09/03/22 21:52:26.475
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":264,"skipped":4936,"failed":0}
------------------------------
• [4.054 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:22.425
    Sep  3 21:52:22.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:52:22.427
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:22.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:22.438
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 09/03/22 21:52:22.441
    Sep  3 21:52:22.451: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265" in namespace "projected-1089" to be "Succeeded or Failed"
    Sep  3 21:52:22.454: INFO: Pod "downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.432805ms
    Sep  3 21:52:24.457: INFO: Pod "downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005991588s
    Sep  3 21:52:26.458: INFO: Pod "downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006526465s
    STEP: Saw pod success 09/03/22 21:52:26.458
    Sep  3 21:52:26.458: INFO: Pod "downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265" satisfied condition "Succeeded or Failed"
    Sep  3 21:52:26.460: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265 container client-container: <nil>
    STEP: delete the pod 09/03/22 21:52:26.464
    Sep  3 21:52:26.471: INFO: Waiting for pod downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265 to disappear
    Sep  3 21:52:26.473: INFO: Pod downwardapi-volume-31e57b7e-8f4f-4f1b-89e2-f34e05f4e265 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Sep  3 21:52:26.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1089" for this suite. 09/03/22 21:52:26.475
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:26.499
Sep  3 21:52:26.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 21:52:26.501
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:26.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:26.513
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-5826 09/03/22 21:52:26.515
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5826 to expose endpoints map[] 09/03/22 21:52:26.521
Sep  3 21:52:26.532: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Sep  3 21:52:27.538: INFO: successfully validated that service endpoint-test2 in namespace services-5826 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5826 09/03/22 21:52:27.538
Sep  3 21:52:27.544: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5826" to be "running and ready"
Sep  3 21:52:27.554: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.53112ms
Sep  3 21:52:27.554: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:52:29.556: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011902982s
Sep  3 21:52:29.557: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  3 21:52:29.557: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5826 to expose endpoints map[pod1:[80]] 09/03/22 21:52:29.558
Sep  3 21:52:29.565: INFO: successfully validated that service endpoint-test2 in namespace services-5826 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 09/03/22 21:52:29.566
Sep  3 21:52:29.566: INFO: Creating new exec pod
Sep  3 21:52:29.569: INFO: Waiting up to 5m0s for pod "execpod6q7fl" in namespace "services-5826" to be "running"
Sep  3 21:52:29.571: INFO: Pod "execpod6q7fl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.306304ms
Sep  3 21:52:31.574: INFO: Pod "execpod6q7fl": Phase="Running", Reason="", readiness=true. Elapsed: 2.004913255s
Sep  3 21:52:31.574: INFO: Pod "execpod6q7fl" satisfied condition "running"
Sep  3 21:52:32.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5826 exec execpod6q7fl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Sep  3 21:52:32.712: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Sep  3 21:52:32.712: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 21:52:32.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5826 exec execpod6q7fl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.251 80'
Sep  3 21:52:32.842: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.2.251 80\nConnection to 10.96.2.251 80 port [tcp/http] succeeded!\n"
Sep  3 21:52:32.842: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-5826 09/03/22 21:52:32.842
Sep  3 21:52:32.848: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5826" to be "running and ready"
Sep  3 21:52:32.854: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.153113ms
Sep  3 21:52:32.854: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:52:34.857: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009101465s
Sep  3 21:52:34.857: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  3 21:52:34.857: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5826 to expose endpoints map[pod1:[80] pod2:[80]] 09/03/22 21:52:34.859
Sep  3 21:52:34.867: INFO: successfully validated that service endpoint-test2 in namespace services-5826 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 09/03/22 21:52:34.867
Sep  3 21:52:35.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5826 exec execpod6q7fl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Sep  3 21:52:36.033: INFO: stderr: "+ + echonc hostName\n -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  3 21:52:36.033: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 21:52:36.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5826 exec execpod6q7fl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.251 80'
Sep  3 21:52:36.181: INFO: stderr: "+ nc -v -t -w 2 10.96.2.251 80\nConnection to 10.96.2.251 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Sep  3 21:52:36.181: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5826 09/03/22 21:52:36.181
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5826 to expose endpoints map[pod2:[80]] 09/03/22 21:52:36.187
Sep  3 21:52:36.231: INFO: successfully validated that service endpoint-test2 in namespace services-5826 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 09/03/22 21:52:36.231
Sep  3 21:52:37.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5826 exec execpod6q7fl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Sep  3 21:52:37.362: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  3 21:52:37.362: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 21:52:37.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5826 exec execpod6q7fl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.251 80'
Sep  3 21:52:37.544: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.2.251 80\nConnection to 10.96.2.251 80 port [tcp/http] succeeded!\n"
Sep  3 21:52:37.544: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-5826 09/03/22 21:52:37.544
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5826 to expose endpoints map[] 09/03/22 21:52:37.558
Sep  3 21:52:38.593: INFO: successfully validated that service endpoint-test2 in namespace services-5826 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 21:52:38.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5826" for this suite. 09/03/22 21:52:38.608
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":265,"skipped":5017,"failed":0}
------------------------------
• [SLOW TEST] [12.116 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:26.499
    Sep  3 21:52:26.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 21:52:26.501
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:26.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:26.513
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-5826 09/03/22 21:52:26.515
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5826 to expose endpoints map[] 09/03/22 21:52:26.521
    Sep  3 21:52:26.532: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Sep  3 21:52:27.538: INFO: successfully validated that service endpoint-test2 in namespace services-5826 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5826 09/03/22 21:52:27.538
    Sep  3 21:52:27.544: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5826" to be "running and ready"
    Sep  3 21:52:27.554: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.53112ms
    Sep  3 21:52:27.554: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:52:29.556: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011902982s
    Sep  3 21:52:29.557: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  3 21:52:29.557: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5826 to expose endpoints map[pod1:[80]] 09/03/22 21:52:29.558
    Sep  3 21:52:29.565: INFO: successfully validated that service endpoint-test2 in namespace services-5826 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 09/03/22 21:52:29.566
    Sep  3 21:52:29.566: INFO: Creating new exec pod
    Sep  3 21:52:29.569: INFO: Waiting up to 5m0s for pod "execpod6q7fl" in namespace "services-5826" to be "running"
    Sep  3 21:52:29.571: INFO: Pod "execpod6q7fl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.306304ms
    Sep  3 21:52:31.574: INFO: Pod "execpod6q7fl": Phase="Running", Reason="", readiness=true. Elapsed: 2.004913255s
    Sep  3 21:52:31.574: INFO: Pod "execpod6q7fl" satisfied condition "running"
    Sep  3 21:52:32.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5826 exec execpod6q7fl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Sep  3 21:52:32.712: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Sep  3 21:52:32.712: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 21:52:32.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5826 exec execpod6q7fl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.251 80'
    Sep  3 21:52:32.842: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.2.251 80\nConnection to 10.96.2.251 80 port [tcp/http] succeeded!\n"
    Sep  3 21:52:32.842: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-5826 09/03/22 21:52:32.842
    Sep  3 21:52:32.848: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5826" to be "running and ready"
    Sep  3 21:52:32.854: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.153113ms
    Sep  3 21:52:32.854: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:52:34.857: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009101465s
    Sep  3 21:52:34.857: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  3 21:52:34.857: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5826 to expose endpoints map[pod1:[80] pod2:[80]] 09/03/22 21:52:34.859
    Sep  3 21:52:34.867: INFO: successfully validated that service endpoint-test2 in namespace services-5826 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 09/03/22 21:52:34.867
    Sep  3 21:52:35.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5826 exec execpod6q7fl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Sep  3 21:52:36.033: INFO: stderr: "+ + echonc hostName\n -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  3 21:52:36.033: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 21:52:36.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5826 exec execpod6q7fl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.251 80'
    Sep  3 21:52:36.181: INFO: stderr: "+ nc -v -t -w 2 10.96.2.251 80\nConnection to 10.96.2.251 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Sep  3 21:52:36.181: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-5826 09/03/22 21:52:36.181
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5826 to expose endpoints map[pod2:[80]] 09/03/22 21:52:36.187
    Sep  3 21:52:36.231: INFO: successfully validated that service endpoint-test2 in namespace services-5826 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 09/03/22 21:52:36.231
    Sep  3 21:52:37.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5826 exec execpod6q7fl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Sep  3 21:52:37.362: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  3 21:52:37.362: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 21:52:37.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5826 exec execpod6q7fl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.251 80'
    Sep  3 21:52:37.544: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.2.251 80\nConnection to 10.96.2.251 80 port [tcp/http] succeeded!\n"
    Sep  3 21:52:37.544: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-5826 09/03/22 21:52:37.544
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5826 to expose endpoints map[] 09/03/22 21:52:37.558
    Sep  3 21:52:38.593: INFO: successfully validated that service endpoint-test2 in namespace services-5826 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 21:52:38.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5826" for this suite. 09/03/22 21:52:38.608
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:38.617
Sep  3 21:52:38.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename dns 09/03/22 21:52:38.655
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:38.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:38.674
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2902.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2902.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 09/03/22 21:52:38.681
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2902.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2902.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 09/03/22 21:52:38.682
STEP: creating a pod to probe /etc/hosts 09/03/22 21:52:38.682
STEP: submitting the pod to kubernetes 09/03/22 21:52:38.682
Sep  3 21:52:38.690: INFO: Waiting up to 15m0s for pod "dns-test-29ab8e24-c8bb-4e21-92ea-9123b4729afa" in namespace "dns-2902" to be "running"
Sep  3 21:52:38.714: INFO: Pod "dns-test-29ab8e24-c8bb-4e21-92ea-9123b4729afa": Phase="Pending", Reason="", readiness=false. Elapsed: 23.72735ms
Sep  3 21:52:40.718: INFO: Pod "dns-test-29ab8e24-c8bb-4e21-92ea-9123b4729afa": Phase="Running", Reason="", readiness=true. Elapsed: 2.027340578s
Sep  3 21:52:40.718: INFO: Pod "dns-test-29ab8e24-c8bb-4e21-92ea-9123b4729afa" satisfied condition "running"
STEP: retrieving the pod 09/03/22 21:52:40.718
STEP: looking for the results for each expected name from probers 09/03/22 21:52:40.72
Sep  3 21:52:40.729: INFO: DNS probes using dns-2902/dns-test-29ab8e24-c8bb-4e21-92ea-9123b4729afa succeeded

STEP: deleting the pod 09/03/22 21:52:40.729
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Sep  3 21:52:40.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2902" for this suite. 09/03/22 21:52:40.741
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":266,"skipped":5050,"failed":0}
------------------------------
• [2.127 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:38.617
    Sep  3 21:52:38.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename dns 09/03/22 21:52:38.655
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:38.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:38.674
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2902.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2902.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     09/03/22 21:52:38.681
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2902.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2902.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     09/03/22 21:52:38.682
    STEP: creating a pod to probe /etc/hosts 09/03/22 21:52:38.682
    STEP: submitting the pod to kubernetes 09/03/22 21:52:38.682
    Sep  3 21:52:38.690: INFO: Waiting up to 15m0s for pod "dns-test-29ab8e24-c8bb-4e21-92ea-9123b4729afa" in namespace "dns-2902" to be "running"
    Sep  3 21:52:38.714: INFO: Pod "dns-test-29ab8e24-c8bb-4e21-92ea-9123b4729afa": Phase="Pending", Reason="", readiness=false. Elapsed: 23.72735ms
    Sep  3 21:52:40.718: INFO: Pod "dns-test-29ab8e24-c8bb-4e21-92ea-9123b4729afa": Phase="Running", Reason="", readiness=true. Elapsed: 2.027340578s
    Sep  3 21:52:40.718: INFO: Pod "dns-test-29ab8e24-c8bb-4e21-92ea-9123b4729afa" satisfied condition "running"
    STEP: retrieving the pod 09/03/22 21:52:40.718
    STEP: looking for the results for each expected name from probers 09/03/22 21:52:40.72
    Sep  3 21:52:40.729: INFO: DNS probes using dns-2902/dns-test-29ab8e24-c8bb-4e21-92ea-9123b4729afa succeeded

    STEP: deleting the pod 09/03/22 21:52:40.729
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Sep  3 21:52:40.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2902" for this suite. 09/03/22 21:52:40.741
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:40.745
Sep  3 21:52:40.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:52:40.746
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:40.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:40.758
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 09/03/22 21:52:40.76
Sep  3 21:52:40.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 create -f -'
Sep  3 21:52:40.972: INFO: stderr: ""
Sep  3 21:52:40.972: INFO: stdout: "pod/pause created\n"
Sep  3 21:52:40.972: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  3 21:52:40.972: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1824" to be "running and ready"
Sep  3 21:52:40.976: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033408ms
Sep  3 21:52:40.976: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'kind-worker2' to be 'Running' but was 'Pending'
Sep  3 21:52:42.979: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006837635s
Sep  3 21:52:42.979: INFO: Pod "pause" satisfied condition "running and ready"
Sep  3 21:52:42.979: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 09/03/22 21:52:42.979
Sep  3 21:52:42.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 label pods pause testing-label=testing-label-value'
Sep  3 21:52:43.054: INFO: stderr: ""
Sep  3 21:52:43.054: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 09/03/22 21:52:43.054
Sep  3 21:52:43.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 get pod pause -L testing-label'
Sep  3 21:52:43.121: INFO: stderr: ""
Sep  3 21:52:43.121: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 09/03/22 21:52:43.121
Sep  3 21:52:43.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 label pods pause testing-label-'
Sep  3 21:52:43.233: INFO: stderr: ""
Sep  3 21:52:43.233: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 09/03/22 21:52:43.233
Sep  3 21:52:43.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 get pod pause -L testing-label'
Sep  3 21:52:43.315: INFO: stderr: ""
Sep  3 21:52:43.315: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 09/03/22 21:52:43.315
Sep  3 21:52:43.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 delete --grace-period=0 --force -f -'
Sep  3 21:52:43.403: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 21:52:43.403: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  3 21:52:43.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 get rc,svc -l name=pause --no-headers'
Sep  3 21:52:43.507: INFO: stderr: "No resources found in kubectl-1824 namespace.\n"
Sep  3 21:52:43.507: INFO: stdout: ""
Sep  3 21:52:43.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  3 21:52:43.591: INFO: stderr: ""
Sep  3 21:52:43.591: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:52:43.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1824" for this suite. 09/03/22 21:52:43.594
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":267,"skipped":5064,"failed":0}
------------------------------
• [2.860 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:40.745
    Sep  3 21:52:40.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:52:40.746
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:40.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:40.758
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 09/03/22 21:52:40.76
    Sep  3 21:52:40.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 create -f -'
    Sep  3 21:52:40.972: INFO: stderr: ""
    Sep  3 21:52:40.972: INFO: stdout: "pod/pause created\n"
    Sep  3 21:52:40.972: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Sep  3 21:52:40.972: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1824" to be "running and ready"
    Sep  3 21:52:40.976: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033408ms
    Sep  3 21:52:40.976: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'kind-worker2' to be 'Running' but was 'Pending'
    Sep  3 21:52:42.979: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006837635s
    Sep  3 21:52:42.979: INFO: Pod "pause" satisfied condition "running and ready"
    Sep  3 21:52:42.979: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 09/03/22 21:52:42.979
    Sep  3 21:52:42.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 label pods pause testing-label=testing-label-value'
    Sep  3 21:52:43.054: INFO: stderr: ""
    Sep  3 21:52:43.054: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 09/03/22 21:52:43.054
    Sep  3 21:52:43.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 get pod pause -L testing-label'
    Sep  3 21:52:43.121: INFO: stderr: ""
    Sep  3 21:52:43.121: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 09/03/22 21:52:43.121
    Sep  3 21:52:43.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 label pods pause testing-label-'
    Sep  3 21:52:43.233: INFO: stderr: ""
    Sep  3 21:52:43.233: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 09/03/22 21:52:43.233
    Sep  3 21:52:43.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 get pod pause -L testing-label'
    Sep  3 21:52:43.315: INFO: stderr: ""
    Sep  3 21:52:43.315: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 09/03/22 21:52:43.315
    Sep  3 21:52:43.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 delete --grace-period=0 --force -f -'
    Sep  3 21:52:43.403: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  3 21:52:43.403: INFO: stdout: "pod \"pause\" force deleted\n"
    Sep  3 21:52:43.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 get rc,svc -l name=pause --no-headers'
    Sep  3 21:52:43.507: INFO: stderr: "No resources found in kubectl-1824 namespace.\n"
    Sep  3 21:52:43.507: INFO: stdout: ""
    Sep  3 21:52:43.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-1824 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  3 21:52:43.591: INFO: stderr: ""
    Sep  3 21:52:43.591: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:52:43.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1824" for this suite. 09/03/22 21:52:43.594
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:43.607
Sep  3 21:52:43.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename replication-controller 09/03/22 21:52:43.608
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:43.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:43.619
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c 09/03/22 21:52:43.622
Sep  3 21:52:43.634: INFO: Pod name my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c: Found 0 pods out of 1
Sep  3 21:52:48.638: INFO: Pod name my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c: Found 1 pods out of 1
Sep  3 21:52:48.638: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c" are running
Sep  3 21:52:48.638: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c-7bzlb" in namespace "replication-controller-7787" to be "running"
Sep  3 21:52:48.640: INFO: Pod "my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c-7bzlb": Phase="Running", Reason="", readiness=true. Elapsed: 2.534006ms
Sep  3 21:52:48.640: INFO: Pod "my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c-7bzlb" satisfied condition "running"
Sep  3 21:52:48.640: INFO: Pod "my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c-7bzlb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 21:52:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 21:52:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 21:52:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 21:52:43 +0000 UTC Reason: Message:}])
Sep  3 21:52:48.640: INFO: Trying to dial the pod
Sep  3 21:52:53.647: INFO: Controller my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c: Got expected result from replica 1 [my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c-7bzlb]: "my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c-7bzlb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Sep  3 21:52:53.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7787" for this suite. 09/03/22 21:52:53.65
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":268,"skipped":5089,"failed":0}
------------------------------
• [SLOW TEST] [10.047 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:43.607
    Sep  3 21:52:43.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename replication-controller 09/03/22 21:52:43.608
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:43.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:43.619
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c 09/03/22 21:52:43.622
    Sep  3 21:52:43.634: INFO: Pod name my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c: Found 0 pods out of 1
    Sep  3 21:52:48.638: INFO: Pod name my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c: Found 1 pods out of 1
    Sep  3 21:52:48.638: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c" are running
    Sep  3 21:52:48.638: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c-7bzlb" in namespace "replication-controller-7787" to be "running"
    Sep  3 21:52:48.640: INFO: Pod "my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c-7bzlb": Phase="Running", Reason="", readiness=true. Elapsed: 2.534006ms
    Sep  3 21:52:48.640: INFO: Pod "my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c-7bzlb" satisfied condition "running"
    Sep  3 21:52:48.640: INFO: Pod "my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c-7bzlb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 21:52:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 21:52:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 21:52:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-03 21:52:43 +0000 UTC Reason: Message:}])
    Sep  3 21:52:48.640: INFO: Trying to dial the pod
    Sep  3 21:52:53.647: INFO: Controller my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c: Got expected result from replica 1 [my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c-7bzlb]: "my-hostname-basic-3cddb5ba-1c1a-411f-94ee-2c47e8f5fc7c-7bzlb", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Sep  3 21:52:53.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7787" for this suite. 09/03/22 21:52:53.65
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:53.668
Sep  3 21:52:53.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 21:52:53.669
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:53.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:53.68
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 09/03/22 21:52:53.682
Sep  3 21:52:53.686: INFO: Waiting up to 5m0s for pod "downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d" in namespace "downward-api-8407" to be "Succeeded or Failed"
Sep  3 21:52:53.694: INFO: Pod "downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.127314ms
Sep  3 21:52:55.697: INFO: Pod "downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010710324s
Sep  3 21:52:57.697: INFO: Pod "downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010203626s
STEP: Saw pod success 09/03/22 21:52:57.697
Sep  3 21:52:57.697: INFO: Pod "downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d" satisfied condition "Succeeded or Failed"
Sep  3 21:52:57.699: INFO: Trying to get logs from node kind-worker2 pod downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d container dapi-container: <nil>
STEP: delete the pod 09/03/22 21:52:57.703
Sep  3 21:52:57.710: INFO: Waiting for pod downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d to disappear
Sep  3 21:52:57.712: INFO: Pod downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Sep  3 21:52:57.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8407" for this suite. 09/03/22 21:52:57.714
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":269,"skipped":5137,"failed":0}
------------------------------
• [4.050 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:53.668
    Sep  3 21:52:53.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 21:52:53.669
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:53.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:53.68
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 09/03/22 21:52:53.682
    Sep  3 21:52:53.686: INFO: Waiting up to 5m0s for pod "downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d" in namespace "downward-api-8407" to be "Succeeded or Failed"
    Sep  3 21:52:53.694: INFO: Pod "downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.127314ms
    Sep  3 21:52:55.697: INFO: Pod "downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010710324s
    Sep  3 21:52:57.697: INFO: Pod "downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010203626s
    STEP: Saw pod success 09/03/22 21:52:57.697
    Sep  3 21:52:57.697: INFO: Pod "downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d" satisfied condition "Succeeded or Failed"
    Sep  3 21:52:57.699: INFO: Trying to get logs from node kind-worker2 pod downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d container dapi-container: <nil>
    STEP: delete the pod 09/03/22 21:52:57.703
    Sep  3 21:52:57.710: INFO: Waiting for pod downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d to disappear
    Sep  3 21:52:57.712: INFO: Pod downward-api-1560839f-e8bf-448e-84ec-d78f1afc800d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Sep  3 21:52:57.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8407" for this suite. 09/03/22 21:52:57.714
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:52:57.724
Sep  3 21:52:57.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pod-network-test 09/03/22 21:52:57.727
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:57.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:57.741
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-2330 09/03/22 21:52:57.744
STEP: creating a selector 09/03/22 21:52:57.745
STEP: Creating the service pods in kubernetes 09/03/22 21:52:57.746
Sep  3 21:52:57.746: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  3 21:52:57.757: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2330" to be "running and ready"
Sep  3 21:52:57.784: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.794151ms
Sep  3 21:52:57.784: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:52:59.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.03058796s
Sep  3 21:52:59.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 21:53:01.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.030937321s
Sep  3 21:53:01.788: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 21:53:03.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.030560246s
Sep  3 21:53:03.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 21:53:05.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.030802872s
Sep  3 21:53:05.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 21:53:07.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.030350896s
Sep  3 21:53:07.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 21:53:09.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.030281789s
Sep  3 21:53:09.787: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  3 21:53:09.787: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  3 21:53:09.789: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2330" to be "running and ready"
Sep  3 21:53:09.791: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.630904ms
Sep  3 21:53:09.791: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  3 21:53:09.791: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/03/22 21:53:09.792
Sep  3 21:53:09.796: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2330" to be "running"
Sep  3 21:53:09.799: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.087807ms
Sep  3 21:53:11.807: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010251284s
Sep  3 21:53:11.807: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  3 21:53:11.808: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  3 21:53:11.808: INFO: Breadth first check of 10.244.2.128 on host 172.18.0.2...
Sep  3 21:53:11.810: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.76:9080/dial?request=hostname&protocol=http&host=10.244.2.128&port=8083&tries=1'] Namespace:pod-network-test-2330 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 21:53:11.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 21:53:11.811: INFO: ExecWithOptions: Clientset creation
Sep  3 21:53:11.811: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2330/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.76%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.128%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  3 21:53:11.889: INFO: Waiting for responses: map[]
Sep  3 21:53:11.889: INFO: reached 10.244.2.128 after 0/1 tries
Sep  3 21:53:11.889: INFO: Breadth first check of 10.244.1.75 on host 172.18.0.3...
Sep  3 21:53:11.892: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.76:9080/dial?request=hostname&protocol=http&host=10.244.1.75&port=8083&tries=1'] Namespace:pod-network-test-2330 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 21:53:11.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 21:53:11.892: INFO: ExecWithOptions: Clientset creation
Sep  3 21:53:11.892: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2330/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.76%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.75%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  3 21:53:11.964: INFO: Waiting for responses: map[]
Sep  3 21:53:11.964: INFO: reached 10.244.1.75 after 0/1 tries
Sep  3 21:53:11.964: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Sep  3 21:53:11.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2330" for this suite. 09/03/22 21:53:11.967
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":270,"skipped":5160,"failed":0}
------------------------------
• [SLOW TEST] [14.248 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:52:57.724
    Sep  3 21:52:57.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pod-network-test 09/03/22 21:52:57.727
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:52:57.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:52:57.741
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-2330 09/03/22 21:52:57.744
    STEP: creating a selector 09/03/22 21:52:57.745
    STEP: Creating the service pods in kubernetes 09/03/22 21:52:57.746
    Sep  3 21:52:57.746: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  3 21:52:57.757: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2330" to be "running and ready"
    Sep  3 21:52:57.784: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.794151ms
    Sep  3 21:52:57.784: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:52:59.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.03058796s
    Sep  3 21:52:59.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 21:53:01.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.030937321s
    Sep  3 21:53:01.788: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 21:53:03.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.030560246s
    Sep  3 21:53:03.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 21:53:05.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.030802872s
    Sep  3 21:53:05.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 21:53:07.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.030350896s
    Sep  3 21:53:07.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 21:53:09.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.030281789s
    Sep  3 21:53:09.787: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  3 21:53:09.787: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  3 21:53:09.789: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2330" to be "running and ready"
    Sep  3 21:53:09.791: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.630904ms
    Sep  3 21:53:09.791: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  3 21:53:09.791: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/03/22 21:53:09.792
    Sep  3 21:53:09.796: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2330" to be "running"
    Sep  3 21:53:09.799: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.087807ms
    Sep  3 21:53:11.807: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010251284s
    Sep  3 21:53:11.807: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  3 21:53:11.808: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  3 21:53:11.808: INFO: Breadth first check of 10.244.2.128 on host 172.18.0.2...
    Sep  3 21:53:11.810: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.76:9080/dial?request=hostname&protocol=http&host=10.244.2.128&port=8083&tries=1'] Namespace:pod-network-test-2330 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 21:53:11.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 21:53:11.811: INFO: ExecWithOptions: Clientset creation
    Sep  3 21:53:11.811: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2330/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.76%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.128%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  3 21:53:11.889: INFO: Waiting for responses: map[]
    Sep  3 21:53:11.889: INFO: reached 10.244.2.128 after 0/1 tries
    Sep  3 21:53:11.889: INFO: Breadth first check of 10.244.1.75 on host 172.18.0.3...
    Sep  3 21:53:11.892: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.76:9080/dial?request=hostname&protocol=http&host=10.244.1.75&port=8083&tries=1'] Namespace:pod-network-test-2330 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 21:53:11.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 21:53:11.892: INFO: ExecWithOptions: Clientset creation
    Sep  3 21:53:11.892: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2330/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.76%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.75%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  3 21:53:11.964: INFO: Waiting for responses: map[]
    Sep  3 21:53:11.964: INFO: reached 10.244.1.75 after 0/1 tries
    Sep  3 21:53:11.964: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Sep  3 21:53:11.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2330" for this suite. 09/03/22 21:53:11.967
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:53:11.975
Sep  3 21:53:11.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubectl 09/03/22 21:53:11.975
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:53:11.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:53:11.988
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 09/03/22 21:53:11.99
Sep  3 21:53:11.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9554 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  3 21:53:12.089: INFO: stderr: ""
Sep  3 21:53:12.089: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 09/03/22 21:53:12.089
STEP: verifying the pod e2e-test-httpd-pod was created 09/03/22 21:53:17.141
Sep  3 21:53:17.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9554 get pod e2e-test-httpd-pod -o json'
Sep  3 21:53:17.417: INFO: stderr: ""
Sep  3 21:53:17.418: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-09-03T21:53:12Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9554\",\n        \"resourceVersion\": \"22085\",\n        \"uid\": \"61293959-139d-4786-a834-65dd55d681fd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-44hbx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kind-worker2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-44hbx\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-03T21:53:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-03T21:53:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-03T21:53:12Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-03T21:53:12Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://46f70ad6f4cf33e2e0c6c0f025448962ffd33e29a6b147cea726197ff9a36e27\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-09-03T21:53:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.18.0.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.77\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.77\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-09-03T21:53:12Z\"\n    }\n}\n"
STEP: replace the image in the pod 09/03/22 21:53:17.418
Sep  3 21:53:17.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9554 replace -f -'
Sep  3 21:53:17.784: INFO: stderr: ""
Sep  3 21:53:17.784: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 09/03/22 21:53:17.784
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Sep  3 21:53:17.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9554 delete pods e2e-test-httpd-pod'
Sep  3 21:53:19.918: INFO: stderr: ""
Sep  3 21:53:19.918: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Sep  3 21:53:19.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9554" for this suite. 09/03/22 21:53:19.922
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":271,"skipped":5163,"failed":0}
------------------------------
• [SLOW TEST] [7.951 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:53:11.975
    Sep  3 21:53:11.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubectl 09/03/22 21:53:11.975
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:53:11.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:53:11.988
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 09/03/22 21:53:11.99
    Sep  3 21:53:11.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9554 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  3 21:53:12.089: INFO: stderr: ""
    Sep  3 21:53:12.089: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 09/03/22 21:53:12.089
    STEP: verifying the pod e2e-test-httpd-pod was created 09/03/22 21:53:17.141
    Sep  3 21:53:17.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9554 get pod e2e-test-httpd-pod -o json'
    Sep  3 21:53:17.417: INFO: stderr: ""
    Sep  3 21:53:17.418: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-09-03T21:53:12Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9554\",\n        \"resourceVersion\": \"22085\",\n        \"uid\": \"61293959-139d-4786-a834-65dd55d681fd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-44hbx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kind-worker2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-44hbx\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-03T21:53:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-03T21:53:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-03T21:53:12Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-03T21:53:12Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://46f70ad6f4cf33e2e0c6c0f025448962ffd33e29a6b147cea726197ff9a36e27\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-09-03T21:53:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.18.0.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.77\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.77\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-09-03T21:53:12Z\"\n    }\n}\n"
    STEP: replace the image in the pod 09/03/22 21:53:17.418
    Sep  3 21:53:17.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9554 replace -f -'
    Sep  3 21:53:17.784: INFO: stderr: ""
    Sep  3 21:53:17.784: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 09/03/22 21:53:17.784
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Sep  3 21:53:17.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=kubectl-9554 delete pods e2e-test-httpd-pod'
    Sep  3 21:53:19.918: INFO: stderr: ""
    Sep  3 21:53:19.918: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Sep  3 21:53:19.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9554" for this suite. 09/03/22 21:53:19.922
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:53:19.927
Sep  3 21:53:19.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 21:53:19.927
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:53:19.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:53:19.995
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 21:53:20.009
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:53:20.721
STEP: Deploying the webhook pod 09/03/22 21:53:20.727
STEP: Wait for the deployment to be ready 09/03/22 21:53:20.734
Sep  3 21:53:20.739: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 21:53:22.745
STEP: Verifying the service has paired with the endpoint 09/03/22 21:53:22.766
Sep  3 21:53:23.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/03/22 21:53:23.769
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/03/22 21:53:23.78
STEP: Creating a dummy validating-webhook-configuration object 09/03/22 21:53:23.786
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/03/22 21:53:23.794
STEP: Creating a dummy mutating-webhook-configuration object 09/03/22 21:53:23.797
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/03/22 21:53:23.802
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:53:23.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1194" for this suite. 09/03/22 21:53:23.813
STEP: Destroying namespace "webhook-1194-markers" for this suite. 09/03/22 21:53:23.817
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":272,"skipped":5169,"failed":0}
------------------------------
• [3.963 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:53:19.927
    Sep  3 21:53:19.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 21:53:19.927
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:53:19.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:53:19.995
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 21:53:20.009
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:53:20.721
    STEP: Deploying the webhook pod 09/03/22 21:53:20.727
    STEP: Wait for the deployment to be ready 09/03/22 21:53:20.734
    Sep  3 21:53:20.739: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 21:53:22.745
    STEP: Verifying the service has paired with the endpoint 09/03/22 21:53:22.766
    Sep  3 21:53:23.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/03/22 21:53:23.769
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/03/22 21:53:23.78
    STEP: Creating a dummy validating-webhook-configuration object 09/03/22 21:53:23.786
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/03/22 21:53:23.794
    STEP: Creating a dummy mutating-webhook-configuration object 09/03/22 21:53:23.797
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/03/22 21:53:23.802
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:53:23.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1194" for this suite. 09/03/22 21:53:23.813
    STEP: Destroying namespace "webhook-1194-markers" for this suite. 09/03/22 21:53:23.817
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:53:23.899
Sep  3 21:53:23.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 21:53:23.902
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:53:23.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:53:23.932
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-03802351-b1ff-4bc3-ae67-f1822b8ca4d1 09/03/22 21:53:23.935
STEP: Creating a pod to test consume secrets 09/03/22 21:53:23.949
Sep  3 21:53:23.973: INFO: Waiting up to 5m0s for pod "pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9" in namespace "secrets-3797" to be "Succeeded or Failed"
Sep  3 21:53:23.986: INFO: Pod "pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.814327ms
Sep  3 21:53:25.990: INFO: Pod "pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016752493s
Sep  3 21:53:27.990: INFO: Pod "pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016552857s
STEP: Saw pod success 09/03/22 21:53:27.99
Sep  3 21:53:27.990: INFO: Pod "pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9" satisfied condition "Succeeded or Failed"
Sep  3 21:53:27.991: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9 container secret-volume-test: <nil>
STEP: delete the pod 09/03/22 21:53:27.995
Sep  3 21:53:28.001: INFO: Waiting for pod pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9 to disappear
Sep  3 21:53:28.003: INFO: Pod pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Sep  3 21:53:28.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3797" for this suite. 09/03/22 21:53:28.005
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":273,"skipped":5178,"failed":0}
------------------------------
• [4.116 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:53:23.899
    Sep  3 21:53:23.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 21:53:23.902
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:53:23.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:53:23.932
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-03802351-b1ff-4bc3-ae67-f1822b8ca4d1 09/03/22 21:53:23.935
    STEP: Creating a pod to test consume secrets 09/03/22 21:53:23.949
    Sep  3 21:53:23.973: INFO: Waiting up to 5m0s for pod "pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9" in namespace "secrets-3797" to be "Succeeded or Failed"
    Sep  3 21:53:23.986: INFO: Pod "pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.814327ms
    Sep  3 21:53:25.990: INFO: Pod "pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016752493s
    Sep  3 21:53:27.990: INFO: Pod "pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016552857s
    STEP: Saw pod success 09/03/22 21:53:27.99
    Sep  3 21:53:27.990: INFO: Pod "pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9" satisfied condition "Succeeded or Failed"
    Sep  3 21:53:27.991: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9 container secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 21:53:27.995
    Sep  3 21:53:28.001: INFO: Waiting for pod pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9 to disappear
    Sep  3 21:53:28.003: INFO: Pod pod-secrets-e234b282-3d16-457b-8f81-64f336774ca9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 21:53:28.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3797" for this suite. 09/03/22 21:53:28.005
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:53:28.015
Sep  3 21:53:28.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename aggregator 09/03/22 21:53:28.02
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:53:28.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:53:28.034
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Sep  3 21:53:28.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 09/03/22 21:53:28.039
Sep  3 21:53:28.767: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  3 21:53:30.812: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:53:32.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:53:34.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:53:36.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:53:38.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:53:40.818: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:53:42.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:53:44.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:53:46.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:53:48.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:53:50.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 21:53:52.938: INFO: Waited 117.41163ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 09/03/22 21:53:52.978
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/03/22 21:53:52.98
STEP: List APIServices 09/03/22 21:53:52.985
Sep  3 21:53:52.990: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Sep  3 21:53:53.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-708" for this suite. 09/03/22 21:53:53.25
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":274,"skipped":5181,"failed":0}
------------------------------
• [SLOW TEST] [25.241 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:53:28.015
    Sep  3 21:53:28.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename aggregator 09/03/22 21:53:28.02
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:53:28.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:53:28.034
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Sep  3 21:53:28.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 09/03/22 21:53:28.039
    Sep  3 21:53:28.767: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Sep  3 21:53:30.812: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:53:32.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:53:34.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:53:36.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:53:38.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:53:40.818: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:53:42.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:53:44.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:53:46.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:53:48.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:53:50.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 3, 21, 53, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  3 21:53:52.938: INFO: Waited 117.41163ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 09/03/22 21:53:52.978
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/03/22 21:53:52.98
    STEP: List APIServices 09/03/22 21:53:52.985
    Sep  3 21:53:52.990: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Sep  3 21:53:53.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-708" for this suite. 09/03/22 21:53:53.25
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:53:53.261
Sep  3 21:53:53.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename server-version 09/03/22 21:53:53.262
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:53:53.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:53:53.289
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 09/03/22 21:53:53.292
STEP: Confirm major version 09/03/22 21:53:53.293
Sep  3 21:53:53.293: INFO: Major version: 1
STEP: Confirm minor version 09/03/22 21:53:53.293
Sep  3 21:53:53.293: INFO: cleanMinorVersion: 25
Sep  3 21:53:53.293: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Sep  3 21:53:53.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-4711" for this suite. 09/03/22 21:53:53.307
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":275,"skipped":5182,"failed":0}
------------------------------
• [0.054 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:53:53.261
    Sep  3 21:53:53.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename server-version 09/03/22 21:53:53.262
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:53:53.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:53:53.289
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 09/03/22 21:53:53.292
    STEP: Confirm major version 09/03/22 21:53:53.293
    Sep  3 21:53:53.293: INFO: Major version: 1
    STEP: Confirm minor version 09/03/22 21:53:53.293
    Sep  3 21:53:53.293: INFO: cleanMinorVersion: 25
    Sep  3 21:53:53.293: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Sep  3 21:53:53.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-4711" for this suite. 09/03/22 21:53:53.307
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:53:53.316
Sep  3 21:53:53.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:53:53.317
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:53:53.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:53:53.338
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-c0b8092b-4d48-40e5-b79f-d53e88bb81fb 09/03/22 21:53:53.347
STEP: Creating configMap with name cm-test-opt-upd-1b98a716-c15b-479a-a1c9-e6c2b141be8a 09/03/22 21:53:53.352
STEP: Creating the pod 09/03/22 21:53:53.355
Sep  3 21:53:53.362: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c" in namespace "projected-398" to be "running and ready"
Sep  3 21:53:53.366: INFO: Pod "pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.055706ms
Sep  3 21:53:53.366: INFO: The phase of Pod pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:53:55.369: INFO: Pod "pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006174943s
Sep  3 21:53:55.369: INFO: The phase of Pod pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:53:57.368: INFO: Pod "pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c": Phase="Running", Reason="", readiness=true. Elapsed: 4.005951318s
Sep  3 21:53:57.369: INFO: The phase of Pod pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c is Running (Ready = true)
Sep  3 21:53:57.369: INFO: Pod "pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-c0b8092b-4d48-40e5-b79f-d53e88bb81fb 09/03/22 21:53:57.38
STEP: Updating configmap cm-test-opt-upd-1b98a716-c15b-479a-a1c9-e6c2b141be8a 09/03/22 21:53:57.383
STEP: Creating configMap with name cm-test-opt-create-455491da-89ef-442f-b597-989438ef7ef9 09/03/22 21:53:57.386
STEP: waiting to observe update in volume 09/03/22 21:53:57.388
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Sep  3 21:55:15.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-398" for this suite. 09/03/22 21:55:15.666
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":276,"skipped":5195,"failed":0}
------------------------------
• [SLOW TEST] [82.354 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:53:53.316
    Sep  3 21:53:53.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:53:53.317
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:53:53.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:53:53.338
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-c0b8092b-4d48-40e5-b79f-d53e88bb81fb 09/03/22 21:53:53.347
    STEP: Creating configMap with name cm-test-opt-upd-1b98a716-c15b-479a-a1c9-e6c2b141be8a 09/03/22 21:53:53.352
    STEP: Creating the pod 09/03/22 21:53:53.355
    Sep  3 21:53:53.362: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c" in namespace "projected-398" to be "running and ready"
    Sep  3 21:53:53.366: INFO: Pod "pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.055706ms
    Sep  3 21:53:53.366: INFO: The phase of Pod pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:53:55.369: INFO: Pod "pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006174943s
    Sep  3 21:53:55.369: INFO: The phase of Pod pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:53:57.368: INFO: Pod "pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c": Phase="Running", Reason="", readiness=true. Elapsed: 4.005951318s
    Sep  3 21:53:57.369: INFO: The phase of Pod pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c is Running (Ready = true)
    Sep  3 21:53:57.369: INFO: Pod "pod-projected-configmaps-2a017385-715c-465b-b9d9-b8ce900e9d3c" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-c0b8092b-4d48-40e5-b79f-d53e88bb81fb 09/03/22 21:53:57.38
    STEP: Updating configmap cm-test-opt-upd-1b98a716-c15b-479a-a1c9-e6c2b141be8a 09/03/22 21:53:57.383
    STEP: Creating configMap with name cm-test-opt-create-455491da-89ef-442f-b597-989438ef7ef9 09/03/22 21:53:57.386
    STEP: waiting to observe update in volume 09/03/22 21:53:57.388
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Sep  3 21:55:15.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-398" for this suite. 09/03/22 21:55:15.666
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:55:15.671
Sep  3 21:55:15.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename crd-webhook 09/03/22 21:55:15.672
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:55:15.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:55:15.683
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/03/22 21:55:15.686
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/03/22 21:55:16.345
STEP: Deploying the custom resource conversion webhook pod 09/03/22 21:55:16.35
STEP: Wait for the deployment to be ready 09/03/22 21:55:16.356
Sep  3 21:55:16.361: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 09/03/22 21:55:18.369
STEP: Verifying the service has paired with the endpoint 09/03/22 21:55:18.38
Sep  3 21:55:19.380: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Sep  3 21:55:19.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Creating a v1 custom resource 09/03/22 21:55:21.978
STEP: Create a v2 custom resource 09/03/22 21:55:21.999
STEP: List CRs in v1 09/03/22 21:55:22.065
STEP: List CRs in v2 09/03/22 21:55:22.073
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:55:22.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3437" for this suite. 09/03/22 21:55:22.593
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":277,"skipped":5195,"failed":0}
------------------------------
• [SLOW TEST] [7.006 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:55:15.671
    Sep  3 21:55:15.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename crd-webhook 09/03/22 21:55:15.672
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:55:15.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:55:15.683
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/03/22 21:55:15.686
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/03/22 21:55:16.345
    STEP: Deploying the custom resource conversion webhook pod 09/03/22 21:55:16.35
    STEP: Wait for the deployment to be ready 09/03/22 21:55:16.356
    Sep  3 21:55:16.361: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 09/03/22 21:55:18.369
    STEP: Verifying the service has paired with the endpoint 09/03/22 21:55:18.38
    Sep  3 21:55:19.380: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Sep  3 21:55:19.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Creating a v1 custom resource 09/03/22 21:55:21.978
    STEP: Create a v2 custom resource 09/03/22 21:55:21.999
    STEP: List CRs in v1 09/03/22 21:55:22.065
    STEP: List CRs in v2 09/03/22 21:55:22.073
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:55:22.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3437" for this suite. 09/03/22 21:55:22.593
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:55:22.68
Sep  3 21:55:22.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename crd-watch 09/03/22 21:55:22.681
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:55:22.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:55:22.729
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Sep  3 21:55:22.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Creating first CR  09/03/22 21:55:25.284
Sep  3 21:55:25.287: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-03T21:55:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-03T21:55:25Z]] name:name1 resourceVersion:22618 uid:1204f85d-2c1b-49cc-9d6c-a41af55c6426] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 09/03/22 21:55:35.287
Sep  3 21:55:35.292: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-03T21:55:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-03T21:55:35Z]] name:name2 resourceVersion:22644 uid:cc7dd67a-8c38-485f-83fd-ebb01485533d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 09/03/22 21:55:45.292
Sep  3 21:55:45.298: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-03T21:55:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-03T21:55:45Z]] name:name1 resourceVersion:22659 uid:1204f85d-2c1b-49cc-9d6c-a41af55c6426] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 09/03/22 21:55:55.298
Sep  3 21:55:55.304: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-03T21:55:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-03T21:55:55Z]] name:name2 resourceVersion:22674 uid:cc7dd67a-8c38-485f-83fd-ebb01485533d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 09/03/22 21:56:05.305
Sep  3 21:56:05.309: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-03T21:55:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-03T21:55:45Z]] name:name1 resourceVersion:22689 uid:1204f85d-2c1b-49cc-9d6c-a41af55c6426] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 09/03/22 21:56:15.31
Sep  3 21:56:15.315: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-03T21:55:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-03T21:55:55Z]] name:name2 resourceVersion:22704 uid:cc7dd67a-8c38-485f-83fd-ebb01485533d] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:56:25.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2203" for this suite. 09/03/22 21:56:25.831
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":278,"skipped":5230,"failed":0}
------------------------------
• [SLOW TEST] [63.167 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:55:22.68
    Sep  3 21:55:22.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename crd-watch 09/03/22 21:55:22.681
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:55:22.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:55:22.729
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Sep  3 21:55:22.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Creating first CR  09/03/22 21:55:25.284
    Sep  3 21:55:25.287: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-03T21:55:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-03T21:55:25Z]] name:name1 resourceVersion:22618 uid:1204f85d-2c1b-49cc-9d6c-a41af55c6426] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 09/03/22 21:55:35.287
    Sep  3 21:55:35.292: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-03T21:55:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-03T21:55:35Z]] name:name2 resourceVersion:22644 uid:cc7dd67a-8c38-485f-83fd-ebb01485533d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 09/03/22 21:55:45.292
    Sep  3 21:55:45.298: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-03T21:55:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-03T21:55:45Z]] name:name1 resourceVersion:22659 uid:1204f85d-2c1b-49cc-9d6c-a41af55c6426] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 09/03/22 21:55:55.298
    Sep  3 21:55:55.304: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-03T21:55:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-03T21:55:55Z]] name:name2 resourceVersion:22674 uid:cc7dd67a-8c38-485f-83fd-ebb01485533d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 09/03/22 21:56:05.305
    Sep  3 21:56:05.309: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-03T21:55:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-03T21:55:45Z]] name:name1 resourceVersion:22689 uid:1204f85d-2c1b-49cc-9d6c-a41af55c6426] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 09/03/22 21:56:15.31
    Sep  3 21:56:15.315: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-03T21:55:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-03T21:55:55Z]] name:name2 resourceVersion:22704 uid:cc7dd67a-8c38-485f-83fd-ebb01485533d] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:56:25.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-2203" for this suite. 09/03/22 21:56:25.831
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:56:25.849
Sep  3 21:56:25.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename security-context-test 09/03/22 21:56:25.85
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:56:25.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:56:25.865
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Sep  3 21:56:25.873: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613" in namespace "security-context-test-5324" to be "Succeeded or Failed"
Sep  3 21:56:25.876: INFO: Pod "busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613": Phase="Pending", Reason="", readiness=false. Elapsed: 2.397006ms
Sep  3 21:56:27.878: INFO: Pod "busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00526091s
Sep  3 21:56:29.878: INFO: Pod "busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004930096s
Sep  3 21:56:29.878: INFO: Pod "busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Sep  3 21:56:29.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5324" for this suite. 09/03/22 21:56:29.88
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":279,"skipped":5247,"failed":0}
------------------------------
• [4.035 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:56:25.849
    Sep  3 21:56:25.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename security-context-test 09/03/22 21:56:25.85
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:56:25.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:56:25.865
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Sep  3 21:56:25.873: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613" in namespace "security-context-test-5324" to be "Succeeded or Failed"
    Sep  3 21:56:25.876: INFO: Pod "busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613": Phase="Pending", Reason="", readiness=false. Elapsed: 2.397006ms
    Sep  3 21:56:27.878: INFO: Pod "busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00526091s
    Sep  3 21:56:29.878: INFO: Pod "busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004930096s
    Sep  3 21:56:29.878: INFO: Pod "busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Sep  3 21:56:29.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5324" for this suite. 09/03/22 21:56:29.88
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:56:29.887
Sep  3 21:56:29.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename endpointslice 09/03/22 21:56:29.888
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:56:29.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:56:29.899
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Sep  3 21:56:29.906: INFO: Endpoints addresses: [172.18.0.4] , ports: [6443]
Sep  3 21:56:29.906: INFO: EndpointSlices addresses: [172.18.0.4] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Sep  3 21:56:29.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4961" for this suite. 09/03/22 21:56:29.909
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":280,"skipped":5252,"failed":0}
------------------------------
• [0.026 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:56:29.887
    Sep  3 21:56:29.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename endpointslice 09/03/22 21:56:29.888
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:56:29.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:56:29.899
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Sep  3 21:56:29.906: INFO: Endpoints addresses: [172.18.0.4] , ports: [6443]
    Sep  3 21:56:29.906: INFO: EndpointSlices addresses: [172.18.0.4] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Sep  3 21:56:29.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4961" for this suite. 09/03/22 21:56:29.909
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:56:29.915
Sep  3 21:56:29.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename sched-pred 09/03/22 21:56:29.916
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:56:29.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:56:29.927
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Sep  3 21:56:29.928: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  3 21:56:29.933: INFO: Waiting for terminating namespaces to be deleted...
Sep  3 21:56:29.935: INFO: 
Logging pods the apiserver thinks is on node kind-worker before test
Sep  3 21:56:29.942: INFO: kindnet-xhxcg from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
Sep  3 21:56:29.942: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  3 21:56:29.942: INFO: kube-proxy-bxm4p from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
Sep  3 21:56:29.942: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 21:56:29.942: INFO: sonobuoy from sonobuoy started at 2022-09-03 20:37:57 +0000 UTC (1 container statuses recorded)
Sep  3 21:56:29.942: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  3 21:56:29.942: INFO: sonobuoy-e2e-job-31a458d0b0c040d4 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
Sep  3 21:56:29.942: INFO: 	Container e2e ready: true, restart count 0
Sep  3 21:56:29.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 21:56:29.942: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-jdmz7 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
Sep  3 21:56:29.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 21:56:29.942: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 21:56:29.942: INFO: 
Logging pods the apiserver thinks is on node kind-worker2 before test
Sep  3 21:56:29.947: INFO: kindnet-zlt4d from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
Sep  3 21:56:29.947: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  3 21:56:29.947: INFO: kube-proxy-wz2c5 from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
Sep  3 21:56:29.947: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 21:56:29.947: INFO: busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613 from security-context-test-5324 started at 2022-09-03 21:56:25 +0000 UTC (1 container statuses recorded)
Sep  3 21:56:29.947: INFO: 	Container busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613 ready: false, restart count 0
Sep  3 21:56:29.947: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-nwk5b from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
Sep  3 21:56:29.947: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 21:56:29.947: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/03/22 21:56:29.948
Sep  3 21:56:29.954: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6380" to be "running"
Sep  3 21:56:29.962: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.733218ms
Sep  3 21:56:31.965: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010873763s
Sep  3 21:56:31.965: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/03/22 21:56:31.967
STEP: Trying to apply a random label on the found node. 09/03/22 21:56:31.975
STEP: verifying the node has the label kubernetes.io/e2e-6b0ca5d4-0f14-4864-abf1-e8a54448655e 42 09/03/22 21:56:31.991
STEP: Trying to relaunch the pod, now with labels. 09/03/22 21:56:32.001
Sep  3 21:56:32.007: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6380" to be "not pending"
Sep  3 21:56:32.021: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 14.229133ms
Sep  3 21:56:34.024: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.017167678s
Sep  3 21:56:34.024: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-6b0ca5d4-0f14-4864-abf1-e8a54448655e off the node kind-worker2 09/03/22 21:56:34.026
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6b0ca5d4-0f14-4864-abf1-e8a54448655e 09/03/22 21:56:34.037
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:56:34.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6380" for this suite. 09/03/22 21:56:34.048
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":281,"skipped":5254,"failed":0}
------------------------------
• [4.143 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:56:29.915
    Sep  3 21:56:29.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename sched-pred 09/03/22 21:56:29.916
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:56:29.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:56:29.927
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Sep  3 21:56:29.928: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  3 21:56:29.933: INFO: Waiting for terminating namespaces to be deleted...
    Sep  3 21:56:29.935: INFO: 
    Logging pods the apiserver thinks is on node kind-worker before test
    Sep  3 21:56:29.942: INFO: kindnet-xhxcg from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
    Sep  3 21:56:29.942: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  3 21:56:29.942: INFO: kube-proxy-bxm4p from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
    Sep  3 21:56:29.942: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  3 21:56:29.942: INFO: sonobuoy from sonobuoy started at 2022-09-03 20:37:57 +0000 UTC (1 container statuses recorded)
    Sep  3 21:56:29.942: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  3 21:56:29.942: INFO: sonobuoy-e2e-job-31a458d0b0c040d4 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
    Sep  3 21:56:29.942: INFO: 	Container e2e ready: true, restart count 0
    Sep  3 21:56:29.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  3 21:56:29.942: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-jdmz7 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
    Sep  3 21:56:29.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  3 21:56:29.942: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  3 21:56:29.942: INFO: 
    Logging pods the apiserver thinks is on node kind-worker2 before test
    Sep  3 21:56:29.947: INFO: kindnet-zlt4d from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
    Sep  3 21:56:29.947: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  3 21:56:29.947: INFO: kube-proxy-wz2c5 from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
    Sep  3 21:56:29.947: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  3 21:56:29.947: INFO: busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613 from security-context-test-5324 started at 2022-09-03 21:56:25 +0000 UTC (1 container statuses recorded)
    Sep  3 21:56:29.947: INFO: 	Container busybox-readonly-false-f7b544f4-81c1-457f-93ba-26096ca58613 ready: false, restart count 0
    Sep  3 21:56:29.947: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-nwk5b from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
    Sep  3 21:56:29.947: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  3 21:56:29.947: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/03/22 21:56:29.948
    Sep  3 21:56:29.954: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6380" to be "running"
    Sep  3 21:56:29.962: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.733218ms
    Sep  3 21:56:31.965: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010873763s
    Sep  3 21:56:31.965: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/03/22 21:56:31.967
    STEP: Trying to apply a random label on the found node. 09/03/22 21:56:31.975
    STEP: verifying the node has the label kubernetes.io/e2e-6b0ca5d4-0f14-4864-abf1-e8a54448655e 42 09/03/22 21:56:31.991
    STEP: Trying to relaunch the pod, now with labels. 09/03/22 21:56:32.001
    Sep  3 21:56:32.007: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6380" to be "not pending"
    Sep  3 21:56:32.021: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 14.229133ms
    Sep  3 21:56:34.024: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.017167678s
    Sep  3 21:56:34.024: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-6b0ca5d4-0f14-4864-abf1-e8a54448655e off the node kind-worker2 09/03/22 21:56:34.026
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-6b0ca5d4-0f14-4864-abf1-e8a54448655e 09/03/22 21:56:34.037
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:56:34.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6380" for this suite. 09/03/22 21:56:34.048
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:56:34.059
Sep  3 21:56:34.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename statefulset 09/03/22 21:56:34.06
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:56:34.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:56:34.077
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1571 09/03/22 21:56:34.08
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-1571 09/03/22 21:56:34.089
Sep  3 21:56:34.100: INFO: Found 0 stateful pods, waiting for 1
Sep  3 21:56:44.103: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 09/03/22 21:56:44.107
STEP: updating a scale subresource 09/03/22 21:56:44.109
STEP: verifying the statefulset Spec.Replicas was modified 09/03/22 21:56:44.115
STEP: Patch a scale subresource 09/03/22 21:56:44.122
STEP: verifying the statefulset Spec.Replicas was modified 09/03/22 21:56:44.142
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Sep  3 21:56:44.151: INFO: Deleting all statefulset in ns statefulset-1571
Sep  3 21:56:44.154: INFO: Scaling statefulset ss to 0
Sep  3 21:56:54.165: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 21:56:54.167: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Sep  3 21:56:54.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1571" for this suite. 09/03/22 21:56:54.184
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":282,"skipped":5296,"failed":0}
------------------------------
• [SLOW TEST] [20.133 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:56:34.059
    Sep  3 21:56:34.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename statefulset 09/03/22 21:56:34.06
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:56:34.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:56:34.077
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1571 09/03/22 21:56:34.08
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-1571 09/03/22 21:56:34.089
    Sep  3 21:56:34.100: INFO: Found 0 stateful pods, waiting for 1
    Sep  3 21:56:44.103: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 09/03/22 21:56:44.107
    STEP: updating a scale subresource 09/03/22 21:56:44.109
    STEP: verifying the statefulset Spec.Replicas was modified 09/03/22 21:56:44.115
    STEP: Patch a scale subresource 09/03/22 21:56:44.122
    STEP: verifying the statefulset Spec.Replicas was modified 09/03/22 21:56:44.142
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Sep  3 21:56:44.151: INFO: Deleting all statefulset in ns statefulset-1571
    Sep  3 21:56:44.154: INFO: Scaling statefulset ss to 0
    Sep  3 21:56:54.165: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  3 21:56:54.167: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Sep  3 21:56:54.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1571" for this suite. 09/03/22 21:56:54.184
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:56:54.199
Sep  3 21:56:54.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 21:56:54.2
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:56:54.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:56:54.213
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-6939 09/03/22 21:56:54.215
Sep  3 21:56:54.220: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6939" to be "running and ready"
Sep  3 21:56:54.222: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 1.889504ms
Sep  3 21:56:54.222: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Sep  3 21:56:56.224: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.004737782s
Sep  3 21:56:56.224: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Sep  3 21:56:56.224: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Sep  3 21:56:56.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep  3 21:56:56.398: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Sep  3 21:56:56.398: INFO: stdout: "iptables"
Sep  3 21:56:56.398: INFO: proxyMode: iptables
Sep  3 21:56:56.404: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  3 21:56:56.405: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-6939 09/03/22 21:56:56.405
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6939 09/03/22 21:56:56.419
I0903 21:56:56.435701      24 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6939, replica count: 3
I0903 21:56:59.486483      24 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 21:56:59.493: INFO: Creating new exec pod
Sep  3 21:56:59.496: INFO: Waiting up to 5m0s for pod "execpod-affinity7b769" in namespace "services-6939" to be "running"
Sep  3 21:56:59.501: INFO: Pod "execpod-affinity7b769": Phase="Pending", Reason="", readiness=false. Elapsed: 5.115512ms
Sep  3 21:57:01.505: INFO: Pod "execpod-affinity7b769": Phase="Running", Reason="", readiness=true. Elapsed: 2.008929588s
Sep  3 21:57:01.505: INFO: Pod "execpod-affinity7b769" satisfied condition "running"
Sep  3 21:57:02.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Sep  3 21:57:02.653: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Sep  3 21:57:02.653: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 21:57:02.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.226.156 80'
Sep  3 21:57:02.812: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.226.156 80\nConnection to 10.96.226.156 80 port [tcp/http] succeeded!\n"
Sep  3 21:57:02.812: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 21:57:02.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.2 31505'
Sep  3 21:57:02.944: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.2 31505\nConnection to 172.18.0.2 31505 port [tcp/*] succeeded!\n"
Sep  3 21:57:02.944: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 21:57:02.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.3 31505'
Sep  3 21:57:03.100: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.3 31505\nConnection to 172.18.0.3 31505 port [tcp/*] succeeded!\n"
Sep  3 21:57:03.100: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 21:57:03.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.18.0.2:31505/ ; done'
Sep  3 21:57:03.341: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n"
Sep  3 21:57:03.341: INFO: stdout: "\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s"
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
Sep  3 21:57:03.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.18.0.2:31505/'
Sep  3 21:57:03.515: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n"
Sep  3 21:57:03.515: INFO: stdout: "affinity-nodeport-timeout-8gf5s"
Sep  3 21:57:23.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.18.0.2:31505/'
Sep  3 21:57:23.677: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n"
Sep  3 21:57:23.677: INFO: stdout: "affinity-nodeport-timeout-sq9mc"
Sep  3 21:57:23.677: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6939, will wait for the garbage collector to delete the pods 09/03/22 21:57:23.684
Sep  3 21:57:23.747: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.30572ms
Sep  3 21:57:23.847: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.353084ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 21:57:25.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6939" for this suite. 09/03/22 21:57:25.671
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":283,"skipped":5310,"failed":0}
------------------------------
• [SLOW TEST] [31.475 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:56:54.199
    Sep  3 21:56:54.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 21:56:54.2
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:56:54.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:56:54.213
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-6939 09/03/22 21:56:54.215
    Sep  3 21:56:54.220: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6939" to be "running and ready"
    Sep  3 21:56:54.222: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 1.889504ms
    Sep  3 21:56:54.222: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 21:56:56.224: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.004737782s
    Sep  3 21:56:56.224: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Sep  3 21:56:56.224: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Sep  3 21:56:56.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Sep  3 21:56:56.398: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Sep  3 21:56:56.398: INFO: stdout: "iptables"
    Sep  3 21:56:56.398: INFO: proxyMode: iptables
    Sep  3 21:56:56.404: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Sep  3 21:56:56.405: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-6939 09/03/22 21:56:56.405
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-6939 09/03/22 21:56:56.419
    I0903 21:56:56.435701      24 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6939, replica count: 3
    I0903 21:56:59.486483      24 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  3 21:56:59.493: INFO: Creating new exec pod
    Sep  3 21:56:59.496: INFO: Waiting up to 5m0s for pod "execpod-affinity7b769" in namespace "services-6939" to be "running"
    Sep  3 21:56:59.501: INFO: Pod "execpod-affinity7b769": Phase="Pending", Reason="", readiness=false. Elapsed: 5.115512ms
    Sep  3 21:57:01.505: INFO: Pod "execpod-affinity7b769": Phase="Running", Reason="", readiness=true. Elapsed: 2.008929588s
    Sep  3 21:57:01.505: INFO: Pod "execpod-affinity7b769" satisfied condition "running"
    Sep  3 21:57:02.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Sep  3 21:57:02.653: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Sep  3 21:57:02.653: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 21:57:02.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.226.156 80'
    Sep  3 21:57:02.812: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.226.156 80\nConnection to 10.96.226.156 80 port [tcp/http] succeeded!\n"
    Sep  3 21:57:02.812: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 21:57:02.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.2 31505'
    Sep  3 21:57:02.944: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.2 31505\nConnection to 172.18.0.2 31505 port [tcp/*] succeeded!\n"
    Sep  3 21:57:02.944: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 21:57:02.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.3 31505'
    Sep  3 21:57:03.100: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.3 31505\nConnection to 172.18.0.3 31505 port [tcp/*] succeeded!\n"
    Sep  3 21:57:03.100: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 21:57:03.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.18.0.2:31505/ ; done'
    Sep  3 21:57:03.341: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n"
    Sep  3 21:57:03.341: INFO: stdout: "\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s\naffinity-nodeport-timeout-8gf5s"
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Received response from host: affinity-nodeport-timeout-8gf5s
    Sep  3 21:57:03.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.18.0.2:31505/'
    Sep  3 21:57:03.515: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n"
    Sep  3 21:57:03.515: INFO: stdout: "affinity-nodeport-timeout-8gf5s"
    Sep  3 21:57:23.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-6939 exec execpod-affinity7b769 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.18.0.2:31505/'
    Sep  3 21:57:23.677: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.18.0.2:31505/\n"
    Sep  3 21:57:23.677: INFO: stdout: "affinity-nodeport-timeout-sq9mc"
    Sep  3 21:57:23.677: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6939, will wait for the garbage collector to delete the pods 09/03/22 21:57:23.684
    Sep  3 21:57:23.747: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.30572ms
    Sep  3 21:57:23.847: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.353084ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 21:57:25.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6939" for this suite. 09/03/22 21:57:25.671
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:57:25.679
Sep  3 21:57:25.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename var-expansion 09/03/22 21:57:25.68
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:57:25.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:57:25.693
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 09/03/22 21:57:25.695
Sep  3 21:57:25.700: INFO: Waiting up to 5m0s for pod "var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799" in namespace "var-expansion-7916" to be "Succeeded or Failed"
Sep  3 21:57:25.703: INFO: Pod "var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799": Phase="Pending", Reason="", readiness=false. Elapsed: 2.86161ms
Sep  3 21:57:27.707: INFO: Pod "var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006523458s
Sep  3 21:57:29.706: INFO: Pod "var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005783091s
Sep  3 21:57:31.706: INFO: Pod "var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006166127s
STEP: Saw pod success 09/03/22 21:57:31.706
Sep  3 21:57:31.707: INFO: Pod "var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799" satisfied condition "Succeeded or Failed"
Sep  3 21:57:31.708: INFO: Trying to get logs from node kind-worker2 pod var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799 container dapi-container: <nil>
STEP: delete the pod 09/03/22 21:57:31.724
Sep  3 21:57:31.731: INFO: Waiting for pod var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799 to disappear
Sep  3 21:57:31.733: INFO: Pod var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Sep  3 21:57:31.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7916" for this suite. 09/03/22 21:57:31.735
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":284,"skipped":5332,"failed":0}
------------------------------
• [SLOW TEST] [6.058 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:57:25.679
    Sep  3 21:57:25.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename var-expansion 09/03/22 21:57:25.68
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:57:25.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:57:25.693
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 09/03/22 21:57:25.695
    Sep  3 21:57:25.700: INFO: Waiting up to 5m0s for pod "var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799" in namespace "var-expansion-7916" to be "Succeeded or Failed"
    Sep  3 21:57:25.703: INFO: Pod "var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799": Phase="Pending", Reason="", readiness=false. Elapsed: 2.86161ms
    Sep  3 21:57:27.707: INFO: Pod "var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006523458s
    Sep  3 21:57:29.706: INFO: Pod "var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005783091s
    Sep  3 21:57:31.706: INFO: Pod "var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006166127s
    STEP: Saw pod success 09/03/22 21:57:31.706
    Sep  3 21:57:31.707: INFO: Pod "var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799" satisfied condition "Succeeded or Failed"
    Sep  3 21:57:31.708: INFO: Trying to get logs from node kind-worker2 pod var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799 container dapi-container: <nil>
    STEP: delete the pod 09/03/22 21:57:31.724
    Sep  3 21:57:31.731: INFO: Waiting for pod var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799 to disappear
    Sep  3 21:57:31.733: INFO: Pod var-expansion-a3748ef1-a29e-4c72-b2b4-ec8048c83799 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Sep  3 21:57:31.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7916" for this suite. 09/03/22 21:57:31.735
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:57:31.746
Sep  3 21:57:31.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename gc 09/03/22 21:57:31.748
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:57:31.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:57:31.757
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 09/03/22 21:57:31.763
STEP: create the rc2 09/03/22 21:57:31.766
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/03/22 21:57:36.779
STEP: delete the rc simpletest-rc-to-be-deleted 09/03/22 21:57:37.841
STEP: wait for the rc to be deleted 09/03/22 21:57:37.851
Sep  3 21:57:42.960: INFO: 69 pods remaining
Sep  3 21:57:42.960: INFO: 69 pods has nil DeletionTimestamp
Sep  3 21:57:42.960: INFO: 
STEP: Gathering metrics 09/03/22 21:57:47.871
Sep  3 21:57:47.911: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
Sep  3 21:57:47.916: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 5.261221ms
Sep  3 21:57:47.917: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
Sep  3 21:57:47.917: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
Sep  3 21:57:48.216: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  3 21:57:48.216: INFO: Deleting pod "simpletest-rc-to-be-deleted-26n5f" in namespace "gc-6774"
Sep  3 21:57:48.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qglj" in namespace "gc-6774"
Sep  3 21:57:48.368: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xdfp" in namespace "gc-6774"
Sep  3 21:57:48.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c269" in namespace "gc-6774"
Sep  3 21:57:48.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ht8k" in namespace "gc-6774"
Sep  3 21:57:48.426: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lqqw" in namespace "gc-6774"
Sep  3 21:57:48.445: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qtk8" in namespace "gc-6774"
Sep  3 21:57:48.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-4r6wx" in namespace "gc-6774"
Sep  3 21:57:48.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-52qmc" in namespace "gc-6774"
Sep  3 21:57:48.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-5txvx" in namespace "gc-6774"
Sep  3 21:57:48.517: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vvr2" in namespace "gc-6774"
Sep  3 21:57:48.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vxlq" in namespace "gc-6774"
Sep  3 21:57:48.553: INFO: Deleting pod "simpletest-rc-to-be-deleted-628ts" in namespace "gc-6774"
Sep  3 21:57:48.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-64cb5" in namespace "gc-6774"
Sep  3 21:57:48.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-65h6f" in namespace "gc-6774"
Sep  3 21:57:48.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tnz6" in namespace "gc-6774"
Sep  3 21:57:48.627: INFO: Deleting pod "simpletest-rc-to-be-deleted-6x7xc" in namespace "gc-6774"
Sep  3 21:57:48.639: INFO: Deleting pod "simpletest-rc-to-be-deleted-726v9" in namespace "gc-6774"
Sep  3 21:57:48.651: INFO: Deleting pod "simpletest-rc-to-be-deleted-75tvd" in namespace "gc-6774"
Sep  3 21:57:48.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-789vz" in namespace "gc-6774"
Sep  3 21:57:48.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p6rc" in namespace "gc-6774"
Sep  3 21:57:48.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qh59" in namespace "gc-6774"
Sep  3 21:57:48.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-8h9fl" in namespace "gc-6774"
Sep  3 21:57:48.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lst9" in namespace "gc-6774"
Sep  3 21:57:48.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-bj8s8" in namespace "gc-6774"
Sep  3 21:57:48.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-bws7j" in namespace "gc-6774"
Sep  3 21:57:48.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbdtf" in namespace "gc-6774"
Sep  3 21:57:48.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-crtrf" in namespace "gc-6774"
Sep  3 21:57:48.808: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctvlx" in namespace "gc-6774"
Sep  3 21:57:48.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxfvh" in namespace "gc-6774"
Sep  3 21:57:48.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5l4b" in namespace "gc-6774"
Sep  3 21:57:48.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5s4q" in namespace "gc-6774"
Sep  3 21:57:48.865: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6sfn" in namespace "gc-6774"
Sep  3 21:57:48.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-dj5ct" in namespace "gc-6774"
Sep  3 21:57:48.888: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpjtd" in namespace "gc-6774"
Sep  3 21:57:48.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8bgz" in namespace "gc-6774"
Sep  3 21:57:48.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-flj28" in namespace "gc-6774"
Sep  3 21:57:48.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsmj4" in namespace "gc-6774"
Sep  3 21:57:48.963: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxkgt" in namespace "gc-6774"
Sep  3 21:57:48.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5q2g" in namespace "gc-6774"
Sep  3 21:57:48.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5zts" in namespace "gc-6774"
Sep  3 21:57:49.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb44z" in namespace "gc-6774"
Sep  3 21:57:49.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdn6f" in namespace "gc-6774"
Sep  3 21:57:49.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjlqx" in namespace "gc-6774"
Sep  3 21:57:49.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkgb6" in namespace "gc-6774"
Sep  3 21:57:49.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtxbl" in namespace "gc-6774"
Sep  3 21:57:49.178: INFO: Deleting pod "simpletest-rc-to-be-deleted-hffj9" in namespace "gc-6774"
Sep  3 21:57:49.190: INFO: Deleting pod "simpletest-rc-to-be-deleted-hfj6r" in namespace "gc-6774"
Sep  3 21:57:49.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhvtc" in namespace "gc-6774"
Sep  3 21:57:49.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-hq2bj" in namespace "gc-6774"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Sep  3 21:57:49.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6774" for this suite. 09/03/22 21:57:49.243
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":285,"skipped":5384,"failed":0}
------------------------------
• [SLOW TEST] [17.501 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:57:31.746
    Sep  3 21:57:31.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename gc 09/03/22 21:57:31.748
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:57:31.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:57:31.757
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 09/03/22 21:57:31.763
    STEP: create the rc2 09/03/22 21:57:31.766
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/03/22 21:57:36.779
    STEP: delete the rc simpletest-rc-to-be-deleted 09/03/22 21:57:37.841
    STEP: wait for the rc to be deleted 09/03/22 21:57:37.851
    Sep  3 21:57:42.960: INFO: 69 pods remaining
    Sep  3 21:57:42.960: INFO: 69 pods has nil DeletionTimestamp
    Sep  3 21:57:42.960: INFO: 
    STEP: Gathering metrics 09/03/22 21:57:47.871
    Sep  3 21:57:47.911: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
    Sep  3 21:57:47.916: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 5.261221ms
    Sep  3 21:57:47.917: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
    Sep  3 21:57:47.917: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
    Sep  3 21:57:48.216: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  3 21:57:48.216: INFO: Deleting pod "simpletest-rc-to-be-deleted-26n5f" in namespace "gc-6774"
    Sep  3 21:57:48.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qglj" in namespace "gc-6774"
    Sep  3 21:57:48.368: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xdfp" in namespace "gc-6774"
    Sep  3 21:57:48.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c269" in namespace "gc-6774"
    Sep  3 21:57:48.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ht8k" in namespace "gc-6774"
    Sep  3 21:57:48.426: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lqqw" in namespace "gc-6774"
    Sep  3 21:57:48.445: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qtk8" in namespace "gc-6774"
    Sep  3 21:57:48.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-4r6wx" in namespace "gc-6774"
    Sep  3 21:57:48.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-52qmc" in namespace "gc-6774"
    Sep  3 21:57:48.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-5txvx" in namespace "gc-6774"
    Sep  3 21:57:48.517: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vvr2" in namespace "gc-6774"
    Sep  3 21:57:48.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vxlq" in namespace "gc-6774"
    Sep  3 21:57:48.553: INFO: Deleting pod "simpletest-rc-to-be-deleted-628ts" in namespace "gc-6774"
    Sep  3 21:57:48.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-64cb5" in namespace "gc-6774"
    Sep  3 21:57:48.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-65h6f" in namespace "gc-6774"
    Sep  3 21:57:48.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tnz6" in namespace "gc-6774"
    Sep  3 21:57:48.627: INFO: Deleting pod "simpletest-rc-to-be-deleted-6x7xc" in namespace "gc-6774"
    Sep  3 21:57:48.639: INFO: Deleting pod "simpletest-rc-to-be-deleted-726v9" in namespace "gc-6774"
    Sep  3 21:57:48.651: INFO: Deleting pod "simpletest-rc-to-be-deleted-75tvd" in namespace "gc-6774"
    Sep  3 21:57:48.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-789vz" in namespace "gc-6774"
    Sep  3 21:57:48.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p6rc" in namespace "gc-6774"
    Sep  3 21:57:48.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qh59" in namespace "gc-6774"
    Sep  3 21:57:48.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-8h9fl" in namespace "gc-6774"
    Sep  3 21:57:48.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lst9" in namespace "gc-6774"
    Sep  3 21:57:48.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-bj8s8" in namespace "gc-6774"
    Sep  3 21:57:48.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-bws7j" in namespace "gc-6774"
    Sep  3 21:57:48.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbdtf" in namespace "gc-6774"
    Sep  3 21:57:48.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-crtrf" in namespace "gc-6774"
    Sep  3 21:57:48.808: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctvlx" in namespace "gc-6774"
    Sep  3 21:57:48.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxfvh" in namespace "gc-6774"
    Sep  3 21:57:48.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5l4b" in namespace "gc-6774"
    Sep  3 21:57:48.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5s4q" in namespace "gc-6774"
    Sep  3 21:57:48.865: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6sfn" in namespace "gc-6774"
    Sep  3 21:57:48.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-dj5ct" in namespace "gc-6774"
    Sep  3 21:57:48.888: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpjtd" in namespace "gc-6774"
    Sep  3 21:57:48.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8bgz" in namespace "gc-6774"
    Sep  3 21:57:48.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-flj28" in namespace "gc-6774"
    Sep  3 21:57:48.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsmj4" in namespace "gc-6774"
    Sep  3 21:57:48.963: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxkgt" in namespace "gc-6774"
    Sep  3 21:57:48.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5q2g" in namespace "gc-6774"
    Sep  3 21:57:48.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5zts" in namespace "gc-6774"
    Sep  3 21:57:49.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb44z" in namespace "gc-6774"
    Sep  3 21:57:49.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdn6f" in namespace "gc-6774"
    Sep  3 21:57:49.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjlqx" in namespace "gc-6774"
    Sep  3 21:57:49.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkgb6" in namespace "gc-6774"
    Sep  3 21:57:49.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtxbl" in namespace "gc-6774"
    Sep  3 21:57:49.178: INFO: Deleting pod "simpletest-rc-to-be-deleted-hffj9" in namespace "gc-6774"
    Sep  3 21:57:49.190: INFO: Deleting pod "simpletest-rc-to-be-deleted-hfj6r" in namespace "gc-6774"
    Sep  3 21:57:49.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhvtc" in namespace "gc-6774"
    Sep  3 21:57:49.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-hq2bj" in namespace "gc-6774"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Sep  3 21:57:49.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6774" for this suite. 09/03/22 21:57:49.243
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:57:49.251
Sep  3 21:57:49.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename sched-preemption 09/03/22 21:57:49.256
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:57:49.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:57:49.301
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Sep  3 21:57:49.333: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  3 21:58:49.383: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 09/03/22 21:58:49.391
Sep  3 21:58:49.458: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  3 21:58:49.527: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  3 21:58:49.588: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  3 21:58:49.654: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/03/22 21:58:49.654
Sep  3 21:58:49.654: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5263" to be "running"
Sep  3 21:58:49.721: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 66.788948ms
Sep  3 21:58:51.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069385763s
Sep  3 21:58:53.733: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078607422s
Sep  3 21:58:55.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069751026s
Sep  3 21:58:57.725: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.070698466s
Sep  3 21:58:59.725: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0703429s
Sep  3 21:59:01.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.070011396s
Sep  3 21:59:03.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.07015123s
Sep  3 21:59:05.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.069763963s
Sep  3 21:59:05.724: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  3 21:59:05.724: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5263" to be "running"
Sep  3 21:59:05.726: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.840005ms
Sep  3 21:59:05.726: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  3 21:59:05.726: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5263" to be "running"
Sep  3 21:59:05.728: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.613004ms
Sep  3 21:59:05.728: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  3 21:59:05.728: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5263" to be "running"
Sep  3 21:59:05.729: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.832405ms
Sep  3 21:59:05.729: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 09/03/22 21:59:05.729
Sep  3 21:59:05.736: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Sep  3 21:59:05.739: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033108ms
Sep  3 21:59:07.741: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005304305s
Sep  3 21:59:09.743: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006594299s
Sep  3 21:59:09.743: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Sep  3 21:59:09.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5263" for this suite. 09/03/22 21:59:09.761
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":286,"skipped":5387,"failed":0}
------------------------------
• [SLOW TEST] [80.537 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:57:49.251
    Sep  3 21:57:49.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename sched-preemption 09/03/22 21:57:49.256
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:57:49.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:57:49.301
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Sep  3 21:57:49.333: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  3 21:58:49.383: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 09/03/22 21:58:49.391
    Sep  3 21:58:49.458: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  3 21:58:49.527: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  3 21:58:49.588: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  3 21:58:49.654: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/03/22 21:58:49.654
    Sep  3 21:58:49.654: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5263" to be "running"
    Sep  3 21:58:49.721: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 66.788948ms
    Sep  3 21:58:51.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069385763s
    Sep  3 21:58:53.733: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078607422s
    Sep  3 21:58:55.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069751026s
    Sep  3 21:58:57.725: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.070698466s
    Sep  3 21:58:59.725: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0703429s
    Sep  3 21:59:01.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.070011396s
    Sep  3 21:59:03.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.07015123s
    Sep  3 21:59:05.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.069763963s
    Sep  3 21:59:05.724: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  3 21:59:05.724: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5263" to be "running"
    Sep  3 21:59:05.726: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.840005ms
    Sep  3 21:59:05.726: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  3 21:59:05.726: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5263" to be "running"
    Sep  3 21:59:05.728: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.613004ms
    Sep  3 21:59:05.728: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  3 21:59:05.728: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5263" to be "running"
    Sep  3 21:59:05.729: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.832405ms
    Sep  3 21:59:05.729: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 09/03/22 21:59:05.729
    Sep  3 21:59:05.736: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Sep  3 21:59:05.739: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033108ms
    Sep  3 21:59:07.741: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005304305s
    Sep  3 21:59:09.743: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006594299s
    Sep  3 21:59:09.743: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 21:59:09.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5263" for this suite. 09/03/22 21:59:09.761
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:59:09.79
Sep  3 21:59:09.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename prestop 09/03/22 21:59:09.791
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:59:09.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:59:09.805
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2298 09/03/22 21:59:09.807
STEP: Waiting for pods to come up. 09/03/22 21:59:09.811
Sep  3 21:59:09.812: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2298" to be "running"
Sep  3 21:59:09.815: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.89991ms
Sep  3 21:59:11.818: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.00580583s
Sep  3 21:59:11.818: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2298 09/03/22 21:59:11.82
Sep  3 21:59:11.823: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2298" to be "running"
Sep  3 21:59:11.828: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.020817ms
Sep  3 21:59:13.831: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.008004238s
Sep  3 21:59:13.831: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 09/03/22 21:59:13.831
Sep  3 21:59:18.839: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 09/03/22 21:59:18.839
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Sep  3 21:59:18.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2298" for this suite. 09/03/22 21:59:18.854
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":287,"skipped":5430,"failed":0}
------------------------------
• [SLOW TEST] [9.075 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:59:09.79
    Sep  3 21:59:09.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename prestop 09/03/22 21:59:09.791
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:59:09.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:59:09.805
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2298 09/03/22 21:59:09.807
    STEP: Waiting for pods to come up. 09/03/22 21:59:09.811
    Sep  3 21:59:09.812: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2298" to be "running"
    Sep  3 21:59:09.815: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.89991ms
    Sep  3 21:59:11.818: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.00580583s
    Sep  3 21:59:11.818: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2298 09/03/22 21:59:11.82
    Sep  3 21:59:11.823: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2298" to be "running"
    Sep  3 21:59:11.828: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.020817ms
    Sep  3 21:59:13.831: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.008004238s
    Sep  3 21:59:13.831: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 09/03/22 21:59:13.831
    Sep  3 21:59:18.839: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 09/03/22 21:59:18.839
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Sep  3 21:59:18.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-2298" for this suite. 09/03/22 21:59:18.854
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:59:18.865
Sep  3 21:59:18.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 21:59:18.867
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:59:18.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:59:18.883
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 21:59:18.893
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:59:19.334
STEP: Deploying the webhook pod 09/03/22 21:59:19.336
STEP: Wait for the deployment to be ready 09/03/22 21:59:19.343
Sep  3 21:59:19.355: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 21:59:21.363
STEP: Verifying the service has paired with the endpoint 09/03/22 21:59:21.369
Sep  3 21:59:22.370: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 09/03/22 21:59:22.373
STEP: Creating a custom resource definition that should be denied by the webhook 09/03/22 21:59:22.398
Sep  3 21:59:22.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 21:59:22.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9112" for this suite. 09/03/22 21:59:22.417
STEP: Destroying namespace "webhook-9112-markers" for this suite. 09/03/22 21:59:22.421
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":288,"skipped":5433,"failed":0}
------------------------------
• [3.626 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:59:18.865
    Sep  3 21:59:18.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 21:59:18.867
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:59:18.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:59:18.883
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 21:59:18.893
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:59:19.334
    STEP: Deploying the webhook pod 09/03/22 21:59:19.336
    STEP: Wait for the deployment to be ready 09/03/22 21:59:19.343
    Sep  3 21:59:19.355: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 21:59:21.363
    STEP: Verifying the service has paired with the endpoint 09/03/22 21:59:21.369
    Sep  3 21:59:22.370: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 09/03/22 21:59:22.373
    STEP: Creating a custom resource definition that should be denied by the webhook 09/03/22 21:59:22.398
    Sep  3 21:59:22.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 21:59:22.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9112" for this suite. 09/03/22 21:59:22.417
    STEP: Destroying namespace "webhook-9112-markers" for this suite. 09/03/22 21:59:22.421
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:59:22.492
Sep  3 21:59:22.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename dns 09/03/22 21:59:22.493
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:59:22.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:59:22.524
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/03/22 21:59:22.53
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/03/22 21:59:22.53
STEP: creating a pod to probe DNS 09/03/22 21:59:22.53
STEP: submitting the pod to kubernetes 09/03/22 21:59:22.53
Sep  3 21:59:22.540: INFO: Waiting up to 15m0s for pod "dns-test-b48a7b90-1771-4be0-ab1f-c78ff7e8cdb4" in namespace "dns-5977" to be "running"
Sep  3 21:59:22.551: INFO: Pod "dns-test-b48a7b90-1771-4be0-ab1f-c78ff7e8cdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.80834ms
Sep  3 21:59:24.555: INFO: Pod "dns-test-b48a7b90-1771-4be0-ab1f-c78ff7e8cdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015141299s
Sep  3 21:59:26.554: INFO: Pod "dns-test-b48a7b90-1771-4be0-ab1f-c78ff7e8cdb4": Phase="Running", Reason="", readiness=true. Elapsed: 4.014504646s
Sep  3 21:59:26.554: INFO: Pod "dns-test-b48a7b90-1771-4be0-ab1f-c78ff7e8cdb4" satisfied condition "running"
STEP: retrieving the pod 09/03/22 21:59:26.554
STEP: looking for the results for each expected name from probers 09/03/22 21:59:26.556
Sep  3 21:59:26.568: INFO: DNS probes using dns-5977/dns-test-b48a7b90-1771-4be0-ab1f-c78ff7e8cdb4 succeeded

STEP: deleting the pod 09/03/22 21:59:26.568
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Sep  3 21:59:26.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5977" for this suite. 09/03/22 21:59:26.58
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":289,"skipped":5445,"failed":0}
------------------------------
• [4.091 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:59:22.492
    Sep  3 21:59:22.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename dns 09/03/22 21:59:22.493
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:59:22.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:59:22.524
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/03/22 21:59:22.53
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/03/22 21:59:22.53
    STEP: creating a pod to probe DNS 09/03/22 21:59:22.53
    STEP: submitting the pod to kubernetes 09/03/22 21:59:22.53
    Sep  3 21:59:22.540: INFO: Waiting up to 15m0s for pod "dns-test-b48a7b90-1771-4be0-ab1f-c78ff7e8cdb4" in namespace "dns-5977" to be "running"
    Sep  3 21:59:22.551: INFO: Pod "dns-test-b48a7b90-1771-4be0-ab1f-c78ff7e8cdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.80834ms
    Sep  3 21:59:24.555: INFO: Pod "dns-test-b48a7b90-1771-4be0-ab1f-c78ff7e8cdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015141299s
    Sep  3 21:59:26.554: INFO: Pod "dns-test-b48a7b90-1771-4be0-ab1f-c78ff7e8cdb4": Phase="Running", Reason="", readiness=true. Elapsed: 4.014504646s
    Sep  3 21:59:26.554: INFO: Pod "dns-test-b48a7b90-1771-4be0-ab1f-c78ff7e8cdb4" satisfied condition "running"
    STEP: retrieving the pod 09/03/22 21:59:26.554
    STEP: looking for the results for each expected name from probers 09/03/22 21:59:26.556
    Sep  3 21:59:26.568: INFO: DNS probes using dns-5977/dns-test-b48a7b90-1771-4be0-ab1f-c78ff7e8cdb4 succeeded

    STEP: deleting the pod 09/03/22 21:59:26.568
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Sep  3 21:59:26.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5977" for this suite. 09/03/22 21:59:26.58
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:59:26.585
Sep  3 21:59:26.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-runtime 09/03/22 21:59:26.586
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:59:26.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:59:26.597
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/03/22 21:59:26.604
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/03/22 21:59:43.663
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/03/22 21:59:43.665
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/03/22 21:59:43.669
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/03/22 21:59:43.669
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/03/22 21:59:43.688
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/03/22 21:59:46.707
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/03/22 21:59:48.716
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/03/22 21:59:48.719
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/03/22 21:59:48.72
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/03/22 21:59:48.731
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/03/22 21:59:49.737
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/03/22 21:59:52.749
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/03/22 21:59:52.752
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/03/22 21:59:52.752
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Sep  3 21:59:52.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7295" for this suite. 09/03/22 21:59:52.766
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":290,"skipped":5469,"failed":0}
------------------------------
• [SLOW TEST] [26.184 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:59:26.585
    Sep  3 21:59:26.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-runtime 09/03/22 21:59:26.586
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:59:26.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:59:26.597
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/03/22 21:59:26.604
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/03/22 21:59:43.663
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/03/22 21:59:43.665
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/03/22 21:59:43.669
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/03/22 21:59:43.669
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/03/22 21:59:43.688
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/03/22 21:59:46.707
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/03/22 21:59:48.716
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/03/22 21:59:48.719
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/03/22 21:59:48.72
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/03/22 21:59:48.731
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/03/22 21:59:49.737
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/03/22 21:59:52.749
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/03/22 21:59:52.752
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/03/22 21:59:52.752
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Sep  3 21:59:52.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7295" for this suite. 09/03/22 21:59:52.766
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:59:52.771
Sep  3 21:59:52.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 21:59:52.773
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:59:52.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:59:52.784
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-8a29d55b-f617-4664-9fea-132fcf586ba6 09/03/22 21:59:52.786
STEP: Creating a pod to test consume secrets 09/03/22 21:59:52.789
Sep  3 21:59:52.794: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22" in namespace "projected-5494" to be "Succeeded or Failed"
Sep  3 21:59:52.796: INFO: Pod "pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.256006ms
Sep  3 21:59:54.800: INFO: Pod "pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005432597s
Sep  3 21:59:56.800: INFO: Pod "pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005826881s
STEP: Saw pod success 09/03/22 21:59:56.8
Sep  3 21:59:56.800: INFO: Pod "pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22" satisfied condition "Succeeded or Failed"
Sep  3 21:59:56.802: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/03/22 21:59:56.813
Sep  3 21:59:56.818: INFO: Waiting for pod pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22 to disappear
Sep  3 21:59:56.820: INFO: Pod pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Sep  3 21:59:56.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5494" for this suite. 09/03/22 21:59:56.823
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":291,"skipped":5478,"failed":0}
------------------------------
• [4.056 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:59:52.771
    Sep  3 21:59:52.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 21:59:52.773
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:59:52.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:59:52.784
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-8a29d55b-f617-4664-9fea-132fcf586ba6 09/03/22 21:59:52.786
    STEP: Creating a pod to test consume secrets 09/03/22 21:59:52.789
    Sep  3 21:59:52.794: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22" in namespace "projected-5494" to be "Succeeded or Failed"
    Sep  3 21:59:52.796: INFO: Pod "pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.256006ms
    Sep  3 21:59:54.800: INFO: Pod "pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005432597s
    Sep  3 21:59:56.800: INFO: Pod "pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005826881s
    STEP: Saw pod success 09/03/22 21:59:56.8
    Sep  3 21:59:56.800: INFO: Pod "pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22" satisfied condition "Succeeded or Failed"
    Sep  3 21:59:56.802: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 21:59:56.813
    Sep  3 21:59:56.818: INFO: Waiting for pod pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22 to disappear
    Sep  3 21:59:56.820: INFO: Pod pod-projected-secrets-017a555a-2aef-4123-be99-6486c7461f22 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Sep  3 21:59:56.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5494" for this suite. 09/03/22 21:59:56.823
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 21:59:56.828
Sep  3 21:59:56.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 21:59:56.829
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:59:56.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:59:56.846
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 21:59:56.858
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:59:57.413
STEP: Deploying the webhook pod 09/03/22 21:59:57.418
STEP: Wait for the deployment to be ready 09/03/22 21:59:57.424
Sep  3 21:59:57.436: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 21:59:59.442
STEP: Verifying the service has paired with the endpoint 09/03/22 21:59:59.452
Sep  3 22:00:00.453: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/03/22 22:00:00.456
STEP: create a namespace for the webhook 09/03/22 22:00:00.467
STEP: create a configmap should be unconditionally rejected by the webhook 09/03/22 22:00:00.47
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 22:00:00.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9273" for this suite. 09/03/22 22:00:00.503
STEP: Destroying namespace "webhook-9273-markers" for this suite. 09/03/22 22:00:00.514
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":292,"skipped":5490,"failed":0}
------------------------------
• [3.830 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 21:59:56.828
    Sep  3 21:59:56.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 21:59:56.829
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 21:59:56.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 21:59:56.846
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 21:59:56.858
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 21:59:57.413
    STEP: Deploying the webhook pod 09/03/22 21:59:57.418
    STEP: Wait for the deployment to be ready 09/03/22 21:59:57.424
    Sep  3 21:59:57.436: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 21:59:59.442
    STEP: Verifying the service has paired with the endpoint 09/03/22 21:59:59.452
    Sep  3 22:00:00.453: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/03/22 22:00:00.456
    STEP: create a namespace for the webhook 09/03/22 22:00:00.467
    STEP: create a configmap should be unconditionally rejected by the webhook 09/03/22 22:00:00.47
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 22:00:00.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9273" for this suite. 09/03/22 22:00:00.503
    STEP: Destroying namespace "webhook-9273-markers" for this suite. 09/03/22 22:00:00.514
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:00:00.662
Sep  3 22:00:00.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubelet-test 09/03/22 22:00:00.663
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:00.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:00.693
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Sep  3 22:00:04.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6561" for this suite. 09/03/22 22:00:04.709
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":293,"skipped":5490,"failed":0}
------------------------------
• [4.050 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:00:00.662
    Sep  3 22:00:00.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubelet-test 09/03/22 22:00:00.663
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:00.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:00.693
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Sep  3 22:00:04.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6561" for this suite. 09/03/22 22:00:04.709
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:00:04.715
Sep  3 22:00:04.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 22:00:04.716
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:04.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:04.727
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-2c39c9aa-d5d2-44a1-99ca-6f37c0f51a3e 09/03/22 22:00:04.73
STEP: Creating a pod to test consume configMaps 09/03/22 22:00:04.733
Sep  3 22:00:04.737: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08" in namespace "projected-9650" to be "Succeeded or Failed"
Sep  3 22:00:04.741: INFO: Pod "pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48981ms
Sep  3 22:00:06.744: INFO: Pod "pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006655751s
Sep  3 22:00:08.744: INFO: Pod "pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007018959s
STEP: Saw pod success 09/03/22 22:00:08.744
Sep  3 22:00:08.745: INFO: Pod "pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08" satisfied condition "Succeeded or Failed"
Sep  3 22:00:08.746: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08 container projected-configmap-volume-test: <nil>
STEP: delete the pod 09/03/22 22:00:08.75
Sep  3 22:00:08.756: INFO: Waiting for pod pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08 to disappear
Sep  3 22:00:08.758: INFO: Pod pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Sep  3 22:00:08.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9650" for this suite. 09/03/22 22:00:08.761
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":294,"skipped":5495,"failed":0}
------------------------------
• [4.049 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:00:04.715
    Sep  3 22:00:04.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 22:00:04.716
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:04.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:04.727
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-2c39c9aa-d5d2-44a1-99ca-6f37c0f51a3e 09/03/22 22:00:04.73
    STEP: Creating a pod to test consume configMaps 09/03/22 22:00:04.733
    Sep  3 22:00:04.737: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08" in namespace "projected-9650" to be "Succeeded or Failed"
    Sep  3 22:00:04.741: INFO: Pod "pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48981ms
    Sep  3 22:00:06.744: INFO: Pod "pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006655751s
    Sep  3 22:00:08.744: INFO: Pod "pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007018959s
    STEP: Saw pod success 09/03/22 22:00:08.744
    Sep  3 22:00:08.745: INFO: Pod "pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08" satisfied condition "Succeeded or Failed"
    Sep  3 22:00:08.746: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 09/03/22 22:00:08.75
    Sep  3 22:00:08.756: INFO: Waiting for pod pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08 to disappear
    Sep  3 22:00:08.758: INFO: Pod pod-projected-configmaps-1748e185-ec71-409f-bf62-e443dd9b1f08 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Sep  3 22:00:08.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9650" for this suite. 09/03/22 22:00:08.761
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:00:08.766
Sep  3 22:00:08.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 22:00:08.767
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:08.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:08.788
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 09/03/22 22:00:08.79
Sep  3 22:00:08.795: INFO: Waiting up to 5m0s for pod "downward-api-d4ca637c-765e-46c4-840b-a319207779ac" in namespace "downward-api-7387" to be "Succeeded or Failed"
Sep  3 22:00:08.800: INFO: Pod "downward-api-d4ca637c-765e-46c4-840b-a319207779ac": Phase="Pending", Reason="", readiness=false. Elapsed: 5.528215ms
Sep  3 22:00:10.803: INFO: Pod "downward-api-d4ca637c-765e-46c4-840b-a319207779ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00857103s
Sep  3 22:00:12.805: INFO: Pod "downward-api-d4ca637c-765e-46c4-840b-a319207779ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00968304s
STEP: Saw pod success 09/03/22 22:00:12.805
Sep  3 22:00:12.805: INFO: Pod "downward-api-d4ca637c-765e-46c4-840b-a319207779ac" satisfied condition "Succeeded or Failed"
Sep  3 22:00:12.807: INFO: Trying to get logs from node kind-worker2 pod downward-api-d4ca637c-765e-46c4-840b-a319207779ac container dapi-container: <nil>
STEP: delete the pod 09/03/22 22:00:12.811
Sep  3 22:00:12.817: INFO: Waiting for pod downward-api-d4ca637c-765e-46c4-840b-a319207779ac to disappear
Sep  3 22:00:12.819: INFO: Pod downward-api-d4ca637c-765e-46c4-840b-a319207779ac no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Sep  3 22:00:12.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7387" for this suite. 09/03/22 22:00:12.821
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":295,"skipped":5530,"failed":0}
------------------------------
• [4.059 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:00:08.766
    Sep  3 22:00:08.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 22:00:08.767
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:08.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:08.788
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 09/03/22 22:00:08.79
    Sep  3 22:00:08.795: INFO: Waiting up to 5m0s for pod "downward-api-d4ca637c-765e-46c4-840b-a319207779ac" in namespace "downward-api-7387" to be "Succeeded or Failed"
    Sep  3 22:00:08.800: INFO: Pod "downward-api-d4ca637c-765e-46c4-840b-a319207779ac": Phase="Pending", Reason="", readiness=false. Elapsed: 5.528215ms
    Sep  3 22:00:10.803: INFO: Pod "downward-api-d4ca637c-765e-46c4-840b-a319207779ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00857103s
    Sep  3 22:00:12.805: INFO: Pod "downward-api-d4ca637c-765e-46c4-840b-a319207779ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00968304s
    STEP: Saw pod success 09/03/22 22:00:12.805
    Sep  3 22:00:12.805: INFO: Pod "downward-api-d4ca637c-765e-46c4-840b-a319207779ac" satisfied condition "Succeeded or Failed"
    Sep  3 22:00:12.807: INFO: Trying to get logs from node kind-worker2 pod downward-api-d4ca637c-765e-46c4-840b-a319207779ac container dapi-container: <nil>
    STEP: delete the pod 09/03/22 22:00:12.811
    Sep  3 22:00:12.817: INFO: Waiting for pod downward-api-d4ca637c-765e-46c4-840b-a319207779ac to disappear
    Sep  3 22:00:12.819: INFO: Pod downward-api-d4ca637c-765e-46c4-840b-a319207779ac no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Sep  3 22:00:12.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7387" for this suite. 09/03/22 22:00:12.821
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:00:12.829
Sep  3 22:00:12.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename proxy 09/03/22 22:00:12.83
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:12.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:12.841
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Sep  3 22:00:12.843: INFO: Creating pod...
Sep  3 22:00:12.848: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-269" to be "running"
Sep  3 22:00:12.853: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.180414ms
Sep  3 22:00:14.857: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008835104s
Sep  3 22:00:14.857: INFO: Pod "agnhost" satisfied condition "running"
Sep  3 22:00:14.857: INFO: Creating service...
Sep  3 22:00:14.869: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/DELETE
Sep  3 22:00:14.878: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  3 22:00:14.879: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/GET
Sep  3 22:00:14.884: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  3 22:00:14.885: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/HEAD
Sep  3 22:00:14.888: INFO: http.Client request:HEAD | StatusCode:200
Sep  3 22:00:14.888: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/OPTIONS
Sep  3 22:00:14.898: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  3 22:00:14.898: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/PATCH
Sep  3 22:00:14.907: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  3 22:00:14.908: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/POST
Sep  3 22:00:14.914: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  3 22:00:14.914: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/PUT
Sep  3 22:00:14.919: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  3 22:00:14.919: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/DELETE
Sep  3 22:00:14.926: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  3 22:00:14.926: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/GET
Sep  3 22:00:14.931: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  3 22:00:14.931: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/HEAD
Sep  3 22:00:14.936: INFO: http.Client request:HEAD | StatusCode:200
Sep  3 22:00:14.936: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/OPTIONS
Sep  3 22:00:14.939: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  3 22:00:14.939: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/PATCH
Sep  3 22:00:14.949: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  3 22:00:14.949: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/POST
Sep  3 22:00:14.956: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  3 22:00:14.956: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/PUT
Sep  3 22:00:14.960: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Sep  3 22:00:14.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-269" for this suite. 09/03/22 22:00:14.969
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":296,"skipped":5567,"failed":0}
------------------------------
• [2.150 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:00:12.829
    Sep  3 22:00:12.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename proxy 09/03/22 22:00:12.83
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:12.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:12.841
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Sep  3 22:00:12.843: INFO: Creating pod...
    Sep  3 22:00:12.848: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-269" to be "running"
    Sep  3 22:00:12.853: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.180414ms
    Sep  3 22:00:14.857: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008835104s
    Sep  3 22:00:14.857: INFO: Pod "agnhost" satisfied condition "running"
    Sep  3 22:00:14.857: INFO: Creating service...
    Sep  3 22:00:14.869: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/DELETE
    Sep  3 22:00:14.878: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  3 22:00:14.879: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/GET
    Sep  3 22:00:14.884: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  3 22:00:14.885: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/HEAD
    Sep  3 22:00:14.888: INFO: http.Client request:HEAD | StatusCode:200
    Sep  3 22:00:14.888: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/OPTIONS
    Sep  3 22:00:14.898: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  3 22:00:14.898: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/PATCH
    Sep  3 22:00:14.907: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  3 22:00:14.908: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/POST
    Sep  3 22:00:14.914: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  3 22:00:14.914: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy/some/path/with/PUT
    Sep  3 22:00:14.919: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  3 22:00:14.919: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/DELETE
    Sep  3 22:00:14.926: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  3 22:00:14.926: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/GET
    Sep  3 22:00:14.931: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  3 22:00:14.931: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/HEAD
    Sep  3 22:00:14.936: INFO: http.Client request:HEAD | StatusCode:200
    Sep  3 22:00:14.936: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/OPTIONS
    Sep  3 22:00:14.939: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  3 22:00:14.939: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/PATCH
    Sep  3 22:00:14.949: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  3 22:00:14.949: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/POST
    Sep  3 22:00:14.956: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  3 22:00:14.956: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-269/services/test-service/proxy/some/path/with/PUT
    Sep  3 22:00:14.960: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Sep  3 22:00:14.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-269" for this suite. 09/03/22 22:00:14.969
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:00:14.989
Sep  3 22:00:14.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 22:00:14.99
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:15.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:15.012
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 22:00:15.033
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 22:00:15.686
STEP: Deploying the webhook pod 09/03/22 22:00:15.689
STEP: Wait for the deployment to be ready 09/03/22 22:00:15.695
Sep  3 22:00:15.702: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 22:00:17.708
STEP: Verifying the service has paired with the endpoint 09/03/22 22:00:17.718
Sep  3 22:00:18.719: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Sep  3 22:00:18.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1701-crds.webhook.example.com via the AdmissionRegistration API 09/03/22 22:00:19.244
STEP: Creating a custom resource that should be mutated by the webhook 09/03/22 22:00:19.255
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 22:00:21.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2541" for this suite. 09/03/22 22:00:21.839
STEP: Destroying namespace "webhook-2541-markers" for this suite. 09/03/22 22:00:21.844
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":297,"skipped":5569,"failed":0}
------------------------------
• [SLOW TEST] [6.933 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:00:14.989
    Sep  3 22:00:14.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 22:00:14.99
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:15.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:15.012
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 22:00:15.033
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 22:00:15.686
    STEP: Deploying the webhook pod 09/03/22 22:00:15.689
    STEP: Wait for the deployment to be ready 09/03/22 22:00:15.695
    Sep  3 22:00:15.702: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 22:00:17.708
    STEP: Verifying the service has paired with the endpoint 09/03/22 22:00:17.718
    Sep  3 22:00:18.719: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Sep  3 22:00:18.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1701-crds.webhook.example.com via the AdmissionRegistration API 09/03/22 22:00:19.244
    STEP: Creating a custom resource that should be mutated by the webhook 09/03/22 22:00:19.255
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 22:00:21.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2541" for this suite. 09/03/22 22:00:21.839
    STEP: Destroying namespace "webhook-2541-markers" for this suite. 09/03/22 22:00:21.844
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:00:21.961
Sep  3 22:00:21.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename kubelet-test 09/03/22 22:00:21.962
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:21.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:21.987
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Sep  3 22:00:22.003: INFO: Waiting up to 5m0s for pod "busybox-scheduling-78dd5758-952c-4cbf-a3be-4d9292589ab5" in namespace "kubelet-test-6818" to be "running and ready"
Sep  3 22:00:22.012: INFO: Pod "busybox-scheduling-78dd5758-952c-4cbf-a3be-4d9292589ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.386422ms
Sep  3 22:00:22.012: INFO: The phase of Pod busybox-scheduling-78dd5758-952c-4cbf-a3be-4d9292589ab5 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:00:24.016: INFO: Pod "busybox-scheduling-78dd5758-952c-4cbf-a3be-4d9292589ab5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012909165s
Sep  3 22:00:24.016: INFO: The phase of Pod busybox-scheduling-78dd5758-952c-4cbf-a3be-4d9292589ab5 is Running (Ready = true)
Sep  3 22:00:24.016: INFO: Pod "busybox-scheduling-78dd5758-952c-4cbf-a3be-4d9292589ab5" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Sep  3 22:00:24.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6818" for this suite. 09/03/22 22:00:24.024
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":298,"skipped":5584,"failed":0}
------------------------------
• [2.068 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:00:21.961
    Sep  3 22:00:21.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename kubelet-test 09/03/22 22:00:21.962
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:21.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:21.987
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Sep  3 22:00:22.003: INFO: Waiting up to 5m0s for pod "busybox-scheduling-78dd5758-952c-4cbf-a3be-4d9292589ab5" in namespace "kubelet-test-6818" to be "running and ready"
    Sep  3 22:00:22.012: INFO: Pod "busybox-scheduling-78dd5758-952c-4cbf-a3be-4d9292589ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.386422ms
    Sep  3 22:00:22.012: INFO: The phase of Pod busybox-scheduling-78dd5758-952c-4cbf-a3be-4d9292589ab5 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 22:00:24.016: INFO: Pod "busybox-scheduling-78dd5758-952c-4cbf-a3be-4d9292589ab5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012909165s
    Sep  3 22:00:24.016: INFO: The phase of Pod busybox-scheduling-78dd5758-952c-4cbf-a3be-4d9292589ab5 is Running (Ready = true)
    Sep  3 22:00:24.016: INFO: Pod "busybox-scheduling-78dd5758-952c-4cbf-a3be-4d9292589ab5" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Sep  3 22:00:24.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6818" for this suite. 09/03/22 22:00:24.024
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:00:24.033
Sep  3 22:00:24.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename replicaset 09/03/22 22:00:24.034
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:24.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:24.047
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/03/22 22:00:24.049
Sep  3 22:00:24.058: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/03/22 22:00:24.058
Sep  3 22:00:24.058: INFO: Waiting up to 5m0s for pod "test-rs-qjjsj" in namespace "replicaset-1847" to be "running"
Sep  3 22:00:24.064: INFO: Pod "test-rs-qjjsj": Phase="Pending", Reason="", readiness=false. Elapsed: 5.632715ms
Sep  3 22:00:26.067: INFO: Pod "test-rs-qjjsj": Phase="Running", Reason="", readiness=true. Elapsed: 2.009003656s
Sep  3 22:00:26.067: INFO: Pod "test-rs-qjjsj" satisfied condition "running"
STEP: getting scale subresource 09/03/22 22:00:26.067
STEP: updating a scale subresource 09/03/22 22:00:26.071
STEP: verifying the replicaset Spec.Replicas was modified 09/03/22 22:00:26.075
STEP: Patch a scale subresource 09/03/22 22:00:26.083
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Sep  3 22:00:26.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1847" for this suite. 09/03/22 22:00:26.121
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":299,"skipped":5631,"failed":0}
------------------------------
• [2.091 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:00:24.033
    Sep  3 22:00:24.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename replicaset 09/03/22 22:00:24.034
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:24.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:24.047
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/03/22 22:00:24.049
    Sep  3 22:00:24.058: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/03/22 22:00:24.058
    Sep  3 22:00:24.058: INFO: Waiting up to 5m0s for pod "test-rs-qjjsj" in namespace "replicaset-1847" to be "running"
    Sep  3 22:00:24.064: INFO: Pod "test-rs-qjjsj": Phase="Pending", Reason="", readiness=false. Elapsed: 5.632715ms
    Sep  3 22:00:26.067: INFO: Pod "test-rs-qjjsj": Phase="Running", Reason="", readiness=true. Elapsed: 2.009003656s
    Sep  3 22:00:26.067: INFO: Pod "test-rs-qjjsj" satisfied condition "running"
    STEP: getting scale subresource 09/03/22 22:00:26.067
    STEP: updating a scale subresource 09/03/22 22:00:26.071
    STEP: verifying the replicaset Spec.Replicas was modified 09/03/22 22:00:26.075
    STEP: Patch a scale subresource 09/03/22 22:00:26.083
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Sep  3 22:00:26.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1847" for this suite. 09/03/22 22:00:26.121
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:00:26.13
Sep  3 22:00:26.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename taint-multiple-pods 09/03/22 22:00:26.13
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:26.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:26.168
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Sep  3 22:00:26.182: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  3 22:01:26.196: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Sep  3 22:01:26.198: INFO: Starting informer...
STEP: Starting pods... 09/03/22 22:01:26.198
Sep  3 22:01:26.413: INFO: Pod1 is running on kind-worker2. Tainting Node
Sep  3 22:01:26.619: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-6911" to be "running"
Sep  3 22:01:26.622: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.262006ms
Sep  3 22:01:28.625: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005710626s
Sep  3 22:01:28.625: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Sep  3 22:01:28.625: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-6911" to be "running"
Sep  3 22:01:28.627: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.638504ms
Sep  3 22:01:28.627: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Sep  3 22:01:28.627: INFO: Pod2 is running on kind-worker2. Tainting Node
STEP: Trying to apply a taint on the Node 09/03/22 22:01:28.627
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/03/22 22:01:28.639
STEP: Waiting for Pod1 and Pod2 to be deleted 09/03/22 22:01:28.641
Sep  3 22:01:34.110: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep  3 22:01:54.156: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/03/22 22:01:54.167
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Sep  3 22:01:54.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6911" for this suite. 09/03/22 22:01:54.173
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":300,"skipped":5664,"failed":0}
------------------------------
• [SLOW TEST] [88.048 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:00:26.13
    Sep  3 22:00:26.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename taint-multiple-pods 09/03/22 22:00:26.13
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:00:26.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:00:26.168
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Sep  3 22:00:26.182: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  3 22:01:26.196: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Sep  3 22:01:26.198: INFO: Starting informer...
    STEP: Starting pods... 09/03/22 22:01:26.198
    Sep  3 22:01:26.413: INFO: Pod1 is running on kind-worker2. Tainting Node
    Sep  3 22:01:26.619: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-6911" to be "running"
    Sep  3 22:01:26.622: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.262006ms
    Sep  3 22:01:28.625: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005710626s
    Sep  3 22:01:28.625: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Sep  3 22:01:28.625: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-6911" to be "running"
    Sep  3 22:01:28.627: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.638504ms
    Sep  3 22:01:28.627: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Sep  3 22:01:28.627: INFO: Pod2 is running on kind-worker2. Tainting Node
    STEP: Trying to apply a taint on the Node 09/03/22 22:01:28.627
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/03/22 22:01:28.639
    STEP: Waiting for Pod1 and Pod2 to be deleted 09/03/22 22:01:28.641
    Sep  3 22:01:34.110: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Sep  3 22:01:54.156: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/03/22 22:01:54.167
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 22:01:54.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-6911" for this suite. 09/03/22 22:01:54.173
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:01:54.178
Sep  3 22:01:54.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-runtime 09/03/22 22:01:54.178
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:01:54.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:01:54.193
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 09/03/22 22:01:54.195
STEP: wait for the container to reach Failed 09/03/22 22:01:54.202
STEP: get the container status 09/03/22 22:01:57.217
STEP: the container should be terminated 09/03/22 22:01:57.219
STEP: the termination message should be set 09/03/22 22:01:57.219
Sep  3 22:01:57.220: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/03/22 22:01:57.22
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Sep  3 22:01:57.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2461" for this suite. 09/03/22 22:01:57.23
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":301,"skipped":5667,"failed":0}
------------------------------
• [3.056 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:01:54.178
    Sep  3 22:01:54.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-runtime 09/03/22 22:01:54.178
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:01:54.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:01:54.193
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 09/03/22 22:01:54.195
    STEP: wait for the container to reach Failed 09/03/22 22:01:54.202
    STEP: get the container status 09/03/22 22:01:57.217
    STEP: the container should be terminated 09/03/22 22:01:57.219
    STEP: the termination message should be set 09/03/22 22:01:57.219
    Sep  3 22:01:57.220: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/03/22 22:01:57.22
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Sep  3 22:01:57.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2461" for this suite. 09/03/22 22:01:57.23
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:01:57.24
Sep  3 22:01:57.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 22:01:57.241
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:01:57.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:01:57.253
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-1366 09/03/22 22:01:57.255
Sep  3 22:01:57.261: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1366" to be "running and ready"
Sep  3 22:01:57.264: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.979408ms
Sep  3 22:01:57.264: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:01:59.267: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.00654048s
Sep  3 22:01:59.267: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Sep  3 22:01:59.267: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Sep  3 22:01:59.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep  3 22:01:59.452: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Sep  3 22:01:59.452: INFO: stdout: "iptables"
Sep  3 22:01:59.452: INFO: proxyMode: iptables
Sep  3 22:01:59.461: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  3 22:01:59.463: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1366 09/03/22 22:01:59.463
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1366 09/03/22 22:01:59.483
I0903 22:01:59.495744      24 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1366, replica count: 3
I0903 22:02:02.553318      24 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 22:02:02.557: INFO: Creating new exec pod
Sep  3 22:02:02.560: INFO: Waiting up to 5m0s for pod "execpod-affinityll6vd" in namespace "services-1366" to be "running"
Sep  3 22:02:02.563: INFO: Pod "execpod-affinityll6vd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.574909ms
Sep  3 22:02:04.566: INFO: Pod "execpod-affinityll6vd": Phase="Running", Reason="", readiness=true. Elapsed: 2.00664198s
Sep  3 22:02:04.566: INFO: Pod "execpod-affinityll6vd" satisfied condition "running"
Sep  3 22:02:05.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec execpod-affinityll6vd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Sep  3 22:02:05.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Sep  3 22:02:05.715: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 22:02:05.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec execpod-affinityll6vd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.237.63 80'
Sep  3 22:02:05.855: INFO: stderr: "+ echo hostName+ nc -v -t -w 2 10.96.237.63 80\nConnection to 10.96.237.63 80 port [tcp/http] succeeded!\n\n"
Sep  3 22:02:05.855: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 22:02:05.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec execpod-affinityll6vd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.237.63:80/ ; done'
Sep  3 22:02:06.114: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n"
Sep  3 22:02:06.114: INFO: stdout: "\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf"
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
Sep  3 22:02:06.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec execpod-affinityll6vd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.237.63:80/'
Sep  3 22:02:06.253: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n"
Sep  3 22:02:06.253: INFO: stdout: "affinity-clusterip-timeout-n9zpf"
Sep  3 22:02:26.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec execpod-affinityll6vd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.237.63:80/'
Sep  3 22:02:26.416: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n"
Sep  3 22:02:26.416: INFO: stdout: "affinity-clusterip-timeout-n9zpf"
Sep  3 22:02:46.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec execpod-affinityll6vd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.237.63:80/'
Sep  3 22:02:46.576: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n"
Sep  3 22:02:46.576: INFO: stdout: "affinity-clusterip-timeout-lwf8k"
Sep  3 22:02:46.576: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1366, will wait for the garbage collector to delete the pods 09/03/22 22:02:46.584
Sep  3 22:02:46.646: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.157217ms
Sep  3 22:02:46.747: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.573772ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 22:02:48.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1366" for this suite. 09/03/22 22:02:48.667
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":302,"skipped":5690,"failed":0}
------------------------------
• [SLOW TEST] [51.430 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:01:57.24
    Sep  3 22:01:57.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 22:01:57.241
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:01:57.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:01:57.253
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-1366 09/03/22 22:01:57.255
    Sep  3 22:01:57.261: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1366" to be "running and ready"
    Sep  3 22:01:57.264: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.979408ms
    Sep  3 22:01:57.264: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 22:01:59.267: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.00654048s
    Sep  3 22:01:59.267: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Sep  3 22:01:59.267: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Sep  3 22:01:59.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Sep  3 22:01:59.452: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Sep  3 22:01:59.452: INFO: stdout: "iptables"
    Sep  3 22:01:59.452: INFO: proxyMode: iptables
    Sep  3 22:01:59.461: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Sep  3 22:01:59.463: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-1366 09/03/22 22:01:59.463
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-1366 09/03/22 22:01:59.483
    I0903 22:01:59.495744      24 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1366, replica count: 3
    I0903 22:02:02.553318      24 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  3 22:02:02.557: INFO: Creating new exec pod
    Sep  3 22:02:02.560: INFO: Waiting up to 5m0s for pod "execpod-affinityll6vd" in namespace "services-1366" to be "running"
    Sep  3 22:02:02.563: INFO: Pod "execpod-affinityll6vd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.574909ms
    Sep  3 22:02:04.566: INFO: Pod "execpod-affinityll6vd": Phase="Running", Reason="", readiness=true. Elapsed: 2.00664198s
    Sep  3 22:02:04.566: INFO: Pod "execpod-affinityll6vd" satisfied condition "running"
    Sep  3 22:02:05.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec execpod-affinityll6vd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Sep  3 22:02:05.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Sep  3 22:02:05.715: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 22:02:05.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec execpod-affinityll6vd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.237.63 80'
    Sep  3 22:02:05.855: INFO: stderr: "+ echo hostName+ nc -v -t -w 2 10.96.237.63 80\nConnection to 10.96.237.63 80 port [tcp/http] succeeded!\n\n"
    Sep  3 22:02:05.855: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 22:02:05.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec execpod-affinityll6vd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.237.63:80/ ; done'
    Sep  3 22:02:06.114: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n"
    Sep  3 22:02:06.114: INFO: stdout: "\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf\naffinity-clusterip-timeout-n9zpf"
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.114: INFO: Received response from host: affinity-clusterip-timeout-n9zpf
    Sep  3 22:02:06.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec execpod-affinityll6vd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.237.63:80/'
    Sep  3 22:02:06.253: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n"
    Sep  3 22:02:06.253: INFO: stdout: "affinity-clusterip-timeout-n9zpf"
    Sep  3 22:02:26.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec execpod-affinityll6vd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.237.63:80/'
    Sep  3 22:02:26.416: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n"
    Sep  3 22:02:26.416: INFO: stdout: "affinity-clusterip-timeout-n9zpf"
    Sep  3 22:02:46.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-1366 exec execpod-affinityll6vd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.237.63:80/'
    Sep  3 22:02:46.576: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.237.63:80/\n"
    Sep  3 22:02:46.576: INFO: stdout: "affinity-clusterip-timeout-lwf8k"
    Sep  3 22:02:46.576: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1366, will wait for the garbage collector to delete the pods 09/03/22 22:02:46.584
    Sep  3 22:02:46.646: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.157217ms
    Sep  3 22:02:46.747: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.573772ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 22:02:48.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1366" for this suite. 09/03/22 22:02:48.667
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:02:48.676
Sep  3 22:02:48.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename disruption 09/03/22 22:02:48.678
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:02:48.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:02:48.689
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 09/03/22 22:02:48.692
STEP: Waiting for the pdb to be processed 09/03/22 22:02:48.694
STEP: updating the pdb 09/03/22 22:02:50.7
STEP: Waiting for the pdb to be processed 09/03/22 22:02:50.707
STEP: patching the pdb 09/03/22 22:02:52.714
STEP: Waiting for the pdb to be processed 09/03/22 22:02:52.72
STEP: Waiting for the pdb to be deleted 09/03/22 22:02:54.727
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Sep  3 22:02:54.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6137" for this suite. 09/03/22 22:02:54.732
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":303,"skipped":5696,"failed":0}
------------------------------
• [SLOW TEST] [6.060 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:02:48.676
    Sep  3 22:02:48.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename disruption 09/03/22 22:02:48.678
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:02:48.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:02:48.689
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 09/03/22 22:02:48.692
    STEP: Waiting for the pdb to be processed 09/03/22 22:02:48.694
    STEP: updating the pdb 09/03/22 22:02:50.7
    STEP: Waiting for the pdb to be processed 09/03/22 22:02:50.707
    STEP: patching the pdb 09/03/22 22:02:52.714
    STEP: Waiting for the pdb to be processed 09/03/22 22:02:52.72
    STEP: Waiting for the pdb to be deleted 09/03/22 22:02:54.727
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Sep  3 22:02:54.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6137" for this suite. 09/03/22 22:02:54.732
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:02:54.74
Sep  3 22:02:54.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 22:02:54.741
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:02:54.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:02:54.754
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-6f799657-8c70-4085-9b51-9e41abe57f89 09/03/22 22:02:54.757
STEP: Creating a pod to test consume secrets 09/03/22 22:02:54.759
Sep  3 22:02:54.764: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65" in namespace "projected-1875" to be "Succeeded or Failed"
Sep  3 22:02:54.771: INFO: Pod "pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65": Phase="Pending", Reason="", readiness=false. Elapsed: 6.490916ms
Sep  3 22:02:56.775: INFO: Pod "pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009986699s
Sep  3 22:02:58.774: INFO: Pod "pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009903573s
STEP: Saw pod success 09/03/22 22:02:58.774
Sep  3 22:02:58.775: INFO: Pod "pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65" satisfied condition "Succeeded or Failed"
Sep  3 22:02:58.776: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/03/22 22:02:58.793
Sep  3 22:02:58.799: INFO: Waiting for pod pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65 to disappear
Sep  3 22:02:58.801: INFO: Pod pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Sep  3 22:02:58.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1875" for this suite. 09/03/22 22:02:58.804
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":304,"skipped":5710,"failed":0}
------------------------------
• [4.068 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:02:54.74
    Sep  3 22:02:54.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 22:02:54.741
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:02:54.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:02:54.754
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-6f799657-8c70-4085-9b51-9e41abe57f89 09/03/22 22:02:54.757
    STEP: Creating a pod to test consume secrets 09/03/22 22:02:54.759
    Sep  3 22:02:54.764: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65" in namespace "projected-1875" to be "Succeeded or Failed"
    Sep  3 22:02:54.771: INFO: Pod "pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65": Phase="Pending", Reason="", readiness=false. Elapsed: 6.490916ms
    Sep  3 22:02:56.775: INFO: Pod "pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009986699s
    Sep  3 22:02:58.774: INFO: Pod "pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009903573s
    STEP: Saw pod success 09/03/22 22:02:58.774
    Sep  3 22:02:58.775: INFO: Pod "pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65" satisfied condition "Succeeded or Failed"
    Sep  3 22:02:58.776: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 22:02:58.793
    Sep  3 22:02:58.799: INFO: Waiting for pod pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65 to disappear
    Sep  3 22:02:58.801: INFO: Pod pod-projected-secrets-27154aa8-7357-442f-9eb6-ddfb29f33e65 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Sep  3 22:02:58.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1875" for this suite. 09/03/22 22:02:58.804
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:02:58.811
Sep  3 22:02:58.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 22:02:58.812
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:02:58.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:02:58.823
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-8003 09/03/22 22:02:58.826
STEP: creating service affinity-nodeport-transition in namespace services-8003 09/03/22 22:02:58.826
STEP: creating replication controller affinity-nodeport-transition in namespace services-8003 09/03/22 22:02:58.834
I0903 22:02:58.857204      24 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8003, replica count: 3
I0903 22:03:01.911514      24 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 22:03:01.917: INFO: Creating new exec pod
Sep  3 22:03:01.921: INFO: Waiting up to 5m0s for pod "execpod-affinityxg4g6" in namespace "services-8003" to be "running"
Sep  3 22:03:01.924: INFO: Pod "execpod-affinityxg4g6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.677007ms
Sep  3 22:03:03.927: INFO: Pod "execpod-affinityxg4g6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005858685s
Sep  3 22:03:03.927: INFO: Pod "execpod-affinityxg4g6" satisfied condition "running"
Sep  3 22:03:04.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8003 exec execpod-affinityxg4g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Sep  3 22:03:05.066: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep  3 22:03:05.066: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 22:03:05.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8003 exec execpod-affinityxg4g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.143.66 80'
Sep  3 22:03:05.223: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.143.66 80\nConnection to 10.96.143.66 80 port [tcp/http] succeeded!\n"
Sep  3 22:03:05.223: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 22:03:05.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8003 exec execpod-affinityxg4g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.2 32687'
Sep  3 22:03:05.382: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.2 32687\nConnection to 172.18.0.2 32687 port [tcp/*] succeeded!\n"
Sep  3 22:03:05.382: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 22:03:05.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8003 exec execpod-affinityxg4g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.3 32687'
Sep  3 22:03:05.641: INFO: stderr: "+ nc -v -t -w 2 172.18.0.3 32687\nConnection to 172.18.0.3 32687 port [tcp/*] succeeded!\n+ echo hostName\n"
Sep  3 22:03:05.641: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 22:03:05.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8003 exec execpod-affinityxg4g6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.18.0.2:32687/ ; done'
Sep  3 22:03:05.957: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n"
Sep  3 22:03:05.957: INFO: stdout: "\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-s5dh6\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-s5dh6\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-s5dh6\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-s5dh6"
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-s5dh6
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-s5dh6
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-s5dh6
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-s5dh6
Sep  3 22:03:05.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8003 exec execpod-affinityxg4g6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.18.0.2:32687/ ; done'
Sep  3 22:03:06.316: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n"
Sep  3 22:03:06.316: INFO: stdout: "\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq"
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
Sep  3 22:03:06.316: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8003, will wait for the garbage collector to delete the pods 09/03/22 22:03:06.328
Sep  3 22:03:06.392: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.82571ms
Sep  3 22:03:06.493: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.63316ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 22:03:08.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8003" for this suite. 09/03/22 22:03:08.739
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":305,"skipped":5728,"failed":0}
------------------------------
• [SLOW TEST] [9.934 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:02:58.811
    Sep  3 22:02:58.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 22:02:58.812
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:02:58.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:02:58.823
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-8003 09/03/22 22:02:58.826
    STEP: creating service affinity-nodeport-transition in namespace services-8003 09/03/22 22:02:58.826
    STEP: creating replication controller affinity-nodeport-transition in namespace services-8003 09/03/22 22:02:58.834
    I0903 22:02:58.857204      24 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8003, replica count: 3
    I0903 22:03:01.911514      24 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  3 22:03:01.917: INFO: Creating new exec pod
    Sep  3 22:03:01.921: INFO: Waiting up to 5m0s for pod "execpod-affinityxg4g6" in namespace "services-8003" to be "running"
    Sep  3 22:03:01.924: INFO: Pod "execpod-affinityxg4g6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.677007ms
    Sep  3 22:03:03.927: INFO: Pod "execpod-affinityxg4g6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005858685s
    Sep  3 22:03:03.927: INFO: Pod "execpod-affinityxg4g6" satisfied condition "running"
    Sep  3 22:03:04.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8003 exec execpod-affinityxg4g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Sep  3 22:03:05.066: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Sep  3 22:03:05.066: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 22:03:05.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8003 exec execpod-affinityxg4g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.143.66 80'
    Sep  3 22:03:05.223: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.143.66 80\nConnection to 10.96.143.66 80 port [tcp/http] succeeded!\n"
    Sep  3 22:03:05.223: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 22:03:05.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8003 exec execpod-affinityxg4g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.2 32687'
    Sep  3 22:03:05.382: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.2 32687\nConnection to 172.18.0.2 32687 port [tcp/*] succeeded!\n"
    Sep  3 22:03:05.382: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 22:03:05.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8003 exec execpod-affinityxg4g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.3 32687'
    Sep  3 22:03:05.641: INFO: stderr: "+ nc -v -t -w 2 172.18.0.3 32687\nConnection to 172.18.0.3 32687 port [tcp/*] succeeded!\n+ echo hostName\n"
    Sep  3 22:03:05.641: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 22:03:05.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8003 exec execpod-affinityxg4g6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.18.0.2:32687/ ; done'
    Sep  3 22:03:05.957: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n"
    Sep  3 22:03:05.957: INFO: stdout: "\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-s5dh6\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-s5dh6\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-s5dh6\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-q8p78\naffinity-nodeport-transition-s5dh6"
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-s5dh6
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-s5dh6
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-s5dh6
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-q8p78
    Sep  3 22:03:05.957: INFO: Received response from host: affinity-nodeport-transition-s5dh6
    Sep  3 22:03:05.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8003 exec execpod-affinityxg4g6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.18.0.2:32687/ ; done'
    Sep  3 22:03:06.316: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.0.2:32687/\n"
    Sep  3 22:03:06.316: INFO: stdout: "\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq\naffinity-nodeport-transition-5s7wq"
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Received response from host: affinity-nodeport-transition-5s7wq
    Sep  3 22:03:06.316: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8003, will wait for the garbage collector to delete the pods 09/03/22 22:03:06.328
    Sep  3 22:03:06.392: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.82571ms
    Sep  3 22:03:06.493: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.63316ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 22:03:08.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8003" for this suite. 09/03/22 22:03:08.739
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:03:08.762
Sep  3 22:03:08.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename gc 09/03/22 22:03:08.763
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:03:08.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:03:08.795
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 09/03/22 22:03:08.808
STEP: delete the rc 09/03/22 22:03:13.83
STEP: wait for the rc to be deleted 09/03/22 22:03:13.855
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/03/22 22:03:18.864
STEP: Gathering metrics 09/03/22 22:03:48.943
Sep  3 22:03:48.995: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
Sep  3 22:03:49.009: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 14.132637ms
Sep  3 22:03:49.010: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
Sep  3 22:03:49.010: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
Sep  3 22:03:49.384: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  3 22:03:49.384: INFO: Deleting pod "simpletest.rc-2cp4l" in namespace "gc-9034"
Sep  3 22:03:49.401: INFO: Deleting pod "simpletest.rc-49rfh" in namespace "gc-9034"
Sep  3 22:03:49.480: INFO: Deleting pod "simpletest.rc-4bm82" in namespace "gc-9034"
Sep  3 22:03:49.522: INFO: Deleting pod "simpletest.rc-4qlsv" in namespace "gc-9034"
Sep  3 22:03:49.547: INFO: Deleting pod "simpletest.rc-4w22x" in namespace "gc-9034"
Sep  3 22:03:49.553: INFO: Deleting pod "simpletest.rc-6cm4l" in namespace "gc-9034"
Sep  3 22:03:49.562: INFO: Deleting pod "simpletest.rc-6db9p" in namespace "gc-9034"
Sep  3 22:03:49.584: INFO: Deleting pod "simpletest.rc-6hgzf" in namespace "gc-9034"
Sep  3 22:03:49.614: INFO: Deleting pod "simpletest.rc-784vd" in namespace "gc-9034"
Sep  3 22:03:49.635: INFO: Deleting pod "simpletest.rc-7h2kb" in namespace "gc-9034"
Sep  3 22:03:49.642: INFO: Deleting pod "simpletest.rc-7lxxc" in namespace "gc-9034"
Sep  3 22:03:49.686: INFO: Deleting pod "simpletest.rc-7mt2p" in namespace "gc-9034"
Sep  3 22:03:49.731: INFO: Deleting pod "simpletest.rc-7pksg" in namespace "gc-9034"
Sep  3 22:03:49.742: INFO: Deleting pod "simpletest.rc-84b5h" in namespace "gc-9034"
Sep  3 22:03:49.770: INFO: Deleting pod "simpletest.rc-8827m" in namespace "gc-9034"
Sep  3 22:03:49.815: INFO: Deleting pod "simpletest.rc-88vgj" in namespace "gc-9034"
Sep  3 22:03:49.865: INFO: Deleting pod "simpletest.rc-8cck4" in namespace "gc-9034"
Sep  3 22:03:49.878: INFO: Deleting pod "simpletest.rc-8r6wz" in namespace "gc-9034"
Sep  3 22:03:49.925: INFO: Deleting pod "simpletest.rc-9tbd7" in namespace "gc-9034"
Sep  3 22:03:49.962: INFO: Deleting pod "simpletest.rc-b552g" in namespace "gc-9034"
Sep  3 22:03:49.987: INFO: Deleting pod "simpletest.rc-b9dkt" in namespace "gc-9034"
Sep  3 22:03:50.004: INFO: Deleting pod "simpletest.rc-bd9pt" in namespace "gc-9034"
Sep  3 22:03:50.021: INFO: Deleting pod "simpletest.rc-bgz2g" in namespace "gc-9034"
Sep  3 22:03:50.050: INFO: Deleting pod "simpletest.rc-bkmjk" in namespace "gc-9034"
Sep  3 22:03:50.086: INFO: Deleting pod "simpletest.rc-c2x5z" in namespace "gc-9034"
Sep  3 22:03:50.096: INFO: Deleting pod "simpletest.rc-cbb5w" in namespace "gc-9034"
Sep  3 22:03:50.155: INFO: Deleting pod "simpletest.rc-cc2xt" in namespace "gc-9034"
Sep  3 22:03:50.178: INFO: Deleting pod "simpletest.rc-cdzn9" in namespace "gc-9034"
Sep  3 22:03:50.200: INFO: Deleting pod "simpletest.rc-cgzrl" in namespace "gc-9034"
Sep  3 22:03:50.217: INFO: Deleting pod "simpletest.rc-chzkt" in namespace "gc-9034"
Sep  3 22:03:50.244: INFO: Deleting pod "simpletest.rc-cn7xk" in namespace "gc-9034"
Sep  3 22:03:50.278: INFO: Deleting pod "simpletest.rc-cxsw8" in namespace "gc-9034"
Sep  3 22:03:50.312: INFO: Deleting pod "simpletest.rc-cz4v6" in namespace "gc-9034"
Sep  3 22:03:50.410: INFO: Deleting pod "simpletest.rc-dbpq7" in namespace "gc-9034"
Sep  3 22:03:50.435: INFO: Deleting pod "simpletest.rc-dch8r" in namespace "gc-9034"
Sep  3 22:03:50.466: INFO: Deleting pod "simpletest.rc-dczt4" in namespace "gc-9034"
Sep  3 22:03:50.495: INFO: Deleting pod "simpletest.rc-dpzcx" in namespace "gc-9034"
Sep  3 22:03:50.525: INFO: Deleting pod "simpletest.rc-dwkjf" in namespace "gc-9034"
Sep  3 22:03:50.557: INFO: Deleting pod "simpletest.rc-fbphv" in namespace "gc-9034"
Sep  3 22:03:50.665: INFO: Deleting pod "simpletest.rc-fjlpg" in namespace "gc-9034"
Sep  3 22:03:50.702: INFO: Deleting pod "simpletest.rc-fvrq7" in namespace "gc-9034"
Sep  3 22:03:50.728: INFO: Deleting pod "simpletest.rc-g8vzm" in namespace "gc-9034"
Sep  3 22:03:50.755: INFO: Deleting pod "simpletest.rc-gh9jh" in namespace "gc-9034"
Sep  3 22:03:50.788: INFO: Deleting pod "simpletest.rc-gxnwx" in namespace "gc-9034"
Sep  3 22:03:50.795: INFO: Deleting pod "simpletest.rc-hfgjt" in namespace "gc-9034"
Sep  3 22:03:50.821: INFO: Deleting pod "simpletest.rc-hwswk" in namespace "gc-9034"
Sep  3 22:03:50.865: INFO: Deleting pod "simpletest.rc-hz526" in namespace "gc-9034"
Sep  3 22:03:50.896: INFO: Deleting pod "simpletest.rc-j4bkm" in namespace "gc-9034"
Sep  3 22:03:50.916: INFO: Deleting pod "simpletest.rc-j6554" in namespace "gc-9034"
Sep  3 22:03:50.971: INFO: Deleting pod "simpletest.rc-j8c9x" in namespace "gc-9034"
Sep  3 22:03:50.983: INFO: Deleting pod "simpletest.rc-jcl4p" in namespace "gc-9034"
Sep  3 22:03:51.029: INFO: Deleting pod "simpletest.rc-jd2zg" in namespace "gc-9034"
Sep  3 22:03:51.090: INFO: Deleting pod "simpletest.rc-jfrxw" in namespace "gc-9034"
Sep  3 22:03:51.137: INFO: Deleting pod "simpletest.rc-jr6x2" in namespace "gc-9034"
Sep  3 22:03:51.186: INFO: Deleting pod "simpletest.rc-k444w" in namespace "gc-9034"
Sep  3 22:03:51.218: INFO: Deleting pod "simpletest.rc-k66wq" in namespace "gc-9034"
Sep  3 22:03:51.252: INFO: Deleting pod "simpletest.rc-k6cxp" in namespace "gc-9034"
Sep  3 22:03:51.286: INFO: Deleting pod "simpletest.rc-kgx8q" in namespace "gc-9034"
Sep  3 22:03:51.312: INFO: Deleting pod "simpletest.rc-kl97k" in namespace "gc-9034"
Sep  3 22:03:51.350: INFO: Deleting pod "simpletest.rc-kzcw6" in namespace "gc-9034"
Sep  3 22:03:51.397: INFO: Deleting pod "simpletest.rc-kzgzf" in namespace "gc-9034"
Sep  3 22:03:51.442: INFO: Deleting pod "simpletest.rc-l2tct" in namespace "gc-9034"
Sep  3 22:03:51.475: INFO: Deleting pod "simpletest.rc-lkz7q" in namespace "gc-9034"
Sep  3 22:03:51.525: INFO: Deleting pod "simpletest.rc-mb8jp" in namespace "gc-9034"
Sep  3 22:03:51.547: INFO: Deleting pod "simpletest.rc-mc9vh" in namespace "gc-9034"
Sep  3 22:03:51.580: INFO: Deleting pod "simpletest.rc-mlxjb" in namespace "gc-9034"
Sep  3 22:03:51.653: INFO: Deleting pod "simpletest.rc-mvmrz" in namespace "gc-9034"
Sep  3 22:03:51.787: INFO: Deleting pod "simpletest.rc-nkzjp" in namespace "gc-9034"
Sep  3 22:03:51.924: INFO: Deleting pod "simpletest.rc-nn7jm" in namespace "gc-9034"
Sep  3 22:03:52.031: INFO: Deleting pod "simpletest.rc-plbjd" in namespace "gc-9034"
Sep  3 22:03:52.048: INFO: Deleting pod "simpletest.rc-qs79w" in namespace "gc-9034"
Sep  3 22:03:52.123: INFO: Deleting pod "simpletest.rc-qs9nl" in namespace "gc-9034"
Sep  3 22:03:52.183: INFO: Deleting pod "simpletest.rc-r8r24" in namespace "gc-9034"
Sep  3 22:03:52.210: INFO: Deleting pod "simpletest.rc-rfmr4" in namespace "gc-9034"
Sep  3 22:03:52.271: INFO: Deleting pod "simpletest.rc-rm6dw" in namespace "gc-9034"
Sep  3 22:03:52.300: INFO: Deleting pod "simpletest.rc-rmmwl" in namespace "gc-9034"
Sep  3 22:03:52.318: INFO: Deleting pod "simpletest.rc-s4ww6" in namespace "gc-9034"
Sep  3 22:03:52.331: INFO: Deleting pod "simpletest.rc-sfpp2" in namespace "gc-9034"
Sep  3 22:03:52.342: INFO: Deleting pod "simpletest.rc-sqh9l" in namespace "gc-9034"
Sep  3 22:03:52.362: INFO: Deleting pod "simpletest.rc-t2zxd" in namespace "gc-9034"
Sep  3 22:03:52.507: INFO: Deleting pod "simpletest.rc-t54xc" in namespace "gc-9034"
Sep  3 22:03:52.555: INFO: Deleting pod "simpletest.rc-tcrzr" in namespace "gc-9034"
Sep  3 22:03:52.607: INFO: Deleting pod "simpletest.rc-td6h6" in namespace "gc-9034"
Sep  3 22:03:52.638: INFO: Deleting pod "simpletest.rc-tdwkb" in namespace "gc-9034"
Sep  3 22:03:52.662: INFO: Deleting pod "simpletest.rc-tgccb" in namespace "gc-9034"
Sep  3 22:03:52.710: INFO: Deleting pod "simpletest.rc-tnbck" in namespace "gc-9034"
Sep  3 22:03:52.743: INFO: Deleting pod "simpletest.rc-vt8pg" in namespace "gc-9034"
Sep  3 22:03:52.805: INFO: Deleting pod "simpletest.rc-vz98w" in namespace "gc-9034"
Sep  3 22:03:52.879: INFO: Deleting pod "simpletest.rc-wmsvj" in namespace "gc-9034"
Sep  3 22:03:52.931: INFO: Deleting pod "simpletest.rc-wn5jc" in namespace "gc-9034"
Sep  3 22:03:52.985: INFO: Deleting pod "simpletest.rc-wnnsl" in namespace "gc-9034"
Sep  3 22:03:53.046: INFO: Deleting pod "simpletest.rc-wscgl" in namespace "gc-9034"
Sep  3 22:03:53.094: INFO: Deleting pod "simpletest.rc-wsgcd" in namespace "gc-9034"
Sep  3 22:03:53.129: INFO: Deleting pod "simpletest.rc-wwt7q" in namespace "gc-9034"
Sep  3 22:03:53.172: INFO: Deleting pod "simpletest.rc-x6vx8" in namespace "gc-9034"
Sep  3 22:03:53.226: INFO: Deleting pod "simpletest.rc-xk2bq" in namespace "gc-9034"
Sep  3 22:03:53.258: INFO: Deleting pod "simpletest.rc-xw8t5" in namespace "gc-9034"
Sep  3 22:03:53.271: INFO: Deleting pod "simpletest.rc-xxg74" in namespace "gc-9034"
Sep  3 22:03:53.294: INFO: Deleting pod "simpletest.rc-z2wlb" in namespace "gc-9034"
Sep  3 22:03:53.318: INFO: Deleting pod "simpletest.rc-zplb5" in namespace "gc-9034"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Sep  3 22:03:53.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9034" for this suite. 09/03/22 22:03:53.349
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":306,"skipped":5749,"failed":0}
------------------------------
• [SLOW TEST] [44.601 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:03:08.762
    Sep  3 22:03:08.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename gc 09/03/22 22:03:08.763
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:03:08.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:03:08.795
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 09/03/22 22:03:08.808
    STEP: delete the rc 09/03/22 22:03:13.83
    STEP: wait for the rc to be deleted 09/03/22 22:03:13.855
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/03/22 22:03:18.864
    STEP: Gathering metrics 09/03/22 22:03:48.943
    Sep  3 22:03:48.995: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
    Sep  3 22:03:49.009: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 14.132637ms
    Sep  3 22:03:49.010: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
    Sep  3 22:03:49.010: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
    Sep  3 22:03:49.384: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  3 22:03:49.384: INFO: Deleting pod "simpletest.rc-2cp4l" in namespace "gc-9034"
    Sep  3 22:03:49.401: INFO: Deleting pod "simpletest.rc-49rfh" in namespace "gc-9034"
    Sep  3 22:03:49.480: INFO: Deleting pod "simpletest.rc-4bm82" in namespace "gc-9034"
    Sep  3 22:03:49.522: INFO: Deleting pod "simpletest.rc-4qlsv" in namespace "gc-9034"
    Sep  3 22:03:49.547: INFO: Deleting pod "simpletest.rc-4w22x" in namespace "gc-9034"
    Sep  3 22:03:49.553: INFO: Deleting pod "simpletest.rc-6cm4l" in namespace "gc-9034"
    Sep  3 22:03:49.562: INFO: Deleting pod "simpletest.rc-6db9p" in namespace "gc-9034"
    Sep  3 22:03:49.584: INFO: Deleting pod "simpletest.rc-6hgzf" in namespace "gc-9034"
    Sep  3 22:03:49.614: INFO: Deleting pod "simpletest.rc-784vd" in namespace "gc-9034"
    Sep  3 22:03:49.635: INFO: Deleting pod "simpletest.rc-7h2kb" in namespace "gc-9034"
    Sep  3 22:03:49.642: INFO: Deleting pod "simpletest.rc-7lxxc" in namespace "gc-9034"
    Sep  3 22:03:49.686: INFO: Deleting pod "simpletest.rc-7mt2p" in namespace "gc-9034"
    Sep  3 22:03:49.731: INFO: Deleting pod "simpletest.rc-7pksg" in namespace "gc-9034"
    Sep  3 22:03:49.742: INFO: Deleting pod "simpletest.rc-84b5h" in namespace "gc-9034"
    Sep  3 22:03:49.770: INFO: Deleting pod "simpletest.rc-8827m" in namespace "gc-9034"
    Sep  3 22:03:49.815: INFO: Deleting pod "simpletest.rc-88vgj" in namespace "gc-9034"
    Sep  3 22:03:49.865: INFO: Deleting pod "simpletest.rc-8cck4" in namespace "gc-9034"
    Sep  3 22:03:49.878: INFO: Deleting pod "simpletest.rc-8r6wz" in namespace "gc-9034"
    Sep  3 22:03:49.925: INFO: Deleting pod "simpletest.rc-9tbd7" in namespace "gc-9034"
    Sep  3 22:03:49.962: INFO: Deleting pod "simpletest.rc-b552g" in namespace "gc-9034"
    Sep  3 22:03:49.987: INFO: Deleting pod "simpletest.rc-b9dkt" in namespace "gc-9034"
    Sep  3 22:03:50.004: INFO: Deleting pod "simpletest.rc-bd9pt" in namespace "gc-9034"
    Sep  3 22:03:50.021: INFO: Deleting pod "simpletest.rc-bgz2g" in namespace "gc-9034"
    Sep  3 22:03:50.050: INFO: Deleting pod "simpletest.rc-bkmjk" in namespace "gc-9034"
    Sep  3 22:03:50.086: INFO: Deleting pod "simpletest.rc-c2x5z" in namespace "gc-9034"
    Sep  3 22:03:50.096: INFO: Deleting pod "simpletest.rc-cbb5w" in namespace "gc-9034"
    Sep  3 22:03:50.155: INFO: Deleting pod "simpletest.rc-cc2xt" in namespace "gc-9034"
    Sep  3 22:03:50.178: INFO: Deleting pod "simpletest.rc-cdzn9" in namespace "gc-9034"
    Sep  3 22:03:50.200: INFO: Deleting pod "simpletest.rc-cgzrl" in namespace "gc-9034"
    Sep  3 22:03:50.217: INFO: Deleting pod "simpletest.rc-chzkt" in namespace "gc-9034"
    Sep  3 22:03:50.244: INFO: Deleting pod "simpletest.rc-cn7xk" in namespace "gc-9034"
    Sep  3 22:03:50.278: INFO: Deleting pod "simpletest.rc-cxsw8" in namespace "gc-9034"
    Sep  3 22:03:50.312: INFO: Deleting pod "simpletest.rc-cz4v6" in namespace "gc-9034"
    Sep  3 22:03:50.410: INFO: Deleting pod "simpletest.rc-dbpq7" in namespace "gc-9034"
    Sep  3 22:03:50.435: INFO: Deleting pod "simpletest.rc-dch8r" in namespace "gc-9034"
    Sep  3 22:03:50.466: INFO: Deleting pod "simpletest.rc-dczt4" in namespace "gc-9034"
    Sep  3 22:03:50.495: INFO: Deleting pod "simpletest.rc-dpzcx" in namespace "gc-9034"
    Sep  3 22:03:50.525: INFO: Deleting pod "simpletest.rc-dwkjf" in namespace "gc-9034"
    Sep  3 22:03:50.557: INFO: Deleting pod "simpletest.rc-fbphv" in namespace "gc-9034"
    Sep  3 22:03:50.665: INFO: Deleting pod "simpletest.rc-fjlpg" in namespace "gc-9034"
    Sep  3 22:03:50.702: INFO: Deleting pod "simpletest.rc-fvrq7" in namespace "gc-9034"
    Sep  3 22:03:50.728: INFO: Deleting pod "simpletest.rc-g8vzm" in namespace "gc-9034"
    Sep  3 22:03:50.755: INFO: Deleting pod "simpletest.rc-gh9jh" in namespace "gc-9034"
    Sep  3 22:03:50.788: INFO: Deleting pod "simpletest.rc-gxnwx" in namespace "gc-9034"
    Sep  3 22:03:50.795: INFO: Deleting pod "simpletest.rc-hfgjt" in namespace "gc-9034"
    Sep  3 22:03:50.821: INFO: Deleting pod "simpletest.rc-hwswk" in namespace "gc-9034"
    Sep  3 22:03:50.865: INFO: Deleting pod "simpletest.rc-hz526" in namespace "gc-9034"
    Sep  3 22:03:50.896: INFO: Deleting pod "simpletest.rc-j4bkm" in namespace "gc-9034"
    Sep  3 22:03:50.916: INFO: Deleting pod "simpletest.rc-j6554" in namespace "gc-9034"
    Sep  3 22:03:50.971: INFO: Deleting pod "simpletest.rc-j8c9x" in namespace "gc-9034"
    Sep  3 22:03:50.983: INFO: Deleting pod "simpletest.rc-jcl4p" in namespace "gc-9034"
    Sep  3 22:03:51.029: INFO: Deleting pod "simpletest.rc-jd2zg" in namespace "gc-9034"
    Sep  3 22:03:51.090: INFO: Deleting pod "simpletest.rc-jfrxw" in namespace "gc-9034"
    Sep  3 22:03:51.137: INFO: Deleting pod "simpletest.rc-jr6x2" in namespace "gc-9034"
    Sep  3 22:03:51.186: INFO: Deleting pod "simpletest.rc-k444w" in namespace "gc-9034"
    Sep  3 22:03:51.218: INFO: Deleting pod "simpletest.rc-k66wq" in namespace "gc-9034"
    Sep  3 22:03:51.252: INFO: Deleting pod "simpletest.rc-k6cxp" in namespace "gc-9034"
    Sep  3 22:03:51.286: INFO: Deleting pod "simpletest.rc-kgx8q" in namespace "gc-9034"
    Sep  3 22:03:51.312: INFO: Deleting pod "simpletest.rc-kl97k" in namespace "gc-9034"
    Sep  3 22:03:51.350: INFO: Deleting pod "simpletest.rc-kzcw6" in namespace "gc-9034"
    Sep  3 22:03:51.397: INFO: Deleting pod "simpletest.rc-kzgzf" in namespace "gc-9034"
    Sep  3 22:03:51.442: INFO: Deleting pod "simpletest.rc-l2tct" in namespace "gc-9034"
    Sep  3 22:03:51.475: INFO: Deleting pod "simpletest.rc-lkz7q" in namespace "gc-9034"
    Sep  3 22:03:51.525: INFO: Deleting pod "simpletest.rc-mb8jp" in namespace "gc-9034"
    Sep  3 22:03:51.547: INFO: Deleting pod "simpletest.rc-mc9vh" in namespace "gc-9034"
    Sep  3 22:03:51.580: INFO: Deleting pod "simpletest.rc-mlxjb" in namespace "gc-9034"
    Sep  3 22:03:51.653: INFO: Deleting pod "simpletest.rc-mvmrz" in namespace "gc-9034"
    Sep  3 22:03:51.787: INFO: Deleting pod "simpletest.rc-nkzjp" in namespace "gc-9034"
    Sep  3 22:03:51.924: INFO: Deleting pod "simpletest.rc-nn7jm" in namespace "gc-9034"
    Sep  3 22:03:52.031: INFO: Deleting pod "simpletest.rc-plbjd" in namespace "gc-9034"
    Sep  3 22:03:52.048: INFO: Deleting pod "simpletest.rc-qs79w" in namespace "gc-9034"
    Sep  3 22:03:52.123: INFO: Deleting pod "simpletest.rc-qs9nl" in namespace "gc-9034"
    Sep  3 22:03:52.183: INFO: Deleting pod "simpletest.rc-r8r24" in namespace "gc-9034"
    Sep  3 22:03:52.210: INFO: Deleting pod "simpletest.rc-rfmr4" in namespace "gc-9034"
    Sep  3 22:03:52.271: INFO: Deleting pod "simpletest.rc-rm6dw" in namespace "gc-9034"
    Sep  3 22:03:52.300: INFO: Deleting pod "simpletest.rc-rmmwl" in namespace "gc-9034"
    Sep  3 22:03:52.318: INFO: Deleting pod "simpletest.rc-s4ww6" in namespace "gc-9034"
    Sep  3 22:03:52.331: INFO: Deleting pod "simpletest.rc-sfpp2" in namespace "gc-9034"
    Sep  3 22:03:52.342: INFO: Deleting pod "simpletest.rc-sqh9l" in namespace "gc-9034"
    Sep  3 22:03:52.362: INFO: Deleting pod "simpletest.rc-t2zxd" in namespace "gc-9034"
    Sep  3 22:03:52.507: INFO: Deleting pod "simpletest.rc-t54xc" in namespace "gc-9034"
    Sep  3 22:03:52.555: INFO: Deleting pod "simpletest.rc-tcrzr" in namespace "gc-9034"
    Sep  3 22:03:52.607: INFO: Deleting pod "simpletest.rc-td6h6" in namespace "gc-9034"
    Sep  3 22:03:52.638: INFO: Deleting pod "simpletest.rc-tdwkb" in namespace "gc-9034"
    Sep  3 22:03:52.662: INFO: Deleting pod "simpletest.rc-tgccb" in namespace "gc-9034"
    Sep  3 22:03:52.710: INFO: Deleting pod "simpletest.rc-tnbck" in namespace "gc-9034"
    Sep  3 22:03:52.743: INFO: Deleting pod "simpletest.rc-vt8pg" in namespace "gc-9034"
    Sep  3 22:03:52.805: INFO: Deleting pod "simpletest.rc-vz98w" in namespace "gc-9034"
    Sep  3 22:03:52.879: INFO: Deleting pod "simpletest.rc-wmsvj" in namespace "gc-9034"
    Sep  3 22:03:52.931: INFO: Deleting pod "simpletest.rc-wn5jc" in namespace "gc-9034"
    Sep  3 22:03:52.985: INFO: Deleting pod "simpletest.rc-wnnsl" in namespace "gc-9034"
    Sep  3 22:03:53.046: INFO: Deleting pod "simpletest.rc-wscgl" in namespace "gc-9034"
    Sep  3 22:03:53.094: INFO: Deleting pod "simpletest.rc-wsgcd" in namespace "gc-9034"
    Sep  3 22:03:53.129: INFO: Deleting pod "simpletest.rc-wwt7q" in namespace "gc-9034"
    Sep  3 22:03:53.172: INFO: Deleting pod "simpletest.rc-x6vx8" in namespace "gc-9034"
    Sep  3 22:03:53.226: INFO: Deleting pod "simpletest.rc-xk2bq" in namespace "gc-9034"
    Sep  3 22:03:53.258: INFO: Deleting pod "simpletest.rc-xw8t5" in namespace "gc-9034"
    Sep  3 22:03:53.271: INFO: Deleting pod "simpletest.rc-xxg74" in namespace "gc-9034"
    Sep  3 22:03:53.294: INFO: Deleting pod "simpletest.rc-z2wlb" in namespace "gc-9034"
    Sep  3 22:03:53.318: INFO: Deleting pod "simpletest.rc-zplb5" in namespace "gc-9034"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Sep  3 22:03:53.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9034" for this suite. 09/03/22 22:03:53.349
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:03:53.373
Sep  3 22:03:53.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pods 09/03/22 22:03:53.374
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:03:53.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:03:53.404
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 09/03/22 22:03:53.418
STEP: submitting the pod to kubernetes 09/03/22 22:03:53.419
STEP: verifying QOS class is set on the pod 09/03/22 22:03:53.44
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Sep  3 22:03:53.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5719" for this suite. 09/03/22 22:03:53.466
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":307,"skipped":5805,"failed":0}
------------------------------
• [0.103 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:03:53.373
    Sep  3 22:03:53.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pods 09/03/22 22:03:53.374
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:03:53.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:03:53.404
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 09/03/22 22:03:53.418
    STEP: submitting the pod to kubernetes 09/03/22 22:03:53.419
    STEP: verifying QOS class is set on the pod 09/03/22 22:03:53.44
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Sep  3 22:03:53.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5719" for this suite. 09/03/22 22:03:53.466
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:03:53.487
Sep  3 22:03:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 22:03:53.493
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:03:53.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:03:53.526
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-5198c3b6-30c6-4c79-a355-802ae3c97611 09/03/22 22:03:53.531
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 22:03:53.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9483" for this suite. 09/03/22 22:03:53.539
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":308,"skipped":5833,"failed":0}
------------------------------
• [0.058 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:03:53.487
    Sep  3 22:03:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 22:03:53.493
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:03:53.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:03:53.526
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-5198c3b6-30c6-4c79-a355-802ae3c97611 09/03/22 22:03:53.531
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 22:03:53.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9483" for this suite. 09/03/22 22:03:53.539
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:03:53.549
Sep  3 22:03:53.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename subpath 09/03/22 22:03:53.551
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:03:53.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:03:53.583
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/03/22 22:03:53.585
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-wphf 09/03/22 22:03:53.594
STEP: Creating a pod to test atomic-volume-subpath 09/03/22 22:03:53.594
Sep  3 22:03:53.601: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-wphf" in namespace "subpath-345" to be "Succeeded or Failed"
Sep  3 22:03:53.607: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.184113ms
Sep  3 22:03:55.624: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022988262s
Sep  3 22:03:57.617: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015826663s
Sep  3 22:03:59.613: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011295507s
Sep  3 22:04:01.626: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024267293s
Sep  3 22:04:03.621: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.019280635s
Sep  3 22:04:05.615: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013838303s
Sep  3 22:04:07.613: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011162632s
Sep  3 22:04:09.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008028661s
Sep  3 22:04:11.613: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011960105s
Sep  3 22:04:13.625: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 20.023338496s
Sep  3 22:04:15.613: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011192196s
Sep  3 22:04:17.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007648152s
Sep  3 22:04:19.613: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011407725s
Sep  3 22:04:21.612: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 28.010237442s
Sep  3 22:04:23.610: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 30.008114305s
Sep  3 22:04:25.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 32.007985773s
Sep  3 22:04:27.610: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 34.008545942s
Sep  3 22:04:29.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 36.007892303s
Sep  3 22:04:31.610: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 38.008833344s
Sep  3 22:04:33.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 40.007887982s
Sep  3 22:04:35.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 42.007436126s
Sep  3 22:04:37.610: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 44.008158871s
Sep  3 22:04:39.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=false. Elapsed: 46.007925819s
Sep  3 22:04:41.611: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 48.009880266s
STEP: Saw pod success 09/03/22 22:04:41.611
Sep  3 22:04:41.612: INFO: Pod "pod-subpath-test-downwardapi-wphf" satisfied condition "Succeeded or Failed"
Sep  3 22:04:41.614: INFO: Trying to get logs from node kind-worker2 pod pod-subpath-test-downwardapi-wphf container test-container-subpath-downwardapi-wphf: <nil>
STEP: delete the pod 09/03/22 22:04:41.625
Sep  3 22:04:41.631: INFO: Waiting for pod pod-subpath-test-downwardapi-wphf to disappear
Sep  3 22:04:41.633: INFO: Pod pod-subpath-test-downwardapi-wphf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-wphf 09/03/22 22:04:41.633
Sep  3 22:04:41.633: INFO: Deleting pod "pod-subpath-test-downwardapi-wphf" in namespace "subpath-345"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Sep  3 22:04:41.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-345" for this suite. 09/03/22 22:04:41.637
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":309,"skipped":5857,"failed":0}
------------------------------
• [SLOW TEST] [48.093 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:03:53.549
    Sep  3 22:03:53.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename subpath 09/03/22 22:03:53.551
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:03:53.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:03:53.583
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/03/22 22:03:53.585
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-wphf 09/03/22 22:03:53.594
    STEP: Creating a pod to test atomic-volume-subpath 09/03/22 22:03:53.594
    Sep  3 22:03:53.601: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-wphf" in namespace "subpath-345" to be "Succeeded or Failed"
    Sep  3 22:03:53.607: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.184113ms
    Sep  3 22:03:55.624: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022988262s
    Sep  3 22:03:57.617: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015826663s
    Sep  3 22:03:59.613: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011295507s
    Sep  3 22:04:01.626: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024267293s
    Sep  3 22:04:03.621: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.019280635s
    Sep  3 22:04:05.615: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013838303s
    Sep  3 22:04:07.613: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011162632s
    Sep  3 22:04:09.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008028661s
    Sep  3 22:04:11.613: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011960105s
    Sep  3 22:04:13.625: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 20.023338496s
    Sep  3 22:04:15.613: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011192196s
    Sep  3 22:04:17.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007648152s
    Sep  3 22:04:19.613: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011407725s
    Sep  3 22:04:21.612: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 28.010237442s
    Sep  3 22:04:23.610: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 30.008114305s
    Sep  3 22:04:25.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 32.007985773s
    Sep  3 22:04:27.610: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 34.008545942s
    Sep  3 22:04:29.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 36.007892303s
    Sep  3 22:04:31.610: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 38.008833344s
    Sep  3 22:04:33.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 40.007887982s
    Sep  3 22:04:35.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 42.007436126s
    Sep  3 22:04:37.610: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=true. Elapsed: 44.008158871s
    Sep  3 22:04:39.609: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Running", Reason="", readiness=false. Elapsed: 46.007925819s
    Sep  3 22:04:41.611: INFO: Pod "pod-subpath-test-downwardapi-wphf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 48.009880266s
    STEP: Saw pod success 09/03/22 22:04:41.611
    Sep  3 22:04:41.612: INFO: Pod "pod-subpath-test-downwardapi-wphf" satisfied condition "Succeeded or Failed"
    Sep  3 22:04:41.614: INFO: Trying to get logs from node kind-worker2 pod pod-subpath-test-downwardapi-wphf container test-container-subpath-downwardapi-wphf: <nil>
    STEP: delete the pod 09/03/22 22:04:41.625
    Sep  3 22:04:41.631: INFO: Waiting for pod pod-subpath-test-downwardapi-wphf to disappear
    Sep  3 22:04:41.633: INFO: Pod pod-subpath-test-downwardapi-wphf no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-wphf 09/03/22 22:04:41.633
    Sep  3 22:04:41.633: INFO: Deleting pod "pod-subpath-test-downwardapi-wphf" in namespace "subpath-345"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Sep  3 22:04:41.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-345" for this suite. 09/03/22 22:04:41.637
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:04:41.645
Sep  3 22:04:41.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename dns 09/03/22 22:04:41.646
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:04:41.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:04:41.657
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 09/03/22 22:04:41.659
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5765 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5765;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5765 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5765;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5765.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5765.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5765.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5765.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5765.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5765.svc;check="$$(dig +notcp +noall +answer +search 126.223.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.223.126_udp@PTR;check="$$(dig +tcp +noall +answer +search 126.223.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.223.126_tcp@PTR;sleep 1; done
 09/03/22 22:04:41.67
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5765 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5765;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5765 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5765;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5765.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5765.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5765.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5765.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5765.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5765.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5765.svc;check="$$(dig +notcp +noall +answer +search 126.223.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.223.126_udp@PTR;check="$$(dig +tcp +noall +answer +search 126.223.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.223.126_tcp@PTR;sleep 1; done
 09/03/22 22:04:41.67
STEP: creating a pod to probe DNS 09/03/22 22:04:41.67
STEP: submitting the pod to kubernetes 09/03/22 22:04:41.67
Sep  3 22:04:41.699: INFO: Waiting up to 15m0s for pod "dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd" in namespace "dns-5765" to be "running"
Sep  3 22:04:41.724: INFO: Pod "dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 25.11778ms
Sep  3 22:04:43.728: INFO: Pod "dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd": Phase="Running", Reason="", readiness=true. Elapsed: 2.02888337s
Sep  3 22:04:43.728: INFO: Pod "dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd" satisfied condition "running"
STEP: retrieving the pod 09/03/22 22:04:43.728
STEP: looking for the results for each expected name from probers 09/03/22 22:04:43.731
Sep  3 22:04:43.733: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.735: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.738: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.741: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.743: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.745: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.749: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.751: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.761: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.763: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.765: INFO: Unable to read jessie_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.767: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.769: INFO: Unable to read jessie_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.771: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.773: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.775: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:43.782: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5765 wheezy_tcp@dns-test-service.dns-5765 wheezy_udp@dns-test-service.dns-5765.svc wheezy_tcp@dns-test-service.dns-5765.svc wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5765 jessie_tcp@dns-test-service.dns-5765 jessie_udp@dns-test-service.dns-5765.svc jessie_tcp@dns-test-service.dns-5765.svc jessie_udp@_http._tcp.dns-test-service.dns-5765.svc jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc]

Sep  3 22:04:48.786: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.789: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.791: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.793: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.797: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.804: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.807: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.819: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.821: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.823: INFO: Unable to read jessie_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.826: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.828: INFO: Unable to read jessie_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.832: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.835: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.838: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:48.849: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5765 wheezy_tcp@dns-test-service.dns-5765 wheezy_udp@dns-test-service.dns-5765.svc wheezy_tcp@dns-test-service.dns-5765.svc wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5765 jessie_tcp@dns-test-service.dns-5765 jessie_udp@dns-test-service.dns-5765.svc jessie_tcp@dns-test-service.dns-5765.svc jessie_udp@_http._tcp.dns-test-service.dns-5765.svc jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc]

Sep  3 22:04:53.786: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.789: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.790: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.792: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.799: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.801: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.828: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.830: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.833: INFO: Unable to read jessie_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.837: INFO: Unable to read jessie_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.839: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.843: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.844: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:53.856: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5765 wheezy_tcp@dns-test-service.dns-5765 wheezy_udp@dns-test-service.dns-5765.svc wheezy_tcp@dns-test-service.dns-5765.svc wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5765 jessie_tcp@dns-test-service.dns-5765 jessie_udp@dns-test-service.dns-5765.svc jessie_tcp@dns-test-service.dns-5765.svc jessie_udp@_http._tcp.dns-test-service.dns-5765.svc jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc]

Sep  3 22:04:58.789: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.791: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.798: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.800: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.802: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.804: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.813: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.815: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.817: INFO: Unable to read jessie_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.819: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.821: INFO: Unable to read jessie_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.823: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.824: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.827: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:04:58.836: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5765 wheezy_tcp@dns-test-service.dns-5765 wheezy_udp@dns-test-service.dns-5765.svc wheezy_tcp@dns-test-service.dns-5765.svc wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5765 jessie_tcp@dns-test-service.dns-5765 jessie_udp@dns-test-service.dns-5765.svc jessie_tcp@dns-test-service.dns-5765.svc jessie_udp@_http._tcp.dns-test-service.dns-5765.svc jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc]

Sep  3 22:05:03.785: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.788: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.790: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.792: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.798: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.810: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.811: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.815: INFO: Unable to read jessie_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.817: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.819: INFO: Unable to read jessie_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.821: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.823: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.825: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:03.833: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5765 wheezy_tcp@dns-test-service.dns-5765 wheezy_udp@dns-test-service.dns-5765.svc wheezy_tcp@dns-test-service.dns-5765.svc wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5765 jessie_tcp@dns-test-service.dns-5765 jessie_udp@dns-test-service.dns-5765.svc jessie_tcp@dns-test-service.dns-5765.svc jessie_udp@_http._tcp.dns-test-service.dns-5765.svc jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc]

Sep  3 22:05:08.785: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.787: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.789: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.791: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.800: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.805: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.815: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.817: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.825: INFO: Unable to read jessie_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.827: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.829: INFO: Unable to read jessie_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.831: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.833: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.834: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:08.844: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5765 wheezy_tcp@dns-test-service.dns-5765 wheezy_udp@dns-test-service.dns-5765.svc wheezy_tcp@dns-test-service.dns-5765.svc wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5765 jessie_tcp@dns-test-service.dns-5765 jessie_udp@dns-test-service.dns-5765.svc jessie_tcp@dns-test-service.dns-5765.svc jessie_udp@_http._tcp.dns-test-service.dns-5765.svc jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc]

Sep  3 22:05:13.802: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:13.804: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
Sep  3 22:05:13.843: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc]

Sep  3 22:05:18.844: INFO: DNS probes using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd succeeded

STEP: deleting the pod 09/03/22 22:05:18.844
STEP: deleting the test service 09/03/22 22:05:18.895
STEP: deleting the test headless service 09/03/22 22:05:18.978
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Sep  3 22:05:19.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5765" for this suite. 09/03/22 22:05:19.028
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":310,"skipped":5859,"failed":0}
------------------------------
• [SLOW TEST] [37.398 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:04:41.645
    Sep  3 22:04:41.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename dns 09/03/22 22:04:41.646
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:04:41.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:04:41.657
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 09/03/22 22:04:41.659
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5765 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5765;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5765 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5765;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5765.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5765.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5765.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5765.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5765.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5765.svc;check="$$(dig +notcp +noall +answer +search 126.223.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.223.126_udp@PTR;check="$$(dig +tcp +noall +answer +search 126.223.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.223.126_tcp@PTR;sleep 1; done
     09/03/22 22:04:41.67
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5765 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5765;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5765 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5765;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5765.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5765.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5765.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5765.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5765.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5765.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5765.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5765.svc;check="$$(dig +notcp +noall +answer +search 126.223.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.223.126_udp@PTR;check="$$(dig +tcp +noall +answer +search 126.223.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.223.126_tcp@PTR;sleep 1; done
     09/03/22 22:04:41.67
    STEP: creating a pod to probe DNS 09/03/22 22:04:41.67
    STEP: submitting the pod to kubernetes 09/03/22 22:04:41.67
    Sep  3 22:04:41.699: INFO: Waiting up to 15m0s for pod "dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd" in namespace "dns-5765" to be "running"
    Sep  3 22:04:41.724: INFO: Pod "dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 25.11778ms
    Sep  3 22:04:43.728: INFO: Pod "dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd": Phase="Running", Reason="", readiness=true. Elapsed: 2.02888337s
    Sep  3 22:04:43.728: INFO: Pod "dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd" satisfied condition "running"
    STEP: retrieving the pod 09/03/22 22:04:43.728
    STEP: looking for the results for each expected name from probers 09/03/22 22:04:43.731
    Sep  3 22:04:43.733: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.735: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.738: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.741: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.743: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.745: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.749: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.751: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.761: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.763: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.765: INFO: Unable to read jessie_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.767: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.769: INFO: Unable to read jessie_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.771: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.773: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.775: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:43.782: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5765 wheezy_tcp@dns-test-service.dns-5765 wheezy_udp@dns-test-service.dns-5765.svc wheezy_tcp@dns-test-service.dns-5765.svc wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5765 jessie_tcp@dns-test-service.dns-5765 jessie_udp@dns-test-service.dns-5765.svc jessie_tcp@dns-test-service.dns-5765.svc jessie_udp@_http._tcp.dns-test-service.dns-5765.svc jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc]

    Sep  3 22:04:48.786: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.789: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.791: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.793: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.797: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.804: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.807: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.819: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.821: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.823: INFO: Unable to read jessie_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.826: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.828: INFO: Unable to read jessie_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.832: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.835: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.838: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:48.849: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5765 wheezy_tcp@dns-test-service.dns-5765 wheezy_udp@dns-test-service.dns-5765.svc wheezy_tcp@dns-test-service.dns-5765.svc wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5765 jessie_tcp@dns-test-service.dns-5765 jessie_udp@dns-test-service.dns-5765.svc jessie_tcp@dns-test-service.dns-5765.svc jessie_udp@_http._tcp.dns-test-service.dns-5765.svc jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc]

    Sep  3 22:04:53.786: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.789: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.790: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.792: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.799: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.801: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.828: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.830: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.833: INFO: Unable to read jessie_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.837: INFO: Unable to read jessie_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.839: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.843: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.844: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:53.856: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5765 wheezy_tcp@dns-test-service.dns-5765 wheezy_udp@dns-test-service.dns-5765.svc wheezy_tcp@dns-test-service.dns-5765.svc wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5765 jessie_tcp@dns-test-service.dns-5765 jessie_udp@dns-test-service.dns-5765.svc jessie_tcp@dns-test-service.dns-5765.svc jessie_udp@_http._tcp.dns-test-service.dns-5765.svc jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc]

    Sep  3 22:04:58.789: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.791: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.798: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.800: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.802: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.804: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.813: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.815: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.817: INFO: Unable to read jessie_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.819: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.821: INFO: Unable to read jessie_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.823: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.824: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.827: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:04:58.836: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5765 wheezy_tcp@dns-test-service.dns-5765 wheezy_udp@dns-test-service.dns-5765.svc wheezy_tcp@dns-test-service.dns-5765.svc wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5765 jessie_tcp@dns-test-service.dns-5765 jessie_udp@dns-test-service.dns-5765.svc jessie_tcp@dns-test-service.dns-5765.svc jessie_udp@_http._tcp.dns-test-service.dns-5765.svc jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc]

    Sep  3 22:05:03.785: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.788: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.790: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.792: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.798: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.810: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.811: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.815: INFO: Unable to read jessie_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.817: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.819: INFO: Unable to read jessie_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.821: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.823: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.825: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:03.833: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5765 wheezy_tcp@dns-test-service.dns-5765 wheezy_udp@dns-test-service.dns-5765.svc wheezy_tcp@dns-test-service.dns-5765.svc wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5765 jessie_tcp@dns-test-service.dns-5765 jessie_udp@dns-test-service.dns-5765.svc jessie_tcp@dns-test-service.dns-5765.svc jessie_udp@_http._tcp.dns-test-service.dns-5765.svc jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc]

    Sep  3 22:05:08.785: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.787: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.789: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.791: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.800: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.805: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.815: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.817: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.825: INFO: Unable to read jessie_udp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.827: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765 from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.829: INFO: Unable to read jessie_udp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.831: INFO: Unable to read jessie_tcp@dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.833: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.834: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:08.844: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5765 wheezy_tcp@dns-test-service.dns-5765 wheezy_udp@dns-test-service.dns-5765.svc wheezy_tcp@dns-test-service.dns-5765.svc wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5765 jessie_tcp@dns-test-service.dns-5765 jessie_udp@dns-test-service.dns-5765.svc jessie_tcp@dns-test-service.dns-5765.svc jessie_udp@_http._tcp.dns-test-service.dns-5765.svc jessie_tcp@_http._tcp.dns-test-service.dns-5765.svc]

    Sep  3 22:05:13.802: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:13.804: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc from pod dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd: the server could not find the requested resource (get pods dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd)
    Sep  3 22:05:13.843: INFO: Lookups using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-5765.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5765.svc]

    Sep  3 22:05:18.844: INFO: DNS probes using dns-5765/dns-test-cb2a8ac0-b8e5-4c9c-96ef-2b18d89f4fdd succeeded

    STEP: deleting the pod 09/03/22 22:05:18.844
    STEP: deleting the test service 09/03/22 22:05:18.895
    STEP: deleting the test headless service 09/03/22 22:05:18.978
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Sep  3 22:05:19.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5765" for this suite. 09/03/22 22:05:19.028
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:05:19.043
Sep  3 22:05:19.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename svcaccounts 09/03/22 22:05:19.044
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:05:19.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:05:19.075
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Sep  3 22:05:19.090: INFO: created pod
Sep  3 22:05:19.090: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2113" to be "Succeeded or Failed"
Sep  3 22:05:19.098: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.889897ms
Sep  3 22:05:21.102: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011971537s
Sep  3 22:05:23.100: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010782893s
STEP: Saw pod success 09/03/22 22:05:23.101
Sep  3 22:05:23.101: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Sep  3 22:05:53.105: INFO: polling logs
Sep  3 22:05:53.110: INFO: Pod logs: 
I0903 22:05:19.767274       1 log.go:195] OK: Got token
I0903 22:05:19.767503       1 log.go:195] validating with in-cluster discovery
I0903 22:05:19.767838       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0903 22:05:19.767882       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2113:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1662243319, NotBefore:1662242719, IssuedAt:1662242719, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2113", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"236d3e86-a4ae-4276-b4b9-b6bdccf17b4d"}}}
I0903 22:05:19.781929       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0903 22:05:19.789075       1 log.go:195] OK: Validated signature on JWT
I0903 22:05:19.789159       1 log.go:195] OK: Got valid claims from token!
I0903 22:05:19.789218       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2113:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1662243319, NotBefore:1662242719, IssuedAt:1662242719, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2113", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"236d3e86-a4ae-4276-b4b9-b6bdccf17b4d"}}}

Sep  3 22:05:53.110: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Sep  3 22:05:53.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2113" for this suite. 09/03/22 22:05:53.117
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":311,"skipped":5866,"failed":0}
------------------------------
• [SLOW TEST] [34.077 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:05:19.043
    Sep  3 22:05:19.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename svcaccounts 09/03/22 22:05:19.044
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:05:19.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:05:19.075
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Sep  3 22:05:19.090: INFO: created pod
    Sep  3 22:05:19.090: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2113" to be "Succeeded or Failed"
    Sep  3 22:05:19.098: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.889897ms
    Sep  3 22:05:21.102: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011971537s
    Sep  3 22:05:23.100: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010782893s
    STEP: Saw pod success 09/03/22 22:05:23.101
    Sep  3 22:05:23.101: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Sep  3 22:05:53.105: INFO: polling logs
    Sep  3 22:05:53.110: INFO: Pod logs: 
    I0903 22:05:19.767274       1 log.go:195] OK: Got token
    I0903 22:05:19.767503       1 log.go:195] validating with in-cluster discovery
    I0903 22:05:19.767838       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0903 22:05:19.767882       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2113:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1662243319, NotBefore:1662242719, IssuedAt:1662242719, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2113", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"236d3e86-a4ae-4276-b4b9-b6bdccf17b4d"}}}
    I0903 22:05:19.781929       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0903 22:05:19.789075       1 log.go:195] OK: Validated signature on JWT
    I0903 22:05:19.789159       1 log.go:195] OK: Got valid claims from token!
    I0903 22:05:19.789218       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2113:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1662243319, NotBefore:1662242719, IssuedAt:1662242719, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2113", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"236d3e86-a4ae-4276-b4b9-b6bdccf17b4d"}}}

    Sep  3 22:05:53.110: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Sep  3 22:05:53.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2113" for this suite. 09/03/22 22:05:53.117
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:05:53.12
Sep  3 22:05:53.121: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename namespaces 09/03/22 22:05:53.122
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:05:53.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:05:53.134
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 09/03/22 22:05:53.137
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:05:53.147
STEP: Creating a pod in the namespace 09/03/22 22:05:53.15
STEP: Waiting for the pod to have running status 09/03/22 22:05:53.155
Sep  3 22:05:53.155: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3799" to be "running"
Sep  3 22:05:53.159: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.092807ms
Sep  3 22:05:55.162: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00620704s
Sep  3 22:05:55.162: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 09/03/22 22:05:55.162
STEP: Waiting for the namespace to be removed. 09/03/22 22:05:55.165
STEP: Recreating the namespace 09/03/22 22:06:06.169
STEP: Verifying there are no pods in the namespace 09/03/22 22:06:06.179
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Sep  3 22:06:06.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9854" for this suite. 09/03/22 22:06:06.183
STEP: Destroying namespace "nsdeletetest-3799" for this suite. 09/03/22 22:06:06.186
Sep  3 22:06:06.188: INFO: Namespace nsdeletetest-3799 was already deleted
STEP: Destroying namespace "nsdeletetest-5268" for this suite. 09/03/22 22:06:06.188
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":312,"skipped":5866,"failed":0}
------------------------------
• [SLOW TEST] [13.073 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:05:53.12
    Sep  3 22:05:53.121: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename namespaces 09/03/22 22:05:53.122
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:05:53.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:05:53.134
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 09/03/22 22:05:53.137
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:05:53.147
    STEP: Creating a pod in the namespace 09/03/22 22:05:53.15
    STEP: Waiting for the pod to have running status 09/03/22 22:05:53.155
    Sep  3 22:05:53.155: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3799" to be "running"
    Sep  3 22:05:53.159: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.092807ms
    Sep  3 22:05:55.162: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00620704s
    Sep  3 22:05:55.162: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 09/03/22 22:05:55.162
    STEP: Waiting for the namespace to be removed. 09/03/22 22:05:55.165
    STEP: Recreating the namespace 09/03/22 22:06:06.169
    STEP: Verifying there are no pods in the namespace 09/03/22 22:06:06.179
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 22:06:06.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9854" for this suite. 09/03/22 22:06:06.183
    STEP: Destroying namespace "nsdeletetest-3799" for this suite. 09/03/22 22:06:06.186
    Sep  3 22:06:06.188: INFO: Namespace nsdeletetest-3799 was already deleted
    STEP: Destroying namespace "nsdeletetest-5268" for this suite. 09/03/22 22:06:06.188
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:06:06.198
Sep  3 22:06:06.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 22:06:06.199
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:06.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:06.21
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 09/03/22 22:06:06.213
Sep  3 22:06:06.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09" in namespace "downward-api-5993" to be "Succeeded or Failed"
Sep  3 22:06:06.221: INFO: Pod "downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272004ms
Sep  3 22:06:08.224: INFO: Pod "downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006433865s
Sep  3 22:06:10.228: INFO: Pod "downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009999427s
STEP: Saw pod success 09/03/22 22:06:10.228
Sep  3 22:06:10.228: INFO: Pod "downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09" satisfied condition "Succeeded or Failed"
Sep  3 22:06:10.230: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09 container client-container: <nil>
STEP: delete the pod 09/03/22 22:06:10.234
Sep  3 22:06:10.241: INFO: Waiting for pod downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09 to disappear
Sep  3 22:06:10.245: INFO: Pod downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Sep  3 22:06:10.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5993" for this suite. 09/03/22 22:06:10.247
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":313,"skipped":5884,"failed":0}
------------------------------
• [4.052 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:06:06.198
    Sep  3 22:06:06.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 22:06:06.199
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:06.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:06.21
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 09/03/22 22:06:06.213
    Sep  3 22:06:06.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09" in namespace "downward-api-5993" to be "Succeeded or Failed"
    Sep  3 22:06:06.221: INFO: Pod "downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272004ms
    Sep  3 22:06:08.224: INFO: Pod "downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006433865s
    Sep  3 22:06:10.228: INFO: Pod "downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009999427s
    STEP: Saw pod success 09/03/22 22:06:10.228
    Sep  3 22:06:10.228: INFO: Pod "downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09" satisfied condition "Succeeded or Failed"
    Sep  3 22:06:10.230: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09 container client-container: <nil>
    STEP: delete the pod 09/03/22 22:06:10.234
    Sep  3 22:06:10.241: INFO: Waiting for pod downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09 to disappear
    Sep  3 22:06:10.245: INFO: Pod downwardapi-volume-e75132fe-846d-44de-9b62-3bd01be81f09 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Sep  3 22:06:10.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5993" for this suite. 09/03/22 22:06:10.247
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:06:10.256
Sep  3 22:06:10.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-lifecycle-hook 09/03/22 22:06:10.257
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:10.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:10.271
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 09/03/22 22:06:10.275
Sep  3 22:06:10.279: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6640" to be "running and ready"
Sep  3 22:06:10.286: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.543608ms
Sep  3 22:06:10.286: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:06:12.290: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01017637s
Sep  3 22:06:12.290: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  3 22:06:12.290: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 09/03/22 22:06:12.292
Sep  3 22:06:12.300: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6640" to be "running and ready"
Sep  3 22:06:12.304: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.868705ms
Sep  3 22:06:12.304: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:06:14.307: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007246708s
Sep  3 22:06:14.307: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Sep  3 22:06:14.307: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/03/22 22:06:14.309
STEP: delete the pod with lifecycle hook 09/03/22 22:06:14.322
Sep  3 22:06:14.326: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  3 22:06:14.329: INFO: Pod pod-with-poststart-http-hook still exists
Sep  3 22:06:16.329: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  3 22:06:16.332: INFO: Pod pod-with-poststart-http-hook still exists
Sep  3 22:06:18.329: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  3 22:06:18.332: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Sep  3 22:06:18.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6640" for this suite. 09/03/22 22:06:18.335
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":314,"skipped":5899,"failed":0}
------------------------------
• [SLOW TEST] [8.082 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:06:10.256
    Sep  3 22:06:10.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/03/22 22:06:10.257
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:10.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:10.271
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 09/03/22 22:06:10.275
    Sep  3 22:06:10.279: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6640" to be "running and ready"
    Sep  3 22:06:10.286: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.543608ms
    Sep  3 22:06:10.286: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 22:06:12.290: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01017637s
    Sep  3 22:06:12.290: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  3 22:06:12.290: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 09/03/22 22:06:12.292
    Sep  3 22:06:12.300: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6640" to be "running and ready"
    Sep  3 22:06:12.304: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.868705ms
    Sep  3 22:06:12.304: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 22:06:14.307: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007246708s
    Sep  3 22:06:14.307: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Sep  3 22:06:14.307: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/03/22 22:06:14.309
    STEP: delete the pod with lifecycle hook 09/03/22 22:06:14.322
    Sep  3 22:06:14.326: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  3 22:06:14.329: INFO: Pod pod-with-poststart-http-hook still exists
    Sep  3 22:06:16.329: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  3 22:06:16.332: INFO: Pod pod-with-poststart-http-hook still exists
    Sep  3 22:06:18.329: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  3 22:06:18.332: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Sep  3 22:06:18.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6640" for this suite. 09/03/22 22:06:18.335
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:06:18.338
Sep  3 22:06:18.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 22:06:18.34
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:18.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:18.352
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-8276 09/03/22 22:06:18.355
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8276 to expose endpoints map[] 09/03/22 22:06:18.361
Sep  3 22:06:18.372: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Sep  3 22:06:19.376: INFO: successfully validated that service multi-endpoint-test in namespace services-8276 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8276 09/03/22 22:06:19.376
Sep  3 22:06:19.381: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8276" to be "running and ready"
Sep  3 22:06:19.384: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158904ms
Sep  3 22:06:19.384: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:06:21.386: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004975411s
Sep  3 22:06:21.386: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  3 22:06:21.386: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8276 to expose endpoints map[pod1:[100]] 09/03/22 22:06:21.388
Sep  3 22:06:21.394: INFO: successfully validated that service multi-endpoint-test in namespace services-8276 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8276 09/03/22 22:06:21.394
Sep  3 22:06:21.397: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8276" to be "running and ready"
Sep  3 22:06:21.400: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.164501ms
Sep  3 22:06:21.400: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:06:23.406: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008704182s
Sep  3 22:06:23.406: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  3 22:06:23.406: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8276 to expose endpoints map[pod1:[100] pod2:[101]] 09/03/22 22:06:23.411
Sep  3 22:06:23.427: INFO: successfully validated that service multi-endpoint-test in namespace services-8276 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 09/03/22 22:06:23.427
Sep  3 22:06:23.427: INFO: Creating new exec pod
Sep  3 22:06:23.437: INFO: Waiting up to 5m0s for pod "execpodnr4nq" in namespace "services-8276" to be "running"
Sep  3 22:06:23.441: INFO: Pod "execpodnr4nq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.533603ms
Sep  3 22:06:25.444: INFO: Pod "execpodnr4nq": Phase="Running", Reason="", readiness=true. Elapsed: 2.007324383s
Sep  3 22:06:25.444: INFO: Pod "execpodnr4nq" satisfied condition "running"
Sep  3 22:06:26.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8276 exec execpodnr4nq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Sep  3 22:06:26.582: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Sep  3 22:06:26.582: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 22:06:26.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8276 exec execpodnr4nq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.9.54 80'
Sep  3 22:06:26.714: INFO: stderr: "+ nc -v -t -w 2 10.96.9.54 80\nConnection to 10.96.9.54 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Sep  3 22:06:26.714: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 22:06:26.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8276 exec execpodnr4nq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Sep  3 22:06:26.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Sep  3 22:06:26.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 22:06:26.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8276 exec execpodnr4nq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.9.54 81'
Sep  3 22:06:27.012: INFO: stderr: "+ nc -v -t -w 2 10.96.9.54 81\n+ echo hostName\nConnection to 10.96.9.54 81 port [tcp/*] succeeded!\n"
Sep  3 22:06:27.012: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8276 09/03/22 22:06:27.012
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8276 to expose endpoints map[pod2:[101]] 09/03/22 22:06:27.032
Sep  3 22:06:27.094: INFO: successfully validated that service multi-endpoint-test in namespace services-8276 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8276 09/03/22 22:06:27.094
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8276 to expose endpoints map[] 09/03/22 22:06:27.233
Sep  3 22:06:27.274: INFO: successfully validated that service multi-endpoint-test in namespace services-8276 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 22:06:27.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8276" for this suite. 09/03/22 22:06:27.395
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":315,"skipped":5899,"failed":0}
------------------------------
• [SLOW TEST] [9.074 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:06:18.338
    Sep  3 22:06:18.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 22:06:18.34
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:18.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:18.352
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-8276 09/03/22 22:06:18.355
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8276 to expose endpoints map[] 09/03/22 22:06:18.361
    Sep  3 22:06:18.372: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Sep  3 22:06:19.376: INFO: successfully validated that service multi-endpoint-test in namespace services-8276 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8276 09/03/22 22:06:19.376
    Sep  3 22:06:19.381: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8276" to be "running and ready"
    Sep  3 22:06:19.384: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158904ms
    Sep  3 22:06:19.384: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 22:06:21.386: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004975411s
    Sep  3 22:06:21.386: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  3 22:06:21.386: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8276 to expose endpoints map[pod1:[100]] 09/03/22 22:06:21.388
    Sep  3 22:06:21.394: INFO: successfully validated that service multi-endpoint-test in namespace services-8276 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-8276 09/03/22 22:06:21.394
    Sep  3 22:06:21.397: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8276" to be "running and ready"
    Sep  3 22:06:21.400: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.164501ms
    Sep  3 22:06:21.400: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 22:06:23.406: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008704182s
    Sep  3 22:06:23.406: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  3 22:06:23.406: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8276 to expose endpoints map[pod1:[100] pod2:[101]] 09/03/22 22:06:23.411
    Sep  3 22:06:23.427: INFO: successfully validated that service multi-endpoint-test in namespace services-8276 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 09/03/22 22:06:23.427
    Sep  3 22:06:23.427: INFO: Creating new exec pod
    Sep  3 22:06:23.437: INFO: Waiting up to 5m0s for pod "execpodnr4nq" in namespace "services-8276" to be "running"
    Sep  3 22:06:23.441: INFO: Pod "execpodnr4nq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.533603ms
    Sep  3 22:06:25.444: INFO: Pod "execpodnr4nq": Phase="Running", Reason="", readiness=true. Elapsed: 2.007324383s
    Sep  3 22:06:25.444: INFO: Pod "execpodnr4nq" satisfied condition "running"
    Sep  3 22:06:26.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8276 exec execpodnr4nq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Sep  3 22:06:26.582: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Sep  3 22:06:26.582: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 22:06:26.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8276 exec execpodnr4nq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.9.54 80'
    Sep  3 22:06:26.714: INFO: stderr: "+ nc -v -t -w 2 10.96.9.54 80\nConnection to 10.96.9.54 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Sep  3 22:06:26.714: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 22:06:26.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8276 exec execpodnr4nq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Sep  3 22:06:26.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Sep  3 22:06:26.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 22:06:26.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-8276 exec execpodnr4nq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.9.54 81'
    Sep  3 22:06:27.012: INFO: stderr: "+ nc -v -t -w 2 10.96.9.54 81\n+ echo hostName\nConnection to 10.96.9.54 81 port [tcp/*] succeeded!\n"
    Sep  3 22:06:27.012: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-8276 09/03/22 22:06:27.012
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8276 to expose endpoints map[pod2:[101]] 09/03/22 22:06:27.032
    Sep  3 22:06:27.094: INFO: successfully validated that service multi-endpoint-test in namespace services-8276 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-8276 09/03/22 22:06:27.094
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8276 to expose endpoints map[] 09/03/22 22:06:27.233
    Sep  3 22:06:27.274: INFO: successfully validated that service multi-endpoint-test in namespace services-8276 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 22:06:27.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8276" for this suite. 09/03/22 22:06:27.395
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:06:27.415
Sep  3 22:06:27.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename configmap 09/03/22 22:06:27.434
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:27.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:27.472
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-814538d8-a8a7-49e5-8d59-b7d853482ea1 09/03/22 22:06:27.481
STEP: Creating a pod to test consume configMaps 09/03/22 22:06:27.491
Sep  3 22:06:27.500: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084" in namespace "configmap-4655" to be "Succeeded or Failed"
Sep  3 22:06:27.507: INFO: Pod "pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084": Phase="Pending", Reason="", readiness=false. Elapsed: 6.520904ms
Sep  3 22:06:29.511: INFO: Pod "pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010559771s
Sep  3 22:06:31.510: INFO: Pod "pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010005213s
STEP: Saw pod success 09/03/22 22:06:31.51
Sep  3 22:06:31.511: INFO: Pod "pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084" satisfied condition "Succeeded or Failed"
Sep  3 22:06:31.512: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084 container agnhost-container: <nil>
STEP: delete the pod 09/03/22 22:06:31.516
Sep  3 22:06:31.522: INFO: Waiting for pod pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084 to disappear
Sep  3 22:06:31.525: INFO: Pod pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Sep  3 22:06:31.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4655" for this suite. 09/03/22 22:06:31.527
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":316,"skipped":5912,"failed":0}
------------------------------
• [4.115 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:06:27.415
    Sep  3 22:06:27.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename configmap 09/03/22 22:06:27.434
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:27.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:27.472
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-814538d8-a8a7-49e5-8d59-b7d853482ea1 09/03/22 22:06:27.481
    STEP: Creating a pod to test consume configMaps 09/03/22 22:06:27.491
    Sep  3 22:06:27.500: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084" in namespace "configmap-4655" to be "Succeeded or Failed"
    Sep  3 22:06:27.507: INFO: Pod "pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084": Phase="Pending", Reason="", readiness=false. Elapsed: 6.520904ms
    Sep  3 22:06:29.511: INFO: Pod "pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010559771s
    Sep  3 22:06:31.510: INFO: Pod "pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010005213s
    STEP: Saw pod success 09/03/22 22:06:31.51
    Sep  3 22:06:31.511: INFO: Pod "pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084" satisfied condition "Succeeded or Failed"
    Sep  3 22:06:31.512: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084 container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 22:06:31.516
    Sep  3 22:06:31.522: INFO: Waiting for pod pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084 to disappear
    Sep  3 22:06:31.525: INFO: Pod pod-configmaps-4b182bd6-0f1b-410b-82c6-da214e324084 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Sep  3 22:06:31.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4655" for this suite. 09/03/22 22:06:31.527
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:06:31.536
Sep  3 22:06:31.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename resourcequota 09/03/22 22:06:31.537
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:31.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:31.553
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 09/03/22 22:06:31.555
STEP: Creating a ResourceQuota 09/03/22 22:06:36.558
STEP: Ensuring resource quota status is calculated 09/03/22 22:06:36.562
STEP: Creating a Pod that fits quota 09/03/22 22:06:38.566
STEP: Ensuring ResourceQuota status captures the pod usage 09/03/22 22:06:38.575
STEP: Not allowing a pod to be created that exceeds remaining quota 09/03/22 22:06:40.578
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/03/22 22:06:40.58
STEP: Ensuring a pod cannot update its resource requirements 09/03/22 22:06:40.582
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/03/22 22:06:40.586
STEP: Deleting the pod 09/03/22 22:06:42.59
STEP: Ensuring resource quota status released the pod usage 09/03/22 22:06:42.596
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Sep  3 22:06:44.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3258" for this suite. 09/03/22 22:06:44.601
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":317,"skipped":5933,"failed":0}
------------------------------
• [SLOW TEST] [13.068 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:06:31.536
    Sep  3 22:06:31.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename resourcequota 09/03/22 22:06:31.537
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:31.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:31.553
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 09/03/22 22:06:31.555
    STEP: Creating a ResourceQuota 09/03/22 22:06:36.558
    STEP: Ensuring resource quota status is calculated 09/03/22 22:06:36.562
    STEP: Creating a Pod that fits quota 09/03/22 22:06:38.566
    STEP: Ensuring ResourceQuota status captures the pod usage 09/03/22 22:06:38.575
    STEP: Not allowing a pod to be created that exceeds remaining quota 09/03/22 22:06:40.578
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/03/22 22:06:40.58
    STEP: Ensuring a pod cannot update its resource requirements 09/03/22 22:06:40.582
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/03/22 22:06:40.586
    STEP: Deleting the pod 09/03/22 22:06:42.59
    STEP: Ensuring resource quota status released the pod usage 09/03/22 22:06:42.596
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Sep  3 22:06:44.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3258" for this suite. 09/03/22 22:06:44.601
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:06:44.607
Sep  3 22:06:44.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename var-expansion 09/03/22 22:06:44.608
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:44.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:44.619
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 09/03/22 22:06:44.621
Sep  3 22:06:44.626: INFO: Waiting up to 5m0s for pod "var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9" in namespace "var-expansion-4799" to be "Succeeded or Failed"
Sep  3 22:06:44.632: INFO: Pod "var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.480913ms
Sep  3 22:06:46.635: INFO: Pod "var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009431194s
Sep  3 22:06:48.636: INFO: Pod "var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01003089s
STEP: Saw pod success 09/03/22 22:06:48.636
Sep  3 22:06:48.636: INFO: Pod "var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9" satisfied condition "Succeeded or Failed"
Sep  3 22:06:48.640: INFO: Trying to get logs from node kind-worker2 pod var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9 container dapi-container: <nil>
STEP: delete the pod 09/03/22 22:06:48.644
Sep  3 22:06:48.651: INFO: Waiting for pod var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9 to disappear
Sep  3 22:06:48.653: INFO: Pod var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Sep  3 22:06:48.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4799" for this suite. 09/03/22 22:06:48.655
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":318,"skipped":5970,"failed":0}
------------------------------
• [4.052 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:06:44.607
    Sep  3 22:06:44.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename var-expansion 09/03/22 22:06:44.608
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:44.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:44.619
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 09/03/22 22:06:44.621
    Sep  3 22:06:44.626: INFO: Waiting up to 5m0s for pod "var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9" in namespace "var-expansion-4799" to be "Succeeded or Failed"
    Sep  3 22:06:44.632: INFO: Pod "var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.480913ms
    Sep  3 22:06:46.635: INFO: Pod "var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009431194s
    Sep  3 22:06:48.636: INFO: Pod "var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01003089s
    STEP: Saw pod success 09/03/22 22:06:48.636
    Sep  3 22:06:48.636: INFO: Pod "var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9" satisfied condition "Succeeded or Failed"
    Sep  3 22:06:48.640: INFO: Trying to get logs from node kind-worker2 pod var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9 container dapi-container: <nil>
    STEP: delete the pod 09/03/22 22:06:48.644
    Sep  3 22:06:48.651: INFO: Waiting for pod var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9 to disappear
    Sep  3 22:06:48.653: INFO: Pod var-expansion-35aecde9-968c-4921-86c7-05b57d4a7ea9 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Sep  3 22:06:48.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4799" for this suite. 09/03/22 22:06:48.655
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:06:48.664
Sep  3 22:06:48.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename events 09/03/22 22:06:48.666
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:48.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:48.675
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 09/03/22 22:06:48.677
Sep  3 22:06:48.680: INFO: created test-event-1
Sep  3 22:06:48.682: INFO: created test-event-2
Sep  3 22:06:48.684: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 09/03/22 22:06:48.684
STEP: delete collection of events 09/03/22 22:06:48.686
Sep  3 22:06:48.686: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/03/22 22:06:48.695
Sep  3 22:06:48.696: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Sep  3 22:06:48.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4491" for this suite. 09/03/22 22:06:48.699
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":319,"skipped":5978,"failed":0}
------------------------------
• [0.038 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:06:48.664
    Sep  3 22:06:48.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename events 09/03/22 22:06:48.666
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:48.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:48.675
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 09/03/22 22:06:48.677
    Sep  3 22:06:48.680: INFO: created test-event-1
    Sep  3 22:06:48.682: INFO: created test-event-2
    Sep  3 22:06:48.684: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 09/03/22 22:06:48.684
    STEP: delete collection of events 09/03/22 22:06:48.686
    Sep  3 22:06:48.686: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/03/22 22:06:48.695
    Sep  3 22:06:48.696: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Sep  3 22:06:48.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4491" for this suite. 09/03/22 22:06:48.699
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:06:48.707
Sep  3 22:06:48.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-probe 09/03/22 22:06:48.708
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:48.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:48.726
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf in namespace container-probe-8619 09/03/22 22:06:48.73
Sep  3 22:06:48.741: INFO: Waiting up to 5m0s for pod "liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf" in namespace "container-probe-8619" to be "not pending"
Sep  3 22:06:48.746: INFO: Pod "liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.76631ms
Sep  3 22:06:50.750: INFO: Pod "liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf": Phase="Running", Reason="", readiness=true. Elapsed: 2.007991111s
Sep  3 22:06:50.750: INFO: Pod "liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf" satisfied condition "not pending"
Sep  3 22:06:50.750: INFO: Started pod liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf in namespace container-probe-8619
STEP: checking the pod's current state and verifying that restartCount is present 09/03/22 22:06:50.75
Sep  3 22:06:50.751: INFO: Initial restart count of pod liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf is 0
Sep  3 22:07:10.783: INFO: Restart count of pod container-probe-8619/liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf is now 1 (20.031842193s elapsed)
STEP: deleting the pod 09/03/22 22:07:10.783
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Sep  3 22:07:10.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8619" for this suite. 09/03/22 22:07:10.792
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":320,"skipped":5986,"failed":0}
------------------------------
• [SLOW TEST] [22.088 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:06:48.707
    Sep  3 22:06:48.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-probe 09/03/22 22:06:48.708
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:06:48.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:06:48.726
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf in namespace container-probe-8619 09/03/22 22:06:48.73
    Sep  3 22:06:48.741: INFO: Waiting up to 5m0s for pod "liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf" in namespace "container-probe-8619" to be "not pending"
    Sep  3 22:06:48.746: INFO: Pod "liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.76631ms
    Sep  3 22:06:50.750: INFO: Pod "liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf": Phase="Running", Reason="", readiness=true. Elapsed: 2.007991111s
    Sep  3 22:06:50.750: INFO: Pod "liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf" satisfied condition "not pending"
    Sep  3 22:06:50.750: INFO: Started pod liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf in namespace container-probe-8619
    STEP: checking the pod's current state and verifying that restartCount is present 09/03/22 22:06:50.75
    Sep  3 22:06:50.751: INFO: Initial restart count of pod liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf is 0
    Sep  3 22:07:10.783: INFO: Restart count of pod container-probe-8619/liveness-26ad5b24-2f95-4503-8eaf-69354e001ecf is now 1 (20.031842193s elapsed)
    STEP: deleting the pod 09/03/22 22:07:10.783
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Sep  3 22:07:10.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8619" for this suite. 09/03/22 22:07:10.792
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:07:10.796
Sep  3 22:07:10.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename sched-pred 09/03/22 22:07:10.797
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:10.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:10.81
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Sep  3 22:07:10.812: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  3 22:07:10.817: INFO: Waiting for terminating namespaces to be deleted...
Sep  3 22:07:10.818: INFO: 
Logging pods the apiserver thinks is on node kind-worker before test
Sep  3 22:07:10.822: INFO: kindnet-xhxcg from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
Sep  3 22:07:10.822: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  3 22:07:10.822: INFO: kube-proxy-bxm4p from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
Sep  3 22:07:10.822: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:07:10.822: INFO: sonobuoy from sonobuoy started at 2022-09-03 20:37:57 +0000 UTC (1 container statuses recorded)
Sep  3 22:07:10.822: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  3 22:07:10.822: INFO: sonobuoy-e2e-job-31a458d0b0c040d4 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
Sep  3 22:07:10.822: INFO: 	Container e2e ready: true, restart count 0
Sep  3 22:07:10.822: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:07:10.822: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-jdmz7 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
Sep  3 22:07:10.822: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:07:10.822: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:07:10.822: INFO: 
Logging pods the apiserver thinks is on node kind-worker2 before test
Sep  3 22:07:10.826: INFO: kindnet-zlt4d from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
Sep  3 22:07:10.826: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  3 22:07:10.826: INFO: kube-proxy-wz2c5 from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
Sep  3 22:07:10.826: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:07:10.826: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-nwk5b from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
Sep  3 22:07:10.826: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:07:10.826: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 09/03/22 22:07:10.826
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1711790c3688da4f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 09/03/22 22:07:10.842
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Sep  3 22:07:11.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5032" for this suite. 09/03/22 22:07:11.844
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":321,"skipped":5986,"failed":0}
------------------------------
• [1.052 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:07:10.796
    Sep  3 22:07:10.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename sched-pred 09/03/22 22:07:10.797
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:10.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:10.81
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Sep  3 22:07:10.812: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  3 22:07:10.817: INFO: Waiting for terminating namespaces to be deleted...
    Sep  3 22:07:10.818: INFO: 
    Logging pods the apiserver thinks is on node kind-worker before test
    Sep  3 22:07:10.822: INFO: kindnet-xhxcg from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
    Sep  3 22:07:10.822: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  3 22:07:10.822: INFO: kube-proxy-bxm4p from kube-system started at 2022-09-03 20:37:48 +0000 UTC (1 container statuses recorded)
    Sep  3 22:07:10.822: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  3 22:07:10.822: INFO: sonobuoy from sonobuoy started at 2022-09-03 20:37:57 +0000 UTC (1 container statuses recorded)
    Sep  3 22:07:10.822: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  3 22:07:10.822: INFO: sonobuoy-e2e-job-31a458d0b0c040d4 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
    Sep  3 22:07:10.822: INFO: 	Container e2e ready: true, restart count 0
    Sep  3 22:07:10.822: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  3 22:07:10.822: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-jdmz7 from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
    Sep  3 22:07:10.822: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  3 22:07:10.822: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  3 22:07:10.822: INFO: 
    Logging pods the apiserver thinks is on node kind-worker2 before test
    Sep  3 22:07:10.826: INFO: kindnet-zlt4d from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
    Sep  3 22:07:10.826: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  3 22:07:10.826: INFO: kube-proxy-wz2c5 from kube-system started at 2022-09-03 20:37:52 +0000 UTC (1 container statuses recorded)
    Sep  3 22:07:10.826: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  3 22:07:10.826: INFO: sonobuoy-systemd-logs-daemon-set-c41264da9a3e4524-nwk5b from sonobuoy started at 2022-09-03 20:38:02 +0000 UTC (2 container statuses recorded)
    Sep  3 22:07:10.826: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  3 22:07:10.826: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 09/03/22 22:07:10.826
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1711790c3688da4f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 09/03/22 22:07:10.842
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 22:07:11.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5032" for this suite. 09/03/22 22:07:11.844
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:07:11.852
Sep  3 22:07:11.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 22:07:11.854
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:11.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:11.865
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/03/22 22:07:11.867
Sep  3 22:07:11.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 22:07:14.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 22:07:23.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7564" for this suite. 09/03/22 22:07:23.78
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":322,"skipped":6005,"failed":0}
------------------------------
• [SLOW TEST] [11.932 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:07:11.852
    Sep  3 22:07:11.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 22:07:11.854
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:11.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:11.865
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/03/22 22:07:11.867
    Sep  3 22:07:11.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 22:07:14.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 22:07:23.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7564" for this suite. 09/03/22 22:07:23.78
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:07:23.788
Sep  3 22:07:23.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename svcaccounts 09/03/22 22:07:23.788
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:23.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:23.8
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Sep  3 22:07:23.813: INFO: created pod pod-service-account-defaultsa
Sep  3 22:07:23.813: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  3 22:07:23.828: INFO: created pod pod-service-account-mountsa
Sep  3 22:07:23.828: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  3 22:07:23.835: INFO: created pod pod-service-account-nomountsa
Sep  3 22:07:23.835: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  3 22:07:23.845: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  3 22:07:23.845: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  3 22:07:23.857: INFO: created pod pod-service-account-mountsa-mountspec
Sep  3 22:07:23.858: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  3 22:07:23.908: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  3 22:07:23.908: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  3 22:07:23.924: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  3 22:07:23.924: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  3 22:07:23.944: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  3 22:07:23.945: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  3 22:07:23.968: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  3 22:07:23.968: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Sep  3 22:07:23.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5036" for this suite. 09/03/22 22:07:23.978
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":323,"skipped":6040,"failed":0}
------------------------------
• [0.211 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:07:23.788
    Sep  3 22:07:23.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename svcaccounts 09/03/22 22:07:23.788
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:23.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:23.8
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Sep  3 22:07:23.813: INFO: created pod pod-service-account-defaultsa
    Sep  3 22:07:23.813: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Sep  3 22:07:23.828: INFO: created pod pod-service-account-mountsa
    Sep  3 22:07:23.828: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Sep  3 22:07:23.835: INFO: created pod pod-service-account-nomountsa
    Sep  3 22:07:23.835: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Sep  3 22:07:23.845: INFO: created pod pod-service-account-defaultsa-mountspec
    Sep  3 22:07:23.845: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Sep  3 22:07:23.857: INFO: created pod pod-service-account-mountsa-mountspec
    Sep  3 22:07:23.858: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Sep  3 22:07:23.908: INFO: created pod pod-service-account-nomountsa-mountspec
    Sep  3 22:07:23.908: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Sep  3 22:07:23.924: INFO: created pod pod-service-account-defaultsa-nomountspec
    Sep  3 22:07:23.924: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Sep  3 22:07:23.944: INFO: created pod pod-service-account-mountsa-nomountspec
    Sep  3 22:07:23.945: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Sep  3 22:07:23.968: INFO: created pod pod-service-account-nomountsa-nomountspec
    Sep  3 22:07:23.968: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Sep  3 22:07:23.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5036" for this suite. 09/03/22 22:07:23.978
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:07:24.026
Sep  3 22:07:24.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename security-context 09/03/22 22:07:24.027
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:24.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:24.061
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/03/22 22:07:24.068
Sep  3 22:07:24.085: INFO: Waiting up to 5m0s for pod "security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5" in namespace "security-context-241" to be "Succeeded or Failed"
Sep  3 22:07:24.099: INFO: Pod "security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.135627ms
Sep  3 22:07:26.101: INFO: Pod "security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015942758s
Sep  3 22:07:28.102: INFO: Pod "security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016689525s
Sep  3 22:07:30.101: INFO: Pod "security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015885288s
STEP: Saw pod success 09/03/22 22:07:30.101
Sep  3 22:07:30.101: INFO: Pod "security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5" satisfied condition "Succeeded or Failed"
Sep  3 22:07:30.103: INFO: Trying to get logs from node kind-worker2 pod security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5 container test-container: <nil>
STEP: delete the pod 09/03/22 22:07:30.107
Sep  3 22:07:30.112: INFO: Waiting for pod security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5 to disappear
Sep  3 22:07:30.115: INFO: Pod security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Sep  3 22:07:30.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-241" for this suite. 09/03/22 22:07:30.118
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":324,"skipped":6061,"failed":0}
------------------------------
• [SLOW TEST] [6.097 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:07:24.026
    Sep  3 22:07:24.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename security-context 09/03/22 22:07:24.027
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:24.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:24.061
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/03/22 22:07:24.068
    Sep  3 22:07:24.085: INFO: Waiting up to 5m0s for pod "security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5" in namespace "security-context-241" to be "Succeeded or Failed"
    Sep  3 22:07:24.099: INFO: Pod "security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.135627ms
    Sep  3 22:07:26.101: INFO: Pod "security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015942758s
    Sep  3 22:07:28.102: INFO: Pod "security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016689525s
    Sep  3 22:07:30.101: INFO: Pod "security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015885288s
    STEP: Saw pod success 09/03/22 22:07:30.101
    Sep  3 22:07:30.101: INFO: Pod "security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5" satisfied condition "Succeeded or Failed"
    Sep  3 22:07:30.103: INFO: Trying to get logs from node kind-worker2 pod security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5 container test-container: <nil>
    STEP: delete the pod 09/03/22 22:07:30.107
    Sep  3 22:07:30.112: INFO: Waiting for pod security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5 to disappear
    Sep  3 22:07:30.115: INFO: Pod security-context-10fd9e74-899b-4ab8-90f7-e54d19a25bd5 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Sep  3 22:07:30.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-241" for this suite. 09/03/22 22:07:30.118
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:07:30.134
Sep  3 22:07:30.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 22:07:30.135
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:30.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:30.163
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 09/03/22 22:07:30.166
Sep  3 22:07:30.171: INFO: Waiting up to 5m0s for pod "labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec" in namespace "downward-api-357" to be "running and ready"
Sep  3 22:07:30.173: INFO: Pod "labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.652203ms
Sep  3 22:07:30.173: INFO: The phase of Pod labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:07:32.176: INFO: Pod "labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004722575s
Sep  3 22:07:32.176: INFO: The phase of Pod labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec is Running (Ready = true)
Sep  3 22:07:32.176: INFO: Pod "labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec" satisfied condition "running and ready"
Sep  3 22:07:32.690: INFO: Successfully updated pod "labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Sep  3 22:07:36.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-357" for this suite. 09/03/22 22:07:36.707
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":325,"skipped":6096,"failed":0}
------------------------------
• [SLOW TEST] [6.578 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:07:30.134
    Sep  3 22:07:30.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 22:07:30.135
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:30.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:30.163
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 09/03/22 22:07:30.166
    Sep  3 22:07:30.171: INFO: Waiting up to 5m0s for pod "labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec" in namespace "downward-api-357" to be "running and ready"
    Sep  3 22:07:30.173: INFO: Pod "labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.652203ms
    Sep  3 22:07:30.173: INFO: The phase of Pod labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 22:07:32.176: INFO: Pod "labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004722575s
    Sep  3 22:07:32.176: INFO: The phase of Pod labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec is Running (Ready = true)
    Sep  3 22:07:32.176: INFO: Pod "labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec" satisfied condition "running and ready"
    Sep  3 22:07:32.690: INFO: Successfully updated pod "labelsupdate2b717ece-6c6d-479a-9afa-ad50f6bf79ec"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Sep  3 22:07:36.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-357" for this suite. 09/03/22 22:07:36.707
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:07:36.713
Sep  3 22:07:36.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 22:07:36.714
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:36.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:36.726
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-23edd6a9-b87d-44af-834f-8d9924174ad0 09/03/22 22:07:36.728
STEP: Creating a pod to test consume secrets 09/03/22 22:07:36.731
Sep  3 22:07:36.735: INFO: Waiting up to 5m0s for pod "pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce" in namespace "secrets-9680" to be "Succeeded or Failed"
Sep  3 22:07:36.737: INFO: Pod "pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365305ms
Sep  3 22:07:38.743: INFO: Pod "pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008051564s
Sep  3 22:07:40.741: INFO: Pod "pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005753506s
STEP: Saw pod success 09/03/22 22:07:40.741
Sep  3 22:07:40.741: INFO: Pod "pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce" satisfied condition "Succeeded or Failed"
Sep  3 22:07:40.743: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce container secret-volume-test: <nil>
STEP: delete the pod 09/03/22 22:07:40.747
Sep  3 22:07:40.752: INFO: Waiting for pod pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce to disappear
Sep  3 22:07:40.754: INFO: Pod pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Sep  3 22:07:40.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9680" for this suite. 09/03/22 22:07:40.757
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":326,"skipped":6113,"failed":0}
------------------------------
• [4.047 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:07:36.713
    Sep  3 22:07:36.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 22:07:36.714
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:36.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:36.726
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-23edd6a9-b87d-44af-834f-8d9924174ad0 09/03/22 22:07:36.728
    STEP: Creating a pod to test consume secrets 09/03/22 22:07:36.731
    Sep  3 22:07:36.735: INFO: Waiting up to 5m0s for pod "pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce" in namespace "secrets-9680" to be "Succeeded or Failed"
    Sep  3 22:07:36.737: INFO: Pod "pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365305ms
    Sep  3 22:07:38.743: INFO: Pod "pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008051564s
    Sep  3 22:07:40.741: INFO: Pod "pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005753506s
    STEP: Saw pod success 09/03/22 22:07:40.741
    Sep  3 22:07:40.741: INFO: Pod "pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce" satisfied condition "Succeeded or Failed"
    Sep  3 22:07:40.743: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce container secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 22:07:40.747
    Sep  3 22:07:40.752: INFO: Waiting for pod pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce to disappear
    Sep  3 22:07:40.754: INFO: Pod pod-secrets-b025f5ca-2a0d-4e18-afde-b59e5e4b7cce no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 22:07:40.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9680" for this suite. 09/03/22 22:07:40.757
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:07:40.766
Sep  3 22:07:40.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 22:07:40.767
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:40.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:40.778
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 09/03/22 22:07:40.78
Sep  3 22:07:40.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3" in namespace "downward-api-3439" to be "Succeeded or Failed"
Sep  3 22:07:40.790: INFO: Pod "downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.590612ms
Sep  3 22:07:42.793: INFO: Pod "downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00825602s
Sep  3 22:07:44.794: INFO: Pod "downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009102392s
STEP: Saw pod success 09/03/22 22:07:44.794
Sep  3 22:07:44.794: INFO: Pod "downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3" satisfied condition "Succeeded or Failed"
Sep  3 22:07:44.797: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3 container client-container: <nil>
STEP: delete the pod 09/03/22 22:07:44.801
Sep  3 22:07:44.807: INFO: Waiting for pod downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3 to disappear
Sep  3 22:07:44.809: INFO: Pod downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Sep  3 22:07:44.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3439" for this suite. 09/03/22 22:07:44.811
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":327,"skipped":6189,"failed":0}
------------------------------
• [4.048 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:07:40.766
    Sep  3 22:07:40.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 22:07:40.767
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:40.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:40.778
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 09/03/22 22:07:40.78
    Sep  3 22:07:40.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3" in namespace "downward-api-3439" to be "Succeeded or Failed"
    Sep  3 22:07:40.790: INFO: Pod "downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.590612ms
    Sep  3 22:07:42.793: INFO: Pod "downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00825602s
    Sep  3 22:07:44.794: INFO: Pod "downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009102392s
    STEP: Saw pod success 09/03/22 22:07:44.794
    Sep  3 22:07:44.794: INFO: Pod "downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3" satisfied condition "Succeeded or Failed"
    Sep  3 22:07:44.797: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3 container client-container: <nil>
    STEP: delete the pod 09/03/22 22:07:44.801
    Sep  3 22:07:44.807: INFO: Waiting for pod downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3 to disappear
    Sep  3 22:07:44.809: INFO: Pod downwardapi-volume-dcbeac16-14e5-4996-8137-02d1945a94d3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Sep  3 22:07:44.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3439" for this suite. 09/03/22 22:07:44.811
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:07:44.818
Sep  3 22:07:44.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename resourcequota 09/03/22 22:07:44.819
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:44.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:44.83
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 09/03/22 22:07:44.832
STEP: Creating a ResourceQuota 09/03/22 22:07:49.836
STEP: Ensuring resource quota status is calculated 09/03/22 22:07:49.84
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Sep  3 22:07:51.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3743" for this suite. 09/03/22 22:07:51.846
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":328,"skipped":6198,"failed":0}
------------------------------
• [SLOW TEST] [7.031 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:07:44.818
    Sep  3 22:07:44.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename resourcequota 09/03/22 22:07:44.819
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:44.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:44.83
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 09/03/22 22:07:44.832
    STEP: Creating a ResourceQuota 09/03/22 22:07:49.836
    STEP: Ensuring resource quota status is calculated 09/03/22 22:07:49.84
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Sep  3 22:07:51.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3743" for this suite. 09/03/22 22:07:51.846
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:07:51.862
Sep  3 22:07:51.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename container-probe 09/03/22 22:07:51.863
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:51.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:51.879
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e in namespace container-probe-7995 09/03/22 22:07:51.881
Sep  3 22:07:51.890: INFO: Waiting up to 5m0s for pod "busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e" in namespace "container-probe-7995" to be "not pending"
Sep  3 22:07:51.895: INFO: Pod "busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.642711ms
Sep  3 22:07:53.898: INFO: Pod "busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008087299s
Sep  3 22:07:53.898: INFO: Pod "busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e" satisfied condition "not pending"
Sep  3 22:07:53.898: INFO: Started pod busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e in namespace container-probe-7995
STEP: checking the pod's current state and verifying that restartCount is present 09/03/22 22:07:53.898
Sep  3 22:07:53.900: INFO: Initial restart count of pod busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e is 0
Sep  3 22:08:43.983: INFO: Restart count of pod container-probe-7995/busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e is now 1 (50.083559347s elapsed)
STEP: deleting the pod 09/03/22 22:08:43.983
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Sep  3 22:08:43.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7995" for this suite. 09/03/22 22:08:44
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":329,"skipped":6228,"failed":0}
------------------------------
• [SLOW TEST] [52.141 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:07:51.862
    Sep  3 22:07:51.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename container-probe 09/03/22 22:07:51.863
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:07:51.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:07:51.879
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e in namespace container-probe-7995 09/03/22 22:07:51.881
    Sep  3 22:07:51.890: INFO: Waiting up to 5m0s for pod "busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e" in namespace "container-probe-7995" to be "not pending"
    Sep  3 22:07:51.895: INFO: Pod "busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.642711ms
    Sep  3 22:07:53.898: INFO: Pod "busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008087299s
    Sep  3 22:07:53.898: INFO: Pod "busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e" satisfied condition "not pending"
    Sep  3 22:07:53.898: INFO: Started pod busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e in namespace container-probe-7995
    STEP: checking the pod's current state and verifying that restartCount is present 09/03/22 22:07:53.898
    Sep  3 22:07:53.900: INFO: Initial restart count of pod busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e is 0
    Sep  3 22:08:43.983: INFO: Restart count of pod container-probe-7995/busybox-c3a7bc59-3bb0-4995-8771-1db213ff280e is now 1 (50.083559347s elapsed)
    STEP: deleting the pod 09/03/22 22:08:43.983
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Sep  3 22:08:43.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7995" for this suite. 09/03/22 22:08:44
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:08:44.006
Sep  3 22:08:44.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename gc 09/03/22 22:08:44.007
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:08:44.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:08:44.018
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Sep  3 22:08:44.041: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ce8f0a34-fd83-415b-b3da-ff283cbf4b94", Controller:(*bool)(0xc00410b446), BlockOwnerDeletion:(*bool)(0xc00410b447)}}
Sep  3 22:08:44.051: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3554f790-5017-4722-87d7-71099be28c01", Controller:(*bool)(0xc0041333c6), BlockOwnerDeletion:(*bool)(0xc0041333c7)}}
Sep  3 22:08:44.066: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"257cd4be-f931-424e-b65e-d178b1da6849", Controller:(*bool)(0xc004133696), BlockOwnerDeletion:(*bool)(0xc004133697)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Sep  3 22:08:49.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1255" for this suite. 09/03/22 22:08:49.084
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":330,"skipped":6233,"failed":0}
------------------------------
• [SLOW TEST] [5.082 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:08:44.006
    Sep  3 22:08:44.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename gc 09/03/22 22:08:44.007
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:08:44.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:08:44.018
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Sep  3 22:08:44.041: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ce8f0a34-fd83-415b-b3da-ff283cbf4b94", Controller:(*bool)(0xc00410b446), BlockOwnerDeletion:(*bool)(0xc00410b447)}}
    Sep  3 22:08:44.051: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3554f790-5017-4722-87d7-71099be28c01", Controller:(*bool)(0xc0041333c6), BlockOwnerDeletion:(*bool)(0xc0041333c7)}}
    Sep  3 22:08:44.066: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"257cd4be-f931-424e-b65e-d178b1da6849", Controller:(*bool)(0xc004133696), BlockOwnerDeletion:(*bool)(0xc004133697)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Sep  3 22:08:49.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1255" for this suite. 09/03/22 22:08:49.084
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:08:49.089
Sep  3 22:08:49.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 22:08:49.09
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:08:49.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:08:49.104
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 22:08:49.116
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 22:08:49.73
STEP: Deploying the webhook pod 09/03/22 22:08:49.735
STEP: Wait for the deployment to be ready 09/03/22 22:08:49.742
Sep  3 22:08:49.754: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 22:08:51.76
STEP: Verifying the service has paired with the endpoint 09/03/22 22:08:51.765
Sep  3 22:08:52.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 09/03/22 22:08:52.768
STEP: Creating a configMap that does not comply to the validation webhook rules 09/03/22 22:08:52.78
STEP: Updating a validating webhook configuration's rules to not include the create operation 09/03/22 22:08:52.789
STEP: Creating a configMap that does not comply to the validation webhook rules 09/03/22 22:08:52.801
STEP: Patching a validating webhook configuration's rules to include the create operation 09/03/22 22:08:52.817
STEP: Creating a configMap that does not comply to the validation webhook rules 09/03/22 22:08:52.824
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 22:08:52.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4335" for this suite. 09/03/22 22:08:52.834
STEP: Destroying namespace "webhook-4335-markers" for this suite. 09/03/22 22:08:52.838
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":331,"skipped":6234,"failed":0}
------------------------------
• [3.845 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:08:49.089
    Sep  3 22:08:49.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 22:08:49.09
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:08:49.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:08:49.104
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 22:08:49.116
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 22:08:49.73
    STEP: Deploying the webhook pod 09/03/22 22:08:49.735
    STEP: Wait for the deployment to be ready 09/03/22 22:08:49.742
    Sep  3 22:08:49.754: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 22:08:51.76
    STEP: Verifying the service has paired with the endpoint 09/03/22 22:08:51.765
    Sep  3 22:08:52.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 09/03/22 22:08:52.768
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/03/22 22:08:52.78
    STEP: Updating a validating webhook configuration's rules to not include the create operation 09/03/22 22:08:52.789
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/03/22 22:08:52.801
    STEP: Patching a validating webhook configuration's rules to include the create operation 09/03/22 22:08:52.817
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/03/22 22:08:52.824
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 22:08:52.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4335" for this suite. 09/03/22 22:08:52.834
    STEP: Destroying namespace "webhook-4335-markers" for this suite. 09/03/22 22:08:52.838
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:08:52.935
Sep  3 22:08:52.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename ingress 09/03/22 22:08:52.935
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:08:52.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:08:52.955
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 09/03/22 22:08:52.958
STEP: getting /apis/networking.k8s.io 09/03/22 22:08:52.96
STEP: getting /apis/networking.k8s.iov1 09/03/22 22:08:52.961
STEP: creating 09/03/22 22:08:52.962
STEP: getting 09/03/22 22:08:52.974
STEP: listing 09/03/22 22:08:52.977
STEP: watching 09/03/22 22:08:52.979
Sep  3 22:08:52.979: INFO: starting watch
STEP: cluster-wide listing 09/03/22 22:08:52.98
STEP: cluster-wide watching 09/03/22 22:08:52.982
Sep  3 22:08:52.982: INFO: starting watch
STEP: patching 09/03/22 22:08:52.983
STEP: updating 09/03/22 22:08:52.987
Sep  3 22:08:52.993: INFO: waiting for watch events with expected annotations
Sep  3 22:08:52.993: INFO: saw patched and updated annotations
STEP: patching /status 09/03/22 22:08:52.994
STEP: updating /status 09/03/22 22:08:52.999
STEP: get /status 09/03/22 22:08:53.007
STEP: deleting 09/03/22 22:08:53.009
STEP: deleting a collection 09/03/22 22:08:53.016
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Sep  3 22:08:53.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6406" for this suite. 09/03/22 22:08:53.028
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":332,"skipped":6257,"failed":0}
------------------------------
• [0.097 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:08:52.935
    Sep  3 22:08:52.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename ingress 09/03/22 22:08:52.935
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:08:52.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:08:52.955
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 09/03/22 22:08:52.958
    STEP: getting /apis/networking.k8s.io 09/03/22 22:08:52.96
    STEP: getting /apis/networking.k8s.iov1 09/03/22 22:08:52.961
    STEP: creating 09/03/22 22:08:52.962
    STEP: getting 09/03/22 22:08:52.974
    STEP: listing 09/03/22 22:08:52.977
    STEP: watching 09/03/22 22:08:52.979
    Sep  3 22:08:52.979: INFO: starting watch
    STEP: cluster-wide listing 09/03/22 22:08:52.98
    STEP: cluster-wide watching 09/03/22 22:08:52.982
    Sep  3 22:08:52.982: INFO: starting watch
    STEP: patching 09/03/22 22:08:52.983
    STEP: updating 09/03/22 22:08:52.987
    Sep  3 22:08:52.993: INFO: waiting for watch events with expected annotations
    Sep  3 22:08:52.993: INFO: saw patched and updated annotations
    STEP: patching /status 09/03/22 22:08:52.994
    STEP: updating /status 09/03/22 22:08:52.999
    STEP: get /status 09/03/22 22:08:53.007
    STEP: deleting 09/03/22 22:08:53.009
    STEP: deleting a collection 09/03/22 22:08:53.016
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Sep  3 22:08:53.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-6406" for this suite. 09/03/22 22:08:53.028
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:08:53.032
Sep  3 22:08:53.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename ephemeral-containers-test 09/03/22 22:08:53.032
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:08:53.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:08:53.045
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 09/03/22 22:08:53.048
Sep  3 22:08:53.060: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6483" to be "running and ready"
Sep  3 22:08:53.062: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.513206ms
Sep  3 22:08:53.062: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:08:55.066: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005826721s
Sep  3 22:08:55.066: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Sep  3 22:08:55.066: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 09/03/22 22:08:55.067
Sep  3 22:08:55.084: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6483" to be "container debugger running"
Sep  3 22:08:55.087: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.531605ms
Sep  3 22:08:57.090: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005452344s
Sep  3 22:08:57.090: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 09/03/22 22:08:57.09
Sep  3 22:08:57.090: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6483 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 22:08:57.090: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 22:08:57.091: INFO: ExecWithOptions: Clientset creation
Sep  3 22:08:57.091: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-6483/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Sep  3 22:08:57.155: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Sep  3 22:08:57.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-6483" for this suite. 09/03/22 22:08:57.164
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":333,"skipped":6260,"failed":0}
------------------------------
• [4.135 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:08:53.032
    Sep  3 22:08:53.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename ephemeral-containers-test 09/03/22 22:08:53.032
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:08:53.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:08:53.045
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 09/03/22 22:08:53.048
    Sep  3 22:08:53.060: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6483" to be "running and ready"
    Sep  3 22:08:53.062: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.513206ms
    Sep  3 22:08:53.062: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 22:08:55.066: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005826721s
    Sep  3 22:08:55.066: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Sep  3 22:08:55.066: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 09/03/22 22:08:55.067
    Sep  3 22:08:55.084: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6483" to be "container debugger running"
    Sep  3 22:08:55.087: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.531605ms
    Sep  3 22:08:57.090: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005452344s
    Sep  3 22:08:57.090: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 09/03/22 22:08:57.09
    Sep  3 22:08:57.090: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6483 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 22:08:57.090: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 22:08:57.091: INFO: ExecWithOptions: Clientset creation
    Sep  3 22:08:57.091: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-6483/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Sep  3 22:08:57.155: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Sep  3 22:08:57.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-6483" for this suite. 09/03/22 22:08:57.164
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:08:57.176
Sep  3 22:08:57.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename var-expansion 09/03/22 22:08:57.178
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:08:57.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:08:57.196
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 09/03/22 22:08:57.199
STEP: waiting for pod running 09/03/22 22:08:57.204
Sep  3 22:08:57.204: INFO: Waiting up to 2m0s for pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c" in namespace "var-expansion-1968" to be "running"
Sep  3 22:08:57.208: INFO: Pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.331209ms
Sep  3 22:08:59.212: INFO: Pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007672749s
Sep  3 22:08:59.212: INFO: Pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c" satisfied condition "running"
STEP: creating a file in subpath 09/03/22 22:08:59.212
Sep  3 22:08:59.214: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1968 PodName:var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 22:08:59.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 22:08:59.214: INFO: ExecWithOptions: Clientset creation
Sep  3 22:08:59.214: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-1968/pods/var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 09/03/22 22:08:59.303
Sep  3 22:08:59.307: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1968 PodName:var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 22:08:59.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 22:08:59.312: INFO: ExecWithOptions: Clientset creation
Sep  3 22:08:59.313: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-1968/pods/var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 09/03/22 22:08:59.376
Sep  3 22:08:59.885: INFO: Successfully updated pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c"
STEP: waiting for annotated pod running 09/03/22 22:08:59.885
Sep  3 22:08:59.885: INFO: Waiting up to 2m0s for pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c" in namespace "var-expansion-1968" to be "running"
Sep  3 22:08:59.887: INFO: Pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.053404ms
Sep  3 22:08:59.888: INFO: Pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c" satisfied condition "running"
STEP: deleting the pod gracefully 09/03/22 22:08:59.888
Sep  3 22:08:59.888: INFO: Deleting pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c" in namespace "var-expansion-1968"
Sep  3 22:08:59.892: INFO: Wait up to 5m0s for pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Sep  3 22:09:33.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1968" for this suite. 09/03/22 22:09:33.901
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":334,"skipped":6277,"failed":0}
------------------------------
• [SLOW TEST] [36.728 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:08:57.176
    Sep  3 22:08:57.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename var-expansion 09/03/22 22:08:57.178
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:08:57.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:08:57.196
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 09/03/22 22:08:57.199
    STEP: waiting for pod running 09/03/22 22:08:57.204
    Sep  3 22:08:57.204: INFO: Waiting up to 2m0s for pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c" in namespace "var-expansion-1968" to be "running"
    Sep  3 22:08:57.208: INFO: Pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.331209ms
    Sep  3 22:08:59.212: INFO: Pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007672749s
    Sep  3 22:08:59.212: INFO: Pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c" satisfied condition "running"
    STEP: creating a file in subpath 09/03/22 22:08:59.212
    Sep  3 22:08:59.214: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1968 PodName:var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 22:08:59.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 22:08:59.214: INFO: ExecWithOptions: Clientset creation
    Sep  3 22:08:59.214: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-1968/pods/var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 09/03/22 22:08:59.303
    Sep  3 22:08:59.307: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1968 PodName:var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 22:08:59.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 22:08:59.312: INFO: ExecWithOptions: Clientset creation
    Sep  3 22:08:59.313: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-1968/pods/var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 09/03/22 22:08:59.376
    Sep  3 22:08:59.885: INFO: Successfully updated pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c"
    STEP: waiting for annotated pod running 09/03/22 22:08:59.885
    Sep  3 22:08:59.885: INFO: Waiting up to 2m0s for pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c" in namespace "var-expansion-1968" to be "running"
    Sep  3 22:08:59.887: INFO: Pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.053404ms
    Sep  3 22:08:59.888: INFO: Pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c" satisfied condition "running"
    STEP: deleting the pod gracefully 09/03/22 22:08:59.888
    Sep  3 22:08:59.888: INFO: Deleting pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c" in namespace "var-expansion-1968"
    Sep  3 22:08:59.892: INFO: Wait up to 5m0s for pod "var-expansion-bafbf80a-45c9-44e5-9a6a-ddf64f9b6c4c" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Sep  3 22:09:33.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1968" for this suite. 09/03/22 22:09:33.901
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:09:33.907
Sep  3 22:09:33.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 22:09:33.909
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:09:33.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:09:33.921
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 09/03/22 22:09:33.923
Sep  3 22:09:33.935: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba" in namespace "projected-6389" to be "Succeeded or Failed"
Sep  3 22:09:33.960: INFO: Pod "downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba": Phase="Pending", Reason="", readiness=false. Elapsed: 24.796452ms
Sep  3 22:09:35.963: INFO: Pod "downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027645289s
Sep  3 22:09:37.964: INFO: Pod "downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028819622s
STEP: Saw pod success 09/03/22 22:09:37.965
Sep  3 22:09:37.965: INFO: Pod "downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba" satisfied condition "Succeeded or Failed"
Sep  3 22:09:37.967: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba container client-container: <nil>
STEP: delete the pod 09/03/22 22:09:37.973
Sep  3 22:09:37.985: INFO: Waiting for pod downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba to disappear
Sep  3 22:09:37.987: INFO: Pod downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Sep  3 22:09:37.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6389" for this suite. 09/03/22 22:09:37.99
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":335,"skipped":6282,"failed":0}
------------------------------
• [4.089 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:09:33.907
    Sep  3 22:09:33.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 22:09:33.909
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:09:33.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:09:33.921
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 09/03/22 22:09:33.923
    Sep  3 22:09:33.935: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba" in namespace "projected-6389" to be "Succeeded or Failed"
    Sep  3 22:09:33.960: INFO: Pod "downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba": Phase="Pending", Reason="", readiness=false. Elapsed: 24.796452ms
    Sep  3 22:09:35.963: INFO: Pod "downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027645289s
    Sep  3 22:09:37.964: INFO: Pod "downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028819622s
    STEP: Saw pod success 09/03/22 22:09:37.965
    Sep  3 22:09:37.965: INFO: Pod "downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba" satisfied condition "Succeeded or Failed"
    Sep  3 22:09:37.967: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba container client-container: <nil>
    STEP: delete the pod 09/03/22 22:09:37.973
    Sep  3 22:09:37.985: INFO: Waiting for pod downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba to disappear
    Sep  3 22:09:37.987: INFO: Pod downwardapi-volume-9c09d650-7742-4170-9805-119c99941cba no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Sep  3 22:09:37.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6389" for this suite. 09/03/22 22:09:37.99
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:09:38.016
Sep  3 22:09:38.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 22:09:38.018
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:09:38.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:09:38.034
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-196 09/03/22 22:09:38.036
STEP: changing the ExternalName service to type=NodePort 09/03/22 22:09:38.039
STEP: creating replication controller externalname-service in namespace services-196 09/03/22 22:09:38.057
I0903 22:09:38.075396      24 runners.go:193] Created replication controller with name: externalname-service, namespace: services-196, replica count: 2
I0903 22:09:41.126529      24 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 22:09:41.126: INFO: Creating new exec pod
Sep  3 22:09:41.130: INFO: Waiting up to 5m0s for pod "execpodt8gwm" in namespace "services-196" to be "running"
Sep  3 22:09:41.133: INFO: Pod "execpodt8gwm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.926606ms
Sep  3 22:09:43.136: INFO: Pod "execpodt8gwm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006153198s
Sep  3 22:09:43.136: INFO: Pod "execpodt8gwm" satisfied condition "running"
Sep  3 22:09:44.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Sep  3 22:09:44.289: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  3 22:09:44.289: INFO: stdout: ""
Sep  3 22:09:45.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Sep  3 22:09:45.450: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  3 22:09:45.450: INFO: stdout: "externalname-service-6vwg9"
Sep  3 22:09:45.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.235.3 80'
Sep  3 22:09:45.623: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.235.3 80\nConnection to 10.96.235.3 80 port [tcp/http] succeeded!\n"
Sep  3 22:09:45.623: INFO: stdout: ""
Sep  3 22:09:46.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.235.3 80'
Sep  3 22:09:46.772: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.235.3 80\nConnection to 10.96.235.3 80 port [tcp/http] succeeded!\n"
Sep  3 22:09:46.772: INFO: stdout: ""
Sep  3 22:09:47.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.235.3 80'
Sep  3 22:09:47.764: INFO: stderr: "+ nc -v -t -w 2 10.96.235.3 80\n+ echo hostName\nConnection to 10.96.235.3 80 port [tcp/http] succeeded!\n"
Sep  3 22:09:47.764: INFO: stdout: ""
Sep  3 22:09:48.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.235.3 80'
Sep  3 22:09:48.766: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.235.3 80\nConnection to 10.96.235.3 80 port [tcp/http] succeeded!\n"
Sep  3 22:09:48.766: INFO: stdout: "externalname-service-lfp99"
Sep  3 22:09:48.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.2 32168'
Sep  3 22:09:48.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.2 32168\nConnection to 172.18.0.2 32168 port [tcp/*] succeeded!\n"
Sep  3 22:09:48.922: INFO: stdout: "externalname-service-lfp99"
Sep  3 22:09:48.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.3 32168'
Sep  3 22:09:49.077: INFO: stderr: "+ nc -v -t -w 2 172.18.0.3 32168\n+ echo hostName\nConnection to 172.18.0.3 32168 port [tcp/*] succeeded!\n"
Sep  3 22:09:49.078: INFO: stdout: "externalname-service-6vwg9"
Sep  3 22:09:49.078: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 22:09:49.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-196" for this suite. 09/03/22 22:09:49.114
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":336,"skipped":6310,"failed":0}
------------------------------
• [SLOW TEST] [11.108 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:09:38.016
    Sep  3 22:09:38.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 22:09:38.018
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:09:38.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:09:38.034
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-196 09/03/22 22:09:38.036
    STEP: changing the ExternalName service to type=NodePort 09/03/22 22:09:38.039
    STEP: creating replication controller externalname-service in namespace services-196 09/03/22 22:09:38.057
    I0903 22:09:38.075396      24 runners.go:193] Created replication controller with name: externalname-service, namespace: services-196, replica count: 2
    I0903 22:09:41.126529      24 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  3 22:09:41.126: INFO: Creating new exec pod
    Sep  3 22:09:41.130: INFO: Waiting up to 5m0s for pod "execpodt8gwm" in namespace "services-196" to be "running"
    Sep  3 22:09:41.133: INFO: Pod "execpodt8gwm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.926606ms
    Sep  3 22:09:43.136: INFO: Pod "execpodt8gwm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006153198s
    Sep  3 22:09:43.136: INFO: Pod "execpodt8gwm" satisfied condition "running"
    Sep  3 22:09:44.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Sep  3 22:09:44.289: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  3 22:09:44.289: INFO: stdout: ""
    Sep  3 22:09:45.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Sep  3 22:09:45.450: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  3 22:09:45.450: INFO: stdout: "externalname-service-6vwg9"
    Sep  3 22:09:45.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.235.3 80'
    Sep  3 22:09:45.623: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.235.3 80\nConnection to 10.96.235.3 80 port [tcp/http] succeeded!\n"
    Sep  3 22:09:45.623: INFO: stdout: ""
    Sep  3 22:09:46.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.235.3 80'
    Sep  3 22:09:46.772: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.235.3 80\nConnection to 10.96.235.3 80 port [tcp/http] succeeded!\n"
    Sep  3 22:09:46.772: INFO: stdout: ""
    Sep  3 22:09:47.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.235.3 80'
    Sep  3 22:09:47.764: INFO: stderr: "+ nc -v -t -w 2 10.96.235.3 80\n+ echo hostName\nConnection to 10.96.235.3 80 port [tcp/http] succeeded!\n"
    Sep  3 22:09:47.764: INFO: stdout: ""
    Sep  3 22:09:48.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.235.3 80'
    Sep  3 22:09:48.766: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.235.3 80\nConnection to 10.96.235.3 80 port [tcp/http] succeeded!\n"
    Sep  3 22:09:48.766: INFO: stdout: "externalname-service-lfp99"
    Sep  3 22:09:48.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.2 32168'
    Sep  3 22:09:48.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.18.0.2 32168\nConnection to 172.18.0.2 32168 port [tcp/*] succeeded!\n"
    Sep  3 22:09:48.922: INFO: stdout: "externalname-service-lfp99"
    Sep  3 22:09:48.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-196 exec execpodt8gwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.18.0.3 32168'
    Sep  3 22:09:49.077: INFO: stderr: "+ nc -v -t -w 2 172.18.0.3 32168\n+ echo hostName\nConnection to 172.18.0.3 32168 port [tcp/*] succeeded!\n"
    Sep  3 22:09:49.078: INFO: stdout: "externalname-service-6vwg9"
    Sep  3 22:09:49.078: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 22:09:49.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-196" for this suite. 09/03/22 22:09:49.114
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:09:49.124
Sep  3 22:09:49.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 22:09:49.125
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:09:49.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:09:49.148
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 09/03/22 22:09:49.153
Sep  3 22:09:49.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: rename a version 09/03/22 22:09:56.297
STEP: check the new version name is served 09/03/22 22:09:56.313
STEP: check the old version name is removed 09/03/22 22:09:59.02
STEP: check the other version is not changed 09/03/22 22:10:00.484
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 22:10:05.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7091" for this suite. 09/03/22 22:10:05.626
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":337,"skipped":6316,"failed":0}
------------------------------
• [SLOW TEST] [16.506 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:09:49.124
    Sep  3 22:09:49.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 22:09:49.125
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:09:49.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:09:49.148
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 09/03/22 22:09:49.153
    Sep  3 22:09:49.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: rename a version 09/03/22 22:09:56.297
    STEP: check the new version name is served 09/03/22 22:09:56.313
    STEP: check the old version name is removed 09/03/22 22:09:59.02
    STEP: check the other version is not changed 09/03/22 22:10:00.484
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 22:10:05.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7091" for this suite. 09/03/22 22:10:05.626
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:10:05.631
Sep  3 22:10:05.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 22:10:05.632
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:05.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:05.65
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-abc15b42-1a44-4448-91cf-2359adc79f73 09/03/22 22:10:05.664
STEP: Creating a pod to test consume secrets 09/03/22 22:10:05.666
Sep  3 22:10:05.670: INFO: Waiting up to 5m0s for pod "pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208" in namespace "secrets-5729" to be "Succeeded or Failed"
Sep  3 22:10:05.673: INFO: Pod "pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208": Phase="Pending", Reason="", readiness=false. Elapsed: 2.440805ms
Sep  3 22:10:07.676: INFO: Pod "pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005593831s
Sep  3 22:10:09.676: INFO: Pod "pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005545951s
STEP: Saw pod success 09/03/22 22:10:09.676
Sep  3 22:10:09.676: INFO: Pod "pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208" satisfied condition "Succeeded or Failed"
Sep  3 22:10:09.678: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208 container secret-volume-test: <nil>
STEP: delete the pod 09/03/22 22:10:09.682
Sep  3 22:10:09.687: INFO: Waiting for pod pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208 to disappear
Sep  3 22:10:09.689: INFO: Pod pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Sep  3 22:10:09.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5729" for this suite. 09/03/22 22:10:09.694
STEP: Destroying namespace "secret-namespace-144" for this suite. 09/03/22 22:10:09.697
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":338,"skipped":6322,"failed":0}
------------------------------
• [4.071 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:10:05.631
    Sep  3 22:10:05.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 22:10:05.632
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:05.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:05.65
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-abc15b42-1a44-4448-91cf-2359adc79f73 09/03/22 22:10:05.664
    STEP: Creating a pod to test consume secrets 09/03/22 22:10:05.666
    Sep  3 22:10:05.670: INFO: Waiting up to 5m0s for pod "pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208" in namespace "secrets-5729" to be "Succeeded or Failed"
    Sep  3 22:10:05.673: INFO: Pod "pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208": Phase="Pending", Reason="", readiness=false. Elapsed: 2.440805ms
    Sep  3 22:10:07.676: INFO: Pod "pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005593831s
    Sep  3 22:10:09.676: INFO: Pod "pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005545951s
    STEP: Saw pod success 09/03/22 22:10:09.676
    Sep  3 22:10:09.676: INFO: Pod "pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208" satisfied condition "Succeeded or Failed"
    Sep  3 22:10:09.678: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208 container secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 22:10:09.682
    Sep  3 22:10:09.687: INFO: Waiting for pod pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208 to disappear
    Sep  3 22:10:09.689: INFO: Pod pod-secrets-91f6780c-d406-402e-b2a5-8c52a020e208 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 22:10:09.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5729" for this suite. 09/03/22 22:10:09.694
    STEP: Destroying namespace "secret-namespace-144" for this suite. 09/03/22 22:10:09.697
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:10:09.707
Sep  3 22:10:09.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename disruption 09/03/22 22:10:09.708
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:09.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:09.72
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 09/03/22 22:10:09.728
STEP: Waiting for all pods to be running 09/03/22 22:10:11.761
Sep  3 22:10:11.772: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Sep  3 22:10:13.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5967" for this suite. 09/03/22 22:10:13.78
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":339,"skipped":6337,"failed":0}
------------------------------
• [4.077 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:10:09.707
    Sep  3 22:10:09.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename disruption 09/03/22 22:10:09.708
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:09.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:09.72
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 09/03/22 22:10:09.728
    STEP: Waiting for all pods to be running 09/03/22 22:10:11.761
    Sep  3 22:10:11.772: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Sep  3 22:10:13.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5967" for this suite. 09/03/22 22:10:13.78
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:10:13.785
Sep  3 22:10:13.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 22:10:13.786
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:13.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:13.796
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 09/03/22 22:10:13.799
Sep  3 22:10:13.804: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf" in namespace "downward-api-5132" to be "Succeeded or Failed"
Sep  3 22:10:13.806: INFO: Pod "downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.990504ms
Sep  3 22:10:15.809: INFO: Pod "downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005623457s
Sep  3 22:10:17.809: INFO: Pod "downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005830603s
STEP: Saw pod success 09/03/22 22:10:17.809
Sep  3 22:10:17.810: INFO: Pod "downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf" satisfied condition "Succeeded or Failed"
Sep  3 22:10:17.812: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf container client-container: <nil>
STEP: delete the pod 09/03/22 22:10:17.823
Sep  3 22:10:17.829: INFO: Waiting for pod downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf to disappear
Sep  3 22:10:17.831: INFO: Pod downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Sep  3 22:10:17.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5132" for this suite. 09/03/22 22:10:17.834
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":340,"skipped":6340,"failed":0}
------------------------------
• [4.053 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:10:13.785
    Sep  3 22:10:13.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 22:10:13.786
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:13.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:13.796
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 09/03/22 22:10:13.799
    Sep  3 22:10:13.804: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf" in namespace "downward-api-5132" to be "Succeeded or Failed"
    Sep  3 22:10:13.806: INFO: Pod "downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.990504ms
    Sep  3 22:10:15.809: INFO: Pod "downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005623457s
    Sep  3 22:10:17.809: INFO: Pod "downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005830603s
    STEP: Saw pod success 09/03/22 22:10:17.809
    Sep  3 22:10:17.810: INFO: Pod "downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf" satisfied condition "Succeeded or Failed"
    Sep  3 22:10:17.812: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf container client-container: <nil>
    STEP: delete the pod 09/03/22 22:10:17.823
    Sep  3 22:10:17.829: INFO: Waiting for pod downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf to disappear
    Sep  3 22:10:17.831: INFO: Pod downwardapi-volume-e1d2230e-f0ed-41b7-8905-9a31665d3bdf no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Sep  3 22:10:17.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5132" for this suite. 09/03/22 22:10:17.834
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:10:17.838
Sep  3 22:10:17.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pod-network-test 09/03/22 22:10:17.839
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:17.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:17.851
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6401 09/03/22 22:10:17.853
STEP: creating a selector 09/03/22 22:10:17.853
STEP: Creating the service pods in kubernetes 09/03/22 22:10:17.853
Sep  3 22:10:17.853: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  3 22:10:17.884: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6401" to be "running and ready"
Sep  3 22:10:17.891: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073012ms
Sep  3 22:10:17.891: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:10:19.895: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009789465s
Sep  3 22:10:19.895: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:10:21.895: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010371159s
Sep  3 22:10:21.895: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:10:23.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009178854s
Sep  3 22:10:23.894: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:10:25.895: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010171853s
Sep  3 22:10:25.895: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:10:27.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009658549s
Sep  3 22:10:27.895: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:10:29.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009398466s
Sep  3 22:10:29.894: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  3 22:10:29.894: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  3 22:10:29.896: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6401" to be "running and ready"
Sep  3 22:10:29.901: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.108209ms
Sep  3 22:10:29.901: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  3 22:10:29.901: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/03/22 22:10:29.903
Sep  3 22:10:29.908: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6401" to be "running"
Sep  3 22:10:29.920: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.774724ms
Sep  3 22:10:31.923: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015962385s
Sep  3 22:10:31.924: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  3 22:10:31.925: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  3 22:10:31.925: INFO: Breadth first check of 10.244.2.233 on host 172.18.0.2...
Sep  3 22:10:31.927: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.248:9080/dial?request=hostname&protocol=udp&host=10.244.2.233&port=8081&tries=1'] Namespace:pod-network-test-6401 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 22:10:31.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 22:10:31.928: INFO: ExecWithOptions: Clientset creation
Sep  3 22:10:31.928: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6401/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.248%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.233%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  3 22:10:32.017: INFO: Waiting for responses: map[]
Sep  3 22:10:32.017: INFO: reached 10.244.2.233 after 0/1 tries
Sep  3 22:10:32.017: INFO: Breadth first check of 10.244.1.247 on host 172.18.0.3...
Sep  3 22:10:32.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.248:9080/dial?request=hostname&protocol=udp&host=10.244.1.247&port=8081&tries=1'] Namespace:pod-network-test-6401 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 22:10:32.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 22:10:32.026: INFO: ExecWithOptions: Clientset creation
Sep  3 22:10:32.027: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6401/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.248%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.247%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  3 22:10:32.111: INFO: Waiting for responses: map[]
Sep  3 22:10:32.111: INFO: reached 10.244.1.247 after 0/1 tries
Sep  3 22:10:32.111: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Sep  3 22:10:32.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6401" for this suite. 09/03/22 22:10:32.113
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":341,"skipped":6344,"failed":0}
------------------------------
• [SLOW TEST] [14.279 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:10:17.838
    Sep  3 22:10:17.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pod-network-test 09/03/22 22:10:17.839
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:17.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:17.851
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6401 09/03/22 22:10:17.853
    STEP: creating a selector 09/03/22 22:10:17.853
    STEP: Creating the service pods in kubernetes 09/03/22 22:10:17.853
    Sep  3 22:10:17.853: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  3 22:10:17.884: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6401" to be "running and ready"
    Sep  3 22:10:17.891: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073012ms
    Sep  3 22:10:17.891: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 22:10:19.895: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009789465s
    Sep  3 22:10:19.895: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:10:21.895: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010371159s
    Sep  3 22:10:21.895: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:10:23.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009178854s
    Sep  3 22:10:23.894: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:10:25.895: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010171853s
    Sep  3 22:10:25.895: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:10:27.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009658549s
    Sep  3 22:10:27.895: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:10:29.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009398466s
    Sep  3 22:10:29.894: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  3 22:10:29.894: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  3 22:10:29.896: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6401" to be "running and ready"
    Sep  3 22:10:29.901: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.108209ms
    Sep  3 22:10:29.901: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  3 22:10:29.901: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/03/22 22:10:29.903
    Sep  3 22:10:29.908: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6401" to be "running"
    Sep  3 22:10:29.920: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.774724ms
    Sep  3 22:10:31.923: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015962385s
    Sep  3 22:10:31.924: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  3 22:10:31.925: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  3 22:10:31.925: INFO: Breadth first check of 10.244.2.233 on host 172.18.0.2...
    Sep  3 22:10:31.927: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.248:9080/dial?request=hostname&protocol=udp&host=10.244.2.233&port=8081&tries=1'] Namespace:pod-network-test-6401 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 22:10:31.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 22:10:31.928: INFO: ExecWithOptions: Clientset creation
    Sep  3 22:10:31.928: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6401/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.248%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.233%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  3 22:10:32.017: INFO: Waiting for responses: map[]
    Sep  3 22:10:32.017: INFO: reached 10.244.2.233 after 0/1 tries
    Sep  3 22:10:32.017: INFO: Breadth first check of 10.244.1.247 on host 172.18.0.3...
    Sep  3 22:10:32.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.248:9080/dial?request=hostname&protocol=udp&host=10.244.1.247&port=8081&tries=1'] Namespace:pod-network-test-6401 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 22:10:32.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 22:10:32.026: INFO: ExecWithOptions: Clientset creation
    Sep  3 22:10:32.027: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6401/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.248%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.247%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  3 22:10:32.111: INFO: Waiting for responses: map[]
    Sep  3 22:10:32.111: INFO: reached 10.244.1.247 after 0/1 tries
    Sep  3 22:10:32.111: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Sep  3 22:10:32.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6401" for this suite. 09/03/22 22:10:32.113
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:10:32.117
Sep  3 22:10:32.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename disruption 09/03/22 22:10:32.118
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:32.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:32.129
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 09/03/22 22:10:32.131
STEP: Waiting for the pdb to be processed 09/03/22 22:10:32.133
STEP: First trying to evict a pod which shouldn't be evictable 09/03/22 22:10:34.143
STEP: Waiting for all pods to be running 09/03/22 22:10:34.144
Sep  3 22:10:34.147: INFO: pods: 0 < 3
Sep  3 22:10:36.152: INFO: running pods: 2 < 3
STEP: locating a running pod 09/03/22 22:10:38.151
STEP: Updating the pdb to allow a pod to be evicted 09/03/22 22:10:38.161
STEP: Waiting for the pdb to be processed 09/03/22 22:10:38.165
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/03/22 22:10:38.172
STEP: Waiting for all pods to be running 09/03/22 22:10:38.173
STEP: Waiting for the pdb to observed all healthy pods 09/03/22 22:10:38.176
STEP: Patching the pdb to disallow a pod to be evicted 09/03/22 22:10:38.192
STEP: Waiting for the pdb to be processed 09/03/22 22:10:38.206
STEP: Waiting for all pods to be running 09/03/22 22:10:40.217
STEP: locating a running pod 09/03/22 22:10:40.219
STEP: Deleting the pdb to allow a pod to be evicted 09/03/22 22:10:40.226
STEP: Waiting for the pdb to be deleted 09/03/22 22:10:40.229
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/03/22 22:10:40.231
STEP: Waiting for all pods to be running 09/03/22 22:10:40.231
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Sep  3 22:10:40.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5119" for this suite. 09/03/22 22:10:40.268
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":342,"skipped":6350,"failed":0}
------------------------------
• [SLOW TEST] [8.168 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:10:32.117
    Sep  3 22:10:32.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename disruption 09/03/22 22:10:32.118
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:32.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:32.129
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 09/03/22 22:10:32.131
    STEP: Waiting for the pdb to be processed 09/03/22 22:10:32.133
    STEP: First trying to evict a pod which shouldn't be evictable 09/03/22 22:10:34.143
    STEP: Waiting for all pods to be running 09/03/22 22:10:34.144
    Sep  3 22:10:34.147: INFO: pods: 0 < 3
    Sep  3 22:10:36.152: INFO: running pods: 2 < 3
    STEP: locating a running pod 09/03/22 22:10:38.151
    STEP: Updating the pdb to allow a pod to be evicted 09/03/22 22:10:38.161
    STEP: Waiting for the pdb to be processed 09/03/22 22:10:38.165
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/03/22 22:10:38.172
    STEP: Waiting for all pods to be running 09/03/22 22:10:38.173
    STEP: Waiting for the pdb to observed all healthy pods 09/03/22 22:10:38.176
    STEP: Patching the pdb to disallow a pod to be evicted 09/03/22 22:10:38.192
    STEP: Waiting for the pdb to be processed 09/03/22 22:10:38.206
    STEP: Waiting for all pods to be running 09/03/22 22:10:40.217
    STEP: locating a running pod 09/03/22 22:10:40.219
    STEP: Deleting the pdb to allow a pod to be evicted 09/03/22 22:10:40.226
    STEP: Waiting for the pdb to be deleted 09/03/22 22:10:40.229
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/03/22 22:10:40.231
    STEP: Waiting for all pods to be running 09/03/22 22:10:40.231
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Sep  3 22:10:40.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5119" for this suite. 09/03/22 22:10:40.268
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:10:40.297
Sep  3 22:10:40.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename cronjob 09/03/22 22:10:40.298
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:40.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:40.331
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 09/03/22 22:10:40.334
STEP: creating 09/03/22 22:10:40.334
STEP: getting 09/03/22 22:10:40.339
STEP: listing 09/03/22 22:10:40.343
STEP: watching 09/03/22 22:10:40.349
Sep  3 22:10:40.350: INFO: starting watch
STEP: cluster-wide listing 09/03/22 22:10:40.351
STEP: cluster-wide watching 09/03/22 22:10:40.358
Sep  3 22:10:40.358: INFO: starting watch
STEP: patching 09/03/22 22:10:40.36
STEP: updating 09/03/22 22:10:40.369
Sep  3 22:10:40.375: INFO: waiting for watch events with expected annotations
Sep  3 22:10:40.376: INFO: saw patched and updated annotations
STEP: patching /status 09/03/22 22:10:40.376
STEP: updating /status 09/03/22 22:10:40.38
STEP: get /status 09/03/22 22:10:40.392
STEP: deleting 09/03/22 22:10:40.398
STEP: deleting a collection 09/03/22 22:10:40.414
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Sep  3 22:10:40.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9734" for this suite. 09/03/22 22:10:40.427
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":343,"skipped":6356,"failed":0}
------------------------------
• [0.135 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:10:40.297
    Sep  3 22:10:40.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename cronjob 09/03/22 22:10:40.298
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:40.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:40.331
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 09/03/22 22:10:40.334
    STEP: creating 09/03/22 22:10:40.334
    STEP: getting 09/03/22 22:10:40.339
    STEP: listing 09/03/22 22:10:40.343
    STEP: watching 09/03/22 22:10:40.349
    Sep  3 22:10:40.350: INFO: starting watch
    STEP: cluster-wide listing 09/03/22 22:10:40.351
    STEP: cluster-wide watching 09/03/22 22:10:40.358
    Sep  3 22:10:40.358: INFO: starting watch
    STEP: patching 09/03/22 22:10:40.36
    STEP: updating 09/03/22 22:10:40.369
    Sep  3 22:10:40.375: INFO: waiting for watch events with expected annotations
    Sep  3 22:10:40.376: INFO: saw patched and updated annotations
    STEP: patching /status 09/03/22 22:10:40.376
    STEP: updating /status 09/03/22 22:10:40.38
    STEP: get /status 09/03/22 22:10:40.392
    STEP: deleting 09/03/22 22:10:40.398
    STEP: deleting a collection 09/03/22 22:10:40.414
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Sep  3 22:10:40.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9734" for this suite. 09/03/22 22:10:40.427
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:10:40.439
Sep  3 22:10:40.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename daemonsets 09/03/22 22:10:40.44
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:40.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:40.455
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Sep  3 22:10:40.473: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 09/03/22 22:10:40.476
Sep  3 22:10:40.480: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 22:10:40.483: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 22:10:40.483: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 22:10:41.486: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 22:10:41.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  3 22:10:41.494: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 22:10:42.490: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 22:10:42.492: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  3 22:10:42.493: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 09/03/22 22:10:42.503
STEP: Check that daemon pods images are updated. 09/03/22 22:10:42.514
Sep  3 22:10:42.517: INFO: Wrong image for pod: daemon-set-2fv4m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Sep  3 22:10:42.517: INFO: Wrong image for pod: daemon-set-djfml. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Sep  3 22:10:42.519: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 22:10:43.523: INFO: Wrong image for pod: daemon-set-2fv4m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Sep  3 22:10:43.525: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 22:10:44.522: INFO: Wrong image for pod: daemon-set-2fv4m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Sep  3 22:10:44.522: INFO: Pod daemon-set-97j28 is not available
Sep  3 22:10:44.525: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 22:10:45.547: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 22:10:46.524: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 22:10:47.522: INFO: Pod daemon-set-mdgs9 is not available
Sep  3 22:10:47.525: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 09/03/22 22:10:47.525
Sep  3 22:10:47.527: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 22:10:47.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  3 22:10:47.530: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 22:10:48.534: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 22:10:48.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  3 22:10:48.539: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  3 22:10:49.533: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  3 22:10:49.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  3 22:10:49.536: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 09/03/22 22:10:49.546
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3499, will wait for the garbage collector to delete the pods 09/03/22 22:10:49.546
Sep  3 22:10:49.602: INFO: Deleting DaemonSet.extensions daemon-set took: 3.912608ms
Sep  3 22:10:49.703: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.449898ms
Sep  3 22:10:51.609: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  3 22:10:51.609: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  3 22:10:51.612: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29375"},"items":null}

Sep  3 22:10:51.614: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29375"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Sep  3 22:10:51.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3499" for this suite. 09/03/22 22:10:51.622
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":344,"skipped":6364,"failed":0}
------------------------------
• [SLOW TEST] [11.189 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:10:40.439
    Sep  3 22:10:40.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename daemonsets 09/03/22 22:10:40.44
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:40.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:40.455
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Sep  3 22:10:40.473: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 09/03/22 22:10:40.476
    Sep  3 22:10:40.480: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 22:10:40.483: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 22:10:40.483: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 22:10:41.486: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 22:10:41.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  3 22:10:41.494: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 22:10:42.490: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 22:10:42.492: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  3 22:10:42.493: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 09/03/22 22:10:42.503
    STEP: Check that daemon pods images are updated. 09/03/22 22:10:42.514
    Sep  3 22:10:42.517: INFO: Wrong image for pod: daemon-set-2fv4m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Sep  3 22:10:42.517: INFO: Wrong image for pod: daemon-set-djfml. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Sep  3 22:10:42.519: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 22:10:43.523: INFO: Wrong image for pod: daemon-set-2fv4m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Sep  3 22:10:43.525: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 22:10:44.522: INFO: Wrong image for pod: daemon-set-2fv4m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Sep  3 22:10:44.522: INFO: Pod daemon-set-97j28 is not available
    Sep  3 22:10:44.525: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 22:10:45.547: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 22:10:46.524: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 22:10:47.522: INFO: Pod daemon-set-mdgs9 is not available
    Sep  3 22:10:47.525: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 09/03/22 22:10:47.525
    Sep  3 22:10:47.527: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 22:10:47.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  3 22:10:47.530: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 22:10:48.534: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 22:10:48.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  3 22:10:48.539: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  3 22:10:49.533: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  3 22:10:49.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  3 22:10:49.536: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 09/03/22 22:10:49.546
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3499, will wait for the garbage collector to delete the pods 09/03/22 22:10:49.546
    Sep  3 22:10:49.602: INFO: Deleting DaemonSet.extensions daemon-set took: 3.912608ms
    Sep  3 22:10:49.703: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.449898ms
    Sep  3 22:10:51.609: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  3 22:10:51.609: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  3 22:10:51.612: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29375"},"items":null}

    Sep  3 22:10:51.614: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29375"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Sep  3 22:10:51.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3499" for this suite. 09/03/22 22:10:51.622
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:10:51.631
Sep  3 22:10:51.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pod-network-test 09/03/22 22:10:51.632
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:51.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:51.644
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-1410 09/03/22 22:10:51.646
STEP: creating a selector 09/03/22 22:10:51.646
STEP: Creating the service pods in kubernetes 09/03/22 22:10:51.646
Sep  3 22:10:51.646: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  3 22:10:51.657: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1410" to be "running and ready"
Sep  3 22:10:51.660: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.329704ms
Sep  3 22:10:51.660: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:10:53.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005125859s
Sep  3 22:10:53.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:10:55.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005311709s
Sep  3 22:10:55.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:10:57.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006629261s
Sep  3 22:10:57.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:10:59.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005291508s
Sep  3 22:10:59.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:11:01.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005929559s
Sep  3 22:11:01.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:11:03.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.005661374s
Sep  3 22:11:03.663: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  3 22:11:03.663: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  3 22:11:03.665: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1410" to be "running and ready"
Sep  3 22:11:03.667: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 1.615803ms
Sep  3 22:11:03.667: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  3 22:11:05.671: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.005401625s
Sep  3 22:11:05.671: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  3 22:11:07.670: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.004733038s
Sep  3 22:11:07.670: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  3 22:11:09.670: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.004523652s
Sep  3 22:11:09.670: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  3 22:11:11.670: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.004345276s
Sep  3 22:11:11.670: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  3 22:11:13.673: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.007948108s
Sep  3 22:11:13.674: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  3 22:11:13.674: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/03/22 22:11:13.676
Sep  3 22:11:13.694: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1410" to be "running"
Sep  3 22:11:13.703: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.635117ms
Sep  3 22:11:15.706: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012222146s
Sep  3 22:11:15.706: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  3 22:11:15.708: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1410" to be "running"
Sep  3 22:11:15.710: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.830103ms
Sep  3 22:11:15.710: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  3 22:11:15.711: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  3 22:11:15.711: INFO: Going to poll 10.244.2.238 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep  3 22:11:15.713: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.238:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1410 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 22:11:15.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 22:11:15.714: INFO: ExecWithOptions: Clientset creation
Sep  3 22:11:15.714: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1410/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.238%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  3 22:11:15.780: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  3 22:11:15.781: INFO: Going to poll 10.244.1.253 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep  3 22:11:15.783: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.253:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1410 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 22:11:15.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 22:11:15.783: INFO: ExecWithOptions: Clientset creation
Sep  3 22:11:15.783: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1410/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.253%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  3 22:11:15.854: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Sep  3 22:11:15.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1410" for this suite. 09/03/22 22:11:15.857
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":345,"skipped":6365,"failed":0}
------------------------------
• [SLOW TEST] [24.230 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:10:51.631
    Sep  3 22:10:51.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pod-network-test 09/03/22 22:10:51.632
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:10:51.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:10:51.644
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-1410 09/03/22 22:10:51.646
    STEP: creating a selector 09/03/22 22:10:51.646
    STEP: Creating the service pods in kubernetes 09/03/22 22:10:51.646
    Sep  3 22:10:51.646: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  3 22:10:51.657: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1410" to be "running and ready"
    Sep  3 22:10:51.660: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.329704ms
    Sep  3 22:10:51.660: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 22:10:53.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005125859s
    Sep  3 22:10:53.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:10:55.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005311709s
    Sep  3 22:10:55.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:10:57.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006629261s
    Sep  3 22:10:57.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:10:59.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005291508s
    Sep  3 22:10:59.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:11:01.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005929559s
    Sep  3 22:11:01.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:11:03.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.005661374s
    Sep  3 22:11:03.663: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  3 22:11:03.663: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  3 22:11:03.665: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1410" to be "running and ready"
    Sep  3 22:11:03.667: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 1.615803ms
    Sep  3 22:11:03.667: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  3 22:11:05.671: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.005401625s
    Sep  3 22:11:05.671: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  3 22:11:07.670: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.004733038s
    Sep  3 22:11:07.670: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  3 22:11:09.670: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.004523652s
    Sep  3 22:11:09.670: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  3 22:11:11.670: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.004345276s
    Sep  3 22:11:11.670: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  3 22:11:13.673: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.007948108s
    Sep  3 22:11:13.674: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  3 22:11:13.674: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/03/22 22:11:13.676
    Sep  3 22:11:13.694: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1410" to be "running"
    Sep  3 22:11:13.703: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.635117ms
    Sep  3 22:11:15.706: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012222146s
    Sep  3 22:11:15.706: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  3 22:11:15.708: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1410" to be "running"
    Sep  3 22:11:15.710: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.830103ms
    Sep  3 22:11:15.710: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  3 22:11:15.711: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  3 22:11:15.711: INFO: Going to poll 10.244.2.238 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Sep  3 22:11:15.713: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.238:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1410 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 22:11:15.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 22:11:15.714: INFO: ExecWithOptions: Clientset creation
    Sep  3 22:11:15.714: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1410/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.238%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  3 22:11:15.780: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  3 22:11:15.781: INFO: Going to poll 10.244.1.253 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Sep  3 22:11:15.783: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.253:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1410 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 22:11:15.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 22:11:15.783: INFO: ExecWithOptions: Clientset creation
    Sep  3 22:11:15.783: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1410/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.253%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  3 22:11:15.854: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Sep  3 22:11:15.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1410" for this suite. 09/03/22 22:11:15.857
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:11:15.866
Sep  3 22:11:15.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename svcaccounts 09/03/22 22:11:15.867
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:15.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:15.88
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  09/03/22 22:11:15.883
Sep  3 22:11:15.888: INFO: Waiting up to 5m0s for pod "test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da" in namespace "svcaccounts-3358" to be "Succeeded or Failed"
Sep  3 22:11:15.895: INFO: Pod "test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da": Phase="Pending", Reason="", readiness=false. Elapsed: 6.860912ms
Sep  3 22:11:17.899: INFO: Pod "test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01097425s
Sep  3 22:11:19.898: INFO: Pod "test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01050423s
STEP: Saw pod success 09/03/22 22:11:19.898
Sep  3 22:11:19.899: INFO: Pod "test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da" satisfied condition "Succeeded or Failed"
Sep  3 22:11:19.900: INFO: Trying to get logs from node kind-worker pod test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da container agnhost-container: <nil>
STEP: delete the pod 09/03/22 22:11:19.906
Sep  3 22:11:19.914: INFO: Waiting for pod test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da to disappear
Sep  3 22:11:19.917: INFO: Pod test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Sep  3 22:11:19.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3358" for this suite. 09/03/22 22:11:19.92
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":346,"skipped":6378,"failed":0}
------------------------------
• [4.057 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:11:15.866
    Sep  3 22:11:15.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename svcaccounts 09/03/22 22:11:15.867
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:15.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:15.88
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  09/03/22 22:11:15.883
    Sep  3 22:11:15.888: INFO: Waiting up to 5m0s for pod "test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da" in namespace "svcaccounts-3358" to be "Succeeded or Failed"
    Sep  3 22:11:15.895: INFO: Pod "test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da": Phase="Pending", Reason="", readiness=false. Elapsed: 6.860912ms
    Sep  3 22:11:17.899: INFO: Pod "test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01097425s
    Sep  3 22:11:19.898: INFO: Pod "test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01050423s
    STEP: Saw pod success 09/03/22 22:11:19.898
    Sep  3 22:11:19.899: INFO: Pod "test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da" satisfied condition "Succeeded or Failed"
    Sep  3 22:11:19.900: INFO: Trying to get logs from node kind-worker pod test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da container agnhost-container: <nil>
    STEP: delete the pod 09/03/22 22:11:19.906
    Sep  3 22:11:19.914: INFO: Waiting for pod test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da to disappear
    Sep  3 22:11:19.917: INFO: Pod test-pod-24a8edb5-d766-4e1b-bfb9-cba66b35f4da no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Sep  3 22:11:19.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3358" for this suite. 09/03/22 22:11:19.92
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:11:19.926
Sep  3 22:11:19.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename runtimeclass 09/03/22 22:11:19.927
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:19.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:19.939
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Sep  3 22:11:19.983: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8767 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Sep  3 22:11:19.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8767" for this suite. 09/03/22 22:11:19.995
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":347,"skipped":6384,"failed":0}
------------------------------
• [0.074 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:11:19.926
    Sep  3 22:11:19.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename runtimeclass 09/03/22 22:11:19.927
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:19.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:19.939
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Sep  3 22:11:19.983: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8767 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Sep  3 22:11:19.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8767" for this suite. 09/03/22 22:11:19.995
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:11:20.002
Sep  3 22:11:20.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir-wrapper 09/03/22 22:11:20.003
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:20.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:20.023
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Sep  3 22:11:20.035: INFO: Waiting up to 5m0s for pod "pod-secrets-6087ebda-76d4-47f3-b149-bb64e98a94c9" in namespace "emptydir-wrapper-7929" to be "running and ready"
Sep  3 22:11:20.038: INFO: Pod "pod-secrets-6087ebda-76d4-47f3-b149-bb64e98a94c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.687305ms
Sep  3 22:11:20.038: INFO: The phase of Pod pod-secrets-6087ebda-76d4-47f3-b149-bb64e98a94c9 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:11:22.041: INFO: Pod "pod-secrets-6087ebda-76d4-47f3-b149-bb64e98a94c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005817691s
Sep  3 22:11:22.041: INFO: The phase of Pod pod-secrets-6087ebda-76d4-47f3-b149-bb64e98a94c9 is Running (Ready = true)
Sep  3 22:11:22.041: INFO: Pod "pod-secrets-6087ebda-76d4-47f3-b149-bb64e98a94c9" satisfied condition "running and ready"
STEP: Cleaning up the secret 09/03/22 22:11:22.043
STEP: Cleaning up the configmap 09/03/22 22:11:22.046
STEP: Cleaning up the pod 09/03/22 22:11:22.05
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Sep  3 22:11:22.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7929" for this suite. 09/03/22 22:11:22.066
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":348,"skipped":6419,"failed":0}
------------------------------
• [2.068 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:11:20.002
    Sep  3 22:11:20.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir-wrapper 09/03/22 22:11:20.003
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:20.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:20.023
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Sep  3 22:11:20.035: INFO: Waiting up to 5m0s for pod "pod-secrets-6087ebda-76d4-47f3-b149-bb64e98a94c9" in namespace "emptydir-wrapper-7929" to be "running and ready"
    Sep  3 22:11:20.038: INFO: Pod "pod-secrets-6087ebda-76d4-47f3-b149-bb64e98a94c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.687305ms
    Sep  3 22:11:20.038: INFO: The phase of Pod pod-secrets-6087ebda-76d4-47f3-b149-bb64e98a94c9 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 22:11:22.041: INFO: Pod "pod-secrets-6087ebda-76d4-47f3-b149-bb64e98a94c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005817691s
    Sep  3 22:11:22.041: INFO: The phase of Pod pod-secrets-6087ebda-76d4-47f3-b149-bb64e98a94c9 is Running (Ready = true)
    Sep  3 22:11:22.041: INFO: Pod "pod-secrets-6087ebda-76d4-47f3-b149-bb64e98a94c9" satisfied condition "running and ready"
    STEP: Cleaning up the secret 09/03/22 22:11:22.043
    STEP: Cleaning up the configmap 09/03/22 22:11:22.046
    STEP: Cleaning up the pod 09/03/22 22:11:22.05
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Sep  3 22:11:22.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-7929" for this suite. 09/03/22 22:11:22.066
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:11:22.076
Sep  3 22:11:22.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename webhook 09/03/22 22:11:22.078
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:22.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:22.089
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 09/03/22 22:11:22.098
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 22:11:22.414
STEP: Deploying the webhook pod 09/03/22 22:11:22.419
STEP: Wait for the deployment to be ready 09/03/22 22:11:22.426
Sep  3 22:11:22.461: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/03/22 22:11:24.469
STEP: Verifying the service has paired with the endpoint 09/03/22 22:11:24.484
Sep  3 22:11:25.487: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/03/22 22:11:25.49
STEP: create a pod that should be updated by the webhook 09/03/22 22:11:25.503
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 22:11:25.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1009" for this suite. 09/03/22 22:11:25.538
STEP: Destroying namespace "webhook-1009-markers" for this suite. 09/03/22 22:11:25.546
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":349,"skipped":6441,"failed":0}
------------------------------
• [3.610 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:11:22.076
    Sep  3 22:11:22.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename webhook 09/03/22 22:11:22.078
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:22.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:22.089
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 09/03/22 22:11:22.098
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/03/22 22:11:22.414
    STEP: Deploying the webhook pod 09/03/22 22:11:22.419
    STEP: Wait for the deployment to be ready 09/03/22 22:11:22.426
    Sep  3 22:11:22.461: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/03/22 22:11:24.469
    STEP: Verifying the service has paired with the endpoint 09/03/22 22:11:24.484
    Sep  3 22:11:25.487: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/03/22 22:11:25.49
    STEP: create a pod that should be updated by the webhook 09/03/22 22:11:25.503
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 22:11:25.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1009" for this suite. 09/03/22 22:11:25.538
    STEP: Destroying namespace "webhook-1009-markers" for this suite. 09/03/22 22:11:25.546
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:11:25.69
Sep  3 22:11:25.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 22:11:25.691
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:25.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:25.708
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-5465 09/03/22 22:11:25.712
STEP: creating service affinity-clusterip-transition in namespace services-5465 09/03/22 22:11:25.713
STEP: creating replication controller affinity-clusterip-transition in namespace services-5465 09/03/22 22:11:25.721
I0903 22:11:25.726758      24 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5465, replica count: 3
I0903 22:11:28.778292      24 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 22:11:28.782: INFO: Creating new exec pod
Sep  3 22:11:28.786: INFO: Waiting up to 5m0s for pod "execpod-affinitykr8bf" in namespace "services-5465" to be "running"
Sep  3 22:11:28.788: INFO: Pod "execpod-affinitykr8bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472205ms
Sep  3 22:11:30.791: INFO: Pod "execpod-affinitykr8bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.005607389s
Sep  3 22:11:30.791: INFO: Pod "execpod-affinitykr8bf" satisfied condition "running"
Sep  3 22:11:31.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5465 exec execpod-affinitykr8bf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Sep  3 22:11:31.967: INFO: stderr: "+ + nc -v -t -w 2 affinity-clusterip-transition 80\necho hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep  3 22:11:31.967: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 22:11:31.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5465 exec execpod-affinitykr8bf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.213.21 80'
Sep  3 22:11:32.142: INFO: stderr: "+ nc -v -t -w 2 10.96.213.21 80\nConnection to 10.96.213.21 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Sep  3 22:11:32.142: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep  3 22:11:32.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5465 exec execpod-affinitykr8bf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.213.21:80/ ; done'
Sep  3 22:11:32.489: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n"
Sep  3 22:11:32.489: INFO: stdout: "\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-v7hhr\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-v7hhr\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-v7hhr\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-z2t8z"
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-v7hhr
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-v7hhr
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-v7hhr
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5465 exec execpod-affinitykr8bf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.213.21:80/ ; done'
Sep  3 22:11:32.817: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n"
Sep  3 22:11:32.817: INFO: stdout: "\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z"
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
Sep  3 22:11:32.817: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5465, will wait for the garbage collector to delete the pods 09/03/22 22:11:32.825
Sep  3 22:11:32.892: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.409111ms
Sep  3 22:11:32.992: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.178894ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 22:11:34.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5465" for this suite. 09/03/22 22:11:34.815
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":350,"skipped":6452,"failed":0}
------------------------------
• [SLOW TEST] [9.130 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:11:25.69
    Sep  3 22:11:25.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 22:11:25.691
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:25.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:25.708
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-5465 09/03/22 22:11:25.712
    STEP: creating service affinity-clusterip-transition in namespace services-5465 09/03/22 22:11:25.713
    STEP: creating replication controller affinity-clusterip-transition in namespace services-5465 09/03/22 22:11:25.721
    I0903 22:11:25.726758      24 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5465, replica count: 3
    I0903 22:11:28.778292      24 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  3 22:11:28.782: INFO: Creating new exec pod
    Sep  3 22:11:28.786: INFO: Waiting up to 5m0s for pod "execpod-affinitykr8bf" in namespace "services-5465" to be "running"
    Sep  3 22:11:28.788: INFO: Pod "execpod-affinitykr8bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472205ms
    Sep  3 22:11:30.791: INFO: Pod "execpod-affinitykr8bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.005607389s
    Sep  3 22:11:30.791: INFO: Pod "execpod-affinitykr8bf" satisfied condition "running"
    Sep  3 22:11:31.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5465 exec execpod-affinitykr8bf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Sep  3 22:11:31.967: INFO: stderr: "+ + nc -v -t -w 2 affinity-clusterip-transition 80\necho hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Sep  3 22:11:31.967: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 22:11:31.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5465 exec execpod-affinitykr8bf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.213.21 80'
    Sep  3 22:11:32.142: INFO: stderr: "+ nc -v -t -w 2 10.96.213.21 80\nConnection to 10.96.213.21 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Sep  3 22:11:32.142: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Sep  3 22:11:32.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5465 exec execpod-affinitykr8bf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.213.21:80/ ; done'
    Sep  3 22:11:32.489: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n"
    Sep  3 22:11:32.489: INFO: stdout: "\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-v7hhr\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-v7hhr\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-v7hhr\naffinity-clusterip-transition-snfp5\naffinity-clusterip-transition-z2t8z"
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-v7hhr
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-v7hhr
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-v7hhr
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-snfp5
    Sep  3 22:11:32.489: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1880162351 --namespace=services-5465 exec execpod-affinitykr8bf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.213.21:80/ ; done'
    Sep  3 22:11:32.817: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.213.21:80/\n"
    Sep  3 22:11:32.817: INFO: stdout: "\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z\naffinity-clusterip-transition-z2t8z"
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Received response from host: affinity-clusterip-transition-z2t8z
    Sep  3 22:11:32.817: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5465, will wait for the garbage collector to delete the pods 09/03/22 22:11:32.825
    Sep  3 22:11:32.892: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.409111ms
    Sep  3 22:11:32.992: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.178894ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 22:11:34.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5465" for this suite. 09/03/22 22:11:34.815
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:11:34.82
Sep  3 22:11:34.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename projected 09/03/22 22:11:34.821
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:34.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:34.838
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 09/03/22 22:11:34.843
Sep  3 22:11:34.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192" in namespace "projected-9120" to be "Succeeded or Failed"
Sep  3 22:11:34.854: INFO: Pod "downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106508ms
Sep  3 22:11:36.857: INFO: Pod "downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006697681s
Sep  3 22:11:38.859: INFO: Pod "downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009253554s
STEP: Saw pod success 09/03/22 22:11:38.859
Sep  3 22:11:38.859: INFO: Pod "downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192" satisfied condition "Succeeded or Failed"
Sep  3 22:11:38.863: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192 container client-container: <nil>
STEP: delete the pod 09/03/22 22:11:38.868
Sep  3 22:11:38.876: INFO: Waiting for pod downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192 to disappear
Sep  3 22:11:38.878: INFO: Pod downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Sep  3 22:11:38.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9120" for this suite. 09/03/22 22:11:38.881
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":351,"skipped":6453,"failed":0}
------------------------------
• [4.064 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:11:34.82
    Sep  3 22:11:34.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename projected 09/03/22 22:11:34.821
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:34.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:34.838
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 09/03/22 22:11:34.843
    Sep  3 22:11:34.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192" in namespace "projected-9120" to be "Succeeded or Failed"
    Sep  3 22:11:34.854: INFO: Pod "downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106508ms
    Sep  3 22:11:36.857: INFO: Pod "downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006697681s
    Sep  3 22:11:38.859: INFO: Pod "downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009253554s
    STEP: Saw pod success 09/03/22 22:11:38.859
    Sep  3 22:11:38.859: INFO: Pod "downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192" satisfied condition "Succeeded or Failed"
    Sep  3 22:11:38.863: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192 container client-container: <nil>
    STEP: delete the pod 09/03/22 22:11:38.868
    Sep  3 22:11:38.876: INFO: Waiting for pod downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192 to disappear
    Sep  3 22:11:38.878: INFO: Pod downwardapi-volume-751636ae-b5a4-48e6-90a1-e46f2a079192 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Sep  3 22:11:38.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9120" for this suite. 09/03/22 22:11:38.881
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:11:38.887
Sep  3 22:11:38.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename emptydir 09/03/22 22:11:38.889
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:38.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:38.9
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/03/22 22:11:38.902
Sep  3 22:11:38.908: INFO: Waiting up to 5m0s for pod "pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5" in namespace "emptydir-1289" to be "Succeeded or Failed"
Sep  3 22:11:38.910: INFO: Pod "pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.383205ms
Sep  3 22:11:40.913: INFO: Pod "pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005511379s
Sep  3 22:11:42.914: INFO: Pod "pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006265518s
Sep  3 22:11:44.914: INFO: Pod "pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00592699s
STEP: Saw pod success 09/03/22 22:11:44.914
Sep  3 22:11:44.914: INFO: Pod "pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5" satisfied condition "Succeeded or Failed"
Sep  3 22:11:44.916: INFO: Trying to get logs from node kind-worker2 pod pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5 container test-container: <nil>
STEP: delete the pod 09/03/22 22:11:44.92
Sep  3 22:11:44.927: INFO: Waiting for pod pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5 to disappear
Sep  3 22:11:44.929: INFO: Pod pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Sep  3 22:11:44.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1289" for this suite. 09/03/22 22:11:44.931
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":352,"skipped":6456,"failed":0}
------------------------------
• [SLOW TEST] [6.047 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:11:38.887
    Sep  3 22:11:38.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename emptydir 09/03/22 22:11:38.889
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:38.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:38.9
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/03/22 22:11:38.902
    Sep  3 22:11:38.908: INFO: Waiting up to 5m0s for pod "pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5" in namespace "emptydir-1289" to be "Succeeded or Failed"
    Sep  3 22:11:38.910: INFO: Pod "pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.383205ms
    Sep  3 22:11:40.913: INFO: Pod "pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005511379s
    Sep  3 22:11:42.914: INFO: Pod "pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006265518s
    Sep  3 22:11:44.914: INFO: Pod "pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00592699s
    STEP: Saw pod success 09/03/22 22:11:44.914
    Sep  3 22:11:44.914: INFO: Pod "pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5" satisfied condition "Succeeded or Failed"
    Sep  3 22:11:44.916: INFO: Trying to get logs from node kind-worker2 pod pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5 container test-container: <nil>
    STEP: delete the pod 09/03/22 22:11:44.92
    Sep  3 22:11:44.927: INFO: Waiting for pod pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5 to disappear
    Sep  3 22:11:44.929: INFO: Pod pod-e052f4e0-ea9d-4dcb-8b7d-dfa12e68c3c5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Sep  3 22:11:44.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1289" for this suite. 09/03/22 22:11:44.931
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:11:44.936
Sep  3 22:11:44.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename downward-api 09/03/22 22:11:44.937
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:44.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:44.948
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 09/03/22 22:11:44.95
Sep  3 22:11:44.965: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c" in namespace "downward-api-9008" to be "Succeeded or Failed"
Sep  3 22:11:44.969: INFO: Pod "downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.458608ms
Sep  3 22:11:46.972: INFO: Pod "downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c": Phase="Running", Reason="", readiness=false. Elapsed: 2.006667588s
Sep  3 22:11:48.973: INFO: Pod "downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007150063s
STEP: Saw pod success 09/03/22 22:11:48.973
Sep  3 22:11:48.973: INFO: Pod "downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c" satisfied condition "Succeeded or Failed"
Sep  3 22:11:48.975: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c container client-container: <nil>
STEP: delete the pod 09/03/22 22:11:48.979
Sep  3 22:11:48.987: INFO: Waiting for pod downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c to disappear
Sep  3 22:11:48.989: INFO: Pod downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Sep  3 22:11:48.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9008" for this suite. 09/03/22 22:11:48.992
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":353,"skipped":6469,"failed":0}
------------------------------
• [4.061 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:11:44.936
    Sep  3 22:11:44.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename downward-api 09/03/22 22:11:44.937
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:44.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:44.948
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 09/03/22 22:11:44.95
    Sep  3 22:11:44.965: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c" in namespace "downward-api-9008" to be "Succeeded or Failed"
    Sep  3 22:11:44.969: INFO: Pod "downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.458608ms
    Sep  3 22:11:46.972: INFO: Pod "downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c": Phase="Running", Reason="", readiness=false. Elapsed: 2.006667588s
    Sep  3 22:11:48.973: INFO: Pod "downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007150063s
    STEP: Saw pod success 09/03/22 22:11:48.973
    Sep  3 22:11:48.973: INFO: Pod "downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c" satisfied condition "Succeeded or Failed"
    Sep  3 22:11:48.975: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c container client-container: <nil>
    STEP: delete the pod 09/03/22 22:11:48.979
    Sep  3 22:11:48.987: INFO: Waiting for pod downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c to disappear
    Sep  3 22:11:48.989: INFO: Pod downwardapi-volume-7b4f75c6-a7ca-4db6-b529-35a24f2f369c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Sep  3 22:11:48.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9008" for this suite. 09/03/22 22:11:48.992
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:11:48.999
Sep  3 22:11:48.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename deployment 09/03/22 22:11:49
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:49.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:49.013
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 09/03/22 22:11:49.018
STEP: waiting for Deployment to be created 09/03/22 22:11:49.023
STEP: waiting for all Replicas to be Ready 09/03/22 22:11:49.025
Sep  3 22:11:49.028: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  3 22:11:49.028: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  3 22:11:49.036: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  3 22:11:49.036: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  3 22:11:49.091: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  3 22:11:49.092: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  3 22:11:49.109: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  3 22:11:49.109: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  3 22:11:50.662: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  3 22:11:50.662: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  3 22:11:50.750: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 09/03/22 22:11:50.75
W0903 22:11:50.755823      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  3 22:11:50.757: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 09/03/22 22:11:50.757
Sep  3 22:11:50.759: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
Sep  3 22:11:50.759: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
Sep  3 22:11:50.759: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
Sep  3 22:11:50.759: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:50.769: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:50.769: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:50.787: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:50.787: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:50.795: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
Sep  3 22:11:50.795: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
Sep  3 22:11:50.823: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
Sep  3 22:11:50.823: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
Sep  3 22:11:51.673: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:51.673: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:51.690: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
STEP: listing Deployments 09/03/22 22:11:51.69
Sep  3 22:11:51.693: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 09/03/22 22:11:51.693
Sep  3 22:11:51.714: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 09/03/22 22:11:51.714
Sep  3 22:11:51.719: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  3 22:11:51.731: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  3 22:11:51.746: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  3 22:11:51.798: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  3 22:11:51.804: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  3 22:11:52.695: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  3 22:11:52.723: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  3 22:11:52.763: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  3 22:11:52.786: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  3 22:11:53.808: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 09/03/22 22:11:53.84
STEP: fetching the DeploymentStatus 09/03/22 22:11:53.846
Sep  3 22:11:53.851: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
Sep  3 22:11:53.851: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
Sep  3 22:11:53.851: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
Sep  3 22:11:53.852: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
Sep  3 22:11:53.852: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
Sep  3 22:11:53.852: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:53.852: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:53.853: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:53.853: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
Sep  3 22:11:53.853: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 3
STEP: deleting the Deployment 09/03/22 22:11:53.853
Sep  3 22:11:53.868: INFO: observed event type MODIFIED
Sep  3 22:11:53.868: INFO: observed event type MODIFIED
Sep  3 22:11:53.869: INFO: observed event type MODIFIED
Sep  3 22:11:53.869: INFO: observed event type MODIFIED
Sep  3 22:11:53.869: INFO: observed event type MODIFIED
Sep  3 22:11:53.870: INFO: observed event type MODIFIED
Sep  3 22:11:53.870: INFO: observed event type MODIFIED
Sep  3 22:11:53.871: INFO: observed event type MODIFIED
Sep  3 22:11:53.871: INFO: observed event type MODIFIED
Sep  3 22:11:53.872: INFO: observed event type MODIFIED
Sep  3 22:11:53.873: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  3 22:11:53.896: INFO: Log out all the ReplicaSets if there is no deployment created
Sep  3 22:11:53.903: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-3645  7ae95a47-f69b-413f-a050-3a739b533d78 30034 4 2022-09-03 22:11:50 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e2b86643-38e5-4786-bfb1-c746da8d15e7 0xc003f6b9c7 0xc003f6b9c8}] [] [{kube-controller-manager Update apps/v1 2022-09-03 22:11:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2b86643-38e5-4786-bfb1-c746da8d15e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 22:11:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f6ba50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Sep  3 22:11:53.910: INFO: pod: "test-deployment-54cc775c4b-pwdpb":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-pwdpb test-deployment-54cc775c4b- deployment-3645  f8c45760-6cb4-4d9d-813a-31a2159a313d 30030 0 2022-09-03 22:11:50 +0000 UTC 2022-09-03 22:11:54 +0000 UTC 0xc003f9bbd0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 7ae95a47-f69b-413f-a050-3a739b533d78 0xc003f9bc07 0xc003f9bc08}] [] [{kube-controller-manager Update v1 2022-09-03 22:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ae95a47-f69b-413f-a050-3a739b533d78\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 22:11:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2bx7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2bx7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.10,StartTime:2022-09-03 22:11:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 22:11:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://d0b1d7033394467db0574301c2359da8daef423684bcb67621f698ec50c8c6a1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  3 22:11:53.910: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-3645  cd7388c8-1ac1-4ec5-a4e4-c07983a524b5 30026 2 2022-09-03 22:11:51 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e2b86643-38e5-4786-bfb1-c746da8d15e7 0xc003f6bab7 0xc003f6bab8}] [] [{kube-controller-manager Update apps/v1 2022-09-03 22:11:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2b86643-38e5-4786-bfb1-c746da8d15e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 22:11:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f6bb40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Sep  3 22:11:53.916: INFO: pod: "test-deployment-7c7d8d58c8-6kp5n":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-6kp5n test-deployment-7c7d8d58c8- deployment-3645  e857aae4-9ed5-44a4-8051-e6c38b99ec93 29987 0 2022-09-03 22:11:51 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 cd7388c8-1ac1-4ec5-a4e4-c07983a524b5 0xc003f6bf27 0xc003f6bf28}] [] [{kube-controller-manager Update v1 2022-09-03 22:11:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd7388c8-1ac1-4ec5-a4e4-c07983a524b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 22:11:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bd5pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bd5pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.11,StartTime:2022-09-03 22:11:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 22:11:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://02dd08d3ced11e82fb8ead86b43773f827197719e54aa3a06e2f9e8c109eed0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  3 22:11:53.917: INFO: pod: "test-deployment-7c7d8d58c8-vd5f7":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-vd5f7 test-deployment-7c7d8d58c8- deployment-3645  b9631d51-2320-41fe-aeb0-dcaa1ca4a590 30025 0 2022-09-03 22:11:52 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 cd7388c8-1ac1-4ec5-a4e4-c07983a524b5 0xc002d72127 0xc002d72128}] [] [{kube-controller-manager Update v1 2022-09-03 22:11:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd7388c8-1ac1-4ec5-a4e4-c07983a524b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 22:11:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5c4cj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5c4cj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:10.244.2.244,StartTime:2022-09-03 22:11:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 22:11:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0c6ca8bcc1efe22987d9e50090f4fa84d23314d979fb9394d7558d691cc0dc44,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  3 22:11:53.918: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-3645  5d09c52b-5aeb-42aa-b4db-4915d19a9935 29954 3 2022-09-03 22:11:49 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e2b86643-38e5-4786-bfb1-c746da8d15e7 0xc003f6bba7 0xc003f6bba8}] [] [{kube-controller-manager Update apps/v1 2022-09-03 22:11:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2b86643-38e5-4786-bfb1-c746da8d15e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 22:11:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f6bc40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Sep  3 22:11:53.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3645" for this suite. 09/03/22 22:11:53.942
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":354,"skipped":6469,"failed":0}
------------------------------
• [4.960 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:11:48.999
    Sep  3 22:11:48.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename deployment 09/03/22 22:11:49
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:49.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:49.013
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 09/03/22 22:11:49.018
    STEP: waiting for Deployment to be created 09/03/22 22:11:49.023
    STEP: waiting for all Replicas to be Ready 09/03/22 22:11:49.025
    Sep  3 22:11:49.028: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  3 22:11:49.028: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  3 22:11:49.036: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  3 22:11:49.036: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  3 22:11:49.091: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  3 22:11:49.092: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  3 22:11:49.109: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  3 22:11:49.109: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  3 22:11:50.662: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  3 22:11:50.662: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  3 22:11:50.750: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 09/03/22 22:11:50.75
    W0903 22:11:50.755823      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  3 22:11:50.757: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 09/03/22 22:11:50.757
    Sep  3 22:11:50.759: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
    Sep  3 22:11:50.759: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
    Sep  3 22:11:50.759: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
    Sep  3 22:11:50.759: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
    Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
    Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
    Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
    Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 0
    Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:50.760: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:50.769: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:50.769: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:50.787: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:50.787: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:50.795: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    Sep  3 22:11:50.795: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    Sep  3 22:11:50.823: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    Sep  3 22:11:50.823: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    Sep  3 22:11:51.673: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:51.673: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:51.690: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    STEP: listing Deployments 09/03/22 22:11:51.69
    Sep  3 22:11:51.693: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 09/03/22 22:11:51.693
    Sep  3 22:11:51.714: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 09/03/22 22:11:51.714
    Sep  3 22:11:51.719: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  3 22:11:51.731: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  3 22:11:51.746: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  3 22:11:51.798: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  3 22:11:51.804: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  3 22:11:52.695: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  3 22:11:52.723: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  3 22:11:52.763: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  3 22:11:52.786: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  3 22:11:53.808: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 09/03/22 22:11:53.84
    STEP: fetching the DeploymentStatus 09/03/22 22:11:53.846
    Sep  3 22:11:53.851: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    Sep  3 22:11:53.851: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    Sep  3 22:11:53.851: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    Sep  3 22:11:53.852: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    Sep  3 22:11:53.852: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 1
    Sep  3 22:11:53.852: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:53.852: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:53.853: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:53.853: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 2
    Sep  3 22:11:53.853: INFO: observed Deployment test-deployment in namespace deployment-3645 with ReadyReplicas 3
    STEP: deleting the Deployment 09/03/22 22:11:53.853
    Sep  3 22:11:53.868: INFO: observed event type MODIFIED
    Sep  3 22:11:53.868: INFO: observed event type MODIFIED
    Sep  3 22:11:53.869: INFO: observed event type MODIFIED
    Sep  3 22:11:53.869: INFO: observed event type MODIFIED
    Sep  3 22:11:53.869: INFO: observed event type MODIFIED
    Sep  3 22:11:53.870: INFO: observed event type MODIFIED
    Sep  3 22:11:53.870: INFO: observed event type MODIFIED
    Sep  3 22:11:53.871: INFO: observed event type MODIFIED
    Sep  3 22:11:53.871: INFO: observed event type MODIFIED
    Sep  3 22:11:53.872: INFO: observed event type MODIFIED
    Sep  3 22:11:53.873: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  3 22:11:53.896: INFO: Log out all the ReplicaSets if there is no deployment created
    Sep  3 22:11:53.903: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-3645  7ae95a47-f69b-413f-a050-3a739b533d78 30034 4 2022-09-03 22:11:50 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e2b86643-38e5-4786-bfb1-c746da8d15e7 0xc003f6b9c7 0xc003f6b9c8}] [] [{kube-controller-manager Update apps/v1 2022-09-03 22:11:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2b86643-38e5-4786-bfb1-c746da8d15e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 22:11:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f6ba50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Sep  3 22:11:53.910: INFO: pod: "test-deployment-54cc775c4b-pwdpb":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-pwdpb test-deployment-54cc775c4b- deployment-3645  f8c45760-6cb4-4d9d-813a-31a2159a313d 30030 0 2022-09-03 22:11:50 +0000 UTC 2022-09-03 22:11:54 +0000 UTC 0xc003f9bbd0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 7ae95a47-f69b-413f-a050-3a739b533d78 0xc003f9bc07 0xc003f9bc08}] [] [{kube-controller-manager Update v1 2022-09-03 22:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ae95a47-f69b-413f-a050-3a739b533d78\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 22:11:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2bx7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2bx7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.10,StartTime:2022-09-03 22:11:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 22:11:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://d0b1d7033394467db0574301c2359da8daef423684bcb67621f698ec50c8c6a1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  3 22:11:53.910: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-3645  cd7388c8-1ac1-4ec5-a4e4-c07983a524b5 30026 2 2022-09-03 22:11:51 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e2b86643-38e5-4786-bfb1-c746da8d15e7 0xc003f6bab7 0xc003f6bab8}] [] [{kube-controller-manager Update apps/v1 2022-09-03 22:11:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2b86643-38e5-4786-bfb1-c746da8d15e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 22:11:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f6bb40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Sep  3 22:11:53.916: INFO: pod: "test-deployment-7c7d8d58c8-6kp5n":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-6kp5n test-deployment-7c7d8d58c8- deployment-3645  e857aae4-9ed5-44a4-8051-e6c38b99ec93 29987 0 2022-09-03 22:11:51 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 cd7388c8-1ac1-4ec5-a4e4-c07983a524b5 0xc003f6bf27 0xc003f6bf28}] [] [{kube-controller-manager Update v1 2022-09-03 22:11:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd7388c8-1ac1-4ec5-a4e4-c07983a524b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 22:11:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bd5pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bd5pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:10.244.1.11,StartTime:2022-09-03 22:11:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 22:11:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://02dd08d3ced11e82fb8ead86b43773f827197719e54aa3a06e2f9e8c109eed0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  3 22:11:53.917: INFO: pod: "test-deployment-7c7d8d58c8-vd5f7":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-vd5f7 test-deployment-7c7d8d58c8- deployment-3645  b9631d51-2320-41fe-aeb0-dcaa1ca4a590 30025 0 2022-09-03 22:11:52 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 cd7388c8-1ac1-4ec5-a4e4-c07983a524b5 0xc002d72127 0xc002d72128}] [] [{kube-controller-manager Update v1 2022-09-03 22:11:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd7388c8-1ac1-4ec5-a4e4-c07983a524b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-09-03 22:11:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5c4cj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5c4cj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-03 22:11:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:10.244.2.244,StartTime:2022-09-03 22:11:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-03 22:11:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0c6ca8bcc1efe22987d9e50090f4fa84d23314d979fb9394d7558d691cc0dc44,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  3 22:11:53.918: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-3645  5d09c52b-5aeb-42aa-b4db-4915d19a9935 29954 3 2022-09-03 22:11:49 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e2b86643-38e5-4786-bfb1-c746da8d15e7 0xc003f6bba7 0xc003f6bba8}] [] [{kube-controller-manager Update apps/v1 2022-09-03 22:11:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2b86643-38e5-4786-bfb1-c746da8d15e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-09-03 22:11:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f6bc40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Sep  3 22:11:53.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3645" for this suite. 09/03/22 22:11:53.942
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:11:53.97
Sep  3 22:11:53.970: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 22:11:53.971
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:54.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:54.015
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 09/03/22 22:11:54.028
Sep  3 22:11:54.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: mark a version not serverd 09/03/22 22:12:00.799
STEP: check the unserved version gets removed 09/03/22 22:12:00.813
STEP: check the other version is not changed 09/03/22 22:12:03.537
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Sep  3 22:12:08.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2557" for this suite. 09/03/22 22:12:08.714
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":355,"skipped":6477,"failed":0}
------------------------------
• [SLOW TEST] [14.748 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:11:53.97
    Sep  3 22:11:53.970: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename crd-publish-openapi 09/03/22 22:11:53.971
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:11:54.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:11:54.015
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 09/03/22 22:11:54.028
    Sep  3 22:11:54.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: mark a version not serverd 09/03/22 22:12:00.799
    STEP: check the unserved version gets removed 09/03/22 22:12:00.813
    STEP: check the other version is not changed 09/03/22 22:12:03.537
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Sep  3 22:12:08.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2557" for this suite. 09/03/22 22:12:08.714
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:12:08.722
Sep  3 22:12:08.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename pod-network-test 09/03/22 22:12:08.723
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:12:08.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:12:08.737
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-4778 09/03/22 22:12:08.742
STEP: creating a selector 09/03/22 22:12:08.742
STEP: Creating the service pods in kubernetes 09/03/22 22:12:08.742
Sep  3 22:12:08.743: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  3 22:12:08.771: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4778" to be "running and ready"
Sep  3 22:12:08.783: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.994124ms
Sep  3 22:12:08.783: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:12:10.786: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015028202s
Sep  3 22:12:10.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:12:12.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015785875s
Sep  3 22:12:12.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:12:14.786: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01513918s
Sep  3 22:12:14.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:12:16.786: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015444118s
Sep  3 22:12:16.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:12:18.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015830156s
Sep  3 22:12:18.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  3 22:12:20.788: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.016932395s
Sep  3 22:12:20.788: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  3 22:12:20.788: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  3 22:12:20.790: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4778" to be "running and ready"
Sep  3 22:12:20.792: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.683204ms
Sep  3 22:12:20.792: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  3 22:12:20.792: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/03/22 22:12:20.794
Sep  3 22:12:20.804: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4778" to be "running"
Sep  3 22:12:20.825: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.226943ms
Sep  3 22:12:22.829: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02440415s
Sep  3 22:12:22.829: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  3 22:12:22.830: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4778" to be "running"
Sep  3 22:12:22.833: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.080204ms
Sep  3 22:12:22.833: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  3 22:12:22.834: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  3 22:12:22.834: INFO: Going to poll 10.244.2.245 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep  3 22:12:22.836: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.245 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4778 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 22:12:22.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 22:12:22.837: INFO: ExecWithOptions: Clientset creation
Sep  3 22:12:22.837: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4778/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.245+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  3 22:12:23.904: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  3 22:12:23.904: INFO: Going to poll 10.244.1.12 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep  3 22:12:23.908: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.12 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4778 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  3 22:12:23.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
Sep  3 22:12:23.908: INFO: ExecWithOptions: Clientset creation
Sep  3 22:12:23.908: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4778/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.12+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  3 22:12:24.972: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Sep  3 22:12:24.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4778" for this suite. 09/03/22 22:12:24.976
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":356,"skipped":6485,"failed":0}
------------------------------
• [SLOW TEST] [16.257 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:12:08.722
    Sep  3 22:12:08.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename pod-network-test 09/03/22 22:12:08.723
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:12:08.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:12:08.737
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-4778 09/03/22 22:12:08.742
    STEP: creating a selector 09/03/22 22:12:08.742
    STEP: Creating the service pods in kubernetes 09/03/22 22:12:08.742
    Sep  3 22:12:08.743: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  3 22:12:08.771: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4778" to be "running and ready"
    Sep  3 22:12:08.783: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.994124ms
    Sep  3 22:12:08.783: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  3 22:12:10.786: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015028202s
    Sep  3 22:12:10.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:12:12.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015785875s
    Sep  3 22:12:12.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:12:14.786: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01513918s
    Sep  3 22:12:14.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:12:16.786: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015444118s
    Sep  3 22:12:16.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:12:18.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015830156s
    Sep  3 22:12:18.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  3 22:12:20.788: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.016932395s
    Sep  3 22:12:20.788: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  3 22:12:20.788: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  3 22:12:20.790: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4778" to be "running and ready"
    Sep  3 22:12:20.792: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.683204ms
    Sep  3 22:12:20.792: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  3 22:12:20.792: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/03/22 22:12:20.794
    Sep  3 22:12:20.804: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4778" to be "running"
    Sep  3 22:12:20.825: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.226943ms
    Sep  3 22:12:22.829: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02440415s
    Sep  3 22:12:22.829: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  3 22:12:22.830: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4778" to be "running"
    Sep  3 22:12:22.833: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.080204ms
    Sep  3 22:12:22.833: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  3 22:12:22.834: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  3 22:12:22.834: INFO: Going to poll 10.244.2.245 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Sep  3 22:12:22.836: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.245 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4778 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 22:12:22.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 22:12:22.837: INFO: ExecWithOptions: Clientset creation
    Sep  3 22:12:22.837: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4778/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.245+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  3 22:12:23.904: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  3 22:12:23.904: INFO: Going to poll 10.244.1.12 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Sep  3 22:12:23.908: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.12 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4778 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  3 22:12:23.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    Sep  3 22:12:23.908: INFO: ExecWithOptions: Clientset creation
    Sep  3 22:12:23.908: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4778/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.12+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  3 22:12:24.972: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Sep  3 22:12:24.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4778" for this suite. 09/03/22 22:12:24.976
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:12:24.983
Sep  3 22:12:24.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename endpointslice 09/03/22 22:12:24.984
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:12:24.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:12:24.994
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 09/03/22 22:12:24.997
STEP: getting /apis/discovery.k8s.io 09/03/22 22:12:24.999
STEP: getting /apis/discovery.k8s.iov1 09/03/22 22:12:25
STEP: creating 09/03/22 22:12:25.001
STEP: getting 09/03/22 22:12:25.011
STEP: listing 09/03/22 22:12:25.013
STEP: watching 09/03/22 22:12:25.017
Sep  3 22:12:25.017: INFO: starting watch
STEP: cluster-wide listing 09/03/22 22:12:25.018
STEP: cluster-wide watching 09/03/22 22:12:25.02
Sep  3 22:12:25.021: INFO: starting watch
STEP: patching 09/03/22 22:12:25.021
STEP: updating 09/03/22 22:12:25.025
Sep  3 22:12:25.032: INFO: waiting for watch events with expected annotations
Sep  3 22:12:25.032: INFO: saw patched and updated annotations
STEP: deleting 09/03/22 22:12:25.032
STEP: deleting a collection 09/03/22 22:12:25.042
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Sep  3 22:12:25.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1946" for this suite. 09/03/22 22:12:25.054
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":357,"skipped":6507,"failed":0}
------------------------------
• [0.076 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:12:24.983
    Sep  3 22:12:24.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename endpointslice 09/03/22 22:12:24.984
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:12:24.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:12:24.994
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 09/03/22 22:12:24.997
    STEP: getting /apis/discovery.k8s.io 09/03/22 22:12:24.999
    STEP: getting /apis/discovery.k8s.iov1 09/03/22 22:12:25
    STEP: creating 09/03/22 22:12:25.001
    STEP: getting 09/03/22 22:12:25.011
    STEP: listing 09/03/22 22:12:25.013
    STEP: watching 09/03/22 22:12:25.017
    Sep  3 22:12:25.017: INFO: starting watch
    STEP: cluster-wide listing 09/03/22 22:12:25.018
    STEP: cluster-wide watching 09/03/22 22:12:25.02
    Sep  3 22:12:25.021: INFO: starting watch
    STEP: patching 09/03/22 22:12:25.021
    STEP: updating 09/03/22 22:12:25.025
    Sep  3 22:12:25.032: INFO: waiting for watch events with expected annotations
    Sep  3 22:12:25.032: INFO: saw patched and updated annotations
    STEP: deleting 09/03/22 22:12:25.032
    STEP: deleting a collection 09/03/22 22:12:25.042
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Sep  3 22:12:25.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-1946" for this suite. 09/03/22 22:12:25.054
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:12:25.061
Sep  3 22:12:25.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename subpath 09/03/22 22:12:25.062
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:12:25.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:12:25.083
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/03/22 22:12:25.086
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-scgn 09/03/22 22:12:25.091
STEP: Creating a pod to test atomic-volume-subpath 09/03/22 22:12:25.091
Sep  3 22:12:25.096: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-scgn" in namespace "subpath-7650" to be "Succeeded or Failed"
Sep  3 22:12:25.098: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.122704ms
Sep  3 22:12:27.101: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 2.005100226s
Sep  3 22:12:29.101: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 4.005606345s
Sep  3 22:12:31.104: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 6.008109801s
Sep  3 22:12:33.101: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 8.005525608s
Sep  3 22:12:35.101: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 10.00516912s
Sep  3 22:12:37.101: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 12.004873532s
Sep  3 22:12:39.100: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 14.004550494s
Sep  3 22:12:41.101: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 16.005423735s
Sep  3 22:12:43.102: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 18.006291275s
Sep  3 22:12:45.102: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 20.006068413s
Sep  3 22:12:47.102: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=false. Elapsed: 22.006417746s
Sep  3 22:12:49.104: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007857676s
STEP: Saw pod success 09/03/22 22:12:49.104
Sep  3 22:12:49.104: INFO: Pod "pod-subpath-test-secret-scgn" satisfied condition "Succeeded or Failed"
Sep  3 22:12:49.106: INFO: Trying to get logs from node kind-worker2 pod pod-subpath-test-secret-scgn container test-container-subpath-secret-scgn: <nil>
STEP: delete the pod 09/03/22 22:12:49.11
Sep  3 22:12:49.122: INFO: Waiting for pod pod-subpath-test-secret-scgn to disappear
Sep  3 22:12:49.124: INFO: Pod pod-subpath-test-secret-scgn no longer exists
STEP: Deleting pod pod-subpath-test-secret-scgn 09/03/22 22:12:49.124
Sep  3 22:12:49.124: INFO: Deleting pod "pod-subpath-test-secret-scgn" in namespace "subpath-7650"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Sep  3 22:12:49.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7650" for this suite. 09/03/22 22:12:49.128
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":358,"skipped":6550,"failed":0}
------------------------------
• [SLOW TEST] [24.076 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:12:25.061
    Sep  3 22:12:25.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename subpath 09/03/22 22:12:25.062
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:12:25.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:12:25.083
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/03/22 22:12:25.086
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-scgn 09/03/22 22:12:25.091
    STEP: Creating a pod to test atomic-volume-subpath 09/03/22 22:12:25.091
    Sep  3 22:12:25.096: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-scgn" in namespace "subpath-7650" to be "Succeeded or Failed"
    Sep  3 22:12:25.098: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.122704ms
    Sep  3 22:12:27.101: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 2.005100226s
    Sep  3 22:12:29.101: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 4.005606345s
    Sep  3 22:12:31.104: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 6.008109801s
    Sep  3 22:12:33.101: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 8.005525608s
    Sep  3 22:12:35.101: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 10.00516912s
    Sep  3 22:12:37.101: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 12.004873532s
    Sep  3 22:12:39.100: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 14.004550494s
    Sep  3 22:12:41.101: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 16.005423735s
    Sep  3 22:12:43.102: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 18.006291275s
    Sep  3 22:12:45.102: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=true. Elapsed: 20.006068413s
    Sep  3 22:12:47.102: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Running", Reason="", readiness=false. Elapsed: 22.006417746s
    Sep  3 22:12:49.104: INFO: Pod "pod-subpath-test-secret-scgn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007857676s
    STEP: Saw pod success 09/03/22 22:12:49.104
    Sep  3 22:12:49.104: INFO: Pod "pod-subpath-test-secret-scgn" satisfied condition "Succeeded or Failed"
    Sep  3 22:12:49.106: INFO: Trying to get logs from node kind-worker2 pod pod-subpath-test-secret-scgn container test-container-subpath-secret-scgn: <nil>
    STEP: delete the pod 09/03/22 22:12:49.11
    Sep  3 22:12:49.122: INFO: Waiting for pod pod-subpath-test-secret-scgn to disappear
    Sep  3 22:12:49.124: INFO: Pod pod-subpath-test-secret-scgn no longer exists
    STEP: Deleting pod pod-subpath-test-secret-scgn 09/03/22 22:12:49.124
    Sep  3 22:12:49.124: INFO: Deleting pod "pod-subpath-test-secret-scgn" in namespace "subpath-7650"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Sep  3 22:12:49.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7650" for this suite. 09/03/22 22:12:49.128
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:12:49.146
Sep  3 22:12:49.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename services 09/03/22 22:12:49.147
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:12:49.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:12:49.161
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Sep  3 22:12:49.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4302" for this suite. 09/03/22 22:12:49.168
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":359,"skipped":6591,"failed":0}
------------------------------
• [0.027 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:12:49.146
    Sep  3 22:12:49.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename services 09/03/22 22:12:49.147
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:12:49.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:12:49.161
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Sep  3 22:12:49.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4302" for this suite. 09/03/22 22:12:49.168
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:12:49.192
Sep  3 22:12:49.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename job 09/03/22 22:12:49.193
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:12:49.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:12:49.21
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 09/03/22 22:12:49.212
STEP: Ensuring active pods == parallelism 09/03/22 22:12:49.22
STEP: Orphaning one of the Job's Pods 09/03/22 22:12:51.235
Sep  3 22:12:51.750: INFO: Successfully updated pod "adopt-release-hf8s5"
STEP: Checking that the Job readopts the Pod 09/03/22 22:12:51.75
Sep  3 22:12:51.751: INFO: Waiting up to 15m0s for pod "adopt-release-hf8s5" in namespace "job-2959" to be "adopted"
Sep  3 22:12:51.758: INFO: Pod "adopt-release-hf8s5": Phase="Running", Reason="", readiness=true. Elapsed: 6.894612ms
Sep  3 22:12:53.761: INFO: Pod "adopt-release-hf8s5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009996145s
Sep  3 22:12:53.761: INFO: Pod "adopt-release-hf8s5" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 09/03/22 22:12:53.761
Sep  3 22:12:54.273: INFO: Successfully updated pod "adopt-release-hf8s5"
STEP: Checking that the Job releases the Pod 09/03/22 22:12:54.273
Sep  3 22:12:54.274: INFO: Waiting up to 15m0s for pod "adopt-release-hf8s5" in namespace "job-2959" to be "released"
Sep  3 22:12:54.277: INFO: Pod "adopt-release-hf8s5": Phase="Running", Reason="", readiness=true. Elapsed: 2.653905ms
Sep  3 22:12:56.280: INFO: Pod "adopt-release-hf8s5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006045512s
Sep  3 22:12:56.280: INFO: Pod "adopt-release-hf8s5" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Sep  3 22:12:56.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2959" for this suite. 09/03/22 22:12:56.283
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":360,"skipped":6619,"failed":0}
------------------------------
• [SLOW TEST] [7.094 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:12:49.192
    Sep  3 22:12:49.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename job 09/03/22 22:12:49.193
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:12:49.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:12:49.21
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 09/03/22 22:12:49.212
    STEP: Ensuring active pods == parallelism 09/03/22 22:12:49.22
    STEP: Orphaning one of the Job's Pods 09/03/22 22:12:51.235
    Sep  3 22:12:51.750: INFO: Successfully updated pod "adopt-release-hf8s5"
    STEP: Checking that the Job readopts the Pod 09/03/22 22:12:51.75
    Sep  3 22:12:51.751: INFO: Waiting up to 15m0s for pod "adopt-release-hf8s5" in namespace "job-2959" to be "adopted"
    Sep  3 22:12:51.758: INFO: Pod "adopt-release-hf8s5": Phase="Running", Reason="", readiness=true. Elapsed: 6.894612ms
    Sep  3 22:12:53.761: INFO: Pod "adopt-release-hf8s5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009996145s
    Sep  3 22:12:53.761: INFO: Pod "adopt-release-hf8s5" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 09/03/22 22:12:53.761
    Sep  3 22:12:54.273: INFO: Successfully updated pod "adopt-release-hf8s5"
    STEP: Checking that the Job releases the Pod 09/03/22 22:12:54.273
    Sep  3 22:12:54.274: INFO: Waiting up to 15m0s for pod "adopt-release-hf8s5" in namespace "job-2959" to be "released"
    Sep  3 22:12:54.277: INFO: Pod "adopt-release-hf8s5": Phase="Running", Reason="", readiness=true. Elapsed: 2.653905ms
    Sep  3 22:12:56.280: INFO: Pod "adopt-release-hf8s5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006045512s
    Sep  3 22:12:56.280: INFO: Pod "adopt-release-hf8s5" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Sep  3 22:12:56.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2959" for this suite. 09/03/22 22:12:56.283
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:12:56.302
Sep  3 22:12:56.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename resourcequota 09/03/22 22:12:56.304
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:12:56.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:12:56.367
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 09/03/22 22:12:56.369
STEP: Ensuring ResourceQuota status is calculated 09/03/22 22:12:56.372
STEP: Creating a ResourceQuota with not terminating scope 09/03/22 22:12:58.381
STEP: Ensuring ResourceQuota status is calculated 09/03/22 22:12:58.386
STEP: Creating a long running pod 09/03/22 22:13:00.388
STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/03/22 22:13:00.396
STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/03/22 22:13:02.399
STEP: Deleting the pod 09/03/22 22:13:04.402
STEP: Ensuring resource quota status released the pod usage 09/03/22 22:13:04.411
STEP: Creating a terminating pod 09/03/22 22:13:06.414
STEP: Ensuring resource quota with terminating scope captures the pod usage 09/03/22 22:13:06.421
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/03/22 22:13:08.424
STEP: Deleting the pod 09/03/22 22:13:10.427
STEP: Ensuring resource quota status released the pod usage 09/03/22 22:13:10.436
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Sep  3 22:13:12.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2658" for this suite. 09/03/22 22:13:12.442
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":361,"skipped":6670,"failed":0}
------------------------------
• [SLOW TEST] [16.143 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:12:56.302
    Sep  3 22:12:56.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename resourcequota 09/03/22 22:12:56.304
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:12:56.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:12:56.367
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 09/03/22 22:12:56.369
    STEP: Ensuring ResourceQuota status is calculated 09/03/22 22:12:56.372
    STEP: Creating a ResourceQuota with not terminating scope 09/03/22 22:12:58.381
    STEP: Ensuring ResourceQuota status is calculated 09/03/22 22:12:58.386
    STEP: Creating a long running pod 09/03/22 22:13:00.388
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/03/22 22:13:00.396
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/03/22 22:13:02.399
    STEP: Deleting the pod 09/03/22 22:13:04.402
    STEP: Ensuring resource quota status released the pod usage 09/03/22 22:13:04.411
    STEP: Creating a terminating pod 09/03/22 22:13:06.414
    STEP: Ensuring resource quota with terminating scope captures the pod usage 09/03/22 22:13:06.421
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/03/22 22:13:08.424
    STEP: Deleting the pod 09/03/22 22:13:10.427
    STEP: Ensuring resource quota status released the pod usage 09/03/22 22:13:10.436
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Sep  3 22:13:12.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2658" for this suite. 09/03/22 22:13:12.442
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 09/03/22 22:13:12.447
Sep  3 22:13:12.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
STEP: Building a namespace api object, basename secrets 09/03/22 22:13:12.447
STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:13:12.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:13:12.46
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-f1c9e1fb-4ad1-48b0-b04d-0edf70ac9c3c 09/03/22 22:13:12.463
STEP: Creating a pod to test consume secrets 09/03/22 22:13:12.465
Sep  3 22:13:12.470: INFO: Waiting up to 5m0s for pod "pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc" in namespace "secrets-8424" to be "Succeeded or Failed"
Sep  3 22:13:12.496: INFO: Pod "pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 25.927652ms
Sep  3 22:13:14.498: INFO: Pod "pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02864141s
Sep  3 22:13:16.499: INFO: Pod "pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029124863s
STEP: Saw pod success 09/03/22 22:13:16.499
Sep  3 22:13:16.499: INFO: Pod "pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc" satisfied condition "Succeeded or Failed"
Sep  3 22:13:16.501: INFO: Trying to get logs from node kind-worker pod pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc container secret-volume-test: <nil>
STEP: delete the pod 09/03/22 22:13:16.512
Sep  3 22:13:16.518: INFO: Waiting for pod pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc to disappear
Sep  3 22:13:16.520: INFO: Pod pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Sep  3 22:13:16.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8424" for this suite. 09/03/22 22:13:16.522
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":362,"skipped":6702,"failed":0}
------------------------------
• [4.080 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 09/03/22 22:13:12.447
    Sep  3 22:13:12.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1880162351
    STEP: Building a namespace api object, basename secrets 09/03/22 22:13:12.447
    STEP: Waiting for a default service account to be provisioned in namespace 09/03/22 22:13:12.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/03/22 22:13:12.46
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-f1c9e1fb-4ad1-48b0-b04d-0edf70ac9c3c 09/03/22 22:13:12.463
    STEP: Creating a pod to test consume secrets 09/03/22 22:13:12.465
    Sep  3 22:13:12.470: INFO: Waiting up to 5m0s for pod "pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc" in namespace "secrets-8424" to be "Succeeded or Failed"
    Sep  3 22:13:12.496: INFO: Pod "pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 25.927652ms
    Sep  3 22:13:14.498: INFO: Pod "pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02864141s
    Sep  3 22:13:16.499: INFO: Pod "pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029124863s
    STEP: Saw pod success 09/03/22 22:13:16.499
    Sep  3 22:13:16.499: INFO: Pod "pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc" satisfied condition "Succeeded or Failed"
    Sep  3 22:13:16.501: INFO: Trying to get logs from node kind-worker pod pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc container secret-volume-test: <nil>
    STEP: delete the pod 09/03/22 22:13:16.512
    Sep  3 22:13:16.518: INFO: Waiting for pod pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc to disappear
    Sep  3 22:13:16.520: INFO: Pod pod-secrets-6323fd67-b12b-40fc-a4a6-fd14d0bf1dcc no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Sep  3 22:13:16.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8424" for this suite. 09/03/22 22:13:16.522
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6705,"failed":0}
Sep  3 22:13:16.532: INFO: Running AfterSuite actions on all nodes
Sep  3 22:13:16.533: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Sep  3 22:13:16.533: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Sep  3 22:13:16.533: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Sep  3 22:13:16.534: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Sep  3 22:13:16.534: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Sep  3 22:13:16.534: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Sep  3 22:13:16.535: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Sep  3 22:13:16.535: INFO: Running AfterSuite actions on node 1
Sep  3 22:13:16.535: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.004 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Sep  3 22:13:16.532: INFO: Running AfterSuite actions on all nodes
    Sep  3 22:13:16.533: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Sep  3 22:13:16.533: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Sep  3 22:13:16.533: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Sep  3 22:13:16.534: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Sep  3 22:13:16.534: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Sep  3 22:13:16.534: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Sep  3 22:13:16.535: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Sep  3 22:13:16.535: INFO: Running AfterSuite actions on node 1
    Sep  3 22:13:16.535: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.074 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7067 Specs in 5702.123 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6705 Skipped
PASS

Ginkgo ran 1 suite in 1h35m2.832513094s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.4[0m

